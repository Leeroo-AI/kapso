---
title: "Deployment Strategies"
description: "Available deployment strategies and their configurations"
---

## Overview

Each deployment strategy is designed for specific use cases. The system can auto-select the best strategy or you can specify one explicitly.

## LOCAL

Run as a local Python process. Fastest for development and testing.

### Features
- No containerization overhead
- Direct file system access
- Easiest debugging
- No additional setup required

### Usage

```python
software = kapso.deploy(solution, strategy=DeployStrategy.LOCAL)
result = software.run({"data_path": "./data.csv"})
```

### How It Works

1. Adapter creates a `run.py` entry point
2. Runner imports and calls the main function
3. Results returned directly

### Configuration

```yaml
# In strategy config
local:
  module: "main"      # Module to import
  callable: "predict" # Function to call
  timeout: 300        # Execution timeout
```

## DOCKER

Run in an isolated Docker container.

### Features
- Environment isolation
- Reproducible builds
- Easy dependency management
- Cross-platform compatibility

### Prerequisites
- Docker installed and running

### Usage

```python
software = kapso.deploy(solution, strategy=DeployStrategy.DOCKER)
result = software.run({"data_path": "/data/input.csv"})
```

### How It Works

1. Adapter creates `Dockerfile` and `docker-compose.yml`
2. Runner builds and starts container
3. Inputs/outputs mapped via volumes

### Dockerfile Template

```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "run.py"]
```

## MODAL

Deploy to Modal.com for serverless GPU compute.

### Features
- Serverless scaling
- GPU support (A100, H100)
- Pay-per-use pricing
- Fast cold starts

### Prerequisites
- Modal account and CLI
- `modal` package installed

```bash
pip install modal
modal token new
```

### Usage

```python
software = kapso.deploy(solution, strategy=DeployStrategy.MODAL)
result = software.run({"prompt": "Hello, world!"})
```

### How It Works

1. Adapter creates `modal_app.py` with Modal decorators
2. Runner deploys to Modal
3. HTTP endpoint for inference

### Modal App Template

```python
import modal

app = modal.App("kapso-deployment")

@app.function(gpu="A100")
def predict(inputs):
    # Load model and run inference
    return result
```

## BENTOML

Deploy with BentoML for production ML serving.

### Features
- Model versioning
- Automatic batching
- Prometheus metrics
- Kubernetes-ready

### Prerequisites
- `bentoml` package installed

```bash
pip install bentoml
```

### Usage

```python
software = kapso.deploy(solution, strategy=DeployStrategy.BENTOML)
result = software.run({"features": [1, 2, 3]})
```

### How It Works

1. Adapter creates `service.py` with BentoML decorators
2. Runner builds and serves the Bento
3. HTTP API for inference

### BentoML Service Template

```python
import bentoml

@bentoml.service
class KapsoService:
    @bentoml.api
    def predict(self, inputs: dict) -> dict:
        return self.model.predict(inputs)
```

## LANGGRAPH

Deploy as a LangGraph agent workflow.

### Features
- Stateful agent execution
- Tool integration
- Streaming support
- Checkpoint persistence

### Prerequisites
- `langgraph` package installed

```bash
pip install langgraph
```

### Usage

```python
software = kapso.deploy(solution, strategy=DeployStrategy.LANGGRAPH)
result = software.run({"query": "What is 2+2?"})
```

### How It Works

1. Adapter creates `graph.py` with LangGraph nodes
2. Runner compiles and runs the graph
3. Streaming or batch execution

## AUTO Selection

When `DeployStrategy.AUTO` is used, the selector analyzes the code:

```python
software = kapso.deploy(solution, strategy=DeployStrategy.AUTO)
# System chooses based on:
# - Code complexity
# - Resource requirements (GPU, memory)
# - Dependencies
# - Target environment
```

### Selection Criteria

| Factor | LOCAL | DOCKER | MODAL | BENTOML |
|--------|-------|--------|-------|---------|
| Simple script | ✅ | | | |
| Complex deps | | ✅ | | |
| GPU required | | | ✅ | |
| Production ML | | | | ✅ |
| Isolation needed | | ✅ | ✅ | ✅ |

## Strategy Comparison

| Strategy | Startup | Cost | Isolation | GPU | Production |
|----------|---------|------|-----------|-----|------------|
| LOCAL | Fast | Free | None | Local | No |
| DOCKER | Medium | Free | Full | Via nvidia-docker | Yes |
| MODAL | Medium | Pay-per-use | Full | Yes | Yes |
| BENTOML | Medium | Self-hosted | Full | Yes | Yes |
| LANGGRAPH | Fast | Depends | None | Via backend | Yes |

## Run Interface

Each strategy defines how to call the deployed code:

```python
# Returned by adapter
run_interface = {
    "type": "function",     # or "http", "grpc"
    "module": "main",       # Python module
    "callable": "predict",  # Function name
    "endpoint": None,       # HTTP endpoint (if applicable)
}
```

## Environment Variables

Pass environment variables to deployments:

```python
software = kapso.deploy(
    solution,
    strategy=DeployStrategy.DOCKER,
    env_vars={
        "API_KEY": "...",
        "MODEL_PATH": "/models/v1",
    },
)
```

## Best Practices

<Tip>
**Start with LOCAL** for development. It's fastest and easiest to debug.
</Tip>

<Note>
**Use DOCKER** when you need reproducible environments or have complex dependencies.
</Note>

<Warning>
**MODAL and BENTOML** require additional setup and may incur costs.
</Warning>
