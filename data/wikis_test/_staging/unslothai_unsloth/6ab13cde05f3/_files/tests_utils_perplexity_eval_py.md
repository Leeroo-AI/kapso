# File: `tests/utils/perplexity_eval.py`

**Category:** test

| Property | Value |
|----------|-------|
| Lines | 81 |
| Functions | `ppl_model`, `add_to_comparison`, `print_model_comparison` |
| Imports | pandas, torch, tqdm |

## Understanding

**Status:** âœ… Documented

**Purpose:** Perplexity evaluation utility for language models

**Mechanism:** Computes sliding window perplexity and tracks comparisons across models

**Significance:** Provides standardized quality metric for model evaluation and comparison
