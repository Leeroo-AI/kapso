# Implementation: UnslothGRPOConfig

{| class="wikitable" style="float:right; margin-left:1em; width:300px;"
|-
! Knowledge Sources
|
* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]
* [[source::Doc|TRL GRPOConfig|https://huggingface.co/docs/trl/grpo_trainer]]
* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]
|-
! Domains
| [[domain::Reinforcement_Learning]], [[domain::Training]], [[domain::Hyperparameters]]
|-
! Last Updated
| [[last_updated::2026-01-09 16:00 GMT]]
|}

== Overview ==

Wrapper for TRL's GRPOConfig that adds Unsloth-specific parameters for optimized GRPO reinforcement learning training.

=== Description ===

`UnslothGRPOConfig` extends TRL's `GRPOConfig` with Unsloth-specific parameters for vLLM integration and memory optimization. It's dynamically generated by Unsloth to match the installed TRL version.

This is a **Wrapper Doc** - it documents how Unsloth extends TRL's configuration class.

Key Unsloth additions:
* `vllm_sampling_params` - Direct vLLM SamplingParams configuration
* `unsloth_num_chunks` - Memory chunking for gradient computation

=== Usage ===

Create `UnslothGRPOConfig` with GRPO hyperparameters and pass to `UnslothGRPOTrainer`. Key parameters to configure:
* `num_generations` - Completions per prompt (typically 6-16)
* `max_completion_length` - Maximum tokens to generate
* `beta` - KL divergence penalty coefficient

== Code Reference ==

=== Source Location ===
* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]
* '''File:''' unsloth/models/rl.py
* '''Lines:''' L309-334 (dynamically generated template)

=== External Reference ===
* '''Library:''' [https://github.com/huggingface/trl TRL]
* '''Base Class:''' `trl.GRPOConfig`

=== Signature ===
<syntaxhighlight lang="python">
@dataclass
class UnslothGRPOConfig(GRPOConfig):
    """
    GRPO configuration with Unsloth extensions.
    """
    # Unsloth-specific
    vllm_sampling_params: Optional[Any] = field(
        default=None,
        metadata={'help': 'vLLM SamplingParams object'}
    )
    unsloth_num_chunks: Optional[int] = field(
        default=-1,
        metadata={'help': 'Chunk size for memory efficiency. -1 is most efficient.'}
    )

    # Standard GRPOConfig parameters
    output_dir: str = None
    per_device_train_batch_size: int = 1
    gradient_accumulation_steps: int = 1
    num_generations: int = 6  # Completions per prompt
    max_prompt_length: int = 256
    max_completion_length: int = 200
    beta: float = 0.1  # KL penalty coefficient
    learning_rate: float = 5e-6
    num_train_epochs: float = 1.0
    max_steps: int = -1
    logging_steps: int = 1
    save_strategy: str = "no"
    bf16: bool = False
    fp16: bool = False
    **kwargs
</syntaxhighlight>

=== Import ===
<syntaxhighlight lang="python">
from unsloth import UnslothGRPOConfig
# Or access after patching:
from trl import GRPOConfig  # Returns UnslothGRPOConfig after import unsloth
</syntaxhighlight>

== I/O Contract ==

=== Inputs ===
{| class="wikitable"
|-
! Name !! Type !! Required !! Description
|-
| output_dir || str || Yes || Directory for checkpoints
|-
| num_generations || int || No (default: 6) || Completions generated per prompt
|-
| max_completion_length || int || No (default: 200) || Maximum tokens to generate
|-
| max_prompt_length || int || No (default: 256) || Maximum prompt token length
|-
| beta || float || No (default: 0.1) || KL divergence penalty coefficient
|-
| learning_rate || float || No (default: 5e-6) || Learning rate for policy
|-
| per_device_train_batch_size || int || No (default: 1) || Prompts per batch per GPU
|-
| gradient_accumulation_steps || int || No (default: 1) || Steps before gradient update
|}

=== Outputs ===
{| class="wikitable"
|-
! Name !! Type !! Description
|-
| config || UnslothGRPOConfig || Configuration object for UnslothGRPOTrainer
|}

== Usage Examples ==

=== Basic GRPO Configuration ===
<syntaxhighlight lang="python">
from unsloth import UnslothGRPOConfig

grpo_config = UnslothGRPOConfig(
    output_dir = "./grpo_outputs",
    num_generations = 6,           # 6 completions per prompt
    max_completion_length = 200,   # Max tokens per completion
    max_prompt_length = 256,       # Max prompt tokens
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 1,
    beta = 0.1,                    # KL penalty
    learning_rate = 5e-6,
    max_steps = 100,
    logging_steps = 1,
    bf16 = True,
)
</syntaxhighlight>

=== High-Generation Configuration (Math Reasoning) ===
<syntaxhighlight lang="python">
from unsloth import UnslothGRPOConfig

# More generations for better reward estimation
grpo_config = UnslothGRPOConfig(
    output_dir = "./math_grpo",
    num_generations = 16,           # More samples for math tasks
    max_completion_length = 500,    # Longer for reasoning
    max_prompt_length = 200,
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 4,  # Effective batch = 4
    beta = 0.05,                    # Lower KL for more exploration
    learning_rate = 1e-5,
    max_steps = 500,
    bf16 = True,
)
</syntaxhighlight>

=== With Custom vLLM Sampling ===
<syntaxhighlight lang="python">
from unsloth import UnslothGRPOConfig, vLLMSamplingParams

# Custom vLLM sampling parameters
sampling_params = vLLMSamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 300,
)

grpo_config = UnslothGRPOConfig(
    output_dir = "./grpo_outputs",
    num_generations = 8,
    max_completion_length = 300,
    vllm_sampling_params = sampling_params,  # Unsloth extension
    learning_rate = 5e-6,
    max_steps = 200,
    bf16 = True,
)
</syntaxhighlight>

== Related Pages ==

=== Implements Principle ===
* [[implements::Principle:Unslothai_Unsloth_GRPO_Configuration]]

=== Requires Environment ===
* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_vLLM_Environment]]
