{
  "pages": [
    {
      "id": "Workflow/Continued_Training",
      "page_title": "Continued Training",
      "page_type": "Workflow",
      "overview": "End-to-end process for resuming training from a previously saved LoRA adapter checkpoint with Unsloth.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Doc|PEFT Loading|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Fine_Tuning]], [[domain::Checkpointing]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for resuming training from a previously saved LoRA adapter checkpoint with Unsloth.\n\n=== Description ===\nThis workflow enables continuing training from a saved checkpoint, either to extend training, fine-tune on additional data, or recover from interruptions. Unsloth supports loading saved LoRA adapters and resuming training seamlessly. Essential for iterative model improvement and long training runs.\n\n=== Usage ===\nExecute this workflow when you need to resume interrupted training, extend training for more steps/epochs, or fine-tune a previously trained adapter on new data. Also useful for curriculum learning approaches where training progresses through multiple stages.\n\n== Execution Steps ==\n=== Step 1: Load Base Model ===\n[[step::Principle:Quantization]]\nLoad the same base model used in original training.\n\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n</syntaxhighlight>\n\n=== Step 2: Load Saved LoRA Adapter ===\n[[step::Principle:Low_Rank_Adaptation]]\nLoad the previously trained LoRA weights.\n\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n\nmodel = PeftModel.from_pretrained(\n    model,\n    \"path/to/saved/lora_adapter\",\n    is_trainable = True,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    use_gradient_checkpointing = \"unsloth\",\n)\n</syntaxhighlight>\n\n=== Step 3: Prepare New/Additional Dataset ===\n[[step::Principle:Supervised_Fine_Tuning]]\nLoad dataset for continued training.\n\n<syntaxhighlight lang=\"python\">\nfrom datasets import load_dataset\n\nnew_dataset = load_dataset(\"your_new_dataset\", split=\"train\")\n\ndef format_prompt(example):\n    return {\"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"}\n\nnew_dataset = new_dataset.map(format_prompt)\n</syntaxhighlight>\n\n=== Step 4: Configure Training ===\nSet up training arguments, potentially with modified hyperparameters.\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\nargs = SFTConfig(\n    output_dir = \"continued_training_outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    warmup_steps = 5,\n    max_steps = 100,\n    learning_rate = 1e-4,\n    optim = \"adamw_8bit\",\n    max_seq_length = 2048,\n    logging_steps = 1,\n)\n</syntaxhighlight>\n\n=== Step 5: Resume Training ===\nContinue training from checkpoint.\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = new_dataset,\n    args = args,\n)\n\ntrainer.train(resume_from_checkpoint = True)\n</syntaxhighlight>\n\n=== Step 6: Save Updated Model ===\nSave the updated adapter.\n\n<syntaxhighlight lang=\"python\">\nmodel.save_pretrained(\"continued_lora_model\")\ntokenizer.save_pretrained(\"continued_lora_model\")\n</syntaxhighlight>\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Load Base Model] --> B[Load Saved LoRA Adapter]\n    B --> C[Prepare New Dataset]\n    C --> D[Configure Training]\n    D --> E{Resume Options}\n    E --> F[Extend Training]\n    E --> G[New Data Fine-tuning]\n    E --> H[Curriculum Stage]\n    F --> I[Continue SFTTrainer]\n    G --> I\n    H --> I\n    I --> J[Save Updated Adapter]\n}}\n\n== Alternative: Resume from Trainer Checkpoint ==\nIf you saved full trainer state:\n\n<syntaxhighlight lang=\"python\">\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    args = args,\n)\n\ntrainer.train(resume_from_checkpoint = \"outputs/checkpoint-500\")\n</syntaxhighlight>\n\n== Related Pages ==\n=== Execution Steps ===\n* [[step::Principle:Quantization]] - Step 1\n* [[step::Principle:Low_Rank_Adaptation]] - Step 2\n* [[step::Principle:Supervised_Fine_Tuning]] - Steps 3-5\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Unsloth_Gradient_Checkpointing_Optimization]]\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n\n",
      "domains": [
        "LLMs",
        "Fine Tuning",
        "Checkpointing"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Doc",
          "title": "PEFT Loading",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Quantization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Supervised_Fine_Tuning"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Quantization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Supervised_Fine_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unsloth_Gradient_Checkpointing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        }
      ]
    },
    {
      "id": "Workflow/DPO_Alignment",
      "page_title": "DPO Alignment",
      "page_type": "Workflow",
      "overview": "End-to-end process for aligning language models with human preferences using Direct Preference Optimization (DPO) with Unsloth.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Paper|DPO Paper|https://arxiv.org/abs/2305.18290]]\n* [[source::Doc|TRL DPOTrainer|https://huggingface.co/docs/trl/dpo_trainer]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::RLHF]], [[domain::Alignment]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for aligning language models with human preferences using Direct Preference Optimization (DPO) with Unsloth.\n\n=== Description ===\nThis workflow aligns a fine-tuned model to produce preferred responses over rejected alternatives. DPO simplifies the traditional RLHF pipeline by eliminating the need for a separate reward model, directly optimizing on preference pairs. Use after SFT to improve response quality, safety, and helpfulness.\n\n=== Usage ===\nExecute this workflow when you have preference data (chosen vs rejected response pairs) and want to align model behavior. Typically applied after SFT training. Effective for reducing harmful outputs, improving helpfulness, and matching specific writing styles.\n\n== Execution Steps ==\n=== Step 1: Load SFT Model ===\n[[step::Principle:Quantization]]\nLoad a model that has been supervised fine-tuned (SFT).\n\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n</syntaxhighlight>\n\n=== Step 2: Add LoRA Adapters ===\n[[step::Principle:Low_Rank_Adaptation]]\nConfigure LoRA for DPO training.\n\n<syntaxhighlight lang=\"python\">\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    use_gradient_checkpointing = \"unsloth\",\n)\n</syntaxhighlight>\n\n=== Step 3: Prepare Preference Dataset ===\n[[step::Principle:Direct_Preference_Optimization]]\nFormat dataset with prompt, chosen, and rejected columns.\n\n<syntaxhighlight lang=\"python\">\nfrom datasets import load_dataset\n\n# Load preference dataset\ndataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n\n# Dataset should have columns:\n# - prompt: The input/question\n# - chosen: The preferred response\n# - rejected: The dispreferred response\n\n# Example format:\n# {\n#     \"prompt\": \"What is the capital of France?\",\n#     \"chosen\": \"The capital of France is Paris.\",\n#     \"rejected\": \"I don't know the answer.\"\n# }\n</syntaxhighlight>\n\n=== Step 4: Configure DPO Training ===\n[[step::Principle:Direct_Preference_Optimization]]\nSet up DPOTrainer with appropriate hyperparameters.\n\n<syntaxhighlight lang=\"python\">\nfrom trl import DPOTrainer, DPOConfig\n\nargs = DPOConfig(\n    output_dir = \"dpo_outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    learning_rate = 5e-5,\n    max_steps = 200,\n    beta = 0.1,\n    optim = \"adamw_8bit\",\n    logging_steps = 1,\n    max_length = 1024,\n    max_prompt_length = 512,\n)\n</syntaxhighlight>\n\n=== Step 5: Train with DPO ===\nExecute DPO training.\n\n<syntaxhighlight lang=\"python\">\ntrainer = DPOTrainer(\n    model = model,\n    ref_model = None,\n    args = args,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== Step 6: Save Aligned Model ===\nSave the preference-aligned model.\n\n<syntaxhighlight lang=\"python\">\nmodel.save_pretrained(\"dpo_model\")\ntokenizer.save_pretrained(\"dpo_model\")\n\nmodel.save_pretrained_gguf(\"dpo_gguf\", tokenizer, quantization_method=\"q4_k_m\")\n</syntaxhighlight>\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Load SFT Model] --> B[Add LoRA Adapters]\n    B --> C[Prepare Preference Data]\n    C --> D[Configure DPOTrainer]\n    D --> E[DPO Training Loop]\n    E --> F[Compute Chosen Log Probs]\n    E --> G[Compute Rejected Log Probs]\n    F --> H[DPO Loss Calculation]\n    G --> H\n    H --> E\n    E --> I[Save Aligned Model]\n}}\n\n== Related Pages ==\n=== Execution Steps ===\n* [[step::Principle:Quantization]] - Step 1\n* [[step::Principle:Low_Rank_Adaptation]] - Step 2\n* [[step::Principle:Direct_Preference_Optimization]] - Steps 3-5\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Unsloth_Gradient_Checkpointing_Optimization]]\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n\n",
      "domains": [
        "LLMs",
        "RLHF",
        "Alignment"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "DPO Paper",
          "url": "https://arxiv.org/abs/2305.18290"
        },
        {
          "type": "Doc",
          "title": "TRL DPOTrainer",
          "url": "https://huggingface.co/docs/trl/dpo_trainer"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Quantization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Direct_Preference_Optimization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Direct_Preference_Optimization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Quantization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Direct_Preference_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unsloth_Gradient_Checkpointing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        }
      ]
    },
    {
      "id": "Workflow/GRPO_Reinforcement_Learning",
      "page_title": "GRPO Reinforcement Learning",
      "page_type": "Workflow",
      "overview": "End-to-end process for improving model reasoning capabilities using Group Relative Policy Optimization (GRPO) reinforcement learning with Unsloth.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Paper|DeepSeekMath GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Doc|TRL GRPOTrainer|https://huggingface.co/docs/trl]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::RLHF]], [[domain::Reasoning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for improving model reasoning capabilities using Group Relative Policy Optimization (GRPO) reinforcement learning with Unsloth.\n\n=== Description ===\nThis workflow uses GRPO to enhance a model's reasoning abilities, particularly for math, coding, and logic tasks. GRPO generates multiple responses per prompt, scores them with a reward function, and trains the model to prefer higher-scored responses using group-relative normalization. Unsloth's optimizations make GRPO training feasible on consumer GPUs.\n\n=== Usage ===\nExecute this workflow when you want to improve model reasoning beyond what SFT can achieve. Requires a base model (ideally already SFT'd) and a reward function or verifier. Best for tasks with verifiable correctness (math, code execution, factual questions).\n\n== Execution Steps ==\n=== Step 1: Load Pre-trained Model ===\n[[step::Principle:Quantization]]\nLoad a base model, ideally one that has been SFT'd on relevant tasks.\n\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen2.5-7B-bnb-4bit\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n</syntaxhighlight>\n\n=== Step 2: Add LoRA Adapters ===\n[[step::Principle:Low_Rank_Adaptation]]\nConfigure LoRA for GRPO training.\n\n<syntaxhighlight lang=\"python\">\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    use_gradient_checkpointing = \"unsloth\",\n)\n</syntaxhighlight>\n\n=== Step 3: Define Reward Function ===\n[[step::Principle:Group_Relative_Policy_Optimization]]\nCreate a reward function that scores response quality.\n\n<syntaxhighlight lang=\"python\">\ndef reward_function(prompts, responses, ground_truths):\n    \"\"\"\n    Score responses based on correctness.\n    Returns list of rewards (floats).\n    \"\"\"\n    rewards = []\n    for prompt, response, gt in zip(prompts, responses, ground_truths):\n        # Example: Check if answer is correct\n        if extract_answer(response) == gt:\n            rewards.append(1.0)\n        else:\n            rewards.append(0.0)\n    return rewards\n\n# Or use a reward model\n# from transformers import AutoModelForSequenceClassification\n# reward_model = AutoModelForSequenceClassification.from_pretrained(\"...\")\n</syntaxhighlight>\n\n=== Step 4: Prepare Dataset ===\n[[step::Principle:Group_Relative_Policy_Optimization]]\nFormat dataset with prompts and optional ground truth for reward computation.\n\n<syntaxhighlight lang=\"python\">\nfrom datasets import load_dataset\n\n# Math reasoning dataset example\ndataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n\ndef format_grpo(example):\n    return {\n        \"prompt\": example[\"question\"],\n        \"ground_truth\": example[\"answer\"].split(\"####\")[-1].strip()\n    }\n\ndataset = dataset.map(format_grpo)\n</syntaxhighlight>\n\n=== Step 5: Configure GRPO Training ===\n[[step::Principle:Group_Relative_Policy_Optimization]]\nSet up GRPOTrainer with appropriate hyperparameters.\n\n<syntaxhighlight lang=\"python\">\nfrom trl import GRPOTrainer, GRPOConfig\n\nargs = GRPOConfig(\n    output_dir = \"grpo_outputs\",\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 8,\n    learning_rate = 1e-5,\n    max_steps = 500,\n    num_generations = 4,\n    max_new_tokens = 512,\n    optim = \"adamw_8bit\",\n    logging_steps = 1,\n)\n\ntrainer = GRPOTrainer(\n    model = model,\n    args = args,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n    reward_fn = reward_function,\n)\n</syntaxhighlight>\n\n=== Step 6: Train and Save ===\nExecute training and save the improved model.\n\n<syntaxhighlight lang=\"python\">\n# Train\ntrainer.train()\n\n# Save\nmodel.save_pretrained(\"grpo_model\")\ntokenizer.save_pretrained(\"grpo_model\")\n</syntaxhighlight>\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Load Base Model] --> B[Add LoRA Adapters]\n    B --> C[Define Reward Function]\n    C --> D[Prepare Prompt Dataset]\n    D --> E[Configure GRPOTrainer]\n    E --> F[GRPO Training Loop]\n    F --> G[Generate N Responses]\n    G --> H[Score with Reward]\n    H --> I[Normalize Within Group]\n    I --> J[Policy Gradient Update]\n    J --> F\n    F --> K[Save Improved Model]\n}}\n\n== Related Pages ==\n=== Execution Steps ===\n* [[step::Principle:Quantization]] - Step 1\n* [[step::Principle:Low_Rank_Adaptation]] - Step 2\n* [[step::Principle:Group_Relative_Policy_Optimization]] - Steps 3-5\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Unsloth_Gradient_Checkpointing_Optimization]]\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n* [[uses_heuristic::Heuristic:Batch_Size_Optimization]]\n\n",
      "domains": [
        "LLMs",
        "RLHF",
        "Reasoning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "DeepSeekMath GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Doc",
          "title": "TRL GRPOTrainer",
          "url": "https://huggingface.co/docs/trl"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Quantization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Group_Relative_Policy_Optimization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Group_Relative_Policy_Optimization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Group_Relative_Policy_Optimization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Quantization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Group_Relative_Policy_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unsloth_Gradient_Checkpointing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Batch_Size_Optimization"
        }
      ]
    },
    {
      "id": "Workflow/Model_Export_GGUF",
      "page_title": "Model Export GGUF",
      "page_type": "Workflow",
      "overview": "End-to-end process for exporting fine-tuned models to GGUF format for deployment with llama.cpp, Ollama, and local inference engines.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Repo|llama.cpp|https://github.com/ggerganov/llama.cpp]]\n* [[source::Doc|Ollama|https://ollama.ai/]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Deployment]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for exporting fine-tuned models to GGUF format for deployment with llama.cpp, Ollama, and local inference engines.\n\n=== Description ===\nThis workflow converts fine-tuned models from HuggingFace format to GGUF for efficient local inference. GGUF is the standard format for running LLMs on CPUs and consumer hardware. Unsloth handles the full pipeline: merging LoRA adapters, converting to GGUF, and applying quantization.\n\n=== Usage ===\nExecute this workflow after fine-tuning when you need to deploy your model for local inference. Use when targeting edge devices, offline applications, or users without GPU access. GGUF enables running 7B models on laptops and smartphones.\n\n== Execution Steps ==\n=== Step 1: Load Trained Model ===\nLoad your fine-tuned model with LoRA adapters.\n\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"lora_model\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n</syntaxhighlight>\n\n=== Step 2: Choose Quantization Level ===\nSelect quantization based on target deployment.\n\n<syntaxhighlight lang=\"python\">\n# Quantization options for GGUF:\n# - q4_k_m: Best balance (recommended)\n# - q5_k_m: Higher quality, larger file\n# - q8_0: Best quality, largest file\n# - q2_k: Smallest file, lower quality\n# - f16: Full precision, reference only\n\nquantization = \"q4_k_m\"\n</syntaxhighlight>\n\n=== Step 3: Export to GGUF ===\nConvert and save as GGUF file.\n\n<syntaxhighlight lang=\"python\">\nmodel.save_pretrained_gguf(\n    \"model_gguf\",\n    tokenizer,\n    quantization_method = quantization,\n)\n\nprint(f\"Saved to model_gguf/unsloth.{quantization.upper()}.gguf\")\n</syntaxhighlight>\n\n=== Step 4: Export Multiple Quantizations (Optional) ===\nCreate variants for different deployment targets.\n\n<syntaxhighlight lang=\"python\">\nfor quant in [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]:\n    model.save_pretrained_gguf(\n        f\"model_{quant}\",\n        tokenizer,\n        quantization_method = quant,\n    )\n</syntaxhighlight>\n\n=== Step 5: Deploy to Ollama ===\nCreate and run with Ollama.\n\n<syntaxhighlight lang=\"bash\">\n# Create Modelfile\ncat > Modelfile << EOF\nFROM ./model_gguf/unsloth.Q4_K_M.gguf\n\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nPARAMETER stop \"<|im_end|>\"\n\nSYSTEM \"You are a helpful assistant.\"\nEOF\n\n# Create Ollama model\nollama create mymodel -f Modelfile\n\n# Run\nollama run mymodel\n</syntaxhighlight>\n\n=== Step 6: Push to HuggingFace (Optional) ===\nShare GGUF model on HuggingFace Hub.\n\n<syntaxhighlight lang=\"python\">\nmodel.push_to_hub_gguf(\n    \"your-username/model-name-GGUF\",\n    tokenizer,\n    quantization_method = \"q4_k_m\",\n    token = \"hf_...\",\n)\n</syntaxhighlight>\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Load Trained Model] --> B[Select Quantization]\n    B --> C[Export to GGUF]\n    C --> D{Deployment Target}\n    D --> E[Ollama]\n    D --> F[llama.cpp]\n    D --> G[LM Studio]\n    D --> H[HuggingFace Hub]\n    E --> I[Create Modelfile]\n    I --> J[ollama create]\n    J --> K[ollama run]\n}}\n\n== Size Comparison ==\n{| class=\"wikitable\"\n! Format !! 7B Model Size !! Quality !! Speed\n|-\n|| Original (FP16) || 14 GB || 100% || Baseline\n|-\n|| q8_0 || 7.2 GB || ~99% || 1.3x\n|-\n|| q5_k_m || 4.8 GB || ~97% || 1.5x\n|-\n|| q4_k_m || 4.1 GB || ~95% || 1.7x\n|-\n|| q2_k || 2.8 GB || ~85% || 2x\n|}\n\n== Related Pages ==\n=== Execution Steps ===\n(Export workflow - no theoretical principles, implementation focused)\n\n=== Tips and Tricks ===\n(Quantization choice depends on deployment constraints)\n\n",
      "domains": [
        "LLMs",
        "Deployment",
        "Inference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Repo",
          "title": "llama.cpp",
          "url": "https://github.com/ggerganov/llama.cpp"
        },
        {
          "type": "Doc",
          "title": "Ollama",
          "url": "https://ollama.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": []
    },
    {
      "id": "Workflow/QLoRA_Finetuning",
      "page_title": "QLoRA Finetuning",
      "page_type": "Workflow",
      "overview": "End-to-end process for parameter-efficient fine-tuning of large language models using 4-bit quantization (QLoRA) with Unsloth's 2x speedup optimizations.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Paper|QLoRA Paper|https://arxiv.org/abs/2305.14314]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Fine_Tuning]], [[domain::PEFT]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for parameter-efficient fine-tuning of large language models using 4-bit quantization (QLoRA) with Unsloth's 2x speedup optimizations.\n\n=== Description ===\nThis workflow outlines the standard procedure for fine-tuning Large Language Models on consumer hardware. It combines 4-bit quantization (Q) with Low-Rank Adaptation (LoRA) to reduce memory requirements by 75%+, enabling training of 7B+ parameter models on single GPUs with 16GB VRAM. Unsloth's optimizations provide an additional 2x speedup and 30% memory reduction compared to standard QLoRA implementations.\n\n=== Usage ===\nExecute this workflow when you have an instruction dataset and need to adapt a base LLM to follow specific instructions, but have limited GPU resources (e.g., <24GB VRAM). This is the most common fine-tuning workflow for practitioners without access to large GPU clusters.\n\n== Execution Steps ==\n=== Step 1: Model Loading with Quantization ===\n[[step::Principle:Quantization]]\nLoad the base model with 4-bit NF4 quantization to fit in GPU memory. Unsloth provides pre-quantized models for faster loading.\n\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n</syntaxhighlight>\n\n=== Step 2: LoRA Adapter Configuration ===\n[[step::Principle:Low_Rank_Adaptation]]\nAdd LoRA adapters to the model with Unsloth's gradient checkpointing optimizations.\n\n<syntaxhighlight lang=\"python\">\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    use_gradient_checkpointing = \"unsloth\",\n)\n</syntaxhighlight>\n\n=== Step 3: Dataset Preparation ===\n[[step::Principle:Supervised_Fine_Tuning]]\nFormat your dataset into the required instruction format for the model.\n\n<syntaxhighlight lang=\"python\">\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n\n# Optional: Apply chat template\ndef format_prompt(example):\n    return {\"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"}\n\ndataset = dataset.map(format_prompt)\n</syntaxhighlight>\n\n=== Step 4: Training Configuration ===\n[[step::Principle:Supervised_Fine_Tuning]]\nConfigure training hyperparameters using SFTConfig.\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\nargs = SFTConfig(\n    output_dir = \"outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    warmup_steps = 10,\n    max_steps = 60,\n    learning_rate = 2e-4,\n    optim = \"adamw_8bit\",\n    max_seq_length = 2048,\n    logging_steps = 1,\n    seed = 3407,\n)\n</syntaxhighlight>\n\n=== Step 5: Training Execution ===\n[[step::Principle:Supervised_Fine_Tuning]]\nRun the training loop using SFTTrainer.\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    args = args,\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== Step 6: Model Saving ===\nSave the trained adapter or merged model.\n\n<syntaxhighlight lang=\"python\">\n# Save LoRA adapter only (~100MB)\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n\n# Or save merged 16-bit model\nmodel.save_pretrained_merged(\"merged_model\", tokenizer, save_method=\"merged_16bit\")\n\n# Or export to GGUF for local inference\nmodel.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")\n</syntaxhighlight>\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Load Model 4-bit] --> B[Add LoRA Adapters]\n    B --> C[Prepare Dataset]\n    C --> D[Configure Training]\n    D --> E[Train with SFTTrainer]\n    E --> F{Save Model}\n    F --> G[LoRA Adapter]\n    F --> H[Merged Model]\n    F --> I[GGUF Export]\n}}\n\n== Related Pages ==\n=== Execution Steps ===\n* [[step::Principle:Quantization]] - Step 1\n* [[step::Principle:Low_Rank_Adaptation]] - Step 2\n* [[step::Principle:Supervised_Fine_Tuning]] - Steps 3-5\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Unsloth_Gradient_Checkpointing_Optimization]]\n* [[uses_heuristic::Heuristic:QLoRA_Target_Modules_Selection]]\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n* [[uses_heuristic::Heuristic:LoRA_Rank_Selection]]\n* [[uses_heuristic::Heuristic:Batch_Size_Optimization]]\n* [[uses_heuristic::Heuristic:AdamW_8bit_Optimizer_Usage]]\n\n",
      "domains": [
        "LLMs",
        "Fine Tuning",
        "PEFT"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Paper",
          "title": "QLoRA Paper",
          "url": "https://arxiv.org/abs/2305.14314"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Quantization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Supervised_Fine_Tuning"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Supervised_Fine_Tuning"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Supervised_Fine_Tuning"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Quantization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Supervised_Fine_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unsloth_Gradient_Checkpointing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "QLoRA_Target_Modules_Selection"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "LoRA_Rank_Selection"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Batch_Size_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "AdamW_8bit_Optimizer_Usage"
        }
      ]
    },
    {
      "id": "Principle/Causal_Language_Modeling",
      "page_title": "Causal Language Modeling",
      "page_type": "Principle",
      "overview": "Language modeling paradigm where models predict the next token based only on previous tokens, using left-to-right autoregressive generation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|GPT-2|https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf]]\n* [[source::Paper|LLaMA|https://arxiv.org/abs/2302.13971]]\n* [[source::Doc|HuggingFace Causal LM|https://huggingface.co/docs/transformers/tasks/language_modeling]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::NLP]], [[domain::Language_Modeling]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nLanguage modeling paradigm where models predict the next token based only on previous tokens, using left-to-right autoregressive generation.\n\n=== Description ===\nCausal Language Modeling (CLM) is the training objective used by GPT, Llama, Mistral, and most modern LLMs. The model learns to predict each token given all previous tokens, using a causal attention mask to prevent looking at future tokens. This autoregressive structure enables open-ended text generation and forms the foundation of instruction-following capabilities.\n\n=== Usage ===\nUse this principle as the foundation for understanding how LLMs generate text. Causal LM is the base training objective before instruction tuning. Understanding CLM is essential for working with decoder-only transformers, implementing generation strategies, and debugging output quality issues.\n\n== Theoretical Basis ==\n'''Training Objective:'''\nMaximize the likelihood of each token given its left context:\n\n\\[\n\\mathcal{L}_{CLM} = -\\sum_{t=1}^{T} \\log P_\\theta(x_t | x_{<t})\n\\]\n\nWhere \\(x_{<t} = (x_1, x_2, ..., x_{t-1})\\).\n\n'''Causal Attention Mask:'''\nPrevents attending to future tokens:\n\n\\[\n\\text{Mask}_{i,j} = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}\n\\]\n\n'''Generation Process:'''\n<syntaxhighlight lang=\"python\">\ndef generate_causal(model, prompt, max_tokens):\n    \"\"\"\n    Autoregressive generation\n    \"\"\"\n    tokens = tokenize(prompt)\n    \n    for _ in range(max_tokens):\n        # Get logits for next token\n        logits = model(tokens)[:, -1, :]  # Last position only\n        \n        # Sample or argmax\n        next_token = sample(logits)\n        \n        # Append and continue\n        tokens = concat(tokens, next_token)\n        \n        if next_token == EOS:\n            break\n    \n    return tokens\n</syntaxhighlight>\n\n'''Key Properties:'''\n1. **Unidirectional**: Only sees past, not future\n2. **Autoregressive**: Generates one token at a time\n3. **No padding needed**: Natural handling of variable length\n4. **Caching**: KV cache enables efficient generation\n\n'''Causal vs Masked LM:'''\n{| class=\"wikitable\"\n! Aspect !! Causal LM (GPT) !! Masked LM (BERT)\n|-\n|| Direction || Left-to-right || Bidirectional\n|-\n|| Mask || Causal (triangular) || Random positions\n|-\n|| Use case || Generation || Understanding\n|-\n|| Examples || GPT, Llama, Mistral || BERT, RoBERTa\n|}\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:Unsloth_FastModel]]\n\n=== Tips and Tricks ===\n(Fundamental concept - no specific heuristics)\n\n",
      "domains": [
        "Deep Learning",
        "NLP",
        "Language Modeling"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "GPT-2",
          "url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
        },
        {
          "type": "Paper",
          "title": "LLaMA",
          "url": "https://arxiv.org/abs/2302.13971"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Causal LM",
          "url": "https://huggingface.co/docs/transformers/tasks/language_modeling"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastModel"
        }
      ]
    },
    {
      "id": "Principle/Direct_Preference_Optimization",
      "page_title": "Direct Preference Optimization",
      "page_type": "Principle",
      "overview": "Alignment technique that trains language models directly on preference pairs without explicit reward modeling, simplifying the RLHF pipeline.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|DPO: Direct Preference Optimization|https://arxiv.org/abs/2305.18290]]\n* [[source::Doc|TRL DPOTrainer|https://huggingface.co/docs/trl/dpo_trainer]]\n* [[source::Blog|Understanding DPO|https://huggingface.co/blog/dpo-trl]]\n|-\n! Domains\n| [[domain::RLHF]], [[domain::Alignment]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nAlignment technique that trains language models directly on preference pairs without explicit reward modeling, simplifying the RLHF pipeline.\n\n=== Description ===\nDirect Preference Optimization (DPO) provides a simpler alternative to PPO-based RLHF. Instead of training a separate reward model and using reinforcement learning, DPO directly optimizes the policy using preference pairs (chosen vs. rejected responses). It derives a closed-form loss from the RL objective, making alignment more stable and efficient.\n\n=== Usage ===\nUse this principle after SFT when you want to align model outputs with human preferences. Apply when you have preference data (pairs of good and bad responses to the same prompt). Preferred over PPO when training stability is important and you don't need online learning.\n\n== Theoretical Basis ==\n'''Key Insight:'''\nThe optimal policy under the RLHF objective has a closed form:\n\n\\[\n\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) \\exp\\left(\\frac{r(x,y)}{\\beta}\\right)\n\\]\n\nThis can be rearranged to express reward in terms of policies:\n\n\\[\nr(x,y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x)\n\\]\n\n'''DPO Loss:'''\nSubstituting into the Bradley-Terry preference model:\n\n\\[\n\\mathcal{L}_{DPO} = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right]\n\\]\n\nWhere:\n* \\(y_w\\) = chosen/winning response\n* \\(y_l\\) = rejected/losing response\n* \\(\\beta\\) = temperature controlling deviation from reference\n* \\(\\pi_{ref}\\) = reference policy (usually the SFT model)\n\n'''Simplified:'''\n<syntaxhighlight lang=\"python\">\ndef dpo_loss(pi_logprobs_chosen, pi_logprobs_rejected,\n             ref_logprobs_chosen, ref_logprobs_rejected, beta):\n    \"\"\"\n    Compute DPO loss\n    \"\"\"\n    pi_logratios = pi_logprobs_chosen - pi_logprobs_rejected\n    ref_logratios = ref_logprobs_chosen - ref_logprobs_rejected\n    \n    logits = beta * (pi_logratios - ref_logratios)\n    loss = -F.logsigmoid(logits).mean()\n    \n    return loss\n</syntaxhighlight>\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:TRL_DPOTrainer]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n\n",
      "domains": [
        "RLHF",
        "Alignment",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "DPO: Direct Preference Optimization",
          "url": "https://arxiv.org/abs/2305.18290"
        },
        {
          "type": "Doc",
          "title": "TRL DPOTrainer",
          "url": "https://huggingface.co/docs/trl/dpo_trainer"
        },
        {
          "type": "Blog",
          "title": "Understanding DPO",
          "url": "https://huggingface.co/blog/dpo-trl"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "TRL_DPOTrainer"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        }
      ]
    },
    {
      "id": "Principle/Flash_Attention",
      "page_title": "Flash Attention",
      "page_type": "Principle",
      "overview": "IO-aware attention algorithm that reduces memory usage from O(n\u00b2) to O(n) while also improving speed through kernel fusion.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|FlashAttention|https://arxiv.org/abs/2205.14135]]\n* [[source::Paper|FlashAttention-2|https://arxiv.org/abs/2307.08691]]\n* [[source::Repo|FlashAttention GitHub|https://github.com/Dao-AILab/flash-attention]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Attention]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nIO-aware attention algorithm that reduces memory usage from O(n\u00b2) to O(n) while also improving speed through kernel fusion.\n\n=== Description ===\nFlashAttention revolutionizes attention computation by restructuring the algorithm to minimize memory reads/writes to GPU high-bandwidth memory (HBM). Instead of materializing the full NxN attention matrix, it computes attention in blocks using GPU SRAM (fast memory), achieving both memory efficiency and speed improvements. FlashAttention-2 further optimizes this with better parallelization and reduced non-matmul FLOPs.\n\n=== Usage ===\nUse this principle when training or inferencing with long sequences (4K+ tokens). Essential for extending context length of LLMs. Unsloth automatically uses FlashAttention when available, providing 12x longer context compared to standard attention implementations.\n\n== Theoretical Basis ==\n'''Standard Attention Memory Issue:'''\nComputing \\(\\text{softmax}(QK^T)\\) requires materializing an \\(N \\times N\\) matrix, where N = sequence length.\n* 8K sequence: 8192 \u00d7 8192 \u00d7 4 bytes = 256 MB per layer per batch\n* Not scalable for long contexts\n\n'''FlashAttention Algorithm:'''\nKey insight: softmax can be computed incrementally without full materialization.\n\n\\[\n\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} = \\frac{e^{x_i - m}}{\\sum_j e^{x_j - m}}\n\\]\n\nWhere m = max(x) for numerical stability.\n\n'''Tiled Computation:'''\n<syntaxhighlight lang=\"python\">\ndef flash_attention_block(Q_block, K_blocks, V_blocks):\n    \"\"\"\n    Process attention in tiles without full N\u00d7N matrix\n    \"\"\"\n    O = zeros_like(Q_block)  # Output accumulator\n    l = zeros(Q_block.shape[0])  # Normalizer accumulator\n    m = full(Q_block.shape[0], -inf)  # Max accumulator\n    \n    for K_b, V_b in zip(K_blocks, V_blocks):\n        # Compute attention scores for this block\n        S = Q_block @ K_b.T / sqrt(d_k)\n        \n        # Online softmax update\n        m_new = maximum(m, S.max(dim=-1))\n        l = l * exp(m - m_new) + exp(S - m_new).sum(dim=-1)\n        O = O * exp(m - m_new)[:, None] + exp(S - m_new) @ V_b\n        m = m_new\n    \n    return O / l[:, None]\n</syntaxhighlight>\n\n'''Benefits:'''\n{| class=\"wikitable\"\n! Aspect !! Standard !! FlashAttention\n|-\n|| Memory || O(N\u00b2) || O(N)\n|-\n|| HBM Accesses || O(N\u00b2) || O(N\u00b2 / SRAM)\n|-\n|| Speed || Baseline || 2-4x faster\n|}\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:Unsloth_FastLanguageModel]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Memory_Efficient_Attention]]\n\n",
      "domains": [
        "Optimization",
        "Attention",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "FlashAttention",
          "url": "https://arxiv.org/abs/2205.14135"
        },
        {
          "type": "Paper",
          "title": "FlashAttention-2",
          "url": "https://arxiv.org/abs/2307.08691"
        },
        {
          "type": "Repo",
          "title": "FlashAttention GitHub",
          "url": "https://github.com/Dao-AILab/flash-attention"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastLanguageModel"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Memory_Efficient_Attention"
        }
      ]
    },
    {
      "id": "Principle/Gradient_Checkpointing",
      "page_title": "Gradient Checkpointing",
      "page_type": "Principle",
      "overview": "Memory optimization technique that trades compute for memory by recomputing activations during backward pass instead of storing them.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|Training Deep Nets with Sublinear Memory|https://arxiv.org/abs/1604.06174]]\n* [[source::Doc|PyTorch Checkpointing|https://pytorch.org/docs/stable/checkpoint.html]]\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory_Management]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nMemory optimization technique that trades compute for memory by recomputing activations during backward pass instead of storing them.\n\n=== Description ===\nDuring standard backpropagation, all intermediate activations from the forward pass are stored for gradient computation. For deep networks, this consumes massive memory. Gradient checkpointing stores only selected activations (checkpoints) and recomputes the others during the backward pass. This reduces memory from O(n) to O(\u221an) for n layers, at the cost of one additional forward pass.\n\n=== Usage ===\nUse this principle when training deep models that don't fit in GPU memory. Essential for fine-tuning large language models (7B+ parameters) on consumer hardware. Apply when you see CUDA Out of Memory errors during training. Unsloth's \"unsloth\" mode provides enhanced gradient checkpointing with 30% additional savings.\n\n== Theoretical Basis ==\n'''Standard Training Memory:'''\nFor a model with L layers, each producing activations of size A:\n\\[\n\\text{Memory} = L \\times A \\quad \\text{(all activations stored)}\n\\]\n\n'''With Checkpointing:'''\nStore only \u221aL checkpoints, recompute intermediate activations:\n\\[\n\\text{Memory} \\approx \\sqrt{L} \\times A\n\\]\n\n'''Algorithm:'''\n<syntaxhighlight lang=\"python\">\n# Conceptual gradient checkpointing\ndef checkpointed_forward(model, x, checkpoint_layers):\n    activations = {}\n    \n    for i, layer in enumerate(model.layers):\n        x = layer(x)\n        if i in checkpoint_layers:\n            activations[i] = x.detach()  # Store checkpoint\n    \n    return x, activations\n\ndef checkpointed_backward(model, grad, activations, checkpoint_layers):\n    for i in reversed(range(len(model.layers))):\n        if i in checkpoint_layers:\n            # Recompute activations from checkpoint\n            x = activations[nearest_checkpoint(i)]\n            for j in range(nearest_checkpoint(i), i):\n                x = model.layers[j](x)\n        \n        grad = backward_through_layer(model.layers[i], x, grad)\n    \n    return grad\n</syntaxhighlight>\n\n'''Trade-offs:'''\n* Memory reduction: ~50-70% for standard, ~60-80% with Unsloth\n* Compute overhead: ~20-33% (one extra forward pass per segment)\n* Unsloth optimizes this with smart checkpointing and kernel fusion\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:Unsloth_get_peft_model]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Unsloth_Gradient_Checkpointing_Optimization]]\n* [[uses_heuristic::Heuristic:Batch_Size_Optimization]]\n\n",
      "domains": [
        "Optimization",
        "Memory Management",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "Training Deep Nets with Sublinear Memory",
          "url": "https://arxiv.org/abs/1604.06174"
        },
        {
          "type": "Doc",
          "title": "PyTorch Checkpointing",
          "url": "https://pytorch.org/docs/stable/checkpoint.html"
        },
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_get_peft_model"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unsloth_Gradient_Checkpointing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Batch_Size_Optimization"
        }
      ]
    },
    {
      "id": "Principle/Group_Relative_Policy_Optimization",
      "page_title": "Group Relative Policy Optimization",
      "page_type": "Principle",
      "overview": "Reinforcement learning technique that improves reasoning by training on groups of sampled responses with relative rewards.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|DeepSeekMath: GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Doc|TRL GRPOTrainer|https://huggingface.co/docs/trl]]\n* [[source::Doc|Unsloth GRPO Guide|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::RLHF]], [[domain::Reasoning]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nReinforcement learning technique that improves reasoning by training on groups of sampled responses with relative rewards.\n\n=== Description ===\nGroup Relative Policy Optimization (GRPO) is a reinforcement learning algorithm designed to improve model reasoning capabilities. Unlike DPO which uses static preference pairs, GRPO generates multiple responses per prompt online and computes relative rewards within each group. This approach, popularized by DeepSeek for mathematical reasoning, provides richer training signals and adapts to the model's current capabilities.\n\n=== Usage ===\nUse this principle when you want to improve model reasoning abilities (math, coding, logic). Apply when you can define a reward function or have a verifier for response quality. Particularly effective for chain-of-thought improvement and self-consistency enhancement. Requires more compute than DPO but can achieve stronger reasoning performance.\n\n== Theoretical Basis ==\n'''Algorithm Overview:'''\n1. For each prompt x, sample G responses: \\{y_1, ..., y_G\\}\n2. Compute rewards: \\{r_1, ..., r_G\\}\n3. Normalize rewards within group: \\(\\hat{r}_i = (r_i - \\mu_G) / \\sigma_G\\)\n4. Update policy using normalized rewards\n\n'''GRPO Objective:'''\n\\[\n\\mathcal{L}_{GRPO} = -\\mathbb{E}_{x \\sim D, y_i \\sim \\pi_\\theta} \\left[ \\hat{r}_i \\cdot \\log \\pi_\\theta(y_i|x) - \\beta D_{KL}(\\pi_\\theta || \\pi_{ref}) \\right]\n\\]\n\n'''Group Normalization:'''\nKey innovation - rewards are normalized within each group:\n\\[\n\\hat{r}_i = \\frac{r_i - \\text{mean}(\\{r_1, ..., r_G\\})}{\\text{std}(\\{r_1, ..., r_G\\}) + \\epsilon}\n\\]\n\nThis provides:\n* Baseline estimation without separate value network\n* Adaptive difficulty - harder prompts have higher variance\n* Reduced reward hacking\n\n'''Pseudo-code:'''\n<syntaxhighlight lang=\"python\">\ndef grpo_step(model, ref_model, prompts, reward_fn, G=4, beta=0.1):\n    \"\"\"\n    One GRPO training step\n    \"\"\"\n    all_losses = []\n    \n    for prompt in prompts:\n        # Generate G responses\n        responses = model.generate(prompt, num_return_sequences=G)\n        \n        # Compute rewards\n        rewards = [reward_fn(prompt, r) for r in responses]\n        \n        # Normalize within group\n        rewards = (rewards - mean(rewards)) / (std(rewards) + 1e-8)\n        \n        # Compute policy gradient loss\n        for response, reward in zip(responses, rewards):\n            log_prob = model.log_prob(response | prompt)\n            ref_log_prob = ref_model.log_prob(response | prompt)\n            \n            kl_penalty = beta * (log_prob - ref_log_prob)\n            loss = -reward * log_prob + kl_penalty\n            all_losses.append(loss)\n    \n    return mean(all_losses)\n</syntaxhighlight>\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:TRL_GRPOTrainer]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n* [[uses_heuristic::Heuristic:Batch_Size_Optimization]]\n\n",
      "domains": [
        "RLHF",
        "Reasoning",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "DeepSeekMath: GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Doc",
          "title": "TRL GRPOTrainer",
          "url": "https://huggingface.co/docs/trl"
        },
        {
          "type": "Doc",
          "title": "Unsloth GRPO Guide",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "TRL_GRPOTrainer"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Batch_Size_Optimization"
        }
      ]
    },
    {
      "id": "Principle/Low_Rank_Adaptation",
      "page_title": "Low Rank Adaptation",
      "page_type": "Principle",
      "overview": "Parameter-efficient fine-tuning technique that adapts pre-trained models by injecting trainable low-rank decomposition matrices into existing layers.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA: Low-Rank Adaptation|https://arxiv.org/abs/2106.09685]]\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n* [[source::Blog|HuggingFace PEFT|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::PEFT]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nParameter-efficient fine-tuning technique that adapts pre-trained models by injecting trainable low-rank decomposition matrices into existing layers.\n\n=== Description ===\nLow-Rank Adaptation (LoRA) is a technique for fine-tuning large language models efficiently. Instead of updating all model parameters, LoRA freezes the pre-trained weights and injects small trainable matrices into each layer. These matrices are decomposed into two low-rank components (A and B), dramatically reducing the number of trainable parameters. For a weight matrix W, the update is W' = W + BA, where B and A have much lower rank than W.\n\n=== Usage ===\nUse this principle when you need to fine-tune large language models (7B+ parameters) but have limited GPU memory or storage. Ideal for instruction tuning, domain adaptation, and style transfer tasks. Prefer LoRA over full fine-tuning when you want to maintain multiple task-specific adapters that can be swapped at inference time.\n\n== Theoretical Basis ==\nThe core insight is that the weight updates during fine-tuning have low \"intrinsic rank\" - they can be approximated by low-rank matrices without significant loss.\n\nFor a pre-trained weight matrix \\( W_0 \\in \\mathbb{R}^{d \\times k} \\):\n\n\\[\nW = W_0 + \\Delta W = W_0 + BA\n\\]\n\nWhere:\n* \\( B \\in \\mathbb{R}^{d \\times r} \\)\n* \\( A \\in \\mathbb{R}^{r \\times k} \\)  \n* \\( r \\ll \\min(d, k) \\) is the rank\n\n'''Parameter Reduction:'''\n* Full fine-tuning: \\( d \\times k \\) parameters\n* LoRA: \\( r \\times (d + k) \\) parameters\n* For d=k=4096, r=16: 99.6% reduction\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\nclass LoRALayer:\n    def __init__(self, d, k, r, alpha):\n        self.W = frozen_pretrained_weight  # (d, k)\n        self.A = nn.Parameter(torch.randn(r, k))  # Trainable\n        self.B = nn.Parameter(torch.zeros(d, r))  # Trainable\n        self.scaling = alpha / r\n    \n    def forward(self, x):\n        # Original computation + low-rank update\n        return self.W @ x + self.scaling * (self.B @ (self.A @ x))\n</syntaxhighlight>\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:Unsloth_get_peft_model]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:LoRA_Rank_Selection]]\n* [[uses_heuristic::Heuristic:QLoRA_Target_Modules_Selection]]\n\n",
      "domains": [
        "Fine Tuning",
        "PEFT",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA: Low-Rank Adaptation",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Blog",
          "title": "HuggingFace PEFT",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_get_peft_model"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "LoRA_Rank_Selection"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "QLoRA_Target_Modules_Selection"
        }
      ]
    },
    {
      "id": "Principle/Mixed_Precision_Training",
      "page_title": "Mixed Precision Training",
      "page_type": "Principle",
      "overview": "Training technique that uses lower precision (FP16/BF16) for most operations while maintaining FP32 for critical computations, reducing memory and increasing speed.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|Mixed Precision Training|https://arxiv.org/abs/1710.03740]]\n* [[source::Doc|NVIDIA Mixed Precision|https://developer.nvidia.com/automatic-mixed-precision]]\n* [[source::Doc|PyTorch AMP|https://pytorch.org/docs/stable/amp.html]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Training]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nTraining technique that uses lower precision (FP16/BF16) for most operations while maintaining FP32 for critical computations, reducing memory and increasing speed.\n\n=== Description ===\nMixed Precision Training combines different numerical precisions to optimize training efficiency. Forward and backward passes use FP16 or BF16 (16-bit) for faster computation and reduced memory, while master weights and certain sensitive operations remain in FP32 for numerical stability. Modern GPUs have specialized Tensor Cores that accelerate 16-bit operations, providing 2-3x speedup.\n\n=== Usage ===\nUse this principle to accelerate training and reduce memory usage on modern GPUs (Volta and newer). BF16 is preferred for LLM training due to its larger exponent range preventing overflow. Unsloth automatically handles mixed precision through its optimizations.\n\n== Theoretical Basis ==\n'''Precision Formats:'''\n{| class=\"wikitable\"\n! Format !! Bits !! Exponent !! Mantissa !! Range !! Use\n|-\n|| FP32 || 32 || 8 || 23 || \u00b13.4e38 || Master weights\n|-\n|| FP16 || 16 || 5 || 10 || \u00b165504 || Training (with scaling)\n|-\n|| BF16 || 16 || 8 || 7 || \u00b13.4e38 || Training (preferred)\n|}\n\n'''Why Mixed Precision Works:'''\n1. Neural networks are robust to precision reduction\n2. Gradients don't need full precision for direction\n3. Master weights maintain accuracy for accumulation\n\n'''Training Pipeline:'''\n<syntaxhighlight lang=\"python\">\n# Mixed precision training with gradient scaling\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()  # For FP16 (not needed for BF16)\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n    \n    # Forward pass in FP16/BF16\n    with autocast(dtype=torch.float16):  # or bfloat16\n        outputs = model(batch)\n        loss = criterion(outputs)\n    \n    # Backward with scaling (prevents underflow in FP16)\n    scaler.scale(loss).backward()\n    \n    # Unscale and update\n    scaler.step(optimizer)\n    scaler.update()\n</syntaxhighlight>\n\n'''Loss Scaling (FP16):'''\nSmall gradients underflow to zero in FP16. Solution:\n1. Scale loss by large factor (e.g., 1024)\n2. Gradients are scaled up, avoiding underflow\n3. Unscale before optimizer step\n\nBF16 doesn't need loss scaling due to larger exponent range.\n\n'''Memory Savings:'''\n{| class=\"wikitable\"\n! Component !! FP32 !! Mixed Precision\n|-\n|| Model weights || 4 bytes/param || 2 bytes/param\n|-\n|| Activations || 4 bytes || 2 bytes\n|-\n|| Gradients || 4 bytes/param || 2 bytes/param\n|-\n|| Master weights || 4 bytes/param || 4 bytes/param (kept FP32)\n|}\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:Unsloth_FastLanguageModel]]\n\n=== Tips and Tricks ===\n(Handled automatically by Unsloth)\n\n",
      "domains": [
        "Optimization",
        "Training",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "Mixed Precision Training",
          "url": "https://arxiv.org/abs/1710.03740"
        },
        {
          "type": "Doc",
          "title": "NVIDIA Mixed Precision",
          "url": "https://developer.nvidia.com/automatic-mixed-precision"
        },
        {
          "type": "Doc",
          "title": "PyTorch AMP",
          "url": "https://pytorch.org/docs/stable/amp.html"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastLanguageModel"
        }
      ]
    },
    {
      "id": "Principle/Parameter_Efficient_Fine_Tuning",
      "page_title": "Parameter Efficient Fine Tuning",
      "page_type": "Principle",
      "overview": "Family of techniques that adapt large pre-trained models by training only a small subset of parameters, reducing memory and storage requirements.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Paper|Prefix Tuning|https://arxiv.org/abs/2101.00190]]\n* [[source::Doc|HuggingFace PEFT|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Optimization]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nFamily of techniques that adapt large pre-trained models by training only a small subset of parameters, reducing memory and storage requirements.\n\n=== Description ===\nParameter Efficient Fine-Tuning (PEFT) encompasses methods that achieve model adaptation while updating only a fraction of parameters. This includes LoRA (low-rank adapters), prefix tuning (trainable prompt tokens), adapters (bottleneck layers), and prompt tuning. PEFT methods make fine-tuning of billion-parameter models feasible on consumer hardware and enable efficient multi-task deployment through adapter switching.\n\n=== Usage ===\nUse this principle when full fine-tuning is impractical due to memory constraints. Apply when you need to maintain multiple task-specific models efficiently (share base weights, swap adapters). Essential understanding for working with modern LLM fine-tuning where full parameter updates are prohibitively expensive.\n\n== Theoretical Basis ==\n'''PEFT Methods Taxonomy:'''\n\n{| class=\"wikitable\"\n! Method !! Where !! What !! Params\n|-\n|| LoRA || Attention/MLP || Low-rank matrices || 0.1-1%\n|-\n|| Prefix Tuning || Input || Trainable prefixes || <0.1%\n|-\n|| Adapters || Layers || Bottleneck modules || 1-3%\n|-\n|| Prompt Tuning || Embeddings || Soft prompts || <0.1%\n|-\n|| (IA)\u00b3 || Activations || Scaling vectors || <0.01%\n|}\n\n'''Why PEFT Works:'''\nThe \"intrinsic dimensionality\" hypothesis suggests that:\n1. Pre-trained models already contain task-relevant features\n2. Fine-tuning mainly activates/combines existing capabilities\n3. The effective update rank is much lower than full parameter space\n\n'''Mathematical Framework:'''\nAll PEFT methods can be viewed as constraining the update space:\n\n\\[\n\\theta_{finetuned} = \\theta_{pretrained} + \\Delta\\theta\n\\]\n\nWhere \\(\\Delta\\theta\\) is restricted to a low-dimensional subspace:\n* LoRA: \\(\\Delta W = BA\\) where \\(B, A\\) are low-rank\n* Prefix: Only modify input representations\n* Adapters: Add small trainable modules\n\n'''Comparison:'''\n<syntaxhighlight lang=\"python\">\n# Full fine-tuning: All 7B parameters trainable\n# Memory: ~56GB for optimizer states + gradients\n\n# LoRA fine-tuning: ~40M parameters trainable (0.5%)\n# Memory: ~4GB for adapters + optimizer states\n\n# Same final performance on most tasks!\n</syntaxhighlight>\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:Unsloth_get_peft_model]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:LoRA_Rank_Selection]]\n* [[uses_heuristic::Heuristic:QLoRA_Target_Modules_Selection]]\n\n",
      "domains": [
        "Fine Tuning",
        "Optimization",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Paper",
          "title": "Prefix Tuning",
          "url": "https://arxiv.org/abs/2101.00190"
        },
        {
          "type": "Doc",
          "title": "HuggingFace PEFT",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_get_peft_model"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "LoRA_Rank_Selection"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "QLoRA_Target_Modules_Selection"
        }
      ]
    },
    {
      "id": "Principle/Quantization",
      "page_title": "Quantization",
      "page_type": "Principle",
      "overview": "Technique to reduce model memory footprint by representing weights in lower precision formats (4-bit, 8-bit) while preserving model quality.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|QLoRA Paper|https://arxiv.org/abs/2305.14314]]\n* [[source::Paper|8-bit Matrix Multiplication|https://arxiv.org/abs/2208.07339]]\n* [[source::Doc|HuggingFace Quantization|https://huggingface.co/docs/transformers/quantization]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory_Management]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nTechnique to reduce model memory footprint by representing weights in lower precision formats (4-bit, 8-bit) while preserving model quality.\n\n=== Description ===\nQuantization reduces the numerical precision of model weights from standard 32-bit or 16-bit floating point to lower bit-widths. For LLMs, 4-bit quantization (QLoRA) reduces memory by ~75% with minimal quality loss. The key innovation is NormalFloat4 (NF4), a data type optimized for the normal distribution of neural network weights, providing better accuracy than uniform 4-bit quantization.\n\n=== Usage ===\nUse this principle when you need to fit large models into limited GPU memory. Essential for running 7B+ parameter models on consumer GPUs (16-24GB VRAM). Apply when memory is the bottleneck, not compute speed. Works in combination with LoRA for efficient fine-tuning.\n\n== Theoretical Basis ==\nQuantization maps continuous values to a discrete set of levels:\n\n\\[\nQ(w) = \\text{round}\\left(\\frac{w - z}{s}\\right) \\cdot s + z\n\\]\n\nWhere:\n* \\( s \\) = scale factor\n* \\( z \\) = zero point\n\n'''NF4 Quantization (QLoRA):'''\nNF4 assumes weights follow a normal distribution N(0, \u03c3). It creates 16 quantization bins positioned optimally for this distribution:\n\n<syntaxhighlight lang=\"python\">\n# Simplified NF4 concept\ndef nf4_quantize(tensor):\n    # Normalize to unit variance\n    normalized = tensor / tensor.abs().max()\n    \n    # Quantize to 4-bit values optimized for normal distribution\n    # Uses pre-computed optimal bin positions for N(0,1)\n    quantized = map_to_nearest_nf4_bin(normalized)\n    \n    return quantized, scale\n</syntaxhighlight>\n\n'''Double Quantization:'''\nQLoRA also quantizes the quantization constants (scales), providing additional ~0.4 bits/parameter savings.\n\n'''Memory Comparison:'''\n{| class=\"wikitable\"\n! Precision !! Bits/Parameter !! 7B Model Size\n|-\n|| FP32 || 32 || 28 GB\n|-\n|| FP16/BF16 || 16 || 14 GB\n|-\n|| INT8 || 8 || 7 GB\n|-\n|| NF4 || 4 || 3.5 GB\n|}\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:BitsAndBytes_4bit_Quantization]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:AdamW_8bit_Optimizer_Usage]]\n\n",
      "domains": [
        "Optimization",
        "Memory Management",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "QLoRA Paper",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Paper",
          "title": "8-bit Matrix Multiplication",
          "url": "https://arxiv.org/abs/2208.07339"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Quantization",
          "url": "https://huggingface.co/docs/transformers/quantization"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "BitsAndBytes_4bit_Quantization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "AdamW_8bit_Optimizer_Usage"
        }
      ]
    },
    {
      "id": "Principle/Rotary_Position_Embedding",
      "page_title": "Rotary Position Embedding",
      "page_type": "Principle",
      "overview": "Position encoding technique that applies rotations to query and key vectors, enabling relative position awareness and seamless context length extension.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|RoFormer: Enhanced Transformer with Rotary Position|https://arxiv.org/abs/2104.09864]]\n* [[source::Blog|Rotary Position Embeddings|https://blog.eleuther.ai/rotary-embeddings/]]\n* [[source::Paper|LLaMA Paper|https://arxiv.org/abs/2302.13971]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::NLP]], [[domain::Position_Encoding]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nPosition encoding technique that applies rotations to query and key vectors, enabling relative position awareness and seamless context length extension.\n\n=== Description ===\nRotary Position Embedding (RoPE) encodes position information by rotating the query and key vectors in attention. Unlike absolute position embeddings, RoPE naturally captures relative positions through the geometric properties of rotations. This enables better length generalization and is the position encoding used in Llama, Mistral, Qwen, and most modern LLMs.\n\n=== Usage ===\nUse this principle when understanding how position information is encoded in modern LLMs. Critical for extending context length beyond training length (RoPE scaling). Understanding RoPE helps when debugging position-related issues or implementing context extension techniques like NTK-aware scaling or YaRN.\n\n== Theoretical Basis ==\n'''Core Idea:'''\nRotate query and key vectors by an angle proportional to their position:\n\n\\[\n\\text{RoPE}(x_m, m) = x_m \\cdot e^{im\\theta}\n\\]\n\nWhere m is position and \u03b8 is a learned or fixed frequency.\n\n'''2D Rotation Matrix Form:'''\nFor each pair of dimensions:\n\\[\nR_\\theta^m = \\begin{pmatrix} \\cos(m\\theta) & -\\sin(m\\theta) \\\\ \\sin(m\\theta) & \\cos(m\\theta) \\end{pmatrix}\n\\]\n\n'''Key Property - Relative Position:'''\n\\[\n\\langle \\text{RoPE}(q, m), \\text{RoPE}(k, n) \\rangle = \\langle q, k \\rangle \\cdot f(m-n)\n\\]\n\nThe dot product only depends on relative position (m-n), not absolute positions.\n\n'''Implementation:'''\n<syntaxhighlight lang=\"python\">\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    \"\"\"\n    q, k: (batch, heads, seq_len, head_dim)\n    cos, sin: Precomputed rotation matrices\n    \"\"\"\n    # Split into pairs of dimensions\n    q1, q2 = q[..., ::2], q[..., 1::2]\n    k1, k2 = k[..., ::2], k[..., 1::2]\n    \n    # Apply rotation\n    q_rotated = torch.cat([q1 * cos - q2 * sin,\n                           q1 * sin + q2 * cos], dim=-1)\n    k_rotated = torch.cat([k1 * cos - k2 * sin,\n                           k1 * sin + k2 * cos], dim=-1)\n    \n    return q_rotated, k_rotated\n</syntaxhighlight>\n\n'''Context Extension:'''\nRoPE naturally supports length extension through frequency scaling:\n* Linear scaling: \u03b8' = \u03b8 / scale\n* NTK-aware: Adjust base frequency\n* YaRN: Learned attention scaling\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:Unsloth_FastLanguageModel]]\n\n=== Tips and Tricks ===\n(Position encoding is handled automatically by Unsloth)\n\n",
      "domains": [
        "Deep Learning",
        "NLP",
        "Position Encoding"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "RoFormer: Enhanced Transformer with Rotary Position",
          "url": "https://arxiv.org/abs/2104.09864"
        },
        {
          "type": "Blog",
          "title": "Rotary Position Embeddings",
          "url": "https://blog.eleuther.ai/rotary-embeddings/"
        },
        {
          "type": "Paper",
          "title": "LLaMA Paper",
          "url": "https://arxiv.org/abs/2302.13971"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastLanguageModel"
        }
      ]
    },
    {
      "id": "Principle/Self_Attention",
      "page_title": "Self Attention",
      "page_type": "Principle",
      "overview": "Mechanism that allows neural networks to weigh the importance of different input positions dynamically based on their relevance to each other.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|Attention Is All You Need|https://arxiv.org/abs/1706.03762]]\n* [[source::Blog|Illustrated Transformer|https://jalammar.github.io/illustrated-transformer/]]\n* [[source::Paper|BERT|https://arxiv.org/abs/1810.04805]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::NLP]], [[domain::Attention]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nMechanism that allows neural networks to weigh the importance of different input positions dynamically based on their relevance to each other.\n\n=== Description ===\nSelf-Attention is the core mechanism of Transformer architectures. It computes attention scores between all pairs of positions in a sequence, allowing the model to capture long-range dependencies regardless of distance. Unlike RNNs which process sequentially, self-attention processes all positions in parallel, enabling massive scalability. It's the fundamental building block of modern LLMs like GPT, Llama, and Mistral.\n\n=== Usage ===\nUse this principle when designing architectures for sequence modeling tasks (NLP, time series) where capturing long-term context is critical. It is the fundamental building block of all modern Large Language Models. Understanding self-attention is essential for working with Transformers, debugging attention patterns, and optimizing memory usage during training.\n\n== Theoretical Basis ==\nThe core operation is scaled dot-product attention:\n\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\n\nWhere:\n* Q (Query): What we're looking for - shape (seq_len, d_k)\n* K (Key): What we match against - shape (seq_len, d_k)\n* V (Value): What we retrieve - shape (seq_len, d_v)\n* d_k: Key dimension (scaling factor prevents softmax saturation)\n\n'''Multi-Head Attention:'''\nMultiple attention heads capture different relationship types:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n\\]\n\n'''Pseudo-code:'''\n<syntaxhighlight lang=\"python\">\ndef self_attention(x, W_q, W_k, W_v, mask=None):\n    \"\"\"\n    x: input sequence (batch, seq_len, d_model)\n    \"\"\"\n    Q = x @ W_q  # (batch, seq_len, d_k)\n    K = x @ W_k\n    V = x @ W_v\n    \n    # Attention scores\n    scores = Q @ K.transpose(-2, -1) / sqrt(d_k)\n    \n    # Apply causal mask for autoregressive models\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    \n    weights = softmax(scores, dim=-1)\n    output = weights @ V\n    \n    return output\n</syntaxhighlight>\n\n'''Memory Complexity:'''\n* Attention matrix: O(n\u00b2) where n = sequence length\n* This is why long-context models need FlashAttention optimization\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:Unsloth_FastLanguageModel]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Memory_Efficient_Attention]]\n\n",
      "domains": [
        "Deep Learning",
        "NLP",
        "Attention"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "Attention Is All You Need",
          "url": "https://arxiv.org/abs/1706.03762"
        },
        {
          "type": "Blog",
          "title": "Illustrated Transformer",
          "url": "https://jalammar.github.io/illustrated-transformer/"
        },
        {
          "type": "Paper",
          "title": "BERT",
          "url": "https://arxiv.org/abs/1810.04805"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastLanguageModel"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Memory_Efficient_Attention"
        }
      ]
    },
    {
      "id": "Principle/Supervised_Fine_Tuning",
      "page_title": "Supervised Fine Tuning",
      "page_type": "Principle",
      "overview": "Training technique that adapts pre-trained language models to follow instructions by training on input-output demonstration pairs.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|InstructGPT|https://arxiv.org/abs/2203.02155]]\n* [[source::Doc|TRL SFTTrainer|https://huggingface.co/docs/trl/sft_trainer]]\n* [[source::Blog|Fine-tuning LLMs|https://huggingface.co/blog/fine-tune-llama-2]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::LLMs]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nTraining technique that adapts pre-trained language models to follow instructions by training on input-output demonstration pairs.\n\n=== Description ===\nSupervised Fine-Tuning (SFT) is the standard method for teaching language models to follow instructions. A pre-trained base model is trained on curated datasets of (instruction, response) pairs, learning to generate helpful, task-specific outputs. SFT is typically the first step in the RLHF pipeline, creating a model that can be further aligned using preference learning (DPO, PPO, GRPO).\n\n=== Usage ===\nUse this principle when adapting a base language model to a specific task or behavior. Apply SFT when you have demonstration data showing desired inputs and outputs. Common use cases include instruction following, chat capability, domain-specific knowledge injection, and format adherence (JSON, code, etc.).\n\n== Theoretical Basis ==\n'''Objective Function:'''\nStandard cross-entropy loss over target tokens:\n\n\\[\n\\mathcal{L}_{SFT} = -\\sum_{t=1}^{T} \\log P_\\theta(y_t | x, y_{<t})\n\\]\n\nWhere:\n* x = input/instruction\n* y = target response\n* T = response length\n\n'''Training Data Format:'''\n<syntaxhighlight lang=\"python\">\n# Typical instruction format\nexample = {\n    \"instruction\": \"Summarize the following article:\",\n    \"input\": \"Article text here...\",\n    \"output\": \"Summary of the article...\"\n}\n\n# Chat format\nexample = {\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n        {\"role\": \"assistant\", \"content\": \"Machine learning is...\"}\n    ]\n}\n</syntaxhighlight>\n\n'''Key Considerations:'''\n1. **Data Quality**: Model learns from demonstrations; garbage in = garbage out\n2. **Format Consistency**: Use consistent prompt templates\n3. **Loss Masking**: Only compute loss on response tokens, not input\n\n'''Loss Masking:'''\n<syntaxhighlight lang=\"python\">\ndef compute_sft_loss(logits, labels, input_mask):\n    \"\"\"\n    Only compute loss on response tokens\n    \"\"\"\n    # Mask out input tokens (set to -100 for ignore)\n    labels[input_mask] = -100\n    \n    loss = cross_entropy(logits, labels, ignore_index=-100)\n    return loss\n</syntaxhighlight>\n\n== Related Pages ==\n=== Implemented By ===\n* [[implemented_by::Implementation:TRL_SFTTrainer]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n* [[uses_heuristic::Heuristic:Sequence_Packing_Optimization]]\n\n",
      "domains": [
        "Fine Tuning",
        "LLMs",
        "Training"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "InstructGPT",
          "url": "https://arxiv.org/abs/2203.02155"
        },
        {
          "type": "Doc",
          "title": "TRL SFTTrainer",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        },
        {
          "type": "Blog",
          "title": "Fine-tuning LLMs",
          "url": "https://huggingface.co/blog/fine-tune-llama-2"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "TRL_SFTTrainer"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Sequence_Packing_Optimization"
        }
      ]
    },
    {
      "id": "Implementation/BitsAndBytes_4bit_Quantization",
      "page_title": "BitsAndBytes 4bit Quantization",
      "page_type": "Implementation",
      "overview": "Concrete tool for 4-bit model quantization using bitsandbytes NF4 format, enabling large model training on consumer GPUs.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|bitsandbytes GitHub|https://github.com/TimDettmers/bitsandbytes]]\n* [[source::Paper|QLoRA Paper|https://arxiv.org/abs/2305.14314]]\n* [[source::Doc|HuggingFace Quantization|https://huggingface.co/docs/transformers/quantization]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::Memory_Optimization]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConcrete tool for 4-bit model quantization using bitsandbytes NF4 format, enabling large model training on consumer GPUs.\n\n=== Description ===\nThe bitsandbytes library provides 4-bit quantization that reduces model memory by ~75% with minimal quality loss. The NF4 (NormalFloat4) format is specifically designed for normally-distributed neural network weights. Unsloth integrates seamlessly with bitsandbytes, automatically applying 4-bit quantization when `load_in_4bit=True`.\n\n=== Usage ===\nThis implementation is used automatically when loading models with Unsloth's `load_in_4bit=True` parameter. Understanding it helps diagnose issues and optimize memory further. Essential for running 7B+ models on 16GB or less VRAM.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\n# Via Unsloth (recommended)\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"...\",\n    load_in_4bit = True,  # Uses bitsandbytes NF4\n)\n\n# Direct bitsandbytes configuration\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit: bool = True,\n    bnb_4bit_quant_type: str = \"nf4\",         # or \"fp4\"\n    bnb_4bit_compute_dtype: torch.dtype = torch.float16,\n    bnb_4bit_use_double_quant: bool = True,   # Nested quantization\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * Model weights in FP16/BF16/FP32 format\n    * Configuration for quantization type and compute dtype\n* **Produces:**\n    * Quantized model with ~75% memory reduction\n    * Preserved forward/backward computation capability\n\n== Memory Comparison ==\n{| class=\"wikitable\"\n! Model Size !! FP16 VRAM !! 4-bit VRAM !! Reduction\n|-\n|| 7B || ~14GB || ~4GB || 71%\n|-\n|| 13B || ~26GB || ~7GB || 73%\n|-\n|| 70B || ~140GB || ~35GB || 75%\n|}\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Simple approach - Unsloth handles everything\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",  # Pre-quantized\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Or quantize on-the-fly\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"meta-llama/Meta-Llama-3-8B\",   # Full precision source\n    max_seq_length = 2048,\n    load_in_4bit = True,   # Quantize during loading\n)\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:AdamW_8bit_Optimizer_Usage]]\n\n",
      "domains": [
        "Quantization",
        "Memory Optimization",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "bitsandbytes GitHub",
          "url": "https://github.com/TimDettmers/bitsandbytes"
        },
        {
          "type": "Paper",
          "title": "QLoRA Paper",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Quantization",
          "url": "https://huggingface.co/docs/transformers/quantization"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "AdamW_8bit_Optimizer_Usage"
        }
      ]
    },
    {
      "id": "Implementation/HF_TrainingArguments",
      "page_title": "HF TrainingArguments",
      "page_type": "Implementation",
      "overview": "Base configuration class for Trainer hyperparameters provided by HuggingFace Transformers library.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|HuggingFace TrainingArguments|https://huggingface.co/docs/transformers/main_classes/trainer]]\n* [[source::Repo|Transformers GitHub|https://github.com/huggingface/transformers]]\n|-\n! Domains\n| [[domain::Configuration]], [[domain::Training]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nBase configuration class for Trainer hyperparameters provided by HuggingFace Transformers library.\n\n=== Description ===\n`TrainingArguments` is the foundational configuration class for all HuggingFace Trainer-based training. It defines hyperparameters for learning rate, batch size, optimization, logging, checkpointing, and hardware utilization. While `SFTConfig` is preferred for fine-tuning, understanding `TrainingArguments` is essential as it forms the base.\n\n=== Usage ===\nImport this class when using the base HuggingFace `Trainer` directly or when you need fine-grained control over training arguments. In Unsloth workflows, prefer `SFTConfig` which extends this class with SFT-specific options.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\nfrom transformers import TrainingArguments\n\nclass TrainingArguments:\n    def __init__(\n        self,\n        output_dir: str,\n        # Training hyperparameters\n        per_device_train_batch_size: int = 8,\n        per_device_eval_batch_size: int = 8,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 5e-5,\n        num_train_epochs: float = 3.0,\n        max_steps: int = -1,\n        # Optimizer\n        optim: str = \"adamw_torch\",\n        warmup_steps: int = 0,\n        warmup_ratio: float = 0.0,\n        lr_scheduler_type: str = \"linear\",\n        weight_decay: float = 0.0,\n        # Precision\n        fp16: bool = False,\n        bf16: bool = False,\n        # Logging & Saving\n        logging_steps: int = 500,\n        save_steps: int = 500,\n        save_total_limit: Optional[int] = None,\n        # Evaluation\n        evaluation_strategy: str = \"no\",\n        eval_steps: Optional[int] = None,\n        # Hardware\n        dataloader_num_workers: int = 0,\n        # Reproducibility\n        seed: int = 42,\n        **kwargs\n    ):\n        ...\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * Hyperparameter values for training configuration\n* **Produces:**\n    * Configured `TrainingArguments` instance\n\n== Key Parameters ==\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n|| output_dir || str || Directory for checkpoints and logs\n|-\n|| learning_rate || float || Initial learning rate (2e-4 for LoRA)\n|-\n|| per_device_train_batch_size || int || Batch size per GPU\n|-\n|| gradient_accumulation_steps || int || Steps before weight update\n|-\n|| optim || str || Optimizer (\"adamw_8bit\" recommended)\n|-\n|| max_steps || int || Total training steps (-1 for epoch-based)\n|-\n|| warmup_steps || int || LR warmup steps\n|}\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n* [[uses_heuristic::Heuristic:Batch_Size_Optimization]]\n* [[uses_heuristic::Heuristic:Warmup_Steps_Heuristic]]\n* [[uses_heuristic::Heuristic:Gradient_Accumulation_Strategy]]\n\n",
      "domains": [
        "Configuration",
        "Training",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "HuggingFace TrainingArguments",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        },
        {
          "type": "Repo",
          "title": "Transformers GitHub",
          "url": "https://github.com/huggingface/transformers"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Batch_Size_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Warmup_Steps_Heuristic"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Gradient_Accumulation_Strategy"
        }
      ]
    },
    {
      "id": "Implementation/TRL_DPOTrainer",
      "page_title": "TRL DPOTrainer",
      "page_type": "Implementation",
      "overview": "Concrete tool for Direct Preference Optimization (DPO) training provided by HuggingFace TRL library, compatible with Unsloth.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|TRL GitHub|https://github.com/huggingface/trl]]\n* [[source::Doc|TRL DPOTrainer|https://huggingface.co/docs/trl/dpo_trainer]]\n* [[source::Paper|DPO Paper|https://arxiv.org/abs/2305.18290]]\n|-\n! Domains\n| [[domain::RLHF]], [[domain::Alignment]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConcrete tool for Direct Preference Optimization (DPO) training provided by HuggingFace TRL library, compatible with Unsloth.\n\n=== Description ===\n`DPOTrainer` implements the DPO algorithm for aligning language models with human preferences without explicit reward modeling. It takes pairs of chosen/rejected responses and trains the model to prefer chosen responses. Works seamlessly with Unsloth-patched models for 2x faster alignment training.\n\n=== Usage ===\nImport this class when aligning a language model using preference data (chosen vs rejected pairs). Use after initial SFT training to further improve response quality. Requires a dataset with `prompt`, `chosen`, and `rejected` columns.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\nfrom trl import DPOTrainer, DPOConfig\n\nclass DPOTrainer(Trainer):\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        ref_model: Optional[PreTrainedModel] = None,\n        args: Optional[DPOConfig] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Dataset] = None,\n        tokenizer: Optional[PreTrainedTokenizer] = None,\n        beta: float = 0.1,\n        loss_type: str = \"sigmoid\",\n        **kwargs\n    ):\n        ...\n    \n    def train(self) -> TrainOutput:\n        \"\"\"Run DPO training loop.\"\"\"\n        ...\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * `model`: Unsloth-patched model (usually after SFT)\n    * `ref_model`: Reference model (optional, uses initial model if None)\n    * `train_dataset`: Dataset with `prompt`, `chosen`, `rejected` columns\n    * `beta`: Temperature parameter for DPO loss\n* **Produces:**\n    * Aligned model that prefers chosen responses\n    * Training logs and metrics\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\nfrom trl import DPOTrainer, DPOConfig\nfrom datasets import load_dataset\n\n# Load preference dataset\ndataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n\n# Configure DPO training\nargs = DPOConfig(\n    output_dir = \"dpo_outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    learning_rate = 5e-5,\n    max_steps = 100,\n    beta = 0.1,\n    optim = \"adamw_8bit\",\n)\n\n# Create trainer\ntrainer = DPOTrainer(\n    model = model,\n    ref_model = None,  # Uses model's initial state\n    args = args,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n* [[requires_env::Environment:Unsloth_Docker_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n* [[uses_heuristic::Heuristic:Gradient_Accumulation_Strategy]]\n\n",
      "domains": [
        "RLHF",
        "Alignment",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "TRL GitHub",
          "url": "https://github.com/huggingface/trl"
        },
        {
          "type": "Doc",
          "title": "TRL DPOTrainer",
          "url": "https://huggingface.co/docs/trl/dpo_trainer"
        },
        {
          "type": "Paper",
          "title": "DPO Paper",
          "url": "https://arxiv.org/abs/2305.18290"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_Docker_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Gradient_Accumulation_Strategy"
        }
      ]
    },
    {
      "id": "Implementation/TRL_GRPOTrainer",
      "page_title": "TRL GRPOTrainer",
      "page_type": "Implementation",
      "overview": "Concrete tool for Group Relative Policy Optimization (GRPO) training provided by HuggingFace TRL library, optimized for Unsloth.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|TRL GitHub|https://github.com/huggingface/trl]]\n* [[source::Paper|DeepSeekMath GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Doc|Unsloth GRPO Guide|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::RLHF]], [[domain::Reasoning]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConcrete tool for Group Relative Policy Optimization (GRPO) training provided by HuggingFace TRL library, optimized for Unsloth.\n\n=== Description ===\n`GRPOTrainer` implements the GRPO algorithm, a reinforcement learning method that improves reasoning capabilities by training on groups of responses with relative rewards. Popularized by DeepSeek for math reasoning, it generates multiple responses per prompt and uses group-relative rewards for training. Unsloth provides significant speedups for GRPO workflows.\n\n=== Usage ===\nImport this class when training models for improved reasoning (math, code, logic). Use with a reward function or model that can score response quality. Particularly effective for enhancing chain-of-thought and step-by-step reasoning capabilities.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\nfrom trl import GRPOTrainer, GRPOConfig\n\nclass GRPOTrainer(Trainer):\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        args: Optional[GRPOConfig] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Dataset] = None,\n        tokenizer: Optional[PreTrainedTokenizer] = None,\n        reward_model: Optional[PreTrainedModel] = None,\n        reward_fn: Optional[Callable] = None,\n        num_generations: int = 4,\n        **kwargs\n    ):\n        ...\n    \n    def train(self) -> TrainOutput:\n        \"\"\"Run GRPO training loop.\"\"\"\n        ...\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * `model`: Unsloth-patched model with LoRA adapters\n    * `train_dataset`: Dataset with prompts (questions/problems)\n    * `reward_model` or `reward_fn`: Scoring mechanism for responses\n    * `num_generations`: Number of responses to generate per prompt\n* **Produces:**\n    * Model with improved reasoning capabilities\n    * Training logs with reward metrics\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\nfrom trl import GRPOTrainer, GRPOConfig\n\n# Define reward function (example for math)\ndef reward_fn(responses, ground_truths):\n    rewards = []\n    for resp, gt in zip(responses, ground_truths):\n        # Check if answer matches\n        rewards.append(1.0 if gt in resp else 0.0)\n    return rewards\n\n# Configure GRPO training\nargs = GRPOConfig(\n    output_dir = \"grpo_outputs\",\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 8,\n    learning_rate = 1e-5,\n    max_steps = 500,\n    num_generations = 4,  # Generate 4 responses per prompt\n    optim = \"adamw_8bit\",\n)\n\n# Create trainer\ntrainer = GRPOTrainer(\n    model = model,\n    args = args,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n    reward_fn = reward_fn,\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n* [[requires_env::Environment:Unsloth_Docker_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n* [[uses_heuristic::Heuristic:Batch_Size_Optimization]]\n\n",
      "domains": [
        "RLHF",
        "Reasoning",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "TRL GitHub",
          "url": "https://github.com/huggingface/trl"
        },
        {
          "type": "Paper",
          "title": "DeepSeekMath GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Doc",
          "title": "Unsloth GRPO Guide",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_Docker_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Batch_Size_Optimization"
        }
      ]
    },
    {
      "id": "Implementation/TRL_SFTConfig",
      "page_title": "TRL SFTConfig",
      "page_type": "Implementation",
      "overview": "Configuration class for SFTTrainer hyperparameters provided by HuggingFace TRL library.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|TRL GitHub|https://github.com/huggingface/trl]]\n* [[source::Doc|TRL SFTConfig|https://huggingface.co/docs/trl/sft_trainer]]\n* [[source::Doc|HuggingFace TrainingArguments|https://huggingface.co/docs/transformers/main_classes/trainer]]\n|-\n! Domains\n| [[domain::Configuration]], [[domain::Training]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConfiguration class for SFTTrainer hyperparameters provided by HuggingFace TRL library.\n\n=== Description ===\n`SFTConfig` extends `TrainingArguments` with SFT-specific options like `packing` and `max_seq_length`. It provides a clean interface for setting all training hyperparameters including learning rate, batch size, optimization settings, and logging configuration. Fully compatible with Unsloth workflows.\n\n=== Usage ===\nImport this class to configure training hyperparameters for `SFTTrainer`. Preferred over raw `TrainingArguments` when using TRL for fine-tuning. Set parameters based on heuristics for your specific task and hardware.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\nclass SFTConfig(TrainingArguments):\n    def __init__(\n        self,\n        output_dir: str,\n        # Training hyperparameters\n        per_device_train_batch_size: int = 8,\n        per_device_eval_batch_size: int = 8,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 5e-5,\n        num_train_epochs: float = 3.0,\n        max_steps: int = -1,\n        # Optimizer settings\n        optim: str = \"adamw_torch\",\n        warmup_steps: int = 0,\n        warmup_ratio: float = 0.0,\n        lr_scheduler_type: str = \"linear\",\n        weight_decay: float = 0.0,\n        # SFT-specific\n        max_seq_length: Optional[int] = 1024,\n        packing: bool = False,\n        dataset_text_field: Optional[str] = None,\n        # Logging\n        logging_steps: int = 500,\n        save_steps: int = 500,\n        eval_steps: int = 500,\n        # Others\n        fp16: bool = False,\n        bf16: bool = False,\n        seed: int = 42,\n        **kwargs\n    ):\n        ...\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * Hyperparameter values (learning rate, batch size, etc.)\n    * Path strings (output_dir, logging_dir)\n* **Produces:**\n    * Configured `SFTConfig` instance for use with `SFTTrainer`\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\n# Recommended configuration for Unsloth QLoRA\nargs = SFTConfig(\n    output_dir = \"outputs\",\n    \n    # Batch settings\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    \n    # Learning rate\n    learning_rate = 2e-4,\n    lr_scheduler_type = \"linear\",\n    warmup_steps = 10,\n    \n    # Training duration\n    max_steps = 60,  # or num_train_epochs = 1\n    \n    # Optimizer\n    optim = \"adamw_8bit\",\n    weight_decay = 0.01,\n    \n    # SFT-specific\n    max_seq_length = 2048,\n    packing = False,\n    \n    # Logging\n    logging_steps = 1,\n    save_steps = 25,\n    \n    # Reproducibility\n    seed = 3407,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n* [[uses_heuristic::Heuristic:Batch_Size_Optimization]]\n* [[uses_heuristic::Heuristic:Warmup_Steps_Heuristic]]\n* [[uses_heuristic::Heuristic:AdamW_8bit_Optimizer_Usage]]\n* [[uses_heuristic::Heuristic:Gradient_Accumulation_Strategy]]\n\n",
      "domains": [
        "Configuration",
        "Training",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "TRL GitHub",
          "url": "https://github.com/huggingface/trl"
        },
        {
          "type": "Doc",
          "title": "TRL SFTConfig",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        },
        {
          "type": "Doc",
          "title": "HuggingFace TrainingArguments",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Batch_Size_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Warmup_Steps_Heuristic"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "AdamW_8bit_Optimizer_Usage"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Gradient_Accumulation_Strategy"
        }
      ]
    },
    {
      "id": "Implementation/TRL_SFTTrainer",
      "page_title": "TRL SFTTrainer",
      "page_type": "Implementation",
      "overview": "Concrete tool for supervised fine-tuning (SFT) of language models provided by HuggingFace TRL library, fully compatible with Unsloth.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|TRL GitHub|https://github.com/huggingface/trl]]\n* [[source::Doc|TRL SFTTrainer|https://huggingface.co/docs/trl/sft_trainer]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Training]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConcrete tool for supervised fine-tuning (SFT) of language models provided by HuggingFace TRL library, fully compatible with Unsloth.\n\n=== Description ===\n`SFTTrainer` is the standard trainer for instruction tuning and supervised fine-tuning tasks. It extends HuggingFace's Trainer with features specific to language model fine-tuning: chat template formatting, packing, and dataset processing. When used with Unsloth-patched models, it achieves 2x faster training with full compatibility.\n\n=== Usage ===\nImport this class when performing supervised fine-tuning on instruction datasets, chat datasets, or any text-to-text task. Use after loading model with `FastLanguageModel` or `FastModel`. Standard choice for QLoRA fine-tuning workflows.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTTrainer\n\nclass SFTTrainer(Trainer):\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        args: Optional[SFTConfig] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Dataset] = None,\n        tokenizer: Optional[PreTrainedTokenizer] = None,\n        data_collator: Optional[DataCollator] = None,\n        packing: bool = False,\n        formatting_func: Optional[Callable] = None,\n        max_seq_length: Optional[int] = None,\n        dataset_text_field: Optional[str] = None,\n        **kwargs\n    ):\n        ...\n    \n    def train(self, resume_from_checkpoint: Optional[str] = None) -> TrainOutput:\n        \"\"\"Run training loop.\"\"\"\n        ...\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * `model`: Unsloth-patched `PreTrainedModel` with LoRA adapters\n    * `train_dataset`: HuggingFace `Dataset` with text/instruction data\n    * `tokenizer`: Matching tokenizer from model loading\n    * `args`: `SFTConfig` with training hyperparameters\n* **Produces:**\n    * Training logs and metrics\n    * Model checkpoints in `output_dir`\n    * Final trained model state\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\n# Load dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n\n# Configure training\nargs = SFTConfig(\n    output_dir = \"outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    warmup_steps = 10,\n    max_steps = 60,\n    learning_rate = 2e-4,\n    logging_steps = 1,\n    optim = \"adamw_8bit\",\n    max_seq_length = 2048,\n)\n\n# Create trainer\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    args = args,\n)\n\n# Train\ntrainer.train()\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n* [[requires_env::Environment:Unsloth_Colab_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Gradient_Accumulation_Strategy]]\n* [[uses_heuristic::Heuristic:Sequence_Packing_Optimization]]\n* [[uses_heuristic::Heuristic:Learning_Rate_Tuning]]\n\n",
      "domains": [
        "Fine Tuning",
        "Training",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "TRL GitHub",
          "url": "https://github.com/huggingface/trl"
        },
        {
          "type": "Doc",
          "title": "TRL SFTTrainer",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_Colab_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Gradient_Accumulation_Strategy"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Sequence_Packing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Learning_Rate_Tuning"
        }
      ]
    },
    {
      "id": "Implementation/Unsloth_FastLanguageModel",
      "page_title": "Unsloth FastLanguageModel",
      "page_type": "Implementation",
      "overview": "Concrete tool for loading and patching language models with Unsloth's 2x faster optimizations provided by the Unsloth library.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Model_Loading]], [[domain::Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConcrete tool for loading and patching language models with Unsloth's 2x faster optimizations provided by the Unsloth library.\n\n=== Description ===\n`FastLanguageModel` is the primary entry point for using Unsloth. It provides a unified API to load any supported language model (Llama, Mistral, Qwen, Gemma, etc.) with automatic patching for 2x faster training and 70% less VRAM. The class handles model loading, quantization (4-bit, 8-bit, 16-bit), and optimization kernel injection transparently.\n\n=== Usage ===\nImport this class when you need to load a language model for fine-tuning with Unsloth optimizations. Use it as the first step in any QLoRA, SFT, DPO, or GRPO workflow. Preferred over direct HuggingFace model loading when speed and memory efficiency are priorities.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nclass FastLanguageModel:\n    @staticmethod\n    def from_pretrained(\n        model_name: str,\n        max_seq_length: int = 2048,\n        dtype: Optional[torch.dtype] = None,\n        load_in_4bit: bool = True,\n        load_in_8bit: bool = False,\n        full_finetuning: bool = False,\n        token: Optional[str] = None,\n        device_map: str = \"auto\",\n        trust_remote_code: bool = False,\n        **kwargs\n    ) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n        \"\"\"Load model with Unsloth optimizations.\"\"\"\n        ...\n    \n    @staticmethod\n    def get_peft_model(\n        model: PreTrainedModel,\n        r: int = 16,\n        target_modules: List[str] = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_alpha: int = 16,\n        lora_dropout: float = 0,\n        bias: str = \"none\",\n        use_gradient_checkpointing: Union[bool, str] = \"unsloth\",\n        random_state: int = 3407,\n        max_seq_length: int = 2048,\n        use_rslora: bool = False,\n        **kwargs\n    ) -> PeftModel:\n        \"\"\"Add LoRA adapters with Unsloth optimizations.\"\"\"\n        ...\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * `model_name`: String path to HuggingFace model or local path\n    * `max_seq_length`: Integer, maximum context length\n    * `token`: Optional HuggingFace token for gated models\n* **Produces:**\n    * `model`: Patched `PreTrainedModel` with Unsloth optimizations\n    * `tokenizer`: Corresponding `PreTrainedTokenizer`\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load model with 4-bit quantization\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    # token = \"hf_...\",  # For gated models\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    use_gradient_checkpointing = \"unsloth\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n* [[requires_env::Environment:Unsloth_Colab_Environment]]\n* [[requires_env::Environment:Unsloth_Docker_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Unsloth_Gradient_Checkpointing_Optimization]]\n* [[uses_heuristic::Heuristic:Memory_Efficient_Attention]]\n\n",
      "domains": [
        "LLMs",
        "Model Loading",
        "Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_Colab_Environment"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_Docker_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unsloth_Gradient_Checkpointing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Memory_Efficient_Attention"
        }
      ]
    },
    {
      "id": "Implementation/Unsloth_FastModel",
      "page_title": "Unsloth FastModel",
      "page_type": "Implementation",
      "overview": "Unified model loader for all Unsloth-supported models including LLMs, Vision-Language models, and Text-to-Speech models.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Model_Loading]], [[domain::Multi_Modal]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nUnified model loader for all Unsloth-supported models including LLMs, Vision-Language models, and Text-to-Speech models.\n\n=== Description ===\n`FastModel` is the newer, unified API that supersedes `FastLanguageModel` for general use. It automatically detects the model type and applies appropriate optimizations. Supports language models, vision-language models (Qwen2-VL, LLaVA), and text-to-speech models. Provides the same 2x speedup and 70% VRAM reduction as `FastLanguageModel`.\n\n=== Usage ===\nImport this class as the default choice for loading any model type with Unsloth. Use `FastModel` for new projects and when working with non-language models (vision, TTS). Falls back gracefully to appropriate model-specific handling.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastModel\n\nclass FastModel:\n    @staticmethod\n    def from_pretrained(\n        model_name: str,\n        max_seq_length: int = 2048,\n        dtype: Optional[torch.dtype] = None,\n        load_in_4bit: bool = True,\n        load_in_8bit: bool = False,\n        load_in_16bit: bool = False,\n        full_finetuning: bool = False,\n        token: Optional[str] = None,\n        **kwargs\n    ) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n        \"\"\"Load any supported model with Unsloth optimizations.\"\"\"\n        ...\n    \n    @staticmethod\n    def get_peft_model(\n        model: PreTrainedModel,\n        r: int = 16,\n        target_modules: List[str] = [...],\n        lora_alpha: int = 16,\n        lora_dropout: float = 0,\n        bias: str = \"none\",\n        use_gradient_checkpointing: Union[bool, str] = \"unsloth\",\n        **kwargs\n    ) -> PeftModel:\n        \"\"\"Add LoRA adapters with Unsloth optimizations.\"\"\"\n        ...\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * `model_name`: String path to HuggingFace model (LLM, VLM, or TTS)\n    * `max_seq_length`: Integer, maximum sequence length\n    * `load_in_Xbit`: Boolean flags for quantization level\n* **Produces:**\n    * `model`: Optimized model instance (type depends on input model)\n    * `tokenizer`: Corresponding tokenizer/processor\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastModel\n\n# Load any supported model (LLM, Vision, TTS)\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/Qwen2.5-7B-bnb-4bit\",  # or vision/TTS models\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Add LoRA adapters\nmodel = FastModel.get_peft_model(\n    model,\n    r = 16,\n    lora_alpha = 16,\n    use_gradient_checkpointing = \"unsloth\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n* [[requires_env::Environment:Unsloth_Colab_Environment]]\n* [[requires_env::Environment:Unsloth_Docker_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Unsloth_Gradient_Checkpointing_Optimization]]\n* [[uses_heuristic::Heuristic:Memory_Efficient_Attention]]\n\n",
      "domains": [
        "LLMs",
        "Model Loading",
        "Multi Modal"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_Colab_Environment"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_Docker_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unsloth_Gradient_Checkpointing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Memory_Efficient_Attention"
        }
      ]
    },
    {
      "id": "Implementation/Unsloth_get_peft_model",
      "page_title": "Unsloth get peft model",
      "page_type": "Implementation",
      "overview": "Concrete tool for adding LoRA adapters to models with Unsloth's memory-optimized implementation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Paper|LoRA Paper|https://arxiv.org/abs/2106.09685]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::PEFT]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConcrete tool for adding LoRA adapters to models with Unsloth's memory-optimized implementation.\n\n=== Description ===\n`get_peft_model` is the function (accessed via `FastLanguageModel.get_peft_model` or `FastModel.get_peft_model`) that injects LoRA adapters into a loaded model. It applies Unsloth's custom optimizations including the \"unsloth\" gradient checkpointing mode that provides 30% additional VRAM savings. This is where LoRA hyperparameters (rank, alpha, target modules) are configured.\n\n=== Usage ===\nCall this function immediately after loading a model with `FastLanguageModel.from_pretrained` or `FastModel.from_pretrained`. Required step before training - it transforms the frozen model into a parameter-efficient trainable model.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\n# Accessed as a static method\nFastLanguageModel.get_peft_model(\n    model: PreTrainedModel,\n    r: int = 16,\n    target_modules: List[str] = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    lora_alpha: int = 16,\n    lora_dropout: float = 0,\n    bias: str = \"none\",\n    use_gradient_checkpointing: Union[bool, str] = \"unsloth\",\n    random_state: int = 3407,\n    max_seq_length: int = 2048,\n    use_rslora: bool = False,\n    loftq_config: Optional[dict] = None,\n    **kwargs\n) -> PeftModel\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * `model`: Pre-loaded model from `from_pretrained`\n    * `r`: LoRA rank (typically 8, 16, or 32)\n    * `target_modules`: List of layer names to apply LoRA\n    * `lora_alpha`: LoRA scaling factor\n    * `use_gradient_checkpointing`: Memory optimization setting\n* **Produces:**\n    * `PeftModel` with LoRA adapters and Unsloth optimizations\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load model first\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Add LoRA adapters with Unsloth optimizations\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,                          # LoRA rank\n    target_modules = [               # Layers to adapt\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_alpha = 16,                 # Scaling factor\n    lora_dropout = 0,                # 0 is optimized\n    bias = \"none\",                   # \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\",  # 30% extra VRAM savings\n    random_state = 3407,\n)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: 41,943,040 || all params: 8,030,261,248 || trainable%: 0.5222\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n* [[requires_env::Environment:Unsloth_Colab_Environment]]\n\n=== Tips and Tricks ===\n* [[uses_heuristic::Heuristic:Unsloth_Gradient_Checkpointing_Optimization]]\n* [[uses_heuristic::Heuristic:QLoRA_Target_Modules_Selection]]\n* [[uses_heuristic::Heuristic:LoRA_Rank_Selection]]\n\n",
      "domains": [
        "Fine Tuning",
        "PEFT",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Paper",
          "title": "LoRA Paper",
          "url": "https://arxiv.org/abs/2106.09685"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_Colab_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unsloth_Gradient_Checkpointing_Optimization"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "QLoRA_Target_Modules_Selection"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "LoRA_Rank_Selection"
        }
      ]
    },
    {
      "id": "Implementation/Unsloth_save_pretrained",
      "page_title": "Unsloth save pretrained",
      "page_type": "Implementation",
      "overview": "Concrete tool for saving fine-tuned LoRA adapters or merged models in HuggingFace format.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Doc|HuggingFace Model Sharing|https://huggingface.co/docs/hub/models-uploading]]\n|-\n! Domains\n| [[domain::Model_Export]], [[domain::Deployment]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConcrete tool for saving fine-tuned LoRA adapters or merged models in HuggingFace format.\n\n=== Description ===\nThe `save_pretrained` method saves the fine-tuned model for later use or sharing. It can save just the LoRA adapters (small, ~100MB) or merge adapters into the base model and save the full weights. Also supports pushing directly to HuggingFace Hub.\n\n=== Usage ===\nCall this method after training to persist the model. Use adapter-only saving for efficient storage and sharing; use merged saving when deploying to inference systems that don't support PEFT.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\n# Save LoRA adapters only\nmodel.save_pretrained(\n    save_directory: str,\n    save_method: str = \"lora\",  # \"lora\" or \"merged_16bit\" or \"merged_4bit\"\n)\n\n# Push to HuggingFace Hub\nmodel.push_to_hub(\n    repo_id: str,\n    token: str,\n    save_method: str = \"lora\",\n)\n\n# Save merged model (full weights)\nmodel.save_pretrained_merged(\n    save_directory: str,\n    tokenizer: PreTrainedTokenizer,\n    save_method: str = \"merged_16bit\",  # or \"merged_4bit\"\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * Trained model with LoRA adapters\n    * Save directory path or HuggingFace repo ID\n    * Save method selection\n* **Produces:**\n    * **lora**: LoRA adapter weights (~50-200MB)\n    * **merged_16bit**: Full model weights in FP16 (~14GB for 7B)\n    * **merged_4bit**: Full model weights in 4-bit (~4GB for 7B)\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\n# After training...\n\n# Option 1: Save LoRA adapters only (recommended for storage)\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n\n# Option 2: Push adapters to HuggingFace Hub\nmodel.push_to_hub(\n    \"your-username/llama-3-lora-finetuned\",\n    token = \"hf_...\",\n)\n\n# Option 3: Save merged 16-bit model\nmodel.save_pretrained_merged(\n    \"merged_model\",\n    tokenizer,\n    save_method = \"merged_16bit\",\n)\n\n# Option 4: Push merged model to Hub\nmodel.push_to_hub_merged(\n    \"your-username/llama-3-merged-finetuned\",\n    tokenizer,\n    save_method = \"merged_16bit\",\n    token = \"hf_...\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n\n=== Tips and Tricks ===\n(No specific heuristics - straightforward operation)\n\n",
      "domains": [
        "Model Export",
        "Deployment",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Model Sharing",
          "url": "https://huggingface.co/docs/hub/models-uploading"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unsloth_save_to_GGUF",
      "page_title": "Unsloth save to GGUF",
      "page_type": "Implementation",
      "overview": "Concrete tool for exporting fine-tuned models to GGUF format for use with llama.cpp, Ollama, and other local inference engines.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Repo|llama.cpp|https://github.com/ggerganov/llama.cpp]]\n* [[source::Doc|Unsloth GGUF Guide|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::Model_Export]], [[domain::Deployment]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nConcrete tool for exporting fine-tuned models to GGUF format for use with llama.cpp, Ollama, and other local inference engines.\n\n=== Description ===\nThe `save_pretrained_gguf` method exports models to GGUF format, the standard for local LLM inference. GGUF files work with llama.cpp, Ollama, LM Studio, and other popular inference frameworks. Unsloth handles the conversion automatically, including merging LoRA adapters and applying quantization.\n\n=== Usage ===\nCall this method when you need to deploy your fine-tuned model for local inference. Choose quantization level based on target hardware and quality requirements. Essential for deploying to edge devices or systems without GPU.\n\n== Code Signature ==\n<syntaxhighlight lang=\"python\">\n# Save to GGUF\nmodel.save_pretrained_gguf(\n    save_directory: str,\n    tokenizer: PreTrainedTokenizer,\n    quantization_method: str = \"q4_k_m\",  # Quantization type\n)\n\n# Push GGUF to HuggingFace Hub\nmodel.push_to_hub_gguf(\n    repo_id: str,\n    tokenizer: PreTrainedTokenizer,\n    quantization_method: str = \"q4_k_m\",\n    token: str = None,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n* **Consumes:**\n    * Trained model with LoRA adapters\n    * Tokenizer for embedding table\n    * Quantization method selection\n* **Produces:**\n    * `.gguf` file for local inference\n\n== Quantization Methods ==\n{| class=\"wikitable\"\n! Method !! Size (7B) !! Quality !! Speed !! Use Case\n|-\n|| q4_k_m || ~4GB || Good || Fast || Recommended default\n|-\n|| q5_k_m || ~5GB || Better || Fast || Better quality, more RAM\n|-\n|| q8_0 || ~8GB || Best || Medium || Quality-focused\n|-\n|| f16 || ~14GB || Perfect || Slow || Full precision reference\n|-\n|| q2_k || ~3GB || Acceptable || Fastest || Extreme compression\n|}\n\n== Example Usage ==\n<syntaxhighlight lang=\"python\">\n# After training...\n\n# Option 1: Save GGUF locally\nmodel.save_pretrained_gguf(\n    \"model_gguf\",\n    tokenizer,\n    quantization_method = \"q4_k_m\",  # Good balance\n)\n\n# Option 2: Multiple quantizations\nfor quant in [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]:\n    model.save_pretrained_gguf(f\"model_{quant}\", tokenizer, quant)\n\n# Option 3: Push to HuggingFace Hub\nmodel.push_to_hub_gguf(\n    \"your-username/llama-3-finetuned-GGUF\",\n    tokenizer,\n    quantization_method = \"q4_k_m\",\n    token = \"hf_...\",\n)\n\n# Use with Ollama\n# 1. Create Modelfile:\n#    FROM ./model_gguf/unsloth.Q4_K_M.gguf\n# 2. Run: ollama create mymodel -f Modelfile\n# 3. Run: ollama run mymodel\n</syntaxhighlight>\n\n== Related Pages ==\n=== Context & Requirements ===\n* [[requires_env::Environment:Unsloth_CUDA_Environment]]\n\n=== Tips and Tricks ===\n(No specific heuristics - quantization choice depends on deployment target)\n\n",
      "domains": [
        "Model Export",
        "Deployment",
        "Inference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Repo",
          "title": "llama.cpp",
          "url": "https://github.com/ggerganov/llama.cpp"
        },
        {
          "type": "Doc",
          "title": "Unsloth GGUF Guide",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unsloth_CUDA_Environment"
        }
      ]
    },
    {
      "id": "Environment/Unsloth_CUDA_Environment",
      "page_title": "Unsloth CUDA Environment",
      "page_type": "Environment",
      "overview": "Linux environment with CUDA 11.8+/12.x, Python 3.10+, and PyTorch 2.0+ optimized for Unsloth's 2x faster LLM fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Doc|NVIDIA CUDA Toolkit|https://developer.nvidia.com/cuda-toolkit]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::Deep_Learning]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nLinux environment with CUDA 11.8+/12.x, Python 3.10+, and PyTorch 2.0+ optimized for Unsloth's 2x faster LLM fine-tuning.\n\n=== Description ===\nThis environment provides the GPU-accelerated context required for running Unsloth's optimized fine-tuning workflows. It is built around the NVIDIA CUDA toolkit and includes all necessary dependencies for parameter-efficient fine-tuning (PEFT) of large language models. The environment supports both Ampere (A100, RTX 30xx) and newer architectures (H100, RTX 40xx), with specific optimizations for memory-efficient training through gradient checkpointing and 4-bit quantization.\n\n=== Usage ===\nUse this environment for any **Model Fine-Tuning**, **Reinforcement Learning from Human Feedback (RLHF)**, or **Inference** workflow using Unsloth. It is the mandatory prerequisite for running `FastLanguageModel`, `FastModel`, and all Unsloth-optimized training implementations. Required when you need the 2x speedup and 70% VRAM reduction that Unsloth provides.\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n|| OS || Linux (Ubuntu 20.04/22.04 LTS) || WSL2 on Windows also supported\n|-\n|| Hardware || NVIDIA GPU || Minimum 8GB VRAM (RTX 3060+, A10, T4); 24GB+ recommended for 7B+ models\n|-\n|| CUDA || 11.8 or 12.x || Must match PyTorch CUDA version\n|-\n|| Disk || 50GB SSD || For model weights and checkpoints\n|-\n|| RAM || 16GB+ || 32GB recommended for larger models\n|}\n\n== Dependencies ==\n=== System Packages ===\n* `cuda-toolkit` >= 11.8\n* `cudnn` >= 8.6\n* `git`\n* `git-lfs`\n\n=== Python Packages ===\n* `python` >= 3.10\n* `torch` >= 2.0.0\n* `unsloth` (latest)\n* `transformers` >= 4.36.0\n* `trl` >= 0.7.0\n* `peft` >= 0.7.0\n* `bitsandbytes` >= 0.41.0\n* `accelerate` >= 0.25.0\n* `datasets`\n* `xformers` (optional, for memory efficiency)\n\n== Credentials ==\nThe following environment variables may be required:\n* `HF_TOKEN`: HuggingFace API token for gated models (Llama, Mistral, etc.)\n* `WANDB_API_KEY`: Weights & Biases API key for experiment tracking (optional)\n\n== Related Pages ==\n=== Required By ===\n* [[required_by::Implementation:Unsloth_FastLanguageModel]]\n* [[required_by::Implementation:Unsloth_FastModel]]\n* [[required_by::Implementation:TRL_SFTTrainer]]\n* [[required_by::Implementation:TRL_DPOTrainer]]\n* [[required_by::Implementation:TRL_GRPOTrainer]]\n* [[required_by::Implementation:Unsloth_get_peft_model]]\n* [[required_by::Implementation:BitsAndBytes_4bit_Quantization]]\n* [[required_by::Implementation:Unsloth_save_pretrained]]\n* [[required_by::Implementation:Unsloth_save_to_GGUF]]\n\n",
      "domains": [
        "Infrastructure",
        "Deep Learning",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Doc",
          "title": "NVIDIA CUDA Toolkit",
          "url": "https://developer.nvidia.com/cuda-toolkit"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastLanguageModel"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastModel"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "TRL_SFTTrainer"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "TRL_DPOTrainer"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "TRL_GRPOTrainer"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_get_peft_model"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "BitsAndBytes_4bit_Quantization"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_save_pretrained"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_save_to_GGUF"
        }
      ]
    },
    {
      "id": "Environment/Unsloth_Colab_Environment",
      "page_title": "Unsloth Colab Environment",
      "page_type": "Environment",
      "overview": "Google Colab notebook environment with T4/A100 GPU runtime pre-configured for Unsloth fine-tuning workflows.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Google Colab|https://colab.research.google.com/]]\n* [[source::Blog|Unsloth Colab Notebooks|https://docs.unsloth.ai/get-started/unsloth-notebooks]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::Cloud]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nGoogle Colab notebook environment with T4/A100 GPU runtime pre-configured for Unsloth fine-tuning workflows.\n\n=== Description ===\nThis environment leverages Google Colab's free and Pro GPU runtimes for running Unsloth fine-tuning workflows. Colab provides accessible GPU compute (T4 free tier, A100 for Pro users) without local hardware requirements. Unsloth's memory optimizations are particularly valuable here, enabling fine-tuning of 7B+ models on the free T4 GPU (16GB VRAM) that would otherwise be impossible with standard training methods.\n\n=== Usage ===\nUse this environment when you want to **quickly prototype** fine-tuning workflows, **lack local GPU hardware**, or need to **share reproducible notebooks**. Ideal for educational purposes, experimentation, and small-scale fine-tuning jobs. The free T4 tier can fine-tune models up to 7B parameters with Unsloth's optimizations.\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n|| Runtime || GPU Runtime || Must select GPU in Runtime > Change runtime type\n|-\n|| GPU Type || T4 (free) / A100 (Pro) || T4: 16GB VRAM, A100: 40GB VRAM\n|-\n|| RAM || 12-52GB || Depends on Colab tier (free/Pro/Pro+)\n|-\n|| Disk || ~100GB || Colab provides temporary storage\n|-\n|| Session || Up to 12 hours || Free tier disconnects after idle/usage limits\n|}\n\n== Dependencies ==\n=== Installation Commands ===\nRun these cells at the start of your Colab notebook:\n\n<syntaxhighlight lang=\"python\">\n# Install Unsloth (run first)\n%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth for bleeding edge updates\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n</syntaxhighlight>\n\n=== Python Packages (Auto-installed) ===\n* `unsloth` (latest)\n* `torch` (Colab pre-installed)\n* `transformers`\n* `trl`\n* `peft`\n* `bitsandbytes`\n* `accelerate`\n* `datasets`\n\n== Credentials ==\nThe following may be required for gated models:\n* `HF_TOKEN`: Set via `huggingface_hub.login()` or Colab Secrets\n* `WANDB_API_KEY`: For experiment tracking (optional)\n\n== Related Pages ==\n=== Required By ===\n* [[required_by::Implementation:Unsloth_FastLanguageModel]]\n* [[required_by::Implementation:Unsloth_FastModel]]\n* [[required_by::Implementation:TRL_SFTTrainer]]\n* [[required_by::Implementation:Unsloth_get_peft_model]]\n\n",
      "domains": [
        "Infrastructure",
        "Cloud",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Google Colab",
          "url": "https://colab.research.google.com/"
        },
        {
          "type": "Blog",
          "title": "Unsloth Colab Notebooks",
          "url": "https://docs.unsloth.ai/get-started/unsloth-notebooks"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastLanguageModel"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastModel"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "TRL_SFTTrainer"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_get_peft_model"
        }
      ]
    },
    {
      "id": "Environment/Unsloth_Docker_Environment",
      "page_title": "Unsloth Docker Environment",
      "page_type": "Environment",
      "overview": "Containerized Unsloth environment using Docker with NVIDIA GPU support for reproducible fine-tuning deployments.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Docker Guide|https://docs.unsloth.ai/]]\n* [[source::Doc|NVIDIA Container Toolkit|https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::DevOps]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nContainerized Unsloth environment using Docker with NVIDIA GPU support for reproducible fine-tuning deployments.\n\n=== Description ===\nThis environment provides a fully containerized setup for running Unsloth fine-tuning workflows. Using Docker with NVIDIA Container Toolkit ensures reproducibility across different machines and simplifies deployment to cloud instances. The official Unsloth Docker image includes all dependencies pre-configured, with Jupyter Lab for interactive development.\n\n=== Usage ===\nUse this environment when you need **reproducible deployments**, **cloud server setups** (AWS, GCP, Azure), or **team collaboration** with consistent environments. Ideal for production workflows, automated training pipelines, and situations where environment consistency is critical.\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n|| OS || Linux with Docker || Ubuntu 20.04/22.04 recommended\n|-\n|| Docker || Docker Engine 20.10+ || With Docker Compose support\n|-\n|| NVIDIA Driver || 525+ || Must support CUDA 11.8+\n|-\n|| NVIDIA Container Toolkit || Latest || For GPU passthrough to containers\n|-\n|| Hardware || NVIDIA GPU || Minimum 8GB VRAM\n|-\n|| Disk || 100GB+ SSD || For Docker images and model storage\n|}\n\n== Dependencies ==\n=== Docker Setup ===\n\n<syntaxhighlight lang=\"bash\">\n# Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\nsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</syntaxhighlight>\n\n=== Running Unsloth Container ===\n\n<syntaxhighlight lang=\"bash\">\n# Pull and run official Unsloth Docker image\ndocker pull unsloth/unsloth:latest\n\n# Run with GPU support and Jupyter Lab\ndocker run --gpus all -it -p 8888:8888 unsloth/unsloth:latest\n\n# Access Jupyter Lab at http://localhost:8888\n</syntaxhighlight>\n\n=== Docker Compose (Recommended) ===\n\n<syntaxhighlight lang=\"yaml\">\nversion: '3.8'\nservices:\n  unsloth:\n    image: unsloth/unsloth:latest\n    runtime: nvidia\n    environment:\n      - NVIDIA_VISIBLE_DEVICES=all\n      - HF_TOKEN=${HF_TOKEN}\n    ports:\n      - \"8888:8888\"\n    volumes:\n      - ./data:/workspace/data\n      - ./outputs:/workspace/outputs\n</syntaxhighlight>\n\n== Credentials ==\nThe following environment variables should be set:\n* `HF_TOKEN`: HuggingFace API token (pass via `-e HF_TOKEN=...` or `.env` file)\n* `WANDB_API_KEY`: Weights & Biases API key (optional)\n\n== Related Pages ==\n=== Required By ===\n* [[required_by::Implementation:Unsloth_FastLanguageModel]]\n* [[required_by::Implementation:Unsloth_FastModel]]\n* [[required_by::Implementation:TRL_SFTTrainer]]\n* [[required_by::Implementation:TRL_DPOTrainer]]\n* [[required_by::Implementation:TRL_GRPOTrainer]]\n\n",
      "domains": [
        "Infrastructure",
        "DevOps",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Docker Guide",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Doc",
          "title": "NVIDIA Container Toolkit",
          "url": "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastLanguageModel"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastModel"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "TRL_SFTTrainer"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "TRL_DPOTrainer"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "TRL_GRPOTrainer"
        }
      ]
    },
    {
      "id": "Heuristic/AdamW_8bit_Optimizer_Usage",
      "page_title": "AdamW 8bit Optimizer Usage",
      "page_type": "Heuristic",
      "overview": "Memory optimization using 8-bit Adam optimizer to reduce optimizer state memory by 75% with negligible quality impact.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|8-bit Optimizers|https://arxiv.org/abs/2110.02861]]\n* [[source::Repo|bitsandbytes|https://github.com/TimDettmers/bitsandbytes]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory_Management]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nMemory optimization using 8-bit Adam optimizer to reduce optimizer state memory by 75% with negligible quality impact.\n\n=== Description ===\nStandard Adam optimizer stores two momentum states per parameter in FP32, consuming significant memory for large models. The 8-bit Adam variant from bitsandbytes quantizes these states to 8-bit integers, reducing optimizer memory by ~75% with minimal impact on training quality. Unsloth integrates seamlessly with 8-bit optimizers.\n\n=== Usage ===\nUse this heuristic when **VRAM constrained** or training models where optimizer states consume significant memory. Standard practice for QLoRA fine-tuning with Unsloth.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `optim = \"adamw_8bit\"` in training configuration.\n* **Value:** String `\"adamw_8bit\"`\n* **Alternative:** `\"paged_adamw_8bit\"` for additional memory savings (offloads to CPU when needed)\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\nargs = SFTConfig(\n    optim = \"adamw_8bit\",           # 8-bit Adam optimizer\n    # OR for extreme memory constraints:\n    optim = \"paged_adamw_8bit\",     # With CPU offloading\n    # ... other args\n)\n</syntaxhighlight>\n\n* **Memory Savings:**\n\n{| class=\"wikitable\"\n! Optimizer !! Memory per Parameter !! Relative to FP32 Adam\n|-\n|| adamw (FP32) || 8 bytes || 100%\n|-\n|| adamw_8bit || 2 bytes || 25%\n|-\n|| paged_adamw_8bit || 2 bytes + paging || 25% (can offload)\n|}\n\n* **Trade-off:** Negligible quality loss (<0.1% in benchmarks), significant memory savings.\n\n== Reasoning ==\nAdam optimizer maintains two momentum buffers (m and v) per parameter:\n* FP32 model: 8 bytes per parameter for optimizer states\n* 7B model: ~56GB just for optimizer states in FP32!\n\n8-bit quantization of these states reduces this to ~14GB while maintaining training dynamics through dynamic quantization. Combined with 4-bit model quantization, this makes training 7B+ models feasible on consumer hardware.\n\nUnsloth's optimizations stack with 8-bit optimizers for maximum memory efficiency.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Implementation:TRL_SFTTrainer]]\n* [[uses_heuristic::Implementation:TRL_SFTConfig]]\n* [[uses_heuristic::Implementation:BitsAndBytes_4bit_Quantization]]\n\n",
      "domains": [
        "Optimization",
        "Memory Management",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "8-bit Optimizers",
          "url": "https://arxiv.org/abs/2110.02861"
        },
        {
          "type": "Repo",
          "title": "bitsandbytes",
          "url": "https://github.com/TimDettmers/bitsandbytes"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "TRL_SFTTrainer"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "TRL_SFTConfig"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "BitsAndBytes_4bit_Quantization"
        }
      ]
    },
    {
      "id": "Heuristic/Batch_Size_Optimization",
      "page_title": "Batch Size Optimization",
      "page_type": "Heuristic",
      "overview": "Strategy for selecting batch size and gradient accumulation steps to maximize throughput while fitting in GPU memory.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Paper|Large Batch Training|https://arxiv.org/abs/1706.02677]]\n* [[source::Blog|Effective Batch Size Guide|https://huggingface.co/docs/transformers/perf_train_gpu_one]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory_Management]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nStrategy for selecting batch size and gradient accumulation steps to maximize throughput while fitting in GPU memory.\n\n=== Description ===\nBatch size directly impacts training speed, memory usage, and model convergence. Unsloth's optimizations allow 2x larger batch sizes than standard implementations, enabling more efficient training. The effective batch size is `per_device_batch_size \u00d7 gradient_accumulation_steps \u00d7 num_gpus`.\n\n=== Usage ===\nUse this heuristic when configuring `per_device_train_batch_size` and `gradient_accumulation_steps` in `SFTConfig`. Critical for balancing speed and memory constraints.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Maximize `per_device_train_batch_size` until near OOM, then use gradient accumulation.\n* **Values:**\n\n{| class=\"wikitable\"\n! GPU VRAM !! Batch Size (7B model) !! With Unsloth\n|-\n|| 8GB || OOM || 1-2\n|-\n|| 12GB || 1 || 2-4\n|-\n|| 16GB || 1-2 || 4-8\n|-\n|| 24GB || 2-4 || 8-16\n|-\n|| 40GB+ || 8-16 || 16-32\n|}\n\n* **Target Effective Batch Size:** 32-128 for most tasks\n* **Formula:** `effective_batch = per_device_batch \u00d7 grad_accum \u00d7 num_gpus`\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\n# Example: 16GB GPU, targeting effective batch of 32\nargs = SFTConfig(\n    per_device_train_batch_size = 4,     # Max that fits in memory\n    gradient_accumulation_steps = 8,     # 4 * 8 = 32 effective\n    # ... other args\n)\n</syntaxhighlight>\n\n* **Trade-off:**\n  * Larger batch: Faster training, more stable gradients, more VRAM\n  * Smaller batch + accumulation: Less VRAM, same effective batch, slightly slower\n\n== Reasoning ==\nLarger effective batch sizes provide more stable gradient estimates, leading to smoother training curves. However, very large batches may require learning rate scaling. Unsloth's 70% VRAM reduction allows fitting larger batches than standard HuggingFace training, effectively doubling throughput.\n\nStart with the largest batch that fits, then use gradient accumulation to reach the target effective batch size of 32-64 for most instruction tuning tasks.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Implementation:TRL_SFTConfig]]\n* [[uses_heuristic::Implementation:HF_TrainingArguments]]\n* [[uses_heuristic::Principle:Gradient_Checkpointing]]\n\n",
      "domains": [
        "Optimization",
        "Memory Management",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Paper",
          "title": "Large Batch Training",
          "url": "https://arxiv.org/abs/1706.02677"
        },
        {
          "type": "Blog",
          "title": "Effective Batch Size Guide",
          "url": "https://huggingface.co/docs/transformers/perf_train_gpu_one"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "TRL_SFTConfig"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "HF_TrainingArguments"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "Gradient_Checkpointing"
        }
      ]
    },
    {
      "id": "Heuristic/Gradient_Accumulation_Strategy",
      "page_title": "Gradient Accumulation Strategy",
      "page_type": "Heuristic",
      "overview": "Technique to simulate larger batch sizes by accumulating gradients over multiple forward passes before updating weights.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|HuggingFace Trainer|https://huggingface.co/docs/transformers/main_classes/trainer]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Blog|Gradient Accumulation Guide|https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory_Management]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nTechnique to simulate larger batch sizes by accumulating gradients over multiple forward passes before updating weights.\n\n=== Description ===\nGradient accumulation allows training with larger effective batch sizes than GPU memory permits. Instead of updating weights after each batch, gradients are accumulated over `N` steps before performing a single update. This achieves the same effect as training with `N \u00d7 batch_size` samples per update.\n\n=== Usage ===\nUse this heuristic when you need **larger effective batch sizes** than memory allows, or when you want to match batch sizes used in paper reproductions. Standard practice for all Unsloth fine-tuning workflows.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `gradient_accumulation_steps` to achieve target effective batch size.\n* **Formula:** `effective_batch = per_device_batch \u00d7 gradient_accumulation_steps`\n* **Values:**\n\n{| class=\"wikitable\"\n! Target Effective Batch !! Per-Device Batch !! Accumulation Steps\n|-\n|| 32 || 2 || 16\n|-\n|| 32 || 4 || 8\n|-\n|| 32 || 8 || 4\n|-\n|| 64 || 4 || 16\n|-\n|| 64 || 8 || 8\n|}\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\nargs = SFTConfig(\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,  # Effective batch = 8\n    # ... other args\n)\n</syntaxhighlight>\n\n* **Trade-off:**\n  * More accumulation steps: Same memory as small batch, but slower per-step\n  * Fewer accumulation steps: Need more VRAM, but faster wall-clock time\n\n== Reasoning ==\nMathematically, accumulating gradients over N steps and averaging them produces identical gradients to computing them on N \u00d7 batch_size samples at once. This allows:\n\n1. Training with large effective batches on limited hardware\n2. Matching batch sizes from papers for reproducibility\n3. More stable training through better gradient estimates\n\nWith Unsloth's 70% VRAM reduction, you often need less accumulation than with standard HuggingFace, leading to faster training.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Implementation:TRL_SFTTrainer]]\n* [[uses_heuristic::Implementation:TRL_SFTConfig]]\n* [[uses_heuristic::Implementation:HF_TrainingArguments]]\n\n",
      "domains": [
        "Optimization",
        "Memory Management",
        "Training"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "HuggingFace Trainer",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Blog",
          "title": "Gradient Accumulation Guide",
          "url": "https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "TRL_SFTTrainer"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "TRL_SFTConfig"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "HF_TrainingArguments"
        }
      ]
    },
    {
      "id": "Heuristic/Learning_Rate_Tuning",
      "page_title": "Learning Rate Tuning",
      "page_type": "Heuristic",
      "overview": "Optimal learning rate selection for LoRA/QLoRA fine-tuning, typically 10-100x higher than full fine-tuning rates.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA Paper|https://arxiv.org/abs/2106.09685]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Blog|HuggingFace Training Tips|https://huggingface.co/docs/transformers/training]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Fine_Tuning]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nOptimal learning rate selection for LoRA/QLoRA fine-tuning, typically 10-100x higher than full fine-tuning rates.\n\n=== Description ===\nLearning rate is one of the most critical hyperparameters for successful fine-tuning. LoRA adapters require higher learning rates than full fine-tuning because they only update a small subset of parameters. Too low results in slow convergence; too high causes instability and loss spikes.\n\n=== Usage ===\nUse this heuristic when setting the `learning_rate` in `TrainingArguments` or `SFTConfig`. Essential for all fine-tuning workflows with Unsloth.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set learning rate based on task type and model size.\n* **Values:**\n\n{| class=\"wikitable\"\n! Task Type !! Recommended LR !! Notes\n|-\n|| Instruction Fine-tuning (SFT) || 2e-4 to 5e-4 || Standard for most tasks\n|-\n|| Domain Adaptation || 1e-4 to 2e-4 || More conservative\n|-\n|| RLHF/DPO/GRPO || 5e-5 to 2e-4 || Lower for stability\n|-\n|| Small Models (<3B) || 3e-4 to 1e-3 || Can use higher LR\n|-\n|| Large Models (>13B) || 1e-4 to 2e-4 || More conservative\n|}\n\n* **Default Recommendation:** Start with `2e-4` for most QLoRA fine-tuning tasks.\n* **Trade-off:** Higher LR = faster convergence but risk of instability; Lower LR = slower but more stable.\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\nargs = SFTConfig(\n    learning_rate = 2e-4,           # Recommended starting point\n    lr_scheduler_type = \"linear\",   # or \"cosine\" for longer training\n    # ... other args\n)\n</syntaxhighlight>\n\n== Reasoning ==\nLoRA fine-tuning introduces new low-rank matrices that start from random initialization (or zeros for B matrices). These new parameters need to quickly adapt to the task, requiring higher learning rates. The base model weights remain frozen, so there's no risk of catastrophic forgetting from high LR.\n\nEmpirically, 2e-4 works well across most Llama, Mistral, and Qwen models when using 4-bit quantization with Unsloth.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Workflow:GRPO_Reinforcement_Learning]]\n* [[uses_heuristic::Workflow:DPO_Alignment]]\n* [[uses_heuristic::Implementation:TRL_SFTConfig]]\n* [[uses_heuristic::Implementation:HF_TrainingArguments]]\n\n",
      "domains": [
        "Optimization",
        "Fine Tuning",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA Paper",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Blog",
          "title": "HuggingFace Training Tips",
          "url": "https://huggingface.co/docs/transformers/training"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "GRPO_Reinforcement_Learning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "DPO_Alignment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "TRL_SFTConfig"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "HF_TrainingArguments"
        }
      ]
    },
    {
      "id": "Heuristic/LoRA_Rank_Selection",
      "page_title": "LoRA Rank Selection",
      "page_type": "Heuristic",
      "overview": "Guidelines for selecting LoRA rank (r) to balance model capacity, training cost, and fine-tuning quality.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA Paper|https://arxiv.org/abs/2106.09685]]\n* [[source::Paper|QLoRA Paper|https://arxiv.org/abs/2305.14314]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Optimization]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-14 10:51 GMT]]\n|}\n\n== Overview ==\nGuidelines for selecting LoRA rank (r) to balance model capacity, training cost, and fine-tuning quality.\n\n=== Description ===\nLoRA rank determines the dimensionality of the low-rank adaptation matrices. Higher rank means more trainable parameters and greater capacity to learn task-specific patterns, but also more memory and compute. The optimal rank depends on task complexity and the gap between pre-training and target domain.\n\n=== Usage ===\nUse this heuristic when setting the `r` parameter in `FastLanguageModel.get_peft_model()`. Affects model quality, training speed, and adapter size.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `r` based on task complexity and available resources.\n* **Values:**\n\n{| class=\"wikitable\"\n! Rank (r) !! Use Case !! Trainable Params (7B) !! Quality\n|-\n|| 8 || Simple tasks, quick experiments || ~20M || Good\n|-\n|| 16 || Standard instruction tuning || ~40M || Better (recommended)\n|-\n|| 32 || Complex tasks, domain adaptation || ~80M || Best\n|-\n|| 64+ || Near full fine-tuning capacity || ~160M+ || Diminishing returns\n|}\n\n* **Default Recommendation:** `r = 16` for most tasks\n* **Alpha Rule:** Set `lora_alpha = r` or `lora_alpha = 2 * r`\n\n<syntaxhighlight lang=\"python\">\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,              # Rank - start here\n    lora_alpha = 16,     # Usually equal to r\n    lora_dropout = 0,    # 0 is optimized for Unsloth\n    bias = \"none\",\n    # ... other args\n)\n</syntaxhighlight>\n\n* **Trade-off:**\n  * Higher rank: More capacity, more VRAM, larger adapter files\n  * Lower rank: Faster training, smaller adapters, may underfit complex tasks\n\n== Reasoning ==\nThe LoRA paper showed that language models can be effectively adapted with surprisingly low ranks. Rank 16-32 captures most task-specific information for instruction tuning. Higher ranks show diminishing returns because the \"intrinsic rank\" of the update is often low.\n\nFor Unsloth:\n* Use `r = 8` for quick experiments and simple classification\n* Use `r = 16` for standard instruction tuning (default)\n* Use `r = 32` for complex reasoning tasks or significant domain shift\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Implementation:Unsloth_get_peft_model]]\n* [[uses_heuristic::Principle:Low_Rank_Adaptation]]\n\n",
      "domains": [
        "Fine Tuning",
        "Optimization",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA Paper",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Paper",
          "title": "QLoRA Paper",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-14 10:57 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "Unsloth_get_peft_model"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        }
      ]
    },
    {
      "id": "Heuristic/Memory_Efficient_Attention",
      "page_title": "Memory Efficient Attention",
      "page_type": "Heuristic",
      "overview": "Techniques for reducing attention mechanism memory from O(n\u00b2) to O(n) using FlashAttention and Unsloth's optimizations.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|FlashAttention|https://arxiv.org/abs/2205.14135]]\n* [[source::Paper|FlashAttention-2|https://arxiv.org/abs/2307.08691]]\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory_Management]], [[domain::Attention]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nTechniques for reducing attention mechanism memory from O(n\u00b2) to O(n) using FlashAttention and Unsloth's optimizations.\n\n=== Description ===\nStandard attention requires O(n\u00b2) memory for the attention matrix, limiting sequence length. FlashAttention and related techniques compute attention in a memory-efficient manner by tiling the computation and avoiding materialization of the full attention matrix. Unsloth integrates these optimizations automatically.\n\n=== Usage ===\nUse this heuristic when training with **long sequences** (4K+ tokens) or when hitting OOM errors. Unsloth automatically enables FlashAttention when available, but understanding the mechanism helps optimize for your specific use case.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Unsloth automatically uses FlashAttention when available. Ensure xformers is installed for older GPUs.\n* **Value:** Automatic - no manual configuration needed with Unsloth.\n* **Manual Check:** Verify FlashAttention is active in logs.\n\n<syntaxhighlight lang=\"python\">\n# Unsloth handles this automatically, but you can verify:\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = 8192,  # Long context enabled\n    # FlashAttention automatically used if available\n)\n</syntaxhighlight>\n\n* **Context Length Impact (with Unsloth optimizations):**\n\n{| class=\"wikitable\"\n! GPU VRAM !! Max Context (Standard) !! Max Context (Unsloth)\n|-\n|| 8GB || OOM || 2,972\n|-\n|| 16GB || 2,551 || 40,724\n|-\n|| 24GB || 5,789 || 78,475\n|-\n|| 80GB || 28,454 || 342,733\n|}\n\n* **Trade-off:** Negligible - FlashAttention is faster AND more memory efficient.\n\n== Reasoning ==\nFlashAttention achieves memory efficiency by:\n1. Computing attention in blocks (tiling)\n2. Never materializing the full NxN attention matrix\n3. Using hardware-optimized fused kernels\n\nUnsloth further optimizes this with custom CUDA kernels that achieve 2x speedup over standard FlashAttention implementations while maintaining the memory benefits. This enables 12x longer context lengths compared to HuggingFace + FlashAttention-2.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Implementation:Unsloth_FastLanguageModel]]\n* [[uses_heuristic::Principle:Flash_Attention]]\n* [[uses_heuristic::Principle:Self_Attention]]\n\n",
      "domains": [
        "Optimization",
        "Memory Management",
        "Attention"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "FlashAttention",
          "url": "https://arxiv.org/abs/2205.14135"
        },
        {
          "type": "Paper",
          "title": "FlashAttention-2",
          "url": "https://arxiv.org/abs/2307.08691"
        },
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "Unsloth_FastLanguageModel"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "Flash_Attention"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "Self_Attention"
        }
      ]
    },
    {
      "id": "Heuristic/QLoRA_Target_Modules_Selection",
      "page_title": "QLoRA Target Modules Selection",
      "page_type": "Heuristic",
      "overview": "Guidelines for selecting which linear layers to apply LoRA adapters to maximize fine-tuning effectiveness while minimizing trainable parameters.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|QLoRA Paper|https://arxiv.org/abs/2305.14314]]\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Blog|Unsloth LoRA Guide|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Optimization]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nGuidelines for selecting which linear layers to apply LoRA adapters to maximize fine-tuning effectiveness while minimizing trainable parameters.\n\n=== Description ===\nNot all linear layers in a Transformer contribute equally to fine-tuning performance. Selecting the right `target_modules` determines the trade-off between model capacity, training speed, and memory usage. Unsloth recommends applying LoRA to all linear layers for best results, which their optimizations make feasible.\n\n=== Usage ===\nUse this heuristic when configuring `FastLanguageModel.get_peft_model()` to determine which layers to inject LoRA adapters into. Critical decision that affects model quality, training time, and VRAM usage.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `target_modules` to include all major linear projection layers.\n* **Value (Recommended - All Layers):**\n<syntaxhighlight lang=\"python\">\ntarget_modules = [\n    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projections\n    \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP layers\n]\n</syntaxhighlight>\n* **Value (Minimal - Attention Only):**\n<syntaxhighlight lang=\"python\">\ntarget_modules = [\"q_proj\", \"v_proj\"]  # Minimum viable for basic tasks\n</syntaxhighlight>\n* **Trade-off:**\n  * All layers: Better quality, more VRAM, slower training\n  * Attention only: Faster, less VRAM, potentially lower quality\n\n== Layer Selection Guide ==\n\n{| class=\"wikitable\"\n! Layer !! Role !! Impact !! Priority\n|-\n|| q_proj || Query projection || High - critical for attention patterns || Essential\n|-\n|| k_proj || Key projection || High - attention key computation || Recommended\n|-\n|| v_proj || Value projection || High - information aggregation || Essential\n|-\n|| o_proj || Output projection || Medium - final attention output || Recommended\n|-\n|| gate_proj || MLP gating || Medium - controls information flow || For best quality\n|-\n|| up_proj || MLP expansion || Medium - feature expansion || For best quality\n|-\n|| down_proj || MLP compression || Medium - feature compression || For best quality\n|}\n\n== Reasoning ==\nResearch shows that attention layers (Q, K, V, O projections) capture the most task-specific information during fine-tuning. MLP layers (gate, up, down) contribute additional capacity for learning complex patterns. Unsloth's memory optimizations make targeting all layers practical, achieving results competitive with full fine-tuning at a fraction of the cost.\n\nFor most use cases, targeting all 7 modules provides the best quality without significant overhead when using Unsloth's optimizations.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Implementation:Unsloth_get_peft_model]]\n* [[uses_heuristic::Principle:Low_Rank_Adaptation]]\n\n",
      "domains": [
        "Fine Tuning",
        "Optimization",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "QLoRA Paper",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Blog",
          "title": "Unsloth LoRA Guide",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "Unsloth_get_peft_model"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "Low_Rank_Adaptation"
        }
      ]
    },
    {
      "id": "Heuristic/Sequence_Packing_Optimization",
      "page_title": "Sequence Packing Optimization",
      "page_type": "Heuristic",
      "overview": "Training throughput optimization by packing multiple short sequences into a single training example to eliminate padding waste.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|Packing: Towards 2x NLP Training Speedup|https://arxiv.org/abs/2107.02027]]\n* [[source::Doc|TRL Packing|https://huggingface.co/docs/trl/sft_trainer]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Training]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nTraining throughput optimization by packing multiple short sequences into a single training example to eliminate padding waste.\n\n=== Description ===\nStandard training pads all sequences to the maximum length, wasting compute on padding tokens. Sequence packing concatenates multiple short sequences (with separator tokens) into single examples up to max_seq_length, eliminating padding overhead. This can provide up to 2x speedup for datasets with variable-length samples.\n\n=== Usage ===\nUse this heuristic when your dataset has **variable-length sequences**, especially if many are shorter than `max_seq_length`. Most effective for instruction-tuning datasets where conversations vary in length.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Enable packing in SFTTrainer configuration.\n* **Value:** Set `packing = True` in SFTConfig or SFTTrainer.\n* **When to Use:** Dataset avg length << max_seq_length\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTTrainer, SFTConfig\n\ntrainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    args = SFTConfig(\n        packing = True,              # Enable sequence packing\n        max_seq_length = 2048,       # Packed sequences up to this length\n        # ... other args\n    ),\n)\n</syntaxhighlight>\n\n* **Speedup Estimates:**\n\n{| class=\"wikitable\"\n! Avg Sequence Length !! Max Sequence Length !! Approximate Speedup\n|-\n|| 256 || 2048 || ~5-8x\n|-\n|| 512 || 2048 || ~3-4x\n|-\n|| 1024 || 2048 || ~1.5-2x\n|-\n|| 1500 || 2048 || ~1.2-1.3x\n|}\n\n* **Trade-off:**\n  * Packing enabled: Faster training, may slightly affect attention patterns at boundaries\n  * Packing disabled: Cleaner sequence boundaries, more padding waste\n\n== Reasoning ==\nWithout packing, a batch with sequences of length [200, 300, 150] padded to 2048:\n* Total tokens processed: 3 \u00d7 2048 = 6144\n* Actual content: 200 + 300 + 150 = 650\n* Efficiency: ~10%\n\nWith packing, these sequences become one 650-token example:\n* Total tokens: 650\n* Efficiency: 100%\n* Result: 10x less wasted compute\n\nUnsloth's SFTTrainer integrates with TRL's packing implementation, which handles attention masking to prevent cross-sequence attention.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Implementation:TRL_SFTTrainer]]\n* [[uses_heuristic::Principle:Supervised_Fine_Tuning]]\n\n",
      "domains": [
        "Optimization",
        "Training",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "Packing: Towards 2x NLP Training Speedup",
          "url": "https://arxiv.org/abs/2107.02027"
        },
        {
          "type": "Doc",
          "title": "TRL Packing",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "TRL_SFTTrainer"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "Supervised_Fine_Tuning"
        }
      ]
    },
    {
      "id": "Heuristic/Unsloth_Gradient_Checkpointing_Optimization",
      "page_title": "Unsloth Gradient Checkpointing Optimization",
      "page_type": "Heuristic",
      "overview": "Memory optimization using Unsloth's custom gradient checkpointing implementation to reduce VRAM by 50-60% while maintaining 2x training speed.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth GitHub|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai/]]\n* [[source::Paper|Training Deep Nets with Sublinear Memory|https://arxiv.org/abs/1604.06174]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory_Management]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nMemory optimization using Unsloth's custom gradient checkpointing implementation to reduce VRAM by 50-60% while maintaining 2x training speed.\n\n=== Description ===\nUnsloth provides an enhanced gradient checkpointing implementation that goes beyond standard PyTorch checkpointing. By setting `use_gradient_checkpointing = \"unsloth\"`, you activate optimizations that reduce VRAM usage by approximately 30% more than standard gradient checkpointing, while still achieving 2x faster training than HuggingFace baselines. This is achieved through intelligent recomputation strategies and kernel fusion optimizations.\n\n=== Usage ===\nUse this heuristic when you are **VRAM constrained** (e.g., CUDA OOM errors) or need to fit larger batch sizes on limited hardware. Essential for fine-tuning 7B+ parameter models on consumer GPUs (16-24GB VRAM). Standard practice for QLoRA workflows with Unsloth.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `use_gradient_checkpointing = \"unsloth\"` in `FastLanguageModel.get_peft_model()` or `FastModel.get_peft_model()`.\n* **Value:** String `\"unsloth\"` (not boolean `True`).\n* **Trade-off:** Reduces VRAM by ~30% more than `True`, with ~5-10% slower training than no checkpointing (but 2x faster than HF baseline with checkpointing).\n* **Compatibility:** Works with all Unsloth-supported models. Requires `use_cache=False` during training.\n\n<syntaxhighlight lang=\"python\">\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",  # <-- Key setting\n    random_state = 3407,\n)\n</syntaxhighlight>\n\n== Reasoning ==\nDeep Transformer models store massive activation tensors during the forward pass for backpropagation. Unsloth's `\"unsloth\"` mode implements custom CUDA kernels that:\n1. Selectively checkpoint only the most memory-intensive layers\n2. Fuse operations to reduce intermediate tensor storage\n3. Recompute activations efficiently during backward pass\n\nBenchmarks show Llama-3.1 8B training VRAM drops from ~20GB to ~8GB with this setting, enabling fine-tuning on RTX 3060/4060 class GPUs.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Workflow:GRPO_Reinforcement_Learning]]\n* [[uses_heuristic::Workflow:DPO_Alignment]]\n* [[uses_heuristic::Implementation:Unsloth_get_peft_model]]\n* [[uses_heuristic::Principle:Gradient_Checkpointing]]\n\n",
      "domains": [
        "Optimization",
        "Memory Management",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth GitHub",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai/"
        },
        {
          "type": "Paper",
          "title": "Training Deep Nets with Sublinear Memory",
          "url": "https://arxiv.org/abs/1604.06174"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "GRPO_Reinforcement_Learning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "DPO_Alignment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "Unsloth_get_peft_model"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "Gradient_Checkpointing"
        }
      ]
    },
    {
      "id": "Heuristic/Warmup_Steps_Heuristic",
      "page_title": "Warmup Steps Heuristic",
      "page_type": "Heuristic",
      "overview": "Strategy for selecting warmup steps to gradually ramp up learning rate at training start, preventing early instability.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|Attention Is All You Need|https://arxiv.org/abs/1706.03762]]\n* [[source::Doc|HuggingFace Training Arguments|https://huggingface.co/docs/transformers/main_classes/trainer]]\n* [[source::Blog|Learning Rate Scheduling|https://huggingface.co/docs/transformers/training]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Fine_Tuning]], [[domain::LLMs]]\n|-\n! Last Updated\n| [[last_updated::2025-12-12 00:00 GMT]]\n|}\n\n== Overview ==\nStrategy for selecting warmup steps to gradually ramp up learning rate at training start, preventing early instability.\n\n=== Description ===\nWarmup gradually increases the learning rate from 0 to the target value over a number of steps. This prevents large gradient updates early in training when the model hasn't yet found a stable region of the loss landscape. Particularly important for fine-tuning large models where initial gradients can be noisy.\n\n=== Usage ===\nUse this heuristic when configuring `warmup_steps` or `warmup_ratio` in training arguments. Essential for stable fine-tuning, especially with higher learning rates.\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `warmup_steps` based on training duration and dataset size.\n* **Values:**\n\n{| class=\"wikitable\"\n! Training Duration !! Recommended Warmup !! Alternative\n|-\n|| Short (<100 steps) || 5-10 steps || warmup_ratio = 0.1\n|-\n|| Medium (100-1000 steps) || 10-50 steps || warmup_ratio = 0.05\n|-\n|| Long (>1000 steps) || 50-100 steps || warmup_ratio = 0.03\n|}\n\n* **Default Recommendation:** `warmup_steps = 10` for most Unsloth workflows\n* **Quick Rule:** ~5% of total training steps, minimum 5 steps\n\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTConfig\n\nargs = SFTConfig(\n    warmup_steps = 10,             # Fixed number of steps\n    # OR\n    warmup_ratio = 0.05,           # 5% of total steps\n    lr_scheduler_type = \"linear\",  # or \"cosine\"\n    # ... other args\n)\n</syntaxhighlight>\n\n* **Trade-off:**\n  * More warmup: Safer start, slightly slower to reach peak performance\n  * Less warmup: Faster to peak LR, risk of instability\n\n== Reasoning ==\nAt training start:\n1. LoRA adapter weights are randomly initialized\n2. Gradients can be large and noisy\n3. Large LR \u00d7 large gradients = unstable updates\n\nWarmup allows the model to find a stable optimization trajectory before applying the full learning rate. For short fine-tuning runs (common with Unsloth), 5-10 steps is usually sufficient.\n\n== Related Pages ==\n=== Used By ===\n* [[uses_heuristic::Workflow:QLoRA_Finetuning]]\n* [[uses_heuristic::Implementation:TRL_SFTConfig]]\n* [[uses_heuristic::Implementation:HF_TrainingArguments]]\n\n",
      "domains": [
        "Optimization",
        "Fine Tuning",
        "LLMs"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "Attention Is All You Need",
          "url": "https://arxiv.org/abs/1706.03762"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Training Arguments",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        },
        {
          "type": "Blog",
          "title": "Learning Rate Scheduling",
          "url": "https://huggingface.co/docs/transformers/training"
        }
      ],
      "last_updated": "2025-12-12 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "QLoRA_Finetuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "TRL_SFTConfig"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "HF_TrainingArguments"
        }
      ]
    }
  ],
  "metadata": {
    "collection": "KGWikiPages",
    "embedding_model": "text-embedding-3-large"
  }
}