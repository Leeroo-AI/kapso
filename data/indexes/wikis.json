{
  "pages": [
    {
      "id": "Workflow/huggingface_peft_Adapter_Loading_Inference",
      "page_title": "huggingface peft Adapter Loading Inference",
      "page_type": "Workflow",
      "overview": "End-to-end process for loading pretrained PEFT adapters and running inference, enabling deployment of specialized models from small adapter checkpoints.",
      "content": "# Adapter Loading & Inference\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Documentation|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Inference]], [[domain::Model_Serving]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 12:00 GMT]]\n|}\n\n== Overview ==\n\nEnd-to-end process for loading pretrained PEFT adapters and running inference, enabling deployment of specialized models from small adapter checkpoints.\n\n=== Description ===\n\nThis workflow covers loading saved PEFT adapters onto base models for inference. Adapters can be loaded from local paths or the HuggingFace Hub. The workflow supports loading multiple adapters, switching between them at runtime, and optionally merging adapters into the base model for production deployment with zero inference overhead.\n\n=== Usage ===\n\nExecute this workflow when:\n* You have a trained PEFT adapter and need to run inference\n* You want to load adapters from HuggingFace Hub\n* You need to switch between multiple task-specific adapters\n* You're deploying a PEFT model in production\n\n== Execution Steps ==\n\n=== Step 1: Load Base Model ===\n[[step::Principle:huggingface_peft_Base_Model_Loading]]\n\nLoad the same base model that was used during adapter training. The base model ID is stored in the adapter config, but you can also specify it explicitly if using a different checkpoint.\n\n'''Loading options:'''\n* Standard precision: Load in float16/bfloat16 for inference\n* Quantized: Load in 4-bit/8-bit for memory-constrained deployment\n* Device placement: Use `device_map=\"auto\"` for multi-GPU inference\n\n=== Step 2: Load PEFT Adapter ===\n[[step::Principle:huggingface_peft_Adapter_Loading]]\n\nLoad the pretrained adapter using `PeftModel.from_pretrained()`. This reads the adapter configuration and weights, then injects the adapter layers into the base model.\n\n'''Loading sources:'''\n* Local path: `PeftModel.from_pretrained(model, \"./path/to/adapter\")`\n* HuggingFace Hub: `PeftModel.from_pretrained(model, \"username/adapter-name\")`\n* Specific revision: Use `revision` parameter for version control\n\n'''Options:'''\n* `is_trainable=False`: Load in inference mode (default)\n* `adapter_name`: Custom name for multi-adapter scenarios\n\n=== Step 3: Configure Inference Mode ===\n[[step::Principle:huggingface_peft_Inference_Configuration]]\n\nSet up the model for efficient inference. This includes disabling dropout, setting eval mode, and optionally enabling optimizations like torch.compile or Flash Attention.\n\n'''Configuration:'''\n* Call `model.eval()` to disable dropout and batch norm updates\n* Disable gradient computation with `torch.no_grad()` context\n* Enable inference optimizations (e.g., `torch.compile`)\n\n=== Step 4: Run Inference ===\n[[step::Principle:huggingface_peft_Inference_Execution]]\n\nExecute forward passes through the PEFT model. The adapter modifies the base model's behavior according to its learned weights.\n\n'''Inference patterns:'''\n* Text generation: Use `model.generate()` with appropriate parameters\n* Classification: Forward pass and extract logits\n* Embeddings: Extract hidden states from intermediate layers\n\n=== Step 5: (Optional) Merge Adapter ===\n[[step::Principle:huggingface_peft_Adapter_Merging_Into_Base]]\n\nFor production deployment where inference latency is critical, merge the adapter weights into the base model. This eliminates the adapter computation overhead but makes the model permanent (can't switch adapters).\n\n'''Merging:'''\n* `model.merge_and_unload()`: Merge adapter into base and remove PEFT wrapper\n* Results in standard transformers model with adapter effects baked in\n* No runtime overhead from adapter computation\n\n== Execution Diagram ==\n\n{{#mermaid:graph TD\n    A[Load Base Model] --> B[Load PEFT Adapter]\n    B --> C[Configure Inference]\n    C --> D[Run Inference]\n    D --> E{Production Deploy?}\n    E -->|Yes| F[Merge Adapter]\n    E -->|No| G[Keep Adapter Separate]\n    F --> H[Standard Model Inference]\n    G --> D\n}}\n\n== Related Pages ==\n\n* [[step::Principle:huggingface_peft_Base_Model_Loading]]\n* [[step::Principle:huggingface_peft_Adapter_Loading]]\n* [[step::Principle:huggingface_peft_Inference_Configuration]]\n* [[step::Principle:huggingface_peft_Inference_Execution]]\n* [[step::Principle:huggingface_peft_Adapter_Merging_Into_Base]]\n",
      "domains": [
        "LLMs",
        "Inference",
        "Model Serving"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Documentation",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2025-12-18 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Base_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Inference_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Inference_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Merging_Into_Base"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Base_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Inference_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Inference_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Merging_Into_Base"
        }
      ]
    },
    {
      "id": "Workflow/huggingface_peft_Adapter_Merging",
      "page_title": "huggingface peft Adapter Merging",
      "page_type": "Workflow",
      "overview": "End-to-end process for combining multiple trained PEFT adapters into a single adapter using advanced merging algorithms (TIES, DARE, task arithmetic), enabling multi-task models without separate adapter switching.",
      "content": "# Adapter Merging\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Documentation|https://huggingface.co/docs/peft]]\n* [[source::Paper|TIES-Merging Paper|https://arxiv.org/abs/2306.01708]]\n* [[source::Paper|DARE Paper|https://arxiv.org/abs/2311.03099]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Model_Merging]], [[domain::Multi_Task]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 12:00 GMT]]\n|}\n\n== Overview ==\n\nEnd-to-end process for combining multiple trained PEFT adapters into a single adapter using advanced merging algorithms (TIES, DARE, task arithmetic), enabling multi-task models without separate adapter switching.\n\n=== Description ===\n\nThis workflow covers merging multiple task-specific LoRA adapters into a unified adapter that combines their capabilities. PEFT provides several merging algorithms: simple weighted averaging (task arithmetic), TIES (resolving sign conflicts), DARE (random pruning with rescaling), and combinations thereof. Merged adapters can perform multiple tasks without runtime switching overhead.\n\n=== Usage ===\n\nExecute this workflow when:\n* You have multiple task-specific adapters and want a single multi-task adapter\n* You want to combine domain knowledge from different adapters\n* You need to reduce deployment complexity by eliminating adapter switching\n* You're experimenting with model merging techniques\n\n== Execution Steps ==\n\n=== Step 1: Load Base Model ===\n[[step::Principle:huggingface_peft_Base_Model_Loading]]\n\nLoad the base model that all adapters were trained from. All adapters being merged must share the same base model architecture.\n\n'''Requirements:'''\n* Same base model used for all adapters being merged\n* Adapters must have compatible configurations (same target modules)\n\n=== Step 2: Load Primary Adapter ===\n[[step::Principle:huggingface_peft_Adapter_Loading]]\n\nLoad the first adapter using `PeftModel.from_pretrained()`. This creates the PEFT model structure that will hold the merged result.\n\n'''Purpose:'''\n* Establishes the PEFT model structure\n* First adapter becomes the \"default\" adapter slot\n* Configuration defines the structure for merging\n\n=== Step 3: Load Additional Adapters ===\n[[step::Principle:huggingface_peft_Multi_Adapter_Loading]]\n\nLoad additional adapters using `model.load_adapter()`. Each adapter is loaded with a unique name for identification during merging.\n\n'''Loading multiple adapters:'''\n* Use `load_adapter(path, adapter_name=\"unique_name\")` for each\n* All adapters coexist in memory with separate names\n* Verify compatibility: same rank, target modules, and base model\n\n=== Step 4: Configure Merge Strategy ===\n[[step::Principle:huggingface_peft_Merge_Strategy_Configuration]]\n\nSelect and configure the merging algorithm. Different algorithms have different strengths:\n\n'''Merging algorithms:'''\n* **Linear/Task Arithmetic**: Weighted sum of adapter deltas\n* **TIES**: Trims small values, elects majority sign, averages disjoint sets\n* **DARE Linear**: Random pruning with rescaling + linear merge\n* **DARE TIES**: Random pruning + TIES merging\n\n'''Parameters:'''\n* `weights`: List of weights for each adapter (importance)\n* `density`: Fraction of values to keep (for TIES/DARE)\n* `majority_sign_method`: \"total\" or \"frequency\" for sign election\n\n=== Step 5: Execute Adapter Merge ===\n[[step::Principle:huggingface_peft_Adapter_Merge_Execution]]\n\nCall `model.add_weighted_adapter()` to merge the loaded adapters into a new combined adapter. The merging algorithm computes the weighted combination of adapter weights.\n\n'''Merge execution:'''\n* Specify adapters to merge by name\n* Provide weights for each adapter\n* New adapter is created with the combined weights\n* Original adapters remain intact (can be deleted if needed)\n\n=== Step 6: Evaluate Merged Adapter ===\n[[step::Principle:huggingface_peft_Merge_Evaluation]]\n\nTest the merged adapter on tasks from each source adapter to verify capability preservation. Merging may cause degradation on individual tasks.\n\n'''Evaluation:'''\n* Test on each source task to measure capability retention\n* Compare to individual adapter performance\n* Adjust weights or merging algorithm if needed\n\n=== Step 7: Save Merged Adapter ===\n[[step::Principle:huggingface_peft_Adapter_Serialization]]\n\nSave the merged adapter for deployment. The merged adapter can be loaded just like any single adapter.\n\n'''Output:'''\n* Single adapter checkpoint with combined capabilities\n* Same loading procedure as individual adapters\n* Optionally delete source adapters after merge\n\n== Execution Diagram ==\n\n{{#mermaid:graph TD\n    A[Load Base Model] --> B[Load Primary Adapter]\n    B --> C[Load Additional Adapters]\n    C --> D[Configure Merge Strategy]\n    D --> E[Execute Merge]\n    E --> F[Evaluate Merged Adapter]\n    F --> G{Performance OK?}\n    G -->|Yes| H[Save Merged Adapter]\n    G -->|No| D\n}}\n\n== Related Pages ==\n\n* [[step::Principle:huggingface_peft_Base_Model_Loading]]\n* [[step::Principle:huggingface_peft_Adapter_Loading]]\n* [[step::Principle:huggingface_peft_Multi_Adapter_Loading]]\n* [[step::Principle:huggingface_peft_Merge_Strategy_Configuration]]\n* [[step::Principle:huggingface_peft_Adapter_Merge_Execution]]\n* [[step::Principle:huggingface_peft_Merge_Evaluation]]\n* [[step::Principle:huggingface_peft_Adapter_Serialization]]\n",
      "domains": [
        "LLMs",
        "Model Merging",
        "Multi Task"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Documentation",
          "url": "https://huggingface.co/docs/peft"
        },
        {
          "type": "Paper",
          "title": "TIES-Merging Paper",
          "url": "https://arxiv.org/abs/2306.01708"
        },
        {
          "type": "Paper",
          "title": "DARE Paper",
          "url": "https://arxiv.org/abs/2311.03099"
        }
      ],
      "last_updated": "2025-12-18 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Base_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Multi_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Merge_Strategy_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Merge_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Merge_Evaluation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Serialization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Base_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Multi_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Merge_Strategy_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Merge_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Merge_Evaluation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Serialization"
        }
      ]
    },
    {
      "id": "Workflow/huggingface_peft_LoRA_Fine_Tuning",
      "page_title": "huggingface peft LoRA Fine Tuning",
      "page_type": "Workflow",
      "overview": "End-to-end process for parameter-efficient fine-tuning of transformer models using Low-Rank Adaptation (LoRA), training only ~0.1-1% of model parameters while achieving performance comparable to full fine-tuning.",
      "content": "# LoRA Fine-Tuning\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Documentation|https://huggingface.co/docs/peft]]\n* [[source::Paper|LoRA Paper|https://arxiv.org/abs/2106.09685]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Fine_Tuning]], [[domain::Parameter_Efficient]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 12:00 GMT]]\n|}\n\n== Overview ==\n\nEnd-to-end process for parameter-efficient fine-tuning of transformer models using Low-Rank Adaptation (LoRA), training only ~0.1-1% of model parameters while achieving performance comparable to full fine-tuning.\n\n=== Description ===\n\nThis workflow outlines the standard procedure for applying LoRA adapters to pretrained transformer models. LoRA injects trainable low-rank decomposition matrices into transformer layers (typically attention projections), keeping the original model weights frozen. The process covers model preparation, adapter configuration, training loop execution, and adapter serialization. Only the small adapter weights (typically a few MB) need to be saved, rather than the entire model.\n\n=== Usage ===\n\nExecute this workflow when you need to adapt a pretrained model to a downstream task (classification, generation, etc.) and want to minimize GPU memory usage and training time. Suitable when:\n* You have limited GPU memory (8-24GB)\n* You need to train multiple task-specific adapters from the same base model\n* You want to preserve the original model capabilities while adding specialized knowledge\n\n== Execution Steps ==\n\n=== Step 1: Load Base Model ===\n[[step::Principle:huggingface_peft_Base_Model_Loading]]\n\nLoad the pretrained transformer model from HuggingFace Hub or local path. The model should be in evaluation mode with all parameters frozen by default. Consider setting `device_map` for automatic device placement on multi-GPU systems.\n\n'''Key considerations:'''\n* Use `AutoModelForCausalLM`, `AutoModelForSeq2SeqLM`, or task-specific auto classes\n* Set `torch_dtype` appropriately (float16/bfloat16 for memory efficiency)\n* Enable `device_map=\"auto\"` for large models spanning multiple GPUs\n\n=== Step 2: Configure LoRA Adapter ===\n[[step::Principle:huggingface_peft_LoRA_Configuration]]\n\nCreate a `LoraConfig` object specifying the adapter hyperparameters. This defines which layers receive adapter injection, the rank of the decomposition, and training behavior.\n\n'''Core parameters:'''\n* `r` (rank): Dimension of the low-rank matrices (commonly 8-64)\n* `lora_alpha`: Scaling factor, typically set to `r` or `2*r`\n* `target_modules`: List of module names to inject adapters (e.g., `[\"q_proj\", \"v_proj\"]`)\n* `task_type`: Task type for correct forward pass (CAUSAL_LM, SEQ_2_SEQ_LM, etc.)\n* `lora_dropout`: Dropout probability for regularization\n\n=== Step 3: Create PEFT Model ===\n[[step::Principle:huggingface_peft_PEFT_Model_Creation]]\n\nWrap the base model with the LoRA configuration using `get_peft_model()`. This injects adapter layers into the specified target modules and freezes all original parameters. The resulting model has trainable parameters only in the LoRA layers.\n\n'''What happens:'''\n* Original weight matrix W remains frozen\n* Two small matrices A (r x d) and B (d x r) are added: output = W(x) + B(A(x))\n* Only A and B are trainable (typically <1% of total parameters)\n\n=== Step 4: Prepare for Training ===\n[[step::Principle:huggingface_peft_Training_Preparation]]\n\nSet up the training loop with appropriate optimizer, learning rate scheduler, and data loaders. For gradient checkpointing compatibility with quantized models, call `prepare_model_for_kbit_training()` if needed.\n\n'''Considerations:'''\n* Use higher learning rates than full fine-tuning (1e-4 to 3e-4 typical)\n* Enable gradient checkpointing for memory efficiency\n* Set up appropriate evaluation metrics for your task\n\n=== Step 5: Execute Training Loop ===\n[[step::Principle:huggingface_peft_Training_Execution]]\n\nRun the training loop using either a custom loop or HuggingFace Trainer. Only the LoRA parameters receive gradient updates; the base model remains frozen.\n\n'''Training approaches:'''\n* Use `transformers.Trainer` with standard training arguments\n* Or implement custom loop with `model.train()`, forward pass, backward pass, optimizer step\n* Monitor loss convergence - LoRA typically converges faster than full fine-tuning\n\n=== Step 6: Save Adapter Weights ===\n[[step::Principle:huggingface_peft_Adapter_Serialization]]\n\nSave only the trained adapter weights using `model.save_pretrained()`. This creates a small checkpoint (typically a few MB) containing the LoRA weights and configuration, separate from the base model.\n\n'''Output artifacts:'''\n* `adapter_config.json`: LoRA configuration and base model reference\n* `adapter_model.safetensors`: Trained LoRA weights (small file)\n* Model card with training metadata\n\n== Execution Diagram ==\n\n{{#mermaid:graph TD\n    A[Load Base Model] --> B[Configure LoRA]\n    B --> C[Create PEFT Model]\n    C --> D[Prepare Training]\n    D --> E[Execute Training]\n    E --> F[Save Adapter]\n}}\n\n== Related Pages ==\n\n* [[step::Principle:huggingface_peft_Base_Model_Loading]]\n* [[step::Principle:huggingface_peft_LoRA_Configuration]]\n* [[step::Principle:huggingface_peft_PEFT_Model_Creation]]\n* [[step::Principle:huggingface_peft_Training_Preparation]]\n* [[step::Principle:huggingface_peft_Training_Execution]]\n* [[step::Principle:huggingface_peft_Adapter_Serialization]]\n",
      "domains": [
        "LLMs",
        "Fine Tuning",
        "Parameter Efficient"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Documentation",
          "url": "https://huggingface.co/docs/peft"
        },
        {
          "type": "Paper",
          "title": "LoRA Paper",
          "url": "https://arxiv.org/abs/2106.09685"
        }
      ],
      "last_updated": "2025-12-18 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Base_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_PEFT_Model_Creation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Training_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Training_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Serialization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Base_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_PEFT_Model_Creation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Training_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Training_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Serialization"
        }
      ]
    },
    {
      "id": "Workflow/huggingface_peft_Multi_Adapter_Management",
      "page_title": "huggingface peft Multi Adapter Management",
      "page_type": "Workflow",
      "overview": "End-to-end process for managing multiple PEFT adapters on a single base model, enabling dynamic task switching and efficient multi-task inference without reloading models.",
      "content": "# Multi-Adapter Management\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Documentation|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Multi_Task]], [[domain::Model_Serving]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 12:00 GMT]]\n|}\n\n== Overview ==\n\nEnd-to-end process for managing multiple PEFT adapters on a single base model, enabling dynamic task switching and efficient multi-task inference without reloading models.\n\n=== Description ===\n\nThis workflow covers loading, switching, enabling/disabling, and managing multiple adapters on a single PEFT model. Multiple adapters can coexist in memory, and you can switch between them at runtime without reloading the base model. This is useful for serving multiple task-specific models from a single deployment.\n\n=== Usage ===\n\nExecute this workflow when:\n* You need to serve multiple tasks from a single model deployment\n* You want to A/B test different adapters\n* You're building a multi-tenant system with per-user adapters\n* You need to dynamically switch model behavior at runtime\n\n== Execution Steps ==\n\n=== Step 1: Load Base Model with First Adapter ===\n[[step::Principle:huggingface_peft_Adapter_Loading]]\n\nLoad the base model and first adapter using `PeftModel.from_pretrained()`. This establishes the PEFT model structure with the first adapter as \"default\" (or with a custom name).\n\n'''Initial setup:'''\n* Load base model and first adapter\n* Optionally name the adapter for clarity\n* First adapter is automatically set as active\n\n=== Step 2: Load Additional Adapters ===\n[[step::Principle:huggingface_peft_Multi_Adapter_Loading]]\n\nLoad additional adapters using `model.load_adapter()`. Each adapter receives a unique name for identification and switching.\n\n'''Loading pattern:'''\n* `model.load_adapter(\"path/to/adapter2\", adapter_name=\"task2\")`\n* Adapters share base model weights in memory\n* Each adapter adds only its small weights (~MB) to memory\n\n=== Step 3: Switch Active Adapter ===\n[[step::Principle:huggingface_peft_Adapter_Switching]]\n\nSwitch between loaded adapters using `model.set_adapter()`. The active adapter determines which adapter weights are applied during forward passes.\n\n'''Switching:'''\n* `model.set_adapter(\"task2\")` activates the \"task2\" adapter\n* Instant switch, no model reloading\n* Previous adapter remains loaded (can switch back)\n* Check active adapter: `model.active_adapter`\n\n=== Step 4: Disable/Enable Adapters ===\n[[step::Principle:huggingface_peft_Adapter_Enable_Disable]]\n\nTemporarily disable all adapters to run inference with the pure base model, then re-enable for adapter-augmented inference.\n\n'''Disable pattern:'''\n* Use context manager: `with model.disable_adapter(): ...`\n* Or explicit: `model.disable_adapters()` / `model.enable_adapters()`\n* Useful for comparing base vs. adapted model outputs\n\n=== Step 5: Delete Unused Adapters ===\n[[step::Principle:huggingface_peft_Adapter_Deletion]]\n\nRemove adapters that are no longer needed to free memory. This permanently removes the adapter weights from the model.\n\n'''Cleanup:'''\n* `model.delete_adapter(\"adapter_name\")` removes the adapter\n* Cannot be undone - must reload if needed again\n* Useful for managing memory in long-running processes\n\n=== Step 6: Query Adapter State ===\n[[step::Principle:huggingface_peft_Adapter_State_Query]]\n\nCheck the current state of loaded adapters, which is active, and their configurations.\n\n'''State inspection:'''\n* `model.peft_config` - dict of all loaded adapter configs\n* `model.active_adapter` - currently active adapter name\n* `model.get_model_status()` - detailed adapter status\n\n== Execution Diagram ==\n\n{{#mermaid:graph TD\n    A[Load First Adapter] --> B[Load Additional Adapters]\n    B --> C{Select Operation}\n    C -->|Switch| D[Set Active Adapter]\n    C -->|Compare| E[Disable/Enable Adapter]\n    C -->|Cleanup| F[Delete Adapter]\n    C -->|Inspect| G[Query State]\n    D --> H[Run Inference]\n    E --> H\n    G --> C\n    F --> C\n}}\n\n== Related Pages ==\n\n* [[step::Principle:huggingface_peft_Adapter_Loading]]\n* [[step::Principle:huggingface_peft_Multi_Adapter_Loading]]\n* [[step::Principle:huggingface_peft_Adapter_Switching]]\n* [[step::Principle:huggingface_peft_Adapter_Enable_Disable]]\n* [[step::Principle:huggingface_peft_Adapter_Deletion]]\n* [[step::Principle:huggingface_peft_Adapter_State_Query]]\n",
      "domains": [
        "LLMs",
        "Multi Task",
        "Model Serving"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Documentation",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2025-12-18 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Multi_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Switching"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Enable_Disable"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Deletion"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_State_Query"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Multi_Adapter_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Switching"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Enable_Disable"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Deletion"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_State_Query"
        }
      ]
    },
    {
      "id": "Workflow/huggingface_peft_QLoRA_Training",
      "page_title": "huggingface peft QLoRA Training",
      "page_type": "Workflow",
      "overview": "End-to-end process for fine-tuning large language models on consumer GPUs using 4-bit quantization (QLoRA), enabling training of 7B+ parameter models on hardware with 16-24GB VRAM.",
      "content": "# QLoRA Training\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Documentation|https://huggingface.co/docs/peft]]\n* [[source::Paper|QLoRA Paper|https://arxiv.org/abs/2305.14314]]\n* [[source::Blog|Fine-tuning LLMs on Consumer Hardware|https://pytorch.org/blog/finetune-llms/]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Fine_Tuning]], [[domain::Quantization]], [[domain::Memory_Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 12:00 GMT]]\n|}\n\n== Overview ==\n\nEnd-to-end process for fine-tuning large language models on consumer GPUs using 4-bit quantization (QLoRA), enabling training of 7B+ parameter models on hardware with 16-24GB VRAM.\n\n=== Description ===\n\nQLoRA combines 4-bit NormalFloat quantization with LoRA adapters to dramatically reduce memory requirements during fine-tuning. The base model is loaded in 4-bit precision using bitsandbytes, while LoRA adapter weights remain in full precision (float32) for stable training. This allows training models that would otherwise require 4x more memory, making 7B-13B parameter models accessible on consumer GPUs.\n\n=== Usage ===\n\nExecute this workflow when:\n* You have limited GPU memory (8-24GB VRAM)\n* You want to fine-tune large models (7B+ parameters) that don't fit in memory at full precision\n* You're doing instruction tuning, chat fine-tuning, or domain adaptation\n* Training speed is acceptable (4-bit is slower than full precision due to dequantization overhead)\n\n== Execution Steps ==\n\n=== Step 1: Configure Quantization ===\n[[step::Principle:huggingface_peft_Quantization_Configuration]]\n\nCreate a `BitsAndBytesConfig` specifying 4-bit quantization parameters. NormalFloat4 (NF4) is the recommended quantization type as it's optimized for normally distributed weights.\n\n'''Key parameters:'''\n* `load_in_4bit=True`: Enable 4-bit quantization\n* `bnb_4bit_quant_type=\"nf4\"`: Use NormalFloat4 quantization\n* `bnb_4bit_compute_dtype=torch.bfloat16`: Compute in bfloat16 for speed\n* `bnb_4bit_use_double_quant=True`: Enable nested quantization for additional memory savings\n\n=== Step 2: Load Quantized Model ===\n[[step::Principle:huggingface_peft_Quantized_Model_Loading]]\n\nLoad the base model with the quantization configuration applied. The model weights are automatically quantized during loading, reducing memory footprint by approximately 4x compared to float16.\n\n'''What happens:'''\n* Model weights are loaded and quantized to 4-bit on-the-fly\n* Memory footprint reduced from ~14GB to ~4GB for a 7B model\n* Model is automatically placed on GPU with `device_map=\"auto\"`\n* Attention and linear layers use quantized weights\n\n=== Step 3: Prepare for K-bit Training ===\n[[step::Principle:huggingface_peft_Kbit_Training_Preparation]]\n\nApply `prepare_model_for_kbit_training()` to enable gradient computation for quantized models. This enables input embeddings to require gradients (necessary since quantized layers don't propagate gradients directly) and sets up gradient checkpointing.\n\n'''Modifications applied:'''\n* Enable `input_require_grads` for the embedding layer\n* Disable caching for training mode\n* Optionally enable gradient checkpointing for additional memory savings\n\n=== Step 4: Configure LoRA for QLoRA ===\n[[step::Principle:huggingface_peft_QLoRA_Configuration]]\n\nCreate a `LoraConfig` specifically tuned for QLoRA training. The configuration is similar to standard LoRA but typically uses slightly different hyperparameters optimized for quantized training.\n\n'''QLoRA-specific considerations:'''\n* Target all linear layers for maximum effectiveness: `target_modules=\"all-linear\"`\n* Use moderate rank (r=16-64) as quantization adds implicit regularization\n* Set `task_type` appropriately for your use case\n* LoRA weights remain in full precision (float32) despite base model being quantized\n\n=== Step 5: Create QLoRA Model ===\n[[step::Principle:huggingface_peft_PEFT_Model_Creation]]\n\nWrap the quantized model with LoRA configuration using `get_peft_model()`. The LoRA layers are injected in full precision, operating on dequantized outputs from the 4-bit base model.\n\n'''Architecture:'''\n* Base model: 4-bit quantized weights (frozen)\n* LoRA A/B matrices: Full precision trainable weights\n* Forward pass: dequantize \u2192 compute \u2192 apply LoRA delta\n\n=== Step 6: Execute Training ===\n[[step::Principle:huggingface_peft_QLoRA_Training_Execution]]\n\nRun training with appropriate settings for quantized models. Gradient accumulation is often needed to achieve effective batch sizes while staying within memory limits.\n\n'''Training considerations:'''\n* Use gradient accumulation to increase effective batch size\n* Enable gradient checkpointing if memory is still tight\n* Use paged optimizers (e.g., `paged_adamw_8bit`) for additional memory savings\n* Monitor for NaN losses which can indicate precision issues\n\n=== Step 7: Save QLoRA Adapter ===\n[[step::Principle:huggingface_peft_Adapter_Serialization]]\n\nSave the trained LoRA adapter weights. Only the full-precision LoRA weights are saved (not the quantized base model), resulting in a small checkpoint that can be loaded onto any compatible base model.\n\n'''Output:'''\n* Small adapter checkpoint (same size as regular LoRA)\n* Can be loaded with quantized or non-quantized base models\n* Base model reference stored in config for reproducibility\n\n== Execution Diagram ==\n\n{{#mermaid:graph TD\n    A[Configure Quantization] --> B[Load Quantized Model]\n    B --> C[Prepare K-bit Training]\n    C --> D[Configure QLoRA]\n    D --> E[Create QLoRA Model]\n    E --> F[Execute Training]\n    F --> G[Save Adapter]\n}}\n\n== Related Pages ==\n\n* [[step::Principle:huggingface_peft_Quantization_Configuration]]\n* [[step::Principle:huggingface_peft_Quantized_Model_Loading]]\n* [[step::Principle:huggingface_peft_Kbit_Training_Preparation]]\n* [[step::Principle:huggingface_peft_QLoRA_Configuration]]\n* [[step::Principle:huggingface_peft_PEFT_Model_Creation]]\n* [[step::Principle:huggingface_peft_QLoRA_Training_Execution]]\n* [[step::Principle:huggingface_peft_Adapter_Serialization]]\n",
      "domains": [
        "LLMs",
        "Fine Tuning",
        "Quantization",
        "Memory Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Documentation",
          "url": "https://huggingface.co/docs/peft"
        },
        {
          "type": "Paper",
          "title": "QLoRA Paper",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Blog",
          "title": "Fine-tuning LLMs on Consumer Hardware",
          "url": "https://pytorch.org/blog/finetune-llms/"
        }
      ],
      "last_updated": "2025-12-18 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Quantization_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Quantized_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Kbit_Training_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_QLoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_PEFT_Model_Creation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_QLoRA_Training_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Serialization"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Quantization_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Quantized_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Kbit_Training_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_QLoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_PEFT_Model_Creation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_QLoRA_Training_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Serialization"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Adapter_Deletion",
      "page_title": "huggingface peft Adapter Deletion",
      "page_type": "Principle",
      "overview": "Principle for removing adapters from a model to reclaim GPU memory.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Memory_Management]], [[domain::Resource_Cleanup]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for removing adapters from a model to reclaim GPU memory.\n\n=== Description ===\n\nAdapter Deletion permanently removes adapter weights from the model, freeing associated memory. This is essential for:\n* Memory management with many adapters\n* Cleaning up after merging operations\n* Replacing adapters with new versions\n* Resource optimization in production\n\n=== Usage ===\n\nApply this when:\n* An adapter is no longer needed\n* Memory is constrained\n* After merging adapters (delete originals)\n* Cleaning up failed experiments\n\nConstraint: Cannot delete the currently active adapter.\n\n== Theoretical Basis ==\n\n'''Memory Reclamation:'''\n\nDeleting an adapter with rank r frees:\n<math>\\text{Memory freed} = (d \\times r + r \\times k) \\times \\text{sizeof(dtype)}</math>\n\nFor typical dimensions and r=16 in float16:\n<math>\\text{Memory} \\approx 2 \\times 16 \\times (4096 + 4096) \\times 2 \\approx 0.5 \\text{ MB/layer}</math>\n\n'''Deletion Process:'''\n\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for adapter deletion\ndef delete_adapter(self, adapter_name):\n    # Validate not active\n    if adapter_name == self.active_adapter:\n        raise ValueError(\"Cannot delete active adapter\")\n\n    # Remove from all layers\n    for module in self.modules():\n        if isinstance(module, LoraLayer):\n            if adapter_name in module.lora_A:\n                del module.lora_A[adapter_name]\n                del module.lora_B[adapter_name]\n                del module.scaling[adapter_name]\n\n    # Remove config\n    del self.peft_config[adapter_name]\n\n    # Trigger garbage collection\n    torch.cuda.empty_cache()\n</syntaxhighlight>\n\n'''Memory Management Best Practices:'''\n\n<syntaxhighlight lang=\"python\">\n# Pattern: Clean up after merging\nmodel.add_weighted_adapter([\"a\", \"b\"], [0.5, 0.5], \"merged\")\nmodel.set_adapter(\"merged\")\n\n# Now safe to delete originals\nmodel.delete_adapter(\"a\")\nmodel.delete_adapter(\"b\")\n\n# Force GPU memory cleanup\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_delete_adapter]]\n",
      "domains": [
        "Adapter",
        "Memory Management",
        "Resource Cleanup"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_delete_adapter"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Adapter_Enable_Disable",
      "page_title": "huggingface peft Adapter Enable Disable",
      "page_type": "Principle",
      "overview": "Principle for temporarily disabling all adapter effects to run inference on the base model.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Inference]], [[domain::Control]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for temporarily disabling all adapter effects to run inference on the base model.\n\n=== Description ===\n\nAdapter Enable/Disable provides fine-grained control over adapter effects:\n* **Disable:** All adapters bypass, only base model active\n* **Enable:** Re-activate adapter contributions\n* **Scoped:** Context manager ensures adapters re-enable on exit\n\nThis enables base model comparison without unloading adapters.\n\n=== Usage ===\n\nApply this when:\n* Comparing adapted vs. base model outputs\n* Debugging adapter behavior\n* Temporarily needing base model behavior\n* A/B testing adapter effects\n\n== Theoretical Basis ==\n\n'''Disabled Forward Pass:'''\n\nWhen adapters are disabled:\n<math>h = W_0 x</math>\n\nThe LoRA contribution is bypassed entirely.\n\n'''Implementation:'''\n\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for disable mechanism\nclass LoraLayer:\n    def forward(self, x):\n        result = self.base_layer(x)\n\n        if not self.disable_adapters:  # Check flag\n            for adapter_name in self.active_adapters:\n                lora_out = self.lora_B[adapter_name](\n                    self.lora_A[adapter_name](x)\n                )\n                result += lora_out * self.scaling[adapter_name]\n\n        return result\n</syntaxhighlight>\n\n'''Context Manager Pattern:'''\n\nThe context manager ensures proper cleanup:\n<syntaxhighlight lang=\"python\">\n@contextmanager\ndef disable_adapter(self):\n    # Save current state\n    old_state = self._adapters_disabled\n\n    try:\n        # Disable all adapters\n        self._adapters_disabled = True\n        for module in self.modules():\n            if hasattr(module, 'disable_adapters'):\n                module.disable_adapters = True\n        yield\n    finally:\n        # Restore state (even if exception)\n        self._adapters_disabled = old_state\n        for module in self.modules():\n            if hasattr(module, 'disable_adapters'):\n                module.disable_adapters = old_state\n</syntaxhighlight>\n\n'''Use Cases:'''\n\n1. **Baseline comparison:** Compare outputs before/after adaptation\n2. **Debugging:** Isolate issues to adapter vs. base model\n3. **Selective processing:** Some batches need base, some adapted\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_disable_adapter_context]]\n",
      "domains": [
        "Adapter",
        "Inference",
        "Control"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_disable_adapter_context"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Adapter_Loading",
      "page_title": "huggingface peft Adapter Loading",
      "page_type": "Principle",
      "overview": "Principle for loading pre-trained adapter weights onto a base model for inference or continued training.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Model_Loading]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for loading pre-trained adapter weights onto a base model for inference or continued training.\n\n=== Description ===\n\nAdapter Loading reconstructs a trained PEFT model by:\n1. Loading the adapter configuration (JSON)\n2. Injecting adapter layers into the base model\n3. Loading and placing adapter weights\n4. Setting appropriate mode (inference vs. training)\n\nThis enables deployment of task-specific adaptations without redistributing the full model.\n\n=== Usage ===\n\nApply this when you have a trained adapter checkpoint and need to use it:\n* **Inference:** Set `is_trainable=False` to freeze adapters\n* **Continued training:** Set `is_trainable=True` for gradient updates\n* **Hub loading:** Use HuggingFace model ID directly\n* **Local loading:** Use path to saved adapter directory\n\n== Theoretical Basis ==\n\n'''Adapter Reconstruction:'''\n\nLoading restores the adapted weights:\n<math>W_{adapted} = W_0 + BA</math>\n\nWhere:\n* <math>W_0</math> is loaded from the base model\n* <math>B, A</math> are loaded from the adapter checkpoint\n\n'''Weight Placement:'''\n\nThe loading process:\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for adapter loading\nconfig = LoraConfig.from_pretrained(model_id)  # Load JSON config\n\n# Inject layers based on config\nfor target_name in config.target_modules:\n    module = get_module(model, target_name)\n    inject_lora_layer(module, config.r, config.lora_alpha)\n\n# Load trained weights\nstate_dict = load_safetensors(f\"{model_id}/adapter_model.safetensors\")\nmodel.load_state_dict(state_dict, strict=False)\n</syntaxhighlight>\n\n'''Inference Mode:'''\n\nFor inference, adapters are frozen:\n<syntaxhighlight lang=\"python\">\nif not is_trainable:\n    for name, param in model.named_parameters():\n        if \"lora_\" in name:\n            param.requires_grad = False\n    model.eval()  # Set to eval mode\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_PeftModel_from_pretrained]]\n",
      "domains": [
        "Adapter",
        "Model Loading",
        "Inference"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_PeftModel_from_pretrained"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Adapter_Merge_Execution",
      "page_title": "huggingface peft Adapter Merge Execution",
      "page_type": "Principle",
      "overview": "Principle for combining multiple adapters into a single merged adapter using advanced merging algorithms.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|TIES|https://arxiv.org/abs/2306.01708]]\n* [[source::Paper|DARE|https://arxiv.org/abs/2311.03099]]\n* [[source::Paper|Task Arithmetic|https://arxiv.org/abs/2212.04089]]\n|-\n! Domains\n| [[domain::Model_Merging]], [[domain::Multi_Task]], [[domain::Adapter]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for combining multiple adapters into a single merged adapter using advanced merging algorithms.\n\n=== Description ===\n\nAdapter Merge Execution combines task vectors (adapter deltas) using algorithms that handle:\n* Weight interpolation with arbitrary weights\n* Sign conflicts between adapters (TIES)\n* Sparsification for robustness (DARE)\n* Rank reduction via SVD\n\nDifferent algorithms suit different scenarios\u2014linear for simple interpolation, TIES for conflicting adapters, DARE for robust merging.\n\n=== Usage ===\n\nApply this to combine capabilities from multiple trained adapters:\n* **Linear:** Simple weighted average, fast, may average out conflicts\n* **TIES:** Resolves sign conflicts, good for diverse adapters\n* **DARE-TIES:** Adds random sparsification, more robust\n* **SVD:** Controls output rank, memory efficient\n\n== Theoretical Basis ==\n\n'''Task Arithmetic (Linear):'''\n\nSimple weighted sum of task vectors:\n<math>\\Delta W_{merged} = \\sum_i w_i \\cdot \\Delta W_i</math>\n\nWhere <math>\\Delta W_i = B_i A_i \\cdot \\text{scaling}_i</math>\n\n'''TIES (TrIm, Elect Sign, Merge):'''\n\n1. **Trim:** Keep top-k% by magnitude\n2. **Elect Sign:** Determine majority sign per parameter\n3. **Merge:** Sum only agreeing signs\n\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for TIES\ndef ties_merge(task_tensors, weights, density):\n    # 1. Trim: Keep top density% parameters\n    trimmed = [magnitude_prune(t, density) for t in task_tensors]\n\n    # 2. Elect: Find majority sign\n    signs = torch.stack([t.sign() for t in trimmed])\n    majority_sign = signs.sum(dim=0).sign()\n\n    # 3. Merge: Only include agreeing signs\n    mask = (signs == majority_sign)\n    merged = (trimmed * weights * mask).sum(dim=0)\n\n    return merged / mask.sum(dim=0).clamp(min=1)\n</syntaxhighlight>\n\n'''DARE (Drop And REscale):'''\n\nRandom pruning with rescaling:\n<math>\\Delta W_{dare} = \\frac{\\text{Bernoulli}(p) \\cdot \\Delta W}{p}</math>\n\nReduces sensitivity to specific parameters while maintaining expected value.\n\n'''SVD Rank Reduction:'''\n\nReduce merged adapter rank via SVD:\n<syntaxhighlight lang=\"python\">\ndef svd_merge(delta_w, target_rank):\n    U, S, V = torch.linalg.svd(delta_w)\n    U_r = U[:, :target_rank]\n    S_r = S[:target_rank]\n    V_r = V[:target_rank, :]\n\n    # New A and B matrices\n    A_new = (torch.diag(S_r.sqrt()) @ V_r)\n    B_new = (U_r @ torch.diag(S_r.sqrt()))\n\n    return A_new, B_new\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_add_weighted_adapter]]\n",
      "domains": [
        "Model Merging",
        "Multi Task",
        "Adapter"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "TIES",
          "url": "https://arxiv.org/abs/2306.01708"
        },
        {
          "type": "Paper",
          "title": "DARE",
          "url": "https://arxiv.org/abs/2311.03099"
        },
        {
          "type": "Paper",
          "title": "Task Arithmetic",
          "url": "https://arxiv.org/abs/2212.04089"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_add_weighted_adapter"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Adapter_Merging_Into_Base",
      "page_title": "huggingface peft Adapter Merging Into Base",
      "page_type": "Principle",
      "overview": "Principle for permanently merging adapter weights into the base model to create a standalone deployable model.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Deployment]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for permanently merging adapter weights into the base model to create a standalone deployable model.\n\n=== Description ===\n\nAdapter Merging combines the low-rank adapter matrices with the frozen base weights to produce a standard model. The resulting model:\n* Has no PEFT dependencies\n* Runs with standard inference code\n* May have faster inference (no adapter computation)\n* Cannot switch adapters (irreversible)\n\n=== Usage ===\n\nApply this when deploying a model where:\n* Adapter switching is not needed\n* PEFT dependency should be removed\n* Maximum inference speed is required\n* Model will be shared without PEFT context\n\n== Theoretical Basis ==\n\n'''Weight Merging:'''\n\nThe core merge operation:\n<math>W_{merged} = W_0 + \\frac{\\alpha}{r} \\cdot BA</math>\n\nWhere:\n* <math>W_0</math> is the original frozen weight\n* <math>\\frac{\\alpha}{r}</math> is the LoRA scaling factor\n* <math>BA</math> is the learned low-rank update\n\n'''Merge Implementation:'''\n\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for merge operation\ndef merge_lora_weights(layer):\n    delta_w = (layer.lora_B.weight @ layer.lora_A.weight) * layer.scaling\n    layer.weight.data += delta_w\n\n    # Remove LoRA components\n    del layer.lora_A\n    del layer.lora_B\n</syntaxhighlight>\n\n'''Safe Merge:'''\n\nSafe merge checks for numerical issues:\n<syntaxhighlight lang=\"python\">\ndef safe_merge(layer):\n    delta_w = layer.lora_B.weight @ layer.lora_A.weight\n\n    # Check for NaNs or Infs\n    if torch.isnan(delta_w).any() or torch.isinf(delta_w).any():\n        raise ValueError(\"NaN/Inf detected in adapter weights\")\n\n    layer.weight.data += delta_w * layer.scaling\n</syntaxhighlight>\n\n'''Unload Process:'''\n\nAfter merging, the PEFT wrapper is removed:\n<syntaxhighlight lang=\"python\">\n# Replace LoRA layer with merged Linear\nfor parent, name, module in model.named_modules():\n    if isinstance(module, LoraLayer):\n        merged_linear = module.get_base_layer()  # Already merged\n        setattr(parent, name, merged_linear)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_merge_and_unload]]\n",
      "domains": [
        "Adapter",
        "Deployment",
        "Inference"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_merge_and_unload"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Adapter_Serialization",
      "page_title": "huggingface peft Adapter Serialization",
      "page_type": "Principle",
      "overview": "Principle for saving trained adapter weights independently of the base model for efficient storage and sharing.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|Safetensors|https://huggingface.co/docs/safetensors]]\n|-\n! Domains\n| [[domain::Serialization]], [[domain::Adapter]], [[domain::Persistence]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for saving trained adapter weights independently of the base model for efficient storage and sharing.\n\n=== Description ===\n\nAdapter Serialization saves only the trained adapter parameters, not the full model. This results in dramatically smaller checkpoint sizes\u2014typically 10-100MB for adapters vs. multiple GB for full models. The serialized adapter includes:\n1. Adapter weights (in safetensors or PyTorch format)\n2. Configuration (as JSON)\n3. Optional model card for documentation\n\nThis enables efficient sharing, version control, and deployment of task-specific adaptations.\n\n=== Usage ===\n\nApply this principle after training completes:\n* **Standard save:** Use `model.save_pretrained()` for local storage\n* **Hub upload:** Use `model.push_to_hub()` for sharing\n* **Multi-adapter:** Use `selected_adapters` to save specific adapters\n* **Compatibility:** Enable `safe_serialization=True` for safetensors format\n\n== Theoretical Basis ==\n\n'''Selective State Dict:'''\n\nPEFT extracts only adapter-related weights:\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for adapter state dict extraction\ndef get_peft_model_state_dict(model, adapter_name):\n    state_dict = {}\n\n    for name, param in model.named_parameters():\n        if should_save(name, adapter_name):\n            # Include: lora_A, lora_B, modules_to_save\n            state_dict[name] = param.data\n        # Exclude: base model weights (not saved)\n\n    return state_dict\n</syntaxhighlight>\n\n'''Storage Efficiency:'''\n\nFor a 7B parameter model with LoRA (r=16) on attention layers:\n* Base model: ~14 GB (float16)\n* LoRA adapter: ~40 MB\n\nSavings ratio:\n<math>\\text{Reduction} = 1 - \\frac{\\text{adapter size}}{\\text{model size}} \\approx 99.7\\%</math>\n\n'''Safetensors Format:'''\n\nSafetensors provides:\n* Security: No arbitrary code execution\n* Speed: Memory-mapped loading\n* Compatibility: Cross-framework support\n\n'''Configuration Persistence:'''\n\nThe adapter config JSON includes all hyperparameters:\n<syntaxhighlight lang=\"json\">\n{\n  \"peft_type\": \"LORA\",\n  \"r\": 16,\n  \"lora_alpha\": 32,\n  \"target_modules\": [\"q_proj\", \"v_proj\"],\n  \"lora_dropout\": 0.05,\n  \"bias\": \"none\",\n  \"task_type\": \"CAUSAL_LM\",\n  \"base_model_name_or_path\": \"meta-llama/Llama-2-7b-hf\"\n}\n</syntaxhighlight>\n\nThis enables automatic reconstruction when loading.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_PeftModel_save_pretrained]]\n",
      "domains": [
        "Serialization",
        "Adapter",
        "Persistence"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "Safetensors",
          "url": "https://huggingface.co/docs/safetensors"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_PeftModel_save_pretrained"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Adapter_State_Query",
      "page_title": "huggingface peft Adapter State Query",
      "page_type": "Principle",
      "overview": "Principle for inspecting the state of loaded adapters on a PEFT model, including active adapter identification and configuration retrieval.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Documentation|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::Multi_Task]], [[domain::Adapter]], [[domain::Model_Management]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for inspecting the state of loaded adapters on a PEFT model, including active adapter identification and configuration retrieval.\n\n=== Description ===\n\nAdapter State Query provides introspection capabilities for multi-adapter PEFT models. This includes:\n* Identifying which adapter(s) are currently active\n* Retrieving configuration for all loaded adapters\n* Getting detailed model status including adapter types and trainability\n\nThis is essential for debugging, logging, and conditional logic in multi-adapter serving scenarios.\n\n=== Usage ===\n\nApply this principle when:\n* Building multi-adapter serving systems that need runtime introspection\n* Debugging adapter switching issues\n* Logging which adapter was used for a given request\n* Implementing conditional logic based on adapter state\n\n== Theoretical Basis ==\n\n'''Adapter Registry:'''\n\nPEFT maintains internal registries tracking loaded adapters:\n\n<syntaxhighlight lang=\"python\">\nclass PeftModel:\n    peft_config: dict[str, PeftConfig]  # name -> config mapping\n    active_adapter: Union[str, list[str]]  # currently active adapter(s)\n</syntaxhighlight>\n\n'''State Properties:'''\n\n* `model.peft_config` - Dictionary mapping adapter names to their configurations\n* `model.active_adapter` - String or list of strings indicating active adapter(s)\n* `model.get_model_status()` - Detailed breakdown of adapter types and states\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_query_adapter_state]]\n",
      "domains": [
        "Multi Task",
        "Adapter",
        "Model Management"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Documentation",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_query_adapter_state"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Adapter_Switching",
      "page_title": "huggingface peft Adapter Switching",
      "page_type": "Principle",
      "overview": "Principle for dynamically selecting which adapter(s) affect model outputs during inference.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Multi_Task]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for dynamically selecting which adapter(s) affect model outputs during inference.\n\n=== Description ===\n\nAdapter Switching enables rapid task switching by selecting which loaded adapter is \"active\" during forward passes. Only active adapters contribute to the model output. This enables:\n* Single-adapter mode: One adapter active\n* Multi-adapter mode: Multiple adapters sum their contributions\n* No-adapter mode: Base model only (via disable_adapter)\n\n=== Usage ===\n\nApply this when:\n* Different inputs require different adaptations\n* Comparing outputs across adapters\n* Dynamically selecting task behavior\n* Combining multiple adapter effects\n\n== Theoretical Basis ==\n\n'''Single Adapter Selection:'''\n\nWith adapter <math>i</math> selected:\n<math>h = W_0 x + \\frac{\\alpha_i}{r_i} B_i A_i x</math>\n\nOther adapters are present but don't contribute to output.\n\n'''Multi-Adapter Combination:'''\n\nWhen multiple adapters are active:\n<math>h = W_0 x + \\sum_{i \\in \\text{active}} \\frac{\\alpha_i}{r_i} B_i A_i x</math>\n\nContributions are summed (not averaged).\n\n'''Implementation:'''\n\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for adapter selection\nclass LoraLayer:\n    def forward(self, x):\n        result = self.base_layer(x)\n\n        # Only active adapters contribute\n        for adapter_name in self.active_adapters:\n            if adapter_name in self.lora_A:\n                lora_out = self.lora_B[adapter_name](\n                    self.lora_A[adapter_name](x)\n                )\n                result += lora_out * self.scaling[adapter_name]\n\n        return result\n</syntaxhighlight>\n\n'''Switching Cost:'''\n\nAdapter switching is effectively zero-cost:\n* No weight copies\n* No recomputation\n* Just pointer/index update\n\n<syntaxhighlight lang=\"python\">\ndef set_adapter(self, adapter_name):\n    # O(1) operation - just update active list\n    self.active_adapter = adapter_name\n    for module in self.modules():\n        if hasattr(module, 'active_adapter'):\n            module.active_adapter = adapter_name\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_set_adapter]]\n",
      "domains": [
        "Adapter",
        "Multi Task",
        "Inference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_set_adapter"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Base_Model_Loading",
      "page_title": "huggingface peft Base Model Loading",
      "page_type": "Principle",
      "overview": "Principle for loading pre-trained transformer models as the foundation for parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Doc|Transformers Docs|https://huggingface.co/docs/transformers]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Model_Loading]], [[domain::Transfer_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for loading pre-trained transformer models as the foundation for parameter-efficient fine-tuning.\n\n=== Description ===\n\nBase Model Loading is the first step in any PEFT workflow. It involves loading a pre-trained transformer model that will serve as the frozen foundation for adapter training. The principle emphasizes that the base model's weights remain unchanged during fine-tuning\u2014only the adapter layers are trained. This enables efficient training while preserving the model's pre-trained knowledge.\n\nThe key insight is that modern LLMs contain general knowledge that should be preserved, while adapters learn task-specific modifications. Proper loading includes selecting appropriate precision (float16/bfloat16) and device placement strategies.\n\n=== Usage ===\n\nApply this principle at the start of any LoRA/PEFT fine-tuning workflow. Consider:\n* **Model selection:** Choose a base model appropriate for your task (decoder-only for generation, encoder-decoder for seq2seq)\n* **Precision:** Use float16 or bfloat16 for memory efficiency on consumer GPUs\n* **Device mapping:** Use \"auto\" for multi-GPU setups, explicit GPU ID for single GPU\n* **Attention implementation:** Flash Attention 2 for speed, SDPA for compatibility\n\n== Theoretical Basis ==\n\nThe foundational principle behind PEFT is that pre-trained models form a \"good starting point\" in parameter space. The base model represents:\n\n'''Frozen Knowledge:'''\n<math>W_0 \\in \\mathbb{R}^{d \\times k}</math>\n\nWhere <math>W_0</math> contains pre-trained weights that encode:\n- Language understanding (syntax, semantics)\n- World knowledge from pre-training corpus\n- Task-general representations\n\n'''Adaptation Hypothesis:'''\n\nLoRA hypothesizes that task-specific adaptation can be achieved through low-rank updates:\n<math>W = W_0 + \\Delta W</math>\n\nWhere <math>\\Delta W = BA</math> with <math>B \\in \\mathbb{R}^{d \\times r}</math> and <math>A \\in \\mathbb{R}^{r \\times k}</math>, and <math>r \\ll min(d, k)</math>.\n\nThis only works if <math>W_0</math> is properly initialized\u2014hence the importance of correct base model loading.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_AutoModelForCausalLM_from_pretrained]]\n",
      "domains": [
        "NLP",
        "Model Loading",
        "Transfer Learning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Doc",
          "title": "Transformers Docs",
          "url": "https://huggingface.co/docs/transformers"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_AutoModelForCausalLM_from_pretrained"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Inference_Configuration",
      "page_title": "huggingface peft Inference Configuration",
      "page_type": "Principle",
      "overview": "Principle for configuring a PEFT model for efficient inference by setting evaluation mode and disabling gradient computation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PyTorch|https://pytorch.org/docs/stable/generated/torch.nn.Module.html]]\n|-\n! Domains\n| [[domain::Inference]], [[domain::Model_Serving]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for configuring a PEFT model for efficient inference by setting evaluation mode and disabling gradient computation.\n\n=== Description ===\n\nInference Configuration prepares a loaded PEFT model for production inference. This involves:\n* Setting the model to evaluation mode (`model.eval()`)\n* Disabling dropout layers for deterministic outputs\n* Disabling gradient tracking to reduce memory usage\n* Optionally enabling inference optimizations like torch.compile\n\nThis step ensures optimal inference performance and deterministic behavior.\n\n=== Usage ===\n\nApply this principle after loading a PEFT adapter and before running inference:\n* Call `model.eval()` to disable training-specific behaviors\n* Wrap inference in `torch.no_grad()` context for memory efficiency\n* Consider `torch.compile()` for repeated inference workloads\n\n== Theoretical Basis ==\n\n'''Evaluation Mode Effects:'''\n\nWhen `model.eval()` is called:\n* Dropout layers pass inputs unchanged (no random dropping)\n* BatchNorm uses running statistics instead of batch statistics\n* Model becomes deterministic given the same input\n\n'''No Gradient Context:'''\n\nUsing `torch.no_grad()`:\n* Disables gradient computation in autograd\n* Reduces memory usage (no gradient tensors stored)\n* Slightly faster forward passes\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_model_eval]]\n",
      "domains": [
        "Inference",
        "Model Serving",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PyTorch",
          "url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_model_eval"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Inference_Execution",
      "page_title": "huggingface peft Inference Execution",
      "page_type": "Principle",
      "overview": "Principle for executing inference through a PEFT model, including text generation and forward passes with adapter-augmented behavior.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|Transformers Generation|https://huggingface.co/docs/transformers/main_classes/text_generation]]\n|-\n! Domains\n| [[domain::Inference]], [[domain::Text_Generation]], [[domain::Model_Serving]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for executing inference through a PEFT model, including text generation and forward passes with adapter-augmented behavior.\n\n=== Description ===\n\nInference Execution covers running forward passes and generation through PEFT models. The adapter modifies the base model's behavior according to its learned weights. Common patterns include:\n* Text generation using `model.generate()` for autoregressive models\n* Classification via forward pass and logit extraction\n* Embedding extraction from intermediate layers\n\nThe PEFT wrapper is transparent to inference - standard transformers methods work unchanged.\n\n=== Usage ===\n\nApply this principle when running inference on a loaded PEFT model:\n* Use `model.generate()` for text generation with sampling/beam search\n* Use direct forward pass for classification or embedding tasks\n* Configure generation parameters appropriate for your use case\n\n== Theoretical Basis ==\n\n'''Adapter-Augmented Forward Pass:'''\n\nDuring inference, each adapted layer computes:\n<math>h = W_0 x + B A x = (W_0 + BA) x</math>\n\nWhere:\n* <math>W_0</math> is the frozen base model weight\n* <math>B</math>, <math>A</math> are the learned LoRA matrices\n* The adapter adds a task-specific modification to the base output\n\n'''Generation Process:'''\n\nFor autoregressive generation:\n1. Forward pass through adapted model\n2. Sample/select next token from output logits\n3. Append token and repeat\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_model_generate]]\n",
      "domains": [
        "Inference",
        "Text Generation",
        "Model Serving"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "Transformers Generation",
          "url": "https://huggingface.co/docs/transformers/main_classes/text_generation"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_model_generate"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Kbit_Training_Preparation",
      "page_title": "huggingface peft Kbit Training Preparation",
      "page_type": "Principle",
      "overview": "Principle for preparing quantized models for stable training by ensuring proper dtype handling and gradient flow.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n* [[source::Doc|PEFT Quantization|https://huggingface.co/docs/peft/developer_guides/quantization]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::Training]], [[domain::Memory_Efficiency]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for preparing quantized models for stable training by ensuring proper dtype handling and gradient flow.\n\n=== Description ===\n\nK-bit Training Preparation addresses numerical stability challenges when training with quantized models. Key preparations include:\n1. Casting layer norms to float32 (prevents gradient instability)\n2. Enabling gradient checkpointing (trades compute for memory)\n3. Setting up proper gradient flow through input embeddings\n4. Freezing all base model parameters\n\nWithout these preparations, QLoRA training often diverges or produces NaN gradients.\n\n=== Usage ===\n\nApply this immediately after loading a quantized model and before applying PEFT:\n* Always call `prepare_model_for_kbit_training()` for quantized models\n* Enable gradient checkpointing by default (saves ~30% VRAM)\n* Use `use_reentrant=False` for newer PyTorch versions\n\n== Theoretical Basis ==\n\n'''Layer Norm Stability:'''\n\nLayer normalization with quantized inputs can cause instability:\n<math>\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta</math>\n\nWhen <math>x</math> comes from dequantized values, the statistics <math>\\mu, \\sigma^2</math> can have high variance. Keeping layer norms in float32 ensures:\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for dtype handling\nfor module in model.modules():\n    if isinstance(module, LayerNorm):\n        module.weight = module.weight.to(torch.float32)\n        module.bias = module.bias.to(torch.float32)\n</syntaxhighlight>\n\n'''Gradient Checkpointing:'''\n\nStandard memory: <math>O(L)</math> activations (L = layers)\nCheckpointed: <math>O(\\sqrt{L})</math> activations\n\nTrade-off: ~33% extra forward compute for ~50% memory reduction.\n\n'''Input Gradient Flow:'''\n\nQLoRA requires gradients to flow through embeddings:\n<syntaxhighlight lang=\"python\">\n# Without this hook, gradients don't propagate to LoRA layers\ndef make_inputs_require_grad(module, input, output):\n    output.requires_grad_(True)\n\nmodel.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_prepare_model_for_kbit_training]]\n",
      "domains": [
        "Quantization",
        "Training",
        "Memory Efficiency"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Doc",
          "title": "PEFT Quantization",
          "url": "https://huggingface.co/docs/peft/developer_guides/quantization"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_prepare_model_for_kbit_training"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_LoRA_Configuration",
      "page_title": "huggingface peft LoRA Configuration",
      "page_type": "Principle",
      "overview": "Principle for configuring Low-Rank Adaptation hyperparameters to balance model capacity, training efficiency, and task performance.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Paper|RSLoRA|https://arxiv.org/abs/2312.03732]]\n* [[source::Paper|DoRA|https://arxiv.org/abs/2402.09353]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Parameter_Efficient]], [[domain::Configuration]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for configuring Low-Rank Adaptation hyperparameters to balance model capacity, training efficiency, and task performance.\n\n=== Description ===\n\nLoRA Configuration determines how adapter layers modify the base model. The core hyperparameters\u2014rank (r), alpha, target modules, and dropout\u2014control the expressiveness of the adaptation. The configuration represents a trade-off: higher rank increases capacity but also parameter count, while lower rank is more efficient but may underfit complex tasks.\n\nKey configuration decisions include:\n* **Rank (r):** The dimensionality of the low-rank matrices (typical: 8-64)\n* **Alpha:** Scaling factor that controls the magnitude of updates (alpha/r)\n* **Target modules:** Which layers to adapt (attention vs. all linear)\n* **Variants:** RSLoRA (rank-stabilized scaling), DoRA (weight decomposition)\n\n=== Usage ===\n\nApply this principle when designing your LoRA adaptation strategy:\n* **Simple tasks:** Use lower rank (r=4-8) with default alpha\n* **Complex tasks:** Use higher rank (r=16-64) or DoRA\n* **Memory-constrained:** Target only attention (q,v projections)\n* **Best performance:** Target all linear layers with moderate rank\n\n== Theoretical Basis ==\n\n'''Low-Rank Decomposition:'''\n\nLoRA approximates weight updates as a low-rank matrix:\n<math>\\Delta W = BA</math>\n\nWhere:\n* <math>A \\in \\mathbb{R}^{r \\times k}</math> (down-projection)\n* <math>B \\in \\mathbb{R}^{d \\times r}</math> (up-projection)\n* <math>r</math> is the rank (hyperparameter)\n\n'''Scaling Factor:'''\n\nThe effective update is scaled by alpha/r:\n<math>h = W_0 x + \\frac{\\alpha}{r} \\cdot BAx</math>\n\nThis scaling ensures that the initialization (B=0) results in the original model behavior.\n\n'''Rank-Stabilized LoRA (RSLoRA):'''\n\nStandard scaling uses <math>\\alpha/r</math>, but RSLoRA uses:\n<math>h = W_0 x + \\frac{\\alpha}{\\sqrt{r}} \\cdot BAx</math>\n\nThis stabilizes training at different rank values.\n\n'''DoRA (Weight-Decomposed LoRA):'''\n\nDoRA decomposes weight updates into magnitude and direction:\n<math>W' = m \\cdot \\frac{W_0 + BA}{\\|W_0 + BA\\|_c}</math>\n\nWhere <math>m</math> is a learnable magnitude vector, improving performance especially at low ranks.\n\n'''Target Module Selection:'''\n\nPseudo-code for determining which modules to adapt:\n<syntaxhighlight lang=\"python\">\n# Abstract selection logic\nif target_modules == \"all-linear\":\n    # Adapt all nn.Linear layers except output\n    targets = find_all_linear_modules(model)\nelse:\n    # Adapt specific modules by name pattern\n    targets = match_module_names(model, target_modules)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_LoraConfig_init]]\n\n=== Uses Heuristic ===\n* [[uses_heuristic::Heuristic:huggingface_peft_LoRA_Rank_Selection]]\n",
      "domains": [
        "Fine Tuning",
        "Parameter Efficient",
        "Configuration"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Paper",
          "title": "RSLoRA",
          "url": "https://arxiv.org/abs/2312.03732"
        },
        {
          "type": "Paper",
          "title": "DoRA",
          "url": "https://arxiv.org/abs/2402.09353"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_LoraConfig_init"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "huggingface_peft_LoRA_Rank_Selection"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Merge_Evaluation",
      "page_title": "huggingface peft Merge Evaluation",
      "page_type": "Principle",
      "overview": "Principle for evaluating merged adapter performance to verify capability retention from source adapters.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|Task Arithmetic|https://arxiv.org/abs/2212.04089]]\n|-\n! Domains\n| [[domain::Model_Merging]], [[domain::Evaluation]], [[domain::Multi_Task]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for evaluating merged adapter performance to verify capability retention from source adapters.\n\n=== Description ===\n\nMerge Evaluation assesses the quality of a merged adapter by testing on tasks from each source adapter. Merging can cause degradation on individual tasks (negative transfer), so evaluation helps determine if the merge was successful and whether to adjust parameters.\n\nKey concerns:\n* Does the merged adapter retain capabilities from each source?\n* Is performance comparable to individual adapters?\n* Are there signs of negative transfer or catastrophic forgetting?\n\n=== Usage ===\n\nApply this principle after executing adapter merge:\n* Test on representative samples from each source task\n* Compare metrics to individual adapter baselines\n* Iterate on merge parameters (weights, density) if needed\n\n== Theoretical Basis ==\n\n'''Evaluation Metrics:'''\n\nFor merged adapters, evaluate:\n* Task-specific metrics (accuracy, perplexity, F1, etc.)\n* Average performance across all source tasks\n* Performance drop relative to individual adapters\n\n'''Pareto Efficiency:'''\n\nIdeal merges approach the Pareto frontier where improving one task would harm another. Significant degradation indicates the merge parameters need adjustment.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_merged_adapter_evaluation]]\n",
      "domains": [
        "Model Merging",
        "Evaluation",
        "Multi Task"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "Task Arithmetic",
          "url": "https://arxiv.org/abs/2212.04089"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_merged_adapter_evaluation"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Merge_Strategy_Configuration",
      "page_title": "huggingface peft Merge Strategy Configuration",
      "page_type": "Principle",
      "overview": "Principle for selecting and configuring adapter merging algorithms, including strategy selection (TIES, DARE, linear) and parameter tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|TIES|https://arxiv.org/abs/2306.01708]]\n* [[source::Paper|DARE|https://arxiv.org/abs/2311.03099]]\n|-\n! Domains\n| [[domain::Model_Merging]], [[domain::Multi_Task]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for selecting and configuring adapter merging algorithms, including strategy selection (TIES, DARE, linear) and parameter tuning.\n\n=== Description ===\n\nMerge Strategy Configuration involves selecting the appropriate merging algorithm and its hyperparameters. Different algorithms suit different scenarios:\n\n* **Linear**: Simple weighted average, fast but may average out conflicts\n* **TIES**: Trims, elects sign, merges - good for adapters with conflicting updates\n* **DARE**: Random pruning with rescaling - robust to parameter redundancy\n* **SVD**: Reduces output rank for memory efficiency\n\n=== Usage ===\n\nApply this principle before executing adapter merge:\n* Choose algorithm based on adapter characteristics\n* Set weights reflecting adapter importance\n* Tune density for TIES/DARE (0.5-0.9 typical)\n\n== Theoretical Basis ==\n\n'''Algorithm Selection Guide:'''\n\n| Scenario | Recommended Algorithm |\n|----------|----------------------|\n| Similar tasks, no conflicts | Linear |\n| Diverse tasks, potential sign conflicts | TIES |\n| Need robustness to specific weights | DARE Linear |\n| Diverse + robust | DARE TIES |\n| Memory constrained output | SVD variants |\n\n'''Key Parameters:'''\n\n* `weights`: Per-adapter importance (sum to 1.0 typical)\n* `density`: Fraction of weights to keep (0.0-1.0)\n* `majority_sign_method`: \"total\" (sum) or \"frequency\" (count)\n* `svd_rank`: Target rank for SVD methods\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_merge_strategy_selection]]\n",
      "domains": [
        "Model Merging",
        "Multi Task",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "TIES",
          "url": "https://arxiv.org/abs/2306.01708"
        },
        {
          "type": "Paper",
          "title": "DARE",
          "url": "https://arxiv.org/abs/2311.03099"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_merge_strategy_selection"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Multi_Adapter_Loading",
      "page_title": "huggingface peft Multi Adapter Loading",
      "page_type": "Principle",
      "overview": "Principle for loading multiple task-specific adapters into a single model for efficient multi-task inference.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Multi-Adapter|https://huggingface.co/docs/peft/conceptual_guides/lora#manage-multiple-adapters]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Multi_Task]], [[domain::Model_Loading]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for loading multiple task-specific adapters into a single model for efficient multi-task inference.\n\n=== Description ===\n\nMulti-Adapter Loading enables a single base model to serve multiple tasks by:\n1. Loading additional adapters after the initial PEFT setup\n2. Storing multiple sets of adapter weights in memory\n3. Enabling rapid switching between adapters\n4. Sharing the base model parameters across all adapters\n\nThis is more memory-efficient than loading multiple full models.\n\n=== Usage ===\n\nApply this when:\n* One model needs to serve multiple tasks\n* Switching between behaviors at inference time\n* Memory efficiency is important (share base model)\n* Tasks use compatible adapter configurations\n\n== Theoretical Basis ==\n\n'''Memory Efficiency:'''\n\nMulti-adapter vs. multi-model memory:\n\nSingle model, N adapters:\n<math>\\text{Memory} = |W_0| + N \\times |BA|</math>\n\nN separate models:\n<math>\\text{Memory} = N \\times |W_0|</math>\n\nSavings ratio (for typical r=16, 7B model):\n<math>\\text{Savings} = \\frac{N \\times |W_0|}{|W_0| + N \\times |BA|} \\approx N</math>\n\n'''Adapter Storage:'''\n\nEach LoRA layer maintains a dictionary of adapters:\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for multi-adapter storage\nclass LoraLayer:\n    def __init__(self):\n        self.lora_A = {}  # adapter_name -> weight\n        self.lora_B = {}  # adapter_name -> weight\n\n    def add_adapter(self, name, r, alpha):\n        self.lora_A[name] = nn.Parameter(torch.randn(r, in_features))\n        self.lora_B[name] = nn.Parameter(torch.zeros(out_features, r))\n</syntaxhighlight>\n\n'''Forward Pass Selection:'''\n\nDuring forward, only active adapter contributes:\n<syntaxhighlight lang=\"python\">\ndef forward(self, x, adapter_name):\n    base_output = self.base_layer(x)\n\n    if adapter_name in self.lora_A:\n        lora_output = self.lora_B[adapter_name](self.lora_A[adapter_name](x))\n        return base_output + lora_output * self.scaling\n\n    return base_output\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_load_adapter]]\n",
      "domains": [
        "Adapter",
        "Multi Task",
        "Model Loading"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Multi-Adapter",
          "url": "https://huggingface.co/docs/peft/conceptual_guides/lora#manage-multiple-adapters"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_load_adapter"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_PEFT_Model_Creation",
      "page_title": "huggingface peft PEFT Model Creation",
      "page_type": "Principle",
      "overview": "Principle for injecting adapter layers into a pre-trained model to create a parameter-efficient fine-tuning setup.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Adapter]], [[domain::Model_Architecture]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for injecting adapter layers into a pre-trained model to create a parameter-efficient fine-tuning setup.\n\n=== Description ===\n\nPEFT Model Creation transforms a standard pre-trained model into an adapter-enabled model. This involves:\n1. Freezing all base model parameters\n2. Injecting low-rank adapter layers at specified locations\n3. Initializing adapter weights (B=0 for no-op initialization)\n4. Setting up proper forward pass with adapter contributions\n\nThe result is a model where only a small fraction (typically <1%) of parameters are trainable, dramatically reducing memory requirements for fine-tuning.\n\n=== Usage ===\n\nApply this principle after loading the base model and configuring LoRA:\n* **Standard training:** Use default autocast settings for stable training\n* **Multi-adapter:** Pass `adapter_name` to create named adapters\n* **Mixed adapters:** Use `mixed=True` to combine different PEFT types\n* **Memory optimization:** Use `low_cpu_mem_usage=True` for large models\n\n== Theoretical Basis ==\n\n'''Adapter Injection:'''\n\nThe model modification replaces original linear layers with adapter-augmented versions:\n\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for adapter injection\nfor layer in find_target_modules(model, config.target_modules):\n    # Freeze original weights\n    layer.weight.requires_grad = False\n\n    # Add low-rank adapter\n    layer.lora_A = nn.Parameter(torch.randn(r, in_features))\n    layer.lora_B = nn.Parameter(torch.zeros(out_features, r))\n</syntaxhighlight>\n\n'''Forward Pass Modification:'''\n\nThe adapted forward pass becomes:\n<math>h = W_0 x + \\frac{\\alpha}{r} \\cdot B(Ax)</math>\n\nWhere:\n* <math>W_0 x</math> is the original (frozen) computation\n* <math>B(Ax)</math> is the low-rank adaptation\n* <math>\\alpha/r</math> is the scaling factor\n\n'''Initialization:'''\n\nStandard LoRA initialization:\n* <math>A</math>: Kaiming uniform initialization\n* <math>B</math>: Zero initialization\n\nThis ensures the adapted model starts as the original model (since <math>BA = 0</math>).\n\n'''Parameter Efficiency:'''\n\nFor a linear layer with dimensions <math>d \\times k</math>:\n* Original parameters: <math>dk</math>\n* LoRA parameters: <math>r(d + k)</math>\n* Savings ratio: <math>\\frac{r(d+k)}{dk} \\approx \\frac{r}{min(d,k)}</math>\n\nFor typical LLM dimensions (d=k=4096) and r=16:\n<math>\\text{Ratio} = \\frac{16 \\times 8192}{4096 \\times 4096} \\approx 0.8\\%</math>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_get_peft_model]]\n",
      "domains": [
        "Fine Tuning",
        "Adapter",
        "Model Architecture"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_get_peft_model"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_QLoRA_Configuration",
      "page_title": "huggingface peft QLoRA Configuration",
      "page_type": "Principle",
      "overview": "Principle for configuring LoRA parameters specifically optimized for quantized (QLoRA) training.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Quantization]], [[domain::Memory_Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for configuring LoRA parameters specifically optimized for quantized (QLoRA) training.\n\n=== Description ===\n\nQLoRA Configuration creates a LoraConfig tuned for training on 4-bit quantized models. Key differences from standard LoRA config:\n\n* **target_modules=\"all-linear\"**: QLoRA benefits from adapting all linear layers since quantization compresses the base model\n* **Task type specification**: Required for proper adapter injection\n* **Typical ranks**: r=16-64 common as quantization adds implicit regularization\n\nThe adapter weights remain in full precision (float32) despite the base model being quantized.\n\n=== Usage ===\n\nApply when setting up LoRA configuration for a quantized model:\n* Use \"all-linear\" for comprehensive adaptation\n* Consider slightly higher ranks than standard LoRA\n* Always specify task_type for proper layer targeting\n\n== Theoretical Basis ==\n\n'''QLoRA Architecture:'''\n\n<syntaxhighlight lang=\"text\">\nBase Model (4-bit quantized, frozen)\n         |\n         v\n    Dequantize\n         |\n         v\n    W_0 * x (full precision compute)\n         |\n         v\n    + B*A*x (LoRA, float32)\n         |\n         v\n       Output\n</syntaxhighlight>\n\nThe LoRA matrices B and A remain in full precision, ensuring stable gradient flow despite quantized base weights.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_LoraConfig_for_qlora]]\n",
      "domains": [
        "Fine Tuning",
        "Quantization",
        "Memory Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_LoraConfig_for_qlora"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_QLoRA_Training_Execution",
      "page_title": "huggingface peft QLoRA Training Execution",
      "page_type": "Principle",
      "overview": "Principle for executing training on QLoRA models with appropriate settings for quantized training stability and memory efficiency.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n* [[source::Doc|Transformers Trainer|https://huggingface.co/docs/transformers/main_classes/trainer]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Quantization]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for executing training on QLoRA models with appropriate settings for quantized training stability and memory efficiency.\n\n=== Description ===\n\nQLoRA Training Execution runs the training loop on a quantized PEFT model. Key considerations:\n\n* **Gradient accumulation**: Often required to achieve effective batch sizes within memory constraints\n* **Paged optimizers**: Use `paged_adamw_8bit` for additional memory savings\n* **Gradient checkpointing**: Trade compute for memory when needed\n* **Precision handling**: Monitor for NaN losses indicating precision issues\n\n=== Usage ===\n\nApply when training a QLoRA model:\n* Use gradient accumulation to simulate larger batches\n* Enable gradient checkpointing if memory is still tight\n* Choose appropriate paged optimizer\n* Monitor loss for stability\n\n== Theoretical Basis ==\n\n'''Memory-Compute Tradeoffs:'''\n\n| Technique | Memory Saved | Compute Cost |\n|-----------|--------------|--------------|\n| 4-bit quantization | ~4x | ~10-20% slower |\n| Gradient checkpointing | ~50% | ~20% slower |\n| Paged optimizer | ~30% | Minimal |\n| Gradient accumulation | N/A | N/A (same math) |\n\n'''Effective Batch Size:'''\n\n<math>\\text{effective\\_batch} = \\text{per\\_device\\_batch} \\times \\text{grad\\_accum\\_steps} \\times \\text{num\\_devices}</math>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_Trainer_train_qlora]]\n",
      "domains": [
        "Fine Tuning",
        "Quantization",
        "Training"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Doc",
          "title": "Transformers Trainer",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_Trainer_train_qlora"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Quantization_Configuration",
      "page_title": "huggingface peft Quantization Configuration",
      "page_type": "Principle",
      "overview": "Principle for configuring 4-bit quantization to enable training of large language models on consumer hardware.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n* [[source::Paper|Quantization Survey|https://arxiv.org/abs/2103.13630]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::Memory_Efficiency]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for configuring 4-bit quantization to enable training of large language models on consumer hardware.\n\n=== Description ===\n\nQuantization Configuration sets up the numerical precision for model weights and computation. QLoRA uses 4-bit NF4 (Normal Float 4-bit) quantization specifically designed for normally distributed neural network weights. This reduces memory requirements by ~4x while maintaining model quality through careful quantization scheme design.\n\nKey concepts:\n* **NF4:** Information-theoretically optimal for normal distributions\n* **Double quantization:** Quantizes the quantization constants for additional savings\n* **Compute dtype:** Higher precision (bfloat16) for computations ensures stability\n\n=== Usage ===\n\nApply this principle when setting up QLoRA training:\n* **Standard QLoRA:** Use NF4 with bfloat16 compute dtype\n* **Maximum savings:** Enable double quantization for ~0.4 extra bits/param\n* **Older GPUs:** Use float16 compute dtype if bfloat16 unsupported\n\n== Theoretical Basis ==\n\n'''Normal Float 4-bit (NF4):'''\n\nNF4 quantizes weights to 4-bit values optimized for normally distributed data:\n\n<math>Q_{NF4}(w) = \\text{argmin}_{q_i \\in Q} |w - q_i|</math>\n\nWhere <math>Q</math> contains 16 quantization levels positioned at quantiles of <math>\\mathcal{N}(0,1)</math>.\n\n'''Memory Calculation:'''\n\nBase model memory with 4-bit quantization:\n<math>\\text{Memory}_{4bit} \\approx \\frac{P \\times 4}{8} = \\frac{P}{2} \\text{ bytes}</math>\n\nWhere <math>P</math> is parameter count. For a 7B model:\n<math>\\text{Memory} \\approx \\frac{7 \\times 10^9}{2} \\approx 3.5 \\text{ GB}</math>\n\n'''Double Quantization:'''\n\nQuantization uses scale factors per block. Double quantization quantizes these scales:\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for double quantization\nblock_size = 64\nscales = compute_scales(weights, block_size)  # float16 scales\ndouble_quant_scales = quantize_fp8(scales)    # Additional compression\n</syntaxhighlight>\n\nThis saves ~0.4 bits per parameter (8 \u2192 0.5 bytes for 64-element block scale).\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_BitsAndBytesConfig_4bit]]\n",
      "domains": [
        "Quantization",
        "Memory Efficiency",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Paper",
          "title": "Quantization Survey",
          "url": "https://arxiv.org/abs/2103.13630"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_BitsAndBytesConfig_4bit"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Quantized_Model_Loading",
      "page_title": "huggingface peft Quantized Model Loading",
      "page_type": "Principle",
      "overview": "Principle for loading transformer models with 4-bit or 8-bit quantization using bitsandbytes, enabling large model training on consumer hardware.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n* [[source::Doc|Transformers Quantization|https://huggingface.co/docs/transformers/main_classes/quantization]]\n|-\n! Domains\n| [[domain::Model_Loading]], [[domain::Quantization]], [[domain::Memory_Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for loading transformer models with 4-bit or 8-bit quantization using bitsandbytes, enabling large model training on consumer hardware.\n\n=== Description ===\n\nQuantized Model Loading loads a pretrained model with on-the-fly quantization applied. The model weights are converted to 4-bit (NF4) or 8-bit precision during loading, reducing memory footprint by approximately 4x compared to float16.\n\nKey aspects:\n* Weights are quantized during loading, not pre-quantized\n* `device_map=\"auto\"` is required for proper memory management\n* Attention and linear layers use quantized weights\n* Model is frozen after loading (training handled by adapters)\n\n=== Usage ===\n\nApply after configuring BitsAndBytesConfig:\n* Pass quantization_config to `from_pretrained()`\n* Always use `device_map=\"auto\"`\n* Model will have Linear4bit/Linear8bitLt layers\n\n== Theoretical Basis ==\n\n'''NormalFloat4 (NF4) Quantization:'''\n\nNF4 is information-theoretically optimal for normally distributed weights:\n* Divides the range into 16 non-uniform bins\n* Bins are optimized for Gaussian distribution\n* Better preserves weight information than uniform quantization\n\n'''Memory Savings:'''\n\n| Precision | Bytes/Param | 7B Model Size |\n|-----------|-------------|---------------|\n| float32 | 4 | ~28 GB |\n| float16 | 2 | ~14 GB |\n| 4-bit | 0.5 | ~3.5 GB |\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_AutoModel_from_pretrained_quantized]]\n",
      "domains": [
        "Model Loading",
        "Quantization",
        "Memory Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Doc",
          "title": "Transformers Quantization",
          "url": "https://huggingface.co/docs/transformers/main_classes/quantization"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_AutoModel_from_pretrained_quantized"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Training_Execution",
      "page_title": "huggingface peft Training Execution",
      "page_type": "Principle",
      "overview": "Principle for executing the training loop to optimize adapter parameters for task-specific performance.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Doc|Transformers Trainer|https://huggingface.co/docs/transformers/main_classes/trainer]]\n|-\n! Domains\n| [[domain::Training]], [[domain::Optimization]], [[domain::Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for executing the training loop to optimize adapter parameters for task-specific performance.\n\n=== Description ===\n\nTraining Execution runs the optimization loop that updates adapter weights while keeping base model weights frozen. The training process involves:\n1. Forward pass through adapted model\n2. Loss computation (typically cross-entropy for language modeling)\n3. Backward pass computing gradients only for adapter parameters\n4. Optimizer step updating adapter weights\n\nKey considerations include learning rate selection (typically higher than full fine-tuning), gradient accumulation for effective batch sizes, and mixed precision for efficiency.\n\n=== Usage ===\n\nApply this principle using HuggingFace Trainer or custom training loops:\n* **Learning rate:** Use 1e-4 to 2e-4 (higher than full fine-tuning's 1e-5)\n* **Batch size:** Use gradient accumulation to achieve effective batch of 16-64\n* **Precision:** Use fp16/bf16 mixed precision for memory efficiency\n* **Scheduling:** Cosine or linear decay with warmup (3-10% of steps)\n\n== Theoretical Basis ==\n\n'''Optimization Objective:'''\n\nThe training minimizes loss over adapter parameters only:\n<math>\\min_{A, B} \\mathcal{L}(\\theta_0 + BA; \\mathcal{D})</math>\n\nWhere:\n* <math>\\theta_0</math> are frozen base model parameters\n* <math>A, B</math> are trainable LoRA matrices\n* <math>\\mathcal{D}</math> is the training dataset\n\n'''Gradient Flow:'''\n\nDuring backpropagation:\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for gradient computation\ndef backward(loss, model):\n    loss.backward()\n\n    for name, param in model.named_parameters():\n        if param.requires_grad:  # Only adapter params\n            param.grad  # Computed and stored\n        else:  # Base model params\n            param.grad  # None (not computed)\n</syntaxhighlight>\n\n'''Learning Rate Considerations:'''\n\nLoRA typically uses higher learning rates because:\n1. Fewer parameters to optimize \u2192 can use larger steps\n2. Low-rank structure constrains the optimization space\n3. Base model provides strong initialization\n\nEmpirical guidance:\n* Full fine-tuning: 1e-5 to 5e-5\n* LoRA fine-tuning: 1e-4 to 3e-4\n\n'''Gradient Accumulation:'''\n\nFor memory-constrained settings:\n<math>\\text{Effective batch size} = \\text{micro batch} \\times \\text{accumulation steps}</math>\n\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for gradient accumulation\nfor step, batch in enumerate(dataloader):\n    loss = model(**batch).loss / accumulation_steps\n    loss.backward()\n\n    if (step + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_Trainer_train]]\n\n",
      "domains": [
        "Training",
        "Optimization",
        "Fine Tuning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Doc",
          "title": "Transformers Trainer",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_Trainer_train"
        }
      ]
    },
    {
      "id": "Principle/huggingface_peft_Training_Preparation",
      "page_title": "huggingface peft Training Preparation",
      "page_type": "Principle",
      "overview": "Principle for preparing a PEFT model for training by setting the correct mode and enabling gradient computation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|PyTorch Training|https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html]]\n* [[source::Doc|Transformers Training|https://huggingface.co/docs/transformers/training]]\n|-\n! Domains\n| [[domain::Training]], [[domain::Model_State]], [[domain::Deep_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPrinciple for preparing a PEFT model for training by setting the correct mode and enabling gradient computation.\n\n=== Description ===\n\nTraining Preparation ensures the model is in the correct state before training begins. This includes:\n1. Setting training mode (enables dropout, batch norm in train mode)\n2. Verifying only adapter parameters have gradients enabled\n3. Optionally enabling gradient checkpointing for memory efficiency\n4. Setting up optimizer with appropriate learning rates\n\nIn PEFT, this step confirms that base model weights are frozen and only adapter weights will be updated.\n\n=== Usage ===\n\nApply this principle before the training loop:\n* **Standard training:** Call `model.train()` to enable training mode\n* **Memory optimization:** Enable gradient checkpointing for large models\n* **Custom training:** Set up optimizer to target only trainable parameters\n* **Verification:** Use `model.print_trainable_parameters()` to confirm setup\n\n== Theoretical Basis ==\n\n'''Training Mode Effects:'''\n\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for training mode behavior\ndef train_mode(model):\n    for module in model.modules():\n        if isinstance(module, Dropout):\n            module.active = True  # Enable dropout\n        if isinstance(module, BatchNorm):\n            module.track_running_stats = True\n</syntaxhighlight>\n\n'''Gradient Configuration in PEFT:'''\n\nAfter `get_peft_model()`, the gradient state is:\n<syntaxhighlight lang=\"python\">\n# Pseudo-code for PEFT gradient setup\nfor name, param in model.named_parameters():\n    if \"lora_\" in name or \"modules_to_save\" in name:\n        param.requires_grad = True   # Adapter weights: trainable\n    else:\n        param.requires_grad = False  # Base weights: frozen\n</syntaxhighlight>\n\n'''Memory-Efficient Training:'''\n\nGradient checkpointing trades compute for memory:\n<syntaxhighlight lang=\"python\">\n# With gradient checkpointing\n# Forward: activations discarded\n# Backward: activations recomputed on-the-fly\n\nmodel.gradient_checkpointing_enable()\n# Now memory = O(sqrt(layers)) instead of O(layers)\n</syntaxhighlight>\n\n'''Optimizer Setup:'''\n\nFor PEFT models, the optimizer should only receive trainable parameters:\n<syntaxhighlight lang=\"python\">\n# Filter trainable parameters\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\noptimizer = AdamW(trainable_params, lr=2e-4)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:huggingface_peft_model_train_mode]]\n",
      "domains": [
        "Training",
        "Model State",
        "Deep Learning"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "PyTorch Training",
          "url": "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"
        },
        {
          "type": "Doc",
          "title": "Transformers Training",
          "url": "https://huggingface.co/docs/transformers/training"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_model_train_mode"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AdaLoraConfig",
      "page_title": "huggingface peft AdaLoraConfig",
      "page_type": "Implementation",
      "overview": "Configuration class for AdaLoRA that stores parameters for adaptive rank allocation during LoRA fine-tuning, including training schedule and sensitivity-based rank budgeting.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|AdaLoRA|https://arxiv.org/abs/2303.10512]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Adaptive_Rank]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration class for AdaLoRA that stores parameters for adaptive rank allocation during LoRA fine-tuning, including training schedule and sensitivity-based rank budgeting.\n\n=== Description ===\n\nAdaLoraConfig extends LoraConfig to support three-phase training: initial warmup (tinit steps), rank reduction phase, and final fine-tuning (tfinal steps). The init_r parameter sets starting rank per layer, while target_r sets the average target rank after reduction. RankAllocator uses EMA-smoothed sensitivity scores (beta1, beta2) with orthogonal regularization (orth_reg_weight) to allocate ranks across layers.\n\n=== Usage ===\n\nUse AdaLoraConfig when you want automatic rank allocation based on layer importance. Must specify total_step for the training schedule. Note that the standard `r` parameter is ignored - use init_r instead. DoRA and LoftQ are not supported with AdaLoRA.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/adalora/config.py src/peft/tuners/adalora/config.py]\n* '''Lines:''' 1-109\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass AdaLoraConfig(LoraConfig):\n    \"\"\"\n    Configuration for AdaLoRA adaptive rank allocation.\n\n    Args:\n        target_r: Target average rank after reduction\n        init_r: Initial rank for each matrix\n        tinit: Initial warmup steps (no rank reduction)\n        tfinal: Final fine-tuning steps (no rank reduction)\n        deltaT: Steps between budget allocations\n        beta1: EMA hyperparameter for sensitivity smoothing\n        beta2: EMA hyperparameter for uncertainty\n        orth_reg_weight: Orthogonal regularization coefficient\n        total_step: Total training steps (required)\n        rank_pattern: Saved rank allocation pattern\n    \"\"\"\n    target_r: int = 8\n    init_r: int = 12\n    tinit: int = 0\n    tfinal: int = 0\n    deltaT: int = 1\n    beta1: float = 0.85\n    beta2: float = 0.85\n    orth_reg_weight: float = 0.5\n    total_step: Optional[int] = None\n    rank_pattern: Optional[dict] = None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import AdaLoraConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| target_r || int || No || Target average rank (default: 8)\n|-\n| init_r || int || No || Initial rank per layer (default: 12)\n|-\n| total_step || int || Yes || Total training steps (required)\n|-\n| tinit || int || No || Warmup steps before rank reduction\n|-\n| tfinal || int || No || Final fine-tuning steps\n|-\n| target_modules || list[str] || No || Modules to apply AdaLoRA\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| AdaLoraConfig || dataclass || Configuration object for get_peft_model\n|}\n\n== Usage Examples ==\n\n=== Basic AdaLoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import AdaLoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = AdaLoraConfig(\n    init_r=12,              # Start with rank 12\n    target_r=4,             # Reduce to average rank 4\n    total_step=10000,       # Required: total training steps\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== AdaLoRA with Training Schedule ===\n<syntaxhighlight lang=\"python\">\nfrom peft import AdaLoraConfig\n\n# Three-phase training:\n# 1. Steps 0-100: Warmup (no rank reduction)\n# 2. Steps 100-900: Rank reduction phase\n# 3. Steps 900-1000: Final fine-tuning (no reduction)\n\nconfig = AdaLoraConfig(\n    init_r=16,\n    target_r=8,\n    tinit=100,              # 100 steps warmup\n    tfinal=100,             # 100 steps final tuning\n    total_step=1000,        # Total training steps\n    deltaT=10,              # Allocate ranks every 10 steps\n    beta1=0.85,             # Sensitivity EMA\n    beta2=0.85,             # Uncertainty EMA\n    orth_reg_weight=0.5,    # Orthogonal regularization\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n</syntaxhighlight>\n\n=== Validation Rules ===\n<syntaxhighlight lang=\"python\">\n# AdaLoRA requires total_step\nconfig = AdaLoraConfig(target_r=8)  # Raises ValueError\n\n# Schedule must allow budgeting phase\nconfig = AdaLoraConfig(\n    tinit=500,\n    tfinal=500,\n    total_step=1000,  # Raises ValueError: no budgeting phase\n)\n\n# DoRA not supported\nconfig = AdaLoraConfig(\n    total_step=1000,\n    use_dora=True,  # Raises ValueError\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Adaptive Rank"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "AdaLoRA",
          "url": "https://arxiv.org/abs/2303.10512"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AdaLoraGPTQ",
      "page_title": "huggingface peft AdaLoraGPTQ",
      "page_type": "Implementation",
      "overview": "AdaLoRA implementation for GPTQ-quantized linear layers, enabling SVD-based adaptive rank adaptation on GPTQ models.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|GPTQ|https://arxiv.org/abs/2210.17323]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nAdaLoRA implementation for GPTQ-quantized linear layers, enabling SVD-based adaptive rank adaptation on GPTQ models.\n\n=== Description ===\n\nSVDQuantLinear implements AdaLoRA for GPTQ-quantized layers. It wraps a GPTQ quantized linear module and applies SVD-based adaptation with trainable lora_A, lora_B, and lora_E matrices. The forward pass computes: result + ((dropout(x) @ (lora_A * lora_E).T @ lora_B.T) * scaling / ranknum). Input is cast to float32 when autocast is disabled.\n\n=== Usage ===\n\nUse SVDQuantLinear when applying AdaLoRA to GPTQ-quantized models. The layer is automatically dispatched when the base layer is a GPTQ QuantLinear. Works with models loaded via AutoGPTQ or transformers GPTQ support.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/adalora/gptq.py src/peft/tuners/adalora/gptq.py]\n* '''Lines:''' 1-72\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass SVDQuantLinear(torch.nn.Module, AdaLoraLayer):\n    \"\"\"AdaLoRA for GPTQ-quantized layers.\"\"\"\n    def __init__(\n        self,\n        base_layer,\n        adapter_name,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        init_lora_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize AdaLoRA for GPTQ layer.\"\"\"\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Apply SVD adaptation to GPTQ layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.adalora.gptq import SVDQuantLinear\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || QuantLinear || Yes || GPTQ quantized layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| r || int || Yes || Initial rank\n|-\n| lora_alpha || int || No || Scaling factor\n|-\n| lora_dropout || float || No || Dropout probability\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || GPTQ output + SVD adaptation\n|}\n\n== Usage Examples ==\n\n=== AdaLoRA with GPTQ Model ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import AdaLoraConfig, get_peft_model\n\n# Load GPTQ-quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TheBloke/Llama-2-7B-GPTQ\",\n    device_map=\"auto\",\n)\n\n# Apply AdaLoRA\nconfig = AdaLoraConfig(\n    init_r=12,\n    target_r=4,\n    total_step=10000,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n# Automatically uses SVDQuantLinear for GPTQ layers\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_GPTQ_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "GPTQ",
          "url": "https://arxiv.org/abs/2210.17323"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_GPTQ_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AdaLoraLayer",
      "page_title": "huggingface peft AdaLoraLayer",
      "page_type": "Implementation",
      "overview": "SVD-based adaptive low-rank adaptation layers that dynamically prune and allocate ranks during training based on importance scores.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|AdaLoRA|https://openreview.net/forum?id=lq62uWRJjiY]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Adaptive_Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nSVD-based adaptive low-rank adaptation layers that dynamically prune and allocate ranks during training based on importance scores.\n\n=== Description ===\n\nAdaLoraLayer implements an SVD-based parameterization of LoRA that enables adaptive rank allocation during training. Unlike standard LoRA which uses fixed ranks, AdaLoRA decomposes the weight update into three learnable matrices: lora_A (right singular vectors), lora_E (singular values), and lora_B (left singular vectors). During training, a RankAllocator dynamically prunes less important singular values based on gradient-weighted importance scores, redistributing the rank budget to more important layers.\n\n=== Usage ===\n\nUse AdaLoRA when you need to automatically determine optimal rank allocation across layers rather than using a fixed rank. This is particularly useful when you want to minimize trainable parameters while maximizing performance, as AdaLoRA will concentrate parameters in the most important layers.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/adalora/layer.py src/peft/tuners/adalora/layer.py]\n* '''Lines:''' 1-361\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass AdaLoraLayer(LoraLayer):\n    \"\"\"\n    AdaLoRA layer with SVD-based parameterization.\n\n    Attributes:\n        lora_A: Right singular vectors (nn.ParameterDict)\n        lora_B: Left singular vectors (nn.ParameterDict)\n        lora_E: Singular values (nn.ParameterDict)\n        ranknum: Current rank tracker (nn.ParameterDict)\n    \"\"\"\n    adapter_layer_names = (\"lora_A\", \"lora_B\", \"lora_E\", \"lora_embedding_A\", \"lora_embedding_B\")\n    other_param_names = (\"r\", \"lora_alpha\", \"scaling\", \"lora_dropout\", \"ranknum\")\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        r: int,\n        lora_alpha: int,\n        lora_dropout: float,\n        init_lora_weights: bool,\n        inference_mode: bool = False,\n        **kwargs\n    ) -> None:\n        \"\"\"Update layer with AdaLoRA adapter.\"\"\"\n\nclass SVDLinear(nn.Module, AdaLoraLayer):\n    \"\"\"SVD-based adaptation by a dense layer.\"\"\"\n    def __init__(\n        self,\n        base_layer: nn.Module,\n        adapter_name: str,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,\n        init_lora_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize SVDLinear layer.\"\"\"\n\nclass RankAllocator:\n    \"\"\"\n    Rank allocation manager for AdaLoRA.\n\n    Dynamically adjusts rank budget based on importance scores\n    computed from gradient information during training.\n    \"\"\"\n    def __init__(self, model, peft_config, adapter_name):\n        \"\"\"Initialize RankAllocator.\"\"\"\n\n    def update_and_allocate(self, model, global_step, force_mask=False):\n        \"\"\"Update importance scores and allocate budget.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.adalora import AdaLoraLayer, SVDLinear, RankAllocator\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained layer to adapt\n|-\n| adapter_name || str || Yes || Name identifier for the adapter\n|-\n| r || int || Yes || Initial rank for SVD decomposition\n|-\n| lora_alpha || int || No || Scaling factor (default: 1)\n|-\n| lora_dropout || float || No || Dropout probability (default: 0.0)\n|-\n| init_lora_weights || bool || No || Whether to initialize weights (default: True)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input transformed by base layer + SVD low-rank adaptation\n|-\n| delta_weight || torch.Tensor || Computed as (B @ (A * E)) * scaling / ranknum\n|}\n\n== Usage Examples ==\n\n=== Basic AdaLoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import AdaLoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure AdaLoRA with adaptive rank allocation\nconfig = AdaLoraConfig(\n    init_r=12,           # Initial rank\n    target_r=4,          # Target final rank after pruning\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    tinit=200,           # Warmup steps before pruning\n    tfinal=1000,         # Steps before final rank\n    deltaT=10,           # Pruning frequency\n    orth_reg_weight=0.5, # Orthogonal regularization\n)\n\n# Create PEFT model\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Training with Rank Allocation ===\n<syntaxhighlight lang=\"python\">\n# Training loop with AdaLoRA rank updates\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\nfor step, batch in enumerate(dataloader):\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n\n    optimizer.step()\n\n    # Critical: Update rank allocation after backward, before zero_grad\n    model.base_model.update_and_allocate(step)\n\n    optimizer.zero_grad()\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Adaptive Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "AdaLoRA",
          "url": "https://openreview.net/forum?id=lq62uWRJjiY"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AdaLoraModel",
      "page_title": "huggingface peft AdaLoraModel",
      "page_type": "Implementation",
      "overview": "Model class that creates AdaLoRA (Adaptive LoRA) from a pretrained transformer, enabling dynamic rank allocation based on importance scores during training.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|AdaLoRA|https://openreview.net/forum?id=lq62uWRJjiY]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Adaptive_Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nModel class that creates AdaLoRA (Adaptive LoRA) from a pretrained transformer, enabling dynamic rank allocation based on importance scores during training.\n\n=== Description ===\n\nAdaLoraModel extends LoraModel to implement adaptive low-rank adaptation. It maintains a RankAllocator that dynamically adjusts the rank budget across layers during training based on gradient-weighted importance scores. The model supports only one trainable adapter at a time (other adapters must be in inference mode) and adds orthogonal regularization to the loss function to encourage orthogonality in the low-rank matrices.\n\n=== Usage ===\n\nUse AdaLoraModel when you want automatic rank allocation across layers during training. Unlike standard LoRA where you manually set ranks, AdaLoRA starts with an initial rank and prunes to a target rank, concentrating parameters in the most important layers. Best for scenarios where you want to minimize parameters while maximizing performance.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/adalora/model.py src/peft/tuners/adalora/model.py]\n* '''Lines:''' 1-347\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass AdaLoraModel(LoraModel):\n    \"\"\"\n    Creates AdaLoRA model from a pretrained transformers model.\n\n    Args:\n        model: The model to be adapted (PreTrainedModel)\n        config: The configuration of the AdaLora model (AdaLoraConfig)\n        adapter_name: The name of the adapter (default: \"default\")\n        low_cpu_mem_usage: Create empty adapter weights on meta device\n\n    Attributes:\n        trainable_adapter_name: Name of the single trainable adapter\n        rankallocator: RankAllocator for dynamic rank management\n    \"\"\"\n    target_module_mapping = TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING\n\n    def __init__(self, model, config, adapter_name, **kwargs):\n        \"\"\"Initialize AdaLoraModel with rank allocator.\"\"\"\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Forward pass with orthogonal regularization loss.\"\"\"\n\n    def update_and_allocate(self, global_step: int):\n        \"\"\"\n        Update AdaLoRA budget and mask.\n\n        Must be called after loss.backward() and before zero_grad().\n        \"\"\"\n\n    def resize_modules_by_rank_pattern(self, rank_pattern, adapter_name):\n        \"\"\"Resize modules according to computed rank pattern.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import AdaLoraModel, AdaLoraConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || The pretrained model to adapt\n|-\n| config || AdaLoraConfig || Yes || Configuration for AdaLoRA\n|-\n| adapter_name || str || No || Name for the adapter (default: \"default\")\n|-\n| low_cpu_mem_usage || bool || No || Use meta device for initialization\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward() || ModelOutput || Model output with loss including orthogonal regularization\n|-\n| update_and_allocate() || tuple || (budget, rank_pattern) for current step\n|}\n\n== Usage Examples ==\n\n=== Creating AdaLoRA Model ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import AdaLoraModel, AdaLoraConfig\n\n# Load base model\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n# Configure AdaLoRA\nconfig = AdaLoraConfig(\n    init_r=12,           # Start with rank 12\n    target_r=4,          # Prune down to rank 4\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.01,\n    tinit=200,           # Warmup steps\n    tfinal=1000,         # Final pruning step\n    deltaT=10,           # Pruning interval\n    orth_reg_weight=0.5, # Orthogonal regularization weight\n)\n\n# Create AdaLoRA model\nmodel = AdaLoraModel(base_model, config, \"default\")\n</syntaxhighlight>\n\n=== Full Training Loop ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments\n\n# Custom training loop for AdaLoRA\nclass AdaLoraTrainer(Trainer):\n    def training_step(self, model, inputs):\n        loss = super().training_step(model, inputs)\n\n        # Update rank allocation after backward pass\n        # This is called automatically by the AdaLoRA callback\n        return loss\n\n# Or use callback approach\nfrom peft.optimizers import AdaLoRACallback\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    callbacks=[AdaLoRACallback()],  # Handles update_and_allocate\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== Manual Rank Update ===\n<syntaxhighlight lang=\"python\">\n# Manual training loop with explicit rank updates\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\nfor step, batch in enumerate(dataloader):\n    # Forward pass\n    outputs = model(**batch)\n    loss = outputs.loss  # Includes orthogonal regularization\n\n    # Backward pass\n    loss.backward()\n    optimizer.step()\n\n    # Critical: Update rank allocation\n    # Must be called after backward, before zero_grad\n    model.update_and_allocate(step)\n\n    optimizer.zero_grad()\n\n    if step % 100 == 0:\n        # Check current rank budget\n        config = model.peft_config[model.trainable_adapter_name]\n        print(f\"Step {step}, Rank pattern: {config.rank_pattern}\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Adaptive Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "AdaLoRA",
          "url": "https://openreview.net/forum?id=lq62uWRJjiY"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AdaLoraQuantized",
      "page_title": "huggingface peft AdaLoraQuantized",
      "page_type": "Implementation",
      "overview": "Quantized AdaLoRA layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling SVD-based adaptive rank adaptation on quantized models.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|bitsandbytes|https://github.com/bitsandbytes-foundation/bitsandbytes]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nQuantized AdaLoRA layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling SVD-based adaptive rank adaptation on quantized models.\n\n=== Description ===\n\nSVDLinear8bitLt and SVDLinear4bit implement AdaLoRA for bitsandbytes quantized layers. The layers use SVD decomposition with trainable lora_A, lora_B, and lora_E (importance scores) matrices. The forward pass computes: result + ((dropout(x) @ (lora_A * lora_E).T @ lora_B.T) * scaling / ranknum). For 4-bit, result is defensively cloned for backprop compatibility.\n\n=== Usage ===\n\nUse AdaLoRA quantized layers when fine-tuning large quantized models with adaptive rank allocation. Layers are automatically dispatched when base layers are bitsandbytes Linear8bitLt or Linear4bit. Note: merging is not supported for quantized AdaLoRA.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/adalora/bnb.py src/peft/tuners/adalora/bnb.py]\n* '''Lines:''' 1-144\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass SVDLinear8bitLt(torch.nn.Module, AdaLoraLayer):\n    \"\"\"AdaLoRA for 8-bit quantized layers.\"\"\"\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        init_lora_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize AdaLoRA for 8-bit layer.\"\"\"\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Apply SVD-based adaptation to quantized layer.\"\"\"\n\nclass SVDLinear4bit(torch.nn.Module, AdaLoraLayer):\n    \"\"\"AdaLoRA for 4-bit quantized layers.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.adalora.bnb import SVDLinear8bitLt, SVDLinear4bit\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || bnb.nn.Linear8bitLt/Linear4bit || Yes || Quantized base layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| r || int || Yes || Initial rank\n|-\n| lora_alpha || int || No || Scaling factor (default: 1)\n|-\n| lora_dropout || float || No || Dropout probability\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Base output + SVD adaptation\n|}\n\n== Usage Examples ==\n\n=== AdaLoRA with 4-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import AdaLoraConfig, get_peft_model\n\n# Load model in 4-bit\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n)\n\n# Apply AdaLoRA with rank scheduling\nconfig = AdaLoraConfig(\n    init_r=12,\n    target_r=4,\n    total_step=10000,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n# Automatically uses SVDLinear4bit class\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "bitsandbytes",
          "url": "https://github.com/bitsandbytes-foundation/bitsandbytes"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AdaptedAttention",
      "page_title": "huggingface peft AdaptedAttention",
      "page_type": "Implementation",
      "overview": "Attention layer wrapper that injects learnable adaption prompts into key-value attention, implementing LLaMA-Adapter style prefix tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|LLaMA-Adapter|https://arxiv.org/abs/2303.16199]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Prompt_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nAttention layer wrapper that injects learnable adaption prompts into key-value attention, implementing LLaMA-Adapter style prefix tuning.\n\n=== Description ===\n\nAdaptedAttention wraps LLaMA/Mistral attention modules and injects trainable prompt tokens. It stores adaption_prompt (learnable embeddings) and adaption_gate (zero-initialized scalar). During forward, it computes adapter keys/values from the prompt, calculates attention scores between queries and adapter keys, applies the gate, and adds the result to original attention output. AdaptedAttentionGPT provides equivalent functionality for GPT-2 architecture.\n\n=== Usage ===\n\nUse AdaptedAttention when applying LLaMA-Adapter style fine-tuning. The module wraps existing attention layers and is created automatically by AdaptionPromptModel. Supports LLaMA, Mistral (with grouped query attention), and GPT-2 models.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/adaption_prompt/layer.py src/peft/tuners/adaption_prompt/layer.py]\n* '''Lines:''' 1-237\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass _BaseAdaptedAttention(nn.Module):\n    \"\"\"Base class for adapted attention modules.\"\"\"\n    def __init__(\n        self,\n        model_type: str,\n        adapter_len: int,\n        model,\n        target_dtype=torch.float32,\n    ):\n        \"\"\"\n        Args:\n            model_type: Transformer model type (llama, mistral, gpt2)\n            adapter_len: Number of adapter tokens\n            model: Original attention module to wrap\n        \"\"\"\n\nclass AdaptedAttention(_BaseAdaptedAttention):\n    \"\"\"Wraps LLaMA/Mistral attention with adaption prompts.\"\"\"\n    def forward(self, **kwargs):\n        \"\"\"Forward with injected adapter tokens.\"\"\"\n\nclass AdaptedAttentionGPT(_BaseAdaptedAttention):\n    \"\"\"Wraps GPT-2 attention with adaption prompts.\"\"\"\n    def forward(self, hidden_states, ...):\n        \"\"\"Forward with injected adapter tokens.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.adaption_prompt.layer import AdaptedAttention, AdaptedAttentionGPT\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model_type || str || Yes || \"llama\", \"mistral\", or \"gpt2\"\n|-\n| adapter_len || int || Yes || Number of adapter tokens\n|-\n| model || nn.Module || Yes || Original attention to wrap\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Attention output + adapter contribution\n|}\n\n== Usage Examples ==\n\n=== LLaMA-Adapter Forward Pass ===\n<syntaxhighlight lang=\"python\">\n# AdaptedAttention is created automatically by AdaptionPromptModel\n# The forward computation:\n\n# 1. Get original attention output\noutput = self.model(**kwargs)\n\n# 2. Project adapter prompt to keys/values\nkey = self.model.k_proj(self.adaption_prompt)\nvalue = self.model.v_proj(self.adaption_prompt)\n\n# 3. Recompute query states\nquery_states = compute_query_states(self.model, **kwargs)\n\n# 4. Compute adapter attention\nscores = query_states @ adapter_k.T / sqrt(head_dim)\nscores = self.adaption_gate * softmax(scores)\nadapter_output = scores @ adapter_v\n\n# 5. Add to original output\noutput = output + adapter_output\n</syntaxhighlight>\n\n=== Zero-Init Gating ===\n<syntaxhighlight lang=\"python\">\n# Gate is initialized to 0, allowing gradual contribution\nself.adaption_gate = nn.Parameter(torch.zeros(1))\n\n# During training, gate learns to scale adapter influence\n# This provides stable training initialization\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Prompt Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "LLaMA-Adapter",
          "url": "https://arxiv.org/abs/2303.16199"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AdaptionPromptConfig",
      "page_title": "huggingface peft AdaptionPromptConfig",
      "page_type": "Implementation",
      "overview": "Configuration class for Adaption Prompt (LLaMA-Adapter) that stores parameters for inserting learnable prompt tokens into attention layers.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|LLaMA-Adapter|https://arxiv.org/abs/2303.16199]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Prompt_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration class for Adaption Prompt (LLaMA-Adapter) that stores parameters for inserting learnable prompt tokens into attention layers.\n\n=== Description ===\n\nAdaptionPromptConfig stores configuration for inserting adapter tokens into attention computations. The adapter_len parameter controls how many tokens to insert, and adapter_layers specifies how many layers from the top receive adapters. Model-specific configurations (target_modules, projection layers) are defined in TRANSFORMERS_MODEL_CONFIG for llama, mistral, and gpt2 model types.\n\n=== Usage ===\n\nUse AdaptionPromptConfig for LLaMA-Adapter style fine-tuning where learnable tokens are prepended to keys and values in attention. Supports LLaMA, Mistral, and GPT-2 model types. Target modules are auto-configured based on model type if not specified.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/adaption_prompt/config.py src/peft/tuners/adaption_prompt/config.py]\n* '''Lines:''' 1-89\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass AdaptionPromptConfig(PeftConfig):\n    \"\"\"\n    Configuration for Adaption Prompt (LLaMA-Adapter).\n\n    Args:\n        target_modules: Attention submodule name to insert prompts\n        adapter_len: Number of adapter tokens to insert\n        adapter_layers: Number of layers from top to adapt\n    \"\"\"\n    target_modules: str = None\n    adapter_len: int = None\n    adapter_layers: int = None\n\nModelTypeConfig = namedtuple(\n    \"ModelTypeConfig\",\n    [\"compute_query_states\", \"target_modules\", \"k_proj_layer\", \"v_proj_layer\", \"o_proj_layer\"]\n)\n\nTRANSFORMERS_MODEL_CONFIG = {\n    \"llama\": ModelTypeConfig(...),\n    \"mistral\": ModelTypeConfig(...),\n    \"gpt2\": ModelTypeConfig(...),\n}\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import AdaptionPromptConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| adapter_len || int || Yes || Number of adapter tokens\n|-\n| adapter_layers || int || Yes || Number of layers to adapt (from top)\n|-\n| target_modules || str || No || Attention submodule (auto-configured)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| AdaptionPromptConfig || dataclass || Configuration for get_peft_model\n|}\n\n== Usage Examples ==\n\n=== Basic Adaption Prompt ===\n<syntaxhighlight lang=\"python\">\nfrom peft import AdaptionPromptConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = AdaptionPromptConfig(\n    adapter_len=10,         # 10 adapter tokens\n    adapter_layers=30,      # Apply to top 30 layers\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Supported Model Types ===\n<syntaxhighlight lang=\"python\">\n# LLaMA/Mistral: uses self_attn with k_proj, v_proj, o_proj\n# GPT-2: uses attn with c_attn\n\n# Model type is auto-detected from config\nconfig = AdaptionPromptConfig(\n    adapter_len=10,\n    adapter_layers=12,\n    # target_modules auto-set based on model.config.model_type\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Prompt Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "LLaMA-Adapter",
          "url": "https://arxiv.org/abs/2303.16199"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AdaptionPromptModel",
      "page_title": "huggingface peft AdaptionPromptModel",
      "page_type": "Implementation",
      "overview": "Model class that applies LLaMA-Adapter style adaption prompts by replacing top L attention modules with AdaptedAttention wrappers.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|LLaMA-Adapter|https://arxiv.org/abs/2303.16199]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Prompt_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nModel class that applies LLaMA-Adapter style adaption prompts by replacing top L attention modules with AdaptedAttention wrappers.\n\n=== Description ===\n\nAdaptionPromptModel wraps a transformer model and replaces the top adapter_layers attention modules with AdaptedAttention wrappers. It supports multi-adapter management by caching inactive adapters and swapping them in/out. The model freezes all parameters except adaption_prompt and adaption_gate. Different adapters are stored in _cached_adapters dictionary and swapped via set_adapter().\n\n=== Usage ===\n\nUse AdaptionPromptModel for LLaMA-Adapter fine-tuning. Created automatically via get_peft_model with AdaptionPromptConfig. Supports adding multiple adapters, switching between them, and enabling/disabling adapter layers.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/adaption_prompt/model.py src/peft/tuners/adaption_prompt/model.py]\n* '''Lines:''' 1-170\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass AdaptionPromptModel(nn.Module):\n    \"\"\"\n    Implements adaption prompts (LLaMA-Adapter).\n\n    Attributes:\n        model: Wrapped transformer model\n        peft_config: Dict of adapter configs by name\n        _parents: Module parents for attention replacement\n        _cached_adapters: Inactive adapters storage\n        _active_adapter: Currently active adapter name\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        configs: dict,\n        adapter_name: str,\n    ):\n        \"\"\"Initialize and add first adapter.\"\"\"\n\n    def add_adapter(self, adapter_name: str, config: AdaptionPromptConfig):\n        \"\"\"Add a new adapter with given config.\"\"\"\n\n    def set_adapter(self, adapter_name: str):\n        \"\"\"Switch to a different adapter.\"\"\"\n\n    def enable_adapter_layers(self):\n        \"\"\"Enable adapter layers.\"\"\"\n\n    def disable_adapter_layers(self):\n        \"\"\"Disable adapter layers.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.adaption_prompt import AdaptionPromptModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || nn.Module || Yes || Base transformer model\n|-\n| configs || dict || Yes || Adapter configs by name\n|-\n| adapter_name || str || Yes || Initial adapter name\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward() || ModelOutput || Wrapped model output\n|}\n\n== Usage Examples ==\n\n=== Basic Usage ===\n<syntaxhighlight lang=\"python\">\nfrom peft import AdaptionPromptConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = AdaptionPromptConfig(\n    adapter_len=10,\n    adapter_layers=30,\n)\n\n# Creates AdaptionPromptModel internally\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Multi-Adapter Support ===\n<syntaxhighlight lang=\"python\">\n# Add multiple adapters\npeft_model.add_adapter(\"task1\", config1)\npeft_model.add_adapter(\"task2\", config2)\n\n# Switch between adapters\npeft_model.set_adapter(\"task1\")\noutput1 = peft_model(input_ids)\n\npeft_model.set_adapter(\"task2\")\noutput2 = peft_model(input_ids)\n</syntaxhighlight>\n\n=== Enable/Disable Adapters ===\n<syntaxhighlight lang=\"python\">\n# Disable adapters for base model inference\npeft_model.disable_adapter_layers()\nbase_output = peft_model(input_ids)\n\n# Re-enable adapters\npeft_model.enable_adapter_layers()\nadapted_output = peft_model(input_ids)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Prompt Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "LLaMA-Adapter",
          "url": "https://arxiv.org/abs/2303.16199"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_ArrowLoraLinearLayer",
      "page_title": "huggingface peft ArrowLoraLinearLayer",
      "page_type": "Implementation",
      "overview": "Arrow routing layer that performs mixture-of-experts style routing over multiple LoRA adapters using prototype-based cosine similarity for expert selection.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|Arrow|https://arxiv.org/abs/2404.15198]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Mixture_of_Experts]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nArrow routing layer that performs mixture-of-experts style routing over multiple LoRA adapters using prototype-based cosine similarity for expert selection.\n\n=== Description ===\n\nArrowLoraLinearLayer implements the Arrow algorithm which routes tokens to the most relevant LoRA adapters based on cosine similarity with learned prototypes. For each adapter, a prototype vector is computed via SVD of the LoRA weight matrices. During inference, tokens are matched to prototypes using top-k selection, and expert outputs are combined via weighted sum. The layer also supports General Knowledge Subtraction (GKS) to purify task-specific adapters by removing shared knowledge.\n\n=== Usage ===\n\nUse Arrow when you have multiple task-specific LoRA adapters and want token-level routing to appropriate experts. Arrow is particularly effective for multi-task scenarios where different parts of the input may benefit from different specializations. The `create_arrow_model` helper loads adapters and sets up routing automatically.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/arrow.py src/peft/tuners/lora/arrow.py]\n* '''Lines:''' 1-477\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass ArrowLoraLinearLayer(nn.Module):\n    \"\"\"\n    Arrow routing algorithm for LoRA adapters.\n\n    Attributes:\n        top_k: Number of experts to select per token\n        temperature: Softmax temperature for routing\n        task_adapter_names: List of task-specific adapter names\n        gks_adapter_names: List of general knowledge adapter names\n        prototypes: Buffer holding prototype vectors [E, in_features]\n    \"\"\"\n\n    def __init__(self, in_features, arrow_config):\n        \"\"\"Initialize Arrow routing layer.\"\"\"\n\n    def build_prototypes(self, lora_A, lora_B):\n        \"\"\"Compute prototype vectors via SVD for each adapter.\"\"\"\n\n    def gen_know_sub(self, lora_A, lora_B):\n        \"\"\"Perform General Knowledge Subtraction.\"\"\"\n\n    def forward(self, x, lora_A, lora_B, dropout, scaling):\n        \"\"\"Apply Arrow routing with top-k expert selection.\"\"\"\n\ndef create_arrow_model(\n    base_model: PreTrainedModel,\n    task_specific_adapter_paths: list[str],\n    arrow_config: ArrowConfig,\n    general_adapter_paths: list[str] | None = None,\n    **adapter_kwargs,\n):\n    \"\"\"Create model with Arrow routing over loaded adapters.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.arrow import ArrowLoraLinearLayer, create_arrow_model, ArrowConfig\nfrom peft import ArrowConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| in_features || int || Yes || Input dimension for prototype computation\n|-\n| arrow_config || ArrowConfig || Yes || Configuration with top_k, temperature, adapter names\n|-\n| task_specific_adapter_paths || list[str] || Yes || Paths to task LoRA adapters\n|-\n| general_adapter_paths || list[str] || No || Paths to general knowledge adapters for GKS\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Weighted combination of routed expert outputs\n|-\n| prototypes || torch.Tensor || Computed prototype vectors [num_experts, in_features]\n|}\n\n== Usage Examples ==\n\n=== Creating Arrow Model ===\n<syntaxhighlight lang=\"python\">\nfrom peft import ArrowConfig\nfrom peft.tuners.lora.arrow import create_arrow_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure Arrow\narrow_config = ArrowConfig(\n    top_k=2,                    # Select top 2 experts per token\n    router_temperature=0.1,     # Softmax temperature\n    use_gks=False,              # Disable General Knowledge Subtraction\n)\n\n# Create Arrow model with multiple adapters\nmodel = create_arrow_model(\n    base_model=base_model,\n    task_specific_adapter_paths=[\n        \"path/to/math_adapter\",\n        \"path/to/code_adapter\",\n        \"path/to/writing_adapter\",\n    ],\n    arrow_config=arrow_config,\n)\n</syntaxhighlight>\n\n=== Arrow with General Knowledge Subtraction ===\n<syntaxhighlight lang=\"python\">\nfrom peft import ArrowConfig\nfrom peft.tuners.lora.arrow import create_arrow_model\n\n# Enable GKS to purify task-specific adapters\narrow_config = ArrowConfig(\n    top_k=2,\n    router_temperature=0.1,\n    use_gks=True,               # Enable General Knowledge Subtraction\n)\n\nmodel = create_arrow_model(\n    base_model=base_model,\n    task_specific_adapter_paths=[\n        \"path/to/task1_adapter\",\n        \"path/to/task2_adapter\",\n    ],\n    general_adapter_paths=[\n        \"path/to/general_knowledge_adapter\",\n    ],\n    arrow_config=arrow_config,\n)\n</syntaxhighlight>\n\n=== Arrow Inference ===\n<syntaxhighlight lang=\"python\">\n# Arrow automatically routes tokens to appropriate experts\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=100,\n)\n\n# Token routing happens internally:\n# 1. Compute cosine similarity with prototypes\n# 2. Select top-k experts per token\n# 3. Combine expert outputs via weighted sum\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Mixture of Experts"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "Arrow",
          "url": "https://arxiv.org/abs/2404.15198"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AutoModelForCausalLM_from_pretrained",
      "page_title": "huggingface peft AutoModelForCausalLM from pretrained",
      "page_type": "Implementation",
      "overview": "Concrete tool for loading pre-trained transformer models from HuggingFace Hub or local paths, serving as the base model for LoRA fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|Transformers Docs|https://huggingface.co/docs/transformers/main_classes/model]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Model_Loading]], [[domain::Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for loading pre-trained transformer models from HuggingFace Hub or local paths, serving as the base model for LoRA fine-tuning.\n\n=== Description ===\n\n`AutoModelForCausalLM.from_pretrained` is the primary entry point for loading causal language models from the HuggingFace ecosystem. In the context of PEFT/LoRA fine-tuning, this function loads the base model that will be wrapped with adapter layers. It handles automatic model architecture detection, weight loading, and device placement.\n\n=== Usage ===\n\nUse this when beginning a LoRA fine-tuning workflow. This is the first step before applying any PEFT configuration. The loaded model will serve as the frozen base for adapter training. Choose appropriate `torch_dtype` and `device_map` based on your hardware constraints.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Library:''' [https://github.com/huggingface/transformers transformers]\n* '''Class:''' `transformers.AutoModelForCausalLM`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@classmethod\ndef from_pretrained(\n    cls,\n    pretrained_model_name_or_path: str,\n    *model_args,\n    config: Optional[PretrainedConfig] = None,\n    cache_dir: Optional[str] = None,\n    ignore_mismatched_sizes: bool = False,\n    force_download: bool = False,\n    local_files_only: bool = False,\n    token: Optional[Union[str, bool]] = None,\n    revision: str = \"main\",\n    use_safetensors: Optional[bool] = None,\n    torch_dtype: Optional[torch.dtype] = None,\n    device_map: Optional[Union[str, Dict[str, Union[int, str, torch.device]]]] = None,\n    attn_implementation: Optional[str] = None,\n    **kwargs\n) -> PreTrainedModel:\n    \"\"\"\n    Load a pre-trained model from HuggingFace Hub or local directory.\n\n    Args:\n        pretrained_model_name_or_path: HuggingFace Hub model ID or local path\n        torch_dtype: Model precision (torch.float16, torch.bfloat16, torch.float32)\n        device_map: Device placement strategy (\"auto\", \"cuda\", \"cpu\", or dict)\n        attn_implementation: Attention implementation (\"flash_attention_2\", \"sdpa\", \"eager\")\n\n    Returns:\n        PreTrainedModel: Loaded model ready for adaptation\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| pretrained_model_name_or_path || str || Yes || HuggingFace Hub ID (e.g., \"meta-llama/Llama-2-7b-hf\") or local path\n|-\n| torch_dtype || torch.dtype || No || Model precision. Use torch.float16 or torch.bfloat16 for memory efficiency\n|-\n| device_map || str or dict || No || Device placement. \"auto\" for automatic multi-GPU, \"cuda\" for single GPU\n|-\n| attn_implementation || str || No || Attention backend. \"flash_attention_2\" for speed, \"sdpa\" for PyTorch native\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PreTrainedModel || Loaded transformer model ready for PEFT wrapping\n|}\n\n== Usage Examples ==\n\n=== Basic Loading for LoRA ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load base model with half precision for memory efficiency\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# Load tokenizer (typically paired with model loading)\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ntokenizer.pad_token = tokenizer.eos_token  # Required for some models\n</syntaxhighlight>\n\n=== Loading with Flash Attention ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\n\n# Load with Flash Attention 2 for faster training\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Base_Model_Loading]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "Model Loading",
        "Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "Transformers Docs",
          "url": "https://huggingface.co/docs/transformers/main_classes/model"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Base_Model_Loading"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_AutoModel_from_pretrained_quantized",
      "page_title": "huggingface peft AutoModel from pretrained quantized",
      "page_type": "Implementation",
      "overview": "Concrete tool for loading transformer models with bitsandbytes 4-bit/8-bit quantization applied during loading.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|Transformers Quantization|https://huggingface.co/docs/transformers/main_classes/quantization]]\n* [[source::Repo|bitsandbytes|https://github.com/bitsandbytes-foundation/bitsandbytes]]\n|-\n! Domains\n| [[domain::Model_Loading]], [[domain::Quantization]], [[domain::Memory_Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for loading transformer models with bitsandbytes 4-bit/8-bit quantization applied during loading.\n\n=== Description ===\n\n`AutoModelForCausalLM.from_pretrained()` with a `quantization_config` parameter applies on-the-fly quantization. The model weights are loaded, quantized, and placed on GPU in a single operation. Linear layers are replaced with `Linear4bit` or `Linear8bitLt` variants.\n\n=== Usage ===\n\nPass a BitsAndBytesConfig to `from_pretrained()` with `device_map=\"auto\"` to load a quantized model ready for QLoRA training.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Library:''' `transformers.AutoModelForCausalLM` (external)\n* '''Integration:''' `bitsandbytes` for quantization\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@classmethod\ndef from_pretrained(\n    cls,\n    pretrained_model_name_or_path: str,\n    quantization_config: Optional[BitsAndBytesConfig] = None,\n    device_map: Optional[Union[str, dict]] = None,\n    torch_dtype: Optional[torch.dtype] = None,\n    **kwargs,\n) -> PreTrainedModel:\n    \"\"\"\n    Load model with optional quantization.\n\n    Args:\n        pretrained_model_name_or_path: Model ID or path\n        quantization_config: BitsAndBytesConfig for 4-bit/8-bit\n        device_map: Must be \"auto\" for quantization\n\n    Returns:\n        Quantized model with Linear4bit/Linear8bitLt layers\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== 4-bit NF4 Loading ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # Required for quantization\n)\n</syntaxhighlight>\n\n=== 8-bit Loading ===\n<syntaxhighlight lang=\"python\">\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Quantized_Model_Loading]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "Model Loading",
        "Quantization",
        "Memory Optimization"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "Transformers Quantization",
          "url": "https://huggingface.co/docs/transformers/main_classes/quantization"
        },
        {
          "type": "Repo",
          "title": "bitsandbytes",
          "url": "https://github.com/bitsandbytes-foundation/bitsandbytes"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Quantized_Model_Loading"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_BOFTConfig",
      "page_title": "huggingface peft BOFTConfig",
      "page_type": "Implementation",
      "overview": "Configuration class for BOFT (Butterfly Orthogonal Fine-Tuning) that stores parameters for block-diagonal orthogonal transformations via butterfly factorization.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|BOFT|https://arxiv.org/abs/2311.06243]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Orthogonal_Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration class for BOFT (Butterfly Orthogonal Fine-Tuning) that stores parameters for block-diagonal orthogonal transformations via butterfly factorization.\n\n=== Description ===\n\nBOFTConfig stores configuration for butterfly-factorized orthogonal fine-tuning. Key parameters are boft_block_size (or boft_block_num) which control the orthogonal block dimensions, and boft_n_butterfly_factor which determines the number of butterfly factors (1 = vanilla OFT). Only one of block_size or block_num can be specified since block_size * block_num = layer_dimension.\n\n=== Usage ===\n\nUse BOFTConfig for orthogonal fine-tuning with butterfly factorization. BOFT provides more parameter efficiency than vanilla OFT via butterfly decomposition. Supports Linear and Conv2d layers. Set boft_n_butterfly_factor > 1 to increase effective block size while reducing parameters.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/boft/config.py src/peft/tuners/boft/config.py]\n* '''Lines:''' 1-161\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass BOFTConfig(PeftConfig):\n    \"\"\"\n    Configuration for BOFT (Butterfly Orthogonal Fine-Tuning).\n\n    Args:\n        boft_block_size: Block size (mutually exclusive with block_num)\n        boft_block_num: Number of blocks (mutually exclusive with block_size)\n        boft_n_butterfly_factor: Number of butterfly factors (1 = vanilla OFT)\n        target_modules: Modules to apply BOFT to\n        boft_dropout: Multiplicative dropout probability\n        fan_in_fan_out: True for Conv1D layers (e.g., GPT-2)\n        bias: Bias handling ('none', 'all', 'boft_only')\n        modules_to_save: Additional modules to train/save\n    \"\"\"\n    boft_block_size: int = 4\n    boft_block_num: int = 0\n    boft_n_butterfly_factor: int = 1\n    target_modules: Optional[Union[list[str], str]] = None\n    boft_dropout: float = 0.0\n    fan_in_fan_out: bool = False\n    bias: str = \"none\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BOFTConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| boft_block_size || int || No || Block size (default: 4, exclusive with block_num)\n|-\n| boft_block_num || int || No || Number of blocks (default: 0, exclusive with block_size)\n|-\n| boft_n_butterfly_factor || int || No || Butterfly factors (default: 1)\n|-\n| target_modules || list[str] || No || Modules to adapt\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| BOFTConfig || dataclass || Configuration for get_peft_model\n|}\n\n== Usage Examples ==\n\n=== Basic BOFT Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BOFTConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = BOFTConfig(\n    boft_block_size=8,\n    boft_n_butterfly_factor=1,  # Vanilla OFT\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== BOFT with Butterfly Factorization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BOFTConfig\n\n# More butterfly factors = larger effective block, fewer params\nconfig = BOFTConfig(\n    boft_block_size=4,\n    boft_n_butterfly_factor=2,  # Effective block size doubles\n    boft_dropout=0.1,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n)\n</syntaxhighlight>\n\n=== Using Block Num Instead ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BOFTConfig\n\n# Specify number of blocks instead of size\n# block_size * block_num = layer_dimension\nconfig = BOFTConfig(\n    boft_block_num=128,  # Number of blocks\n    target_modules=[\"q_proj\"],\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Orthogonal Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "BOFT",
          "url": "https://arxiv.org/abs/2311.06243"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_BOFTLayer",
      "page_title": "huggingface peft BOFTLayer",
      "page_type": "Implementation",
      "overview": "Butterfly Orthogonal Fine-Tuning layer that applies orthogonal transformations via Cayley parameterization and butterfly factorization for efficient parameter updates.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|BOFT|https://huggingface.co/papers/2311.06243]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Orthogonal_Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nButterfly Orthogonal Fine-Tuning layer that applies orthogonal transformations via Cayley parameterization and butterfly factorization for efficient parameter updates.\n\n=== Description ===\n\nBOFTLayer implements the BOFT (Butterfly Orthogonal Fine-Tuning) method which uses butterfly factorization to efficiently represent orthogonal transformations. The key insight is that orthogonal matrices can be factorized into products of simpler block-diagonal matrices using butterfly permutations, dramatically reducing parameters while maintaining the orthogonality constraint. The layer uses Cayley parameterization to ensure orthogonality and supports optional CUDA acceleration via a custom kernel for fast block-diagonal operations.\n\n=== Usage ===\n\nUse BOFT when you want to apply orthogonal transformations to model weights with fewer parameters than OFT. BOFT is particularly effective for vision models and when you need to preserve the geometry of the weight space. The butterfly factorization allows scaling to larger models while maintaining orthogonality.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/boft/layer.py src/peft/tuners/boft/layer.py]\n* '''Lines:''' 1-1011\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass BOFTLayer(BaseTunerLayer):\n    \"\"\"\n    Implements the BOFT layer.\n\n    Attributes:\n        boft_R: Trainable rotation parameters (nn.ParameterDict)\n        boft_s: Trainable scaling parameters (nn.ParameterDict)\n        boft_block_size: Block size per adapter (dict)\n        boft_block_num: Number of blocks per adapter (dict)\n        boft_dropout: Multiplicative dropout layers (nn.ModuleDict)\n    \"\"\"\n    adapter_layer_names = (\"boft_R\", \"boft_s\")\n    other_param_names = (\"boft_block_size\", \"boft_block_num\", \"boft_dropout\")\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        boft_block_size: int,\n        boft_block_num: int,\n        boft_n_butterfly_factor: int,\n        boft_dropout: float,\n        init_weights: bool,\n        inference_mode: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Update the layer with BOFT adapter parameters.\"\"\"\n\n    def cayley_batch(self, data: torch.Tensor) -> torch.Tensor:\n        \"\"\"Apply Cayley parameterization to ensure orthogonality.\"\"\"\n\nclass Linear(nn.Module, BOFTLayer):\n    \"\"\"BOFT implemented in a dense layer.\"\"\"\n    def __init__(\n        self,\n        base_layer: nn.Module,\n        adapter_name: str,\n        boft_block_size: int = 8,\n        boft_block_num: int = 0,\n        boft_n_butterfly_factor: int = 0,\n        boft_dropout: float = 0.1,\n        fan_in_fan_out: bool = False,\n        init_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize BOFT Linear layer.\"\"\"\n\nclass Conv2d(nn.Module, BOFTLayer):\n    \"\"\"BOFT implemented in a Conv2d layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.boft import BOFTLayer, BOFTConfig, BOFTModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained layer to adapt (Linear or Conv2d)\n|-\n| adapter_name || str || Yes || Name identifier for the adapter\n|-\n| boft_block_size || int || Yes || Size of each orthogonal block (must divide in_features)\n|-\n| boft_block_num || int || Yes || Number of blocks (alternative to block_size, set one to 0)\n|-\n| boft_n_butterfly_factor || int || No || Number of butterfly factors (default: 1)\n|-\n| boft_dropout || float || No || Multiplicative dropout probability (default: 0.1)\n|-\n| init_weights || bool || No || Whether to initialize weights to identity (default: True)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input transformed by orthogonal rotation and scaling\n|-\n| get_delta_weight() || tuple || (butterfly_oft_mat, boft_s) transformation components\n|}\n\n== Usage Examples ==\n\n=== Basic BOFT Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BOFTConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure BOFT\nconfig = BOFTConfig(\n    boft_block_size=8,          # Size of orthogonal blocks\n    boft_n_butterfly_factor=1,   # Butterfly factorization depth\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    boft_dropout=0.1,\n    bias=\"none\",\n)\n\n# Create PEFT model\nmodel = get_peft_model(model, config)\nprint(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n</syntaxhighlight>\n\n=== BOFT for Vision Models ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BOFTConfig, get_peft_model\nfrom transformers import AutoModelForImageClassification\n\n# Load vision model\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\"\n)\n\n# BOFT works well for vision models\nconfig = BOFTConfig(\n    boft_block_num=4,           # Use block_num instead of block_size\n    boft_n_butterfly_factor=2,  # More butterfly factors for larger models\n    target_modules=[\"query\", \"value\"],\n    boft_dropout=0.05,\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Merging BOFT Weights ===\n<syntaxhighlight lang=\"python\">\n# After training, merge BOFT into base weights\nmodel.merge_and_unload()\n\n# Or merge with safety check\nmodel.merge_adapter(safe_merge=True)\n\n# Save merged model\nmodel.save_pretrained(\"./boft_merged_model\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Orthogonal Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "BOFT",
          "url": "https://huggingface.co/papers/2311.06243"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_BOFTModel",
      "page_title": "huggingface peft BOFTModel",
      "page_type": "Implementation",
      "overview": "Model class that applies BOFT (Butterfly Orthogonal Fine-Tuning) by wrapping Linear and Conv2d layers with butterfly-factorized orthogonal transformations.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|BOFT|https://arxiv.org/abs/2311.06243]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Orthogonal_Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nModel class that applies BOFT (Butterfly Orthogonal Fine-Tuning) by wrapping Linear and Conv2d layers with butterfly-factorized orthogonal transformations.\n\n=== Description ===\n\nBOFTModel extends BaseTuner to apply BOFT to transformer models. It creates Linear or Conv2d BOFT layers based on target module type, passing block_size, block_num, and n_butterfly_factor parameters. Uses TRANSFORMERS_MODELS_TO_BOFT_TARGET_MODULES_MAPPING for default target modules. The _create_and_replace method handles both new module creation and updating existing BOFT layers with additional adapters.\n\n=== Usage ===\n\nUse BOFTModel for orthogonal fine-tuning with butterfly factorization. Created automatically via get_peft_model with BOFTConfig. Supports both Linear and Conv2d layers (e.g., for vision models like DinoV2).\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/boft/model.py src/peft/tuners/boft/model.py]\n* '''Lines:''' 1-132\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass BOFTModel(BaseTuner):\n    \"\"\"\n    Creates BOFT model from pretrained transformer.\n\n    Args:\n        model: Base transformer model\n        config: BOFTConfig\n        adapter_name: Name for the adapter\n\n    Attributes:\n        prefix: \"boft_\"\n        tuner_layer_cls: BOFTLayer\n        target_module_mapping: Default target modules per model type\n    \"\"\"\n    prefix: str = \"boft_\"\n    tuner_layer_cls = BOFTLayer\n\n    def _create_and_replace(\n        self,\n        boft_config,\n        adapter_name,\n        target,\n        target_name,\n        parent,\n        current_key,\n        **optional_kwargs,\n    ):\n        \"\"\"Create or update BOFT layers.\"\"\"\n\n    @staticmethod\n    def _create_new_module(boft_config, adapter_name, target, **kwargs):\n        \"\"\"Create Linear or Conv2d BOFT module.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.boft import BOFTModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || nn.Module || Yes || Base transformer model\n|-\n| config || BOFTConfig || Yes || BOFT configuration\n|-\n| adapter_name || str || No || Adapter name (default: \"default\")\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| BOFTModel || nn.Module || Model with BOFT layers\n|}\n\n== Usage Examples ==\n\n=== Vision Model with BOFT ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BOFTConfig, get_peft_model\nimport transformers\n\n# BOFT works well for vision models\nmodel = transformers.Dinov2ForImageClassification.from_pretrained(\n    \"facebook/dinov2-large\",\n    num_labels=100,\n)\n\nconfig = BOFTConfig(\n    boft_block_size=8,\n    boft_n_butterfly_factor=1,\n    target_modules=[\"query\", \"value\", \"key\", \"output.dense\", \"mlp.fc1\", \"mlp.fc2\"],\n    boft_dropout=0.1,\n    bias=\"boft_only\",\n    modules_to_save=[\"classifier\"],\n)\n\nboft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Multi-Adapter BOFT ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BOFTConfig, get_peft_model\n\n# Add multiple BOFT adapters\nconfig1 = BOFTConfig(boft_block_size=4, target_modules=[\"q_proj\"])\nconfig2 = BOFTConfig(boft_block_size=8, target_modules=[\"q_proj\"])\n\nmodel = get_peft_model(base_model, config1, adapter_name=\"adapter1\")\nmodel.add_adapter(\"adapter2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"adapter2\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Orthogonal Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "BOFT",
          "url": "https://arxiv.org/abs/2311.06243"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_BitsAndBytesConfig_4bit",
      "page_title": "huggingface peft BitsAndBytesConfig 4bit",
      "page_type": "Implementation",
      "overview": "Concrete tool for configuring 4-bit quantization using bitsandbytes for memory-efficient QLoRA training.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n* [[source::Doc|Transformers Quantization|https://huggingface.co/docs/transformers/quantization]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::Memory_Efficiency]], [[domain::Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for configuring 4-bit quantization using bitsandbytes for memory-efficient QLoRA training.\n\n=== Description ===\n\n`BitsAndBytesConfig` configures the quantization parameters for loading models in 4-bit precision. This is the foundation of QLoRA, enabling training of large models on consumer GPUs. The NF4 (Normal Float 4-bit) quantization type provides optimal quantization for normally distributed weights.\n\n=== Usage ===\n\nUse this when setting up QLoRA training to reduce VRAM requirements by ~4x. Pass this config to `AutoModelForCausalLM.from_pretrained()`. Use `bnb_4bit_compute_dtype=torch.bfloat16` for faster computation on Ampere+ GPUs, or `torch.float16` for older GPUs.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Library:''' [https://github.com/huggingface/transformers transformers]\n* '''Class:''' `transformers.BitsAndBytesConfig`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass BitsAndBytesConfig:\n    def __init__(\n        self,\n        load_in_8bit: bool = False,\n        load_in_4bit: bool = False,\n        llm_int8_threshold: float = 6.0,\n        llm_int8_skip_modules: Optional[List[str]] = None,\n        llm_int8_enable_fp32_cpu_offload: bool = False,\n        llm_int8_has_fp16_weight: bool = False,\n        bnb_4bit_compute_dtype: Optional[torch.dtype] = None,\n        bnb_4bit_quant_type: str = \"fp4\",\n        bnb_4bit_use_double_quant: bool = False,\n        bnb_4bit_quant_storage: Optional[torch.dtype] = None,\n    ):\n        \"\"\"\n        Configure quantization for model loading.\n\n        Args:\n            load_in_4bit: Enable 4-bit quantization\n            bnb_4bit_compute_dtype: Compute dtype (torch.float16 or torch.bfloat16)\n            bnb_4bit_quant_type: Quantization type (\"nf4\" recommended, \"fp4\" alternative)\n            bnb_4bit_use_double_quant: Nested quantization for extra memory savings\n        \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import BitsAndBytesConfig\nimport torch\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| load_in_4bit || bool || Yes || Enable 4-bit quantization. Set to True for QLoRA\n|-\n| bnb_4bit_compute_dtype || torch.dtype || No || Compute precision. bfloat16 for Ampere+, float16 for older\n|-\n| bnb_4bit_quant_type || str || No || \"nf4\" (recommended) or \"fp4\". NF4 is optimized for normal distributions\n|-\n| bnb_4bit_use_double_quant || bool || No || Nested quantization. Saves ~0.4 bits/param extra. Default: False\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| config || BitsAndBytesConfig || Configuration object for model loading\n|}\n\n== Usage Examples ==\n\n=== Standard QLoRA Config ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import BitsAndBytesConfig\nimport torch\n\n# Standard QLoRA configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 for Ampere GPUs\n    bnb_4bit_quant_type=\"nf4\",              # NF4 quantization\n    bnb_4bit_use_double_quant=False,\n)\n</syntaxhighlight>\n\n=== Maximum Memory Savings ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import BitsAndBytesConfig\nimport torch\n\n# Maximum memory efficiency with double quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,  # Extra ~0.4 bits/param savings\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Quantization_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "Quantization",
        "Memory Efficiency",
        "Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Doc",
          "title": "Transformers Quantization",
          "url": "https://huggingface.co/docs/transformers/quantization"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Quantization_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_BoneConfig",
      "page_title": "huggingface peft BoneConfig",
      "page_type": "Implementation",
      "overview": "Configuration class for Bone (Block Affine) that stores parameters for Householder reflection-based parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|Bone|https://arxiv.org/abs/2409.15371]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Householder_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration class for Bone (Block Affine) that stores parameters for Householder reflection-based parameter-efficient fine-tuning.\n\n=== Description ===\n\nBoneConfig stores configuration for Bone/BAT fine-tuning. The r parameter sets the rank of the adaptation (best set to even numbers). The init_weights parameter selects between Bone (True) and BAT (\"bat\") variants. Note: Bone is deprecated and will be removed in PEFT v0.19.0 - use MissConfig instead.\n\n=== Usage ===\n\nUse BoneConfig for Householder reflection-based fine-tuning. For new projects, prefer MissConfig which supersedes Bone. Existing Bone checkpoints can be converted using the provided script.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/bone/config.py src/peft/tuners/bone/config.py]\n* '''Lines:''' 1-130\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass BoneConfig(PeftConfig):\n    \"\"\"\n    Configuration for Bone (deprecated, use MissConfig).\n\n    Args:\n        r: Rank of Bone (best set to even numbers)\n        target_modules: Modules to apply Bone to\n        init_weights: True for Bone, \"bat\" for BAT variant\n        bias: Bias handling ('none', 'all', 'bone_only')\n        modules_to_save: Additional modules to train/save\n    \"\"\"\n    r: int = 64\n    target_modules: Optional[Union[list[str], str]] = None\n    init_weights: bool | Literal[\"bat\"] = True\n    bias: str = \"none\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BoneConfig  # Deprecated, use MissConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| r || int || No || Rank (default: 64, prefer even numbers)\n|-\n| target_modules || list[str] || No || Modules to adapt\n|-\n| init_weights || bool/\"bat\" || No || Variant selection\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| BoneConfig || dataclass || Configuration for get_peft_model\n|}\n\n== Usage Examples ==\n\n=== Basic Bone Configuration (Deprecated) ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BoneConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Note: Bone is deprecated, use MissConfig instead\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = BoneConfig(\n    r=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== BAT Variant ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BoneConfig\n\n# BAT variant uses different initialization\nconfig = BoneConfig(\n    r=64,\n    init_weights=\"bat\",\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n</syntaxhighlight>\n\n=== Migration to MiSS ===\n<syntaxhighlight lang=\"python\">\n# Convert existing Bone checkpoint to MiSS:\n# python scripts/convert-bone-to-miss.py checkpoint_path\n\n# Use MissConfig for new projects\nfrom peft import MissConfig\n\nconfig = MissConfig(\n    r=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Householder Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "Bone",
          "url": "https://arxiv.org/abs/2409.15371"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_BoneLayer",
      "page_title": "huggingface peft BoneLayer",
      "page_type": "Implementation",
      "overview": "Block-wise orthogonal weight adaptation layer that applies trainable block transformations to model weights with minimal parameters.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Block_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nBlock-wise orthogonal weight adaptation layer that applies trainable block transformations to model weights with minimal parameters.\n\n=== Description ===\n\nBoneLayer implements block-wise adaptation where the weight matrix is divided into blocks and each block is transformed by a trainable parameter matrix. It supports two modes: standard \"bone\" mode which adds block-wise corrections, and \"bat\" (Block Affine Transformation) mode which applies multiplicative block transformations. The method is parameter-efficient as it only learns small block matrices rather than full weight updates.\n\n=== Usage ===\n\nUse BONE when you want a simple block-wise adaptation method that doesn't require complex factorizations. BONE is suitable when the weight matrix can be cleanly divided into blocks and you want direct control over the block size. The \"bat\" mode is useful when multiplicative transformations are preferred over additive ones.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/bone/layer.py src/peft/tuners/bone/layer.py]\n* '''Lines:''' 1-353\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass BoneLayer(BaseTunerLayer):\n    \"\"\"\n    Block orthogonal adaptation layer.\n\n    Attributes:\n        bone_block: Trainable block parameters (nn.ParameterDict)\n        bone_r: Block rank per adapter (dict)\n    \"\"\"\n    adapter_layer_names = (\"bone_block\",)\n    other_param_names = (\"bone_r\",)\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        r: int,\n        init_weights: bool,\n        inference_mode: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Create bone adapter with specified rank.\"\"\"\n\nclass BoneLinear(nn.Module, BoneLayer):\n    \"\"\"Bone implemented in a dense layer.\"\"\"\n    def __init__(\n        self,\n        base_layer: nn.Module,\n        adapter_name: str,\n        r: int = 0,\n        init_weights: Union[bool, str] = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Initialize BoneLinear layer.\n\n        Args:\n            base_layer: The pretrained linear layer\n            adapter_name: Name for the adapter\n            r: Block rank/size\n            init_weights: True for zeros, \"bat\" for BAT mode, False for random\n        \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.bone import BoneLayer, BoneConfig, BoneModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained linear layer to adapt\n|-\n| adapter_name || str || Yes || Name identifier for the adapter\n|-\n| r || int || Yes || Block rank/size for the adaptation\n|-\n| init_weights || bool or str || No || True=zeros, \"bat\"=BAT mode, False=random\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input transformed by base layer + block adaptation\n|-\n| get_delta_weight() || torch.Tensor || Delta weight computed via block operations\n|}\n\n== Usage Examples ==\n\n=== Basic BONE Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BoneConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure BONE\nconfig = BoneConfig(\n    r=64,                # Block size\n    target_modules=[\"q_proj\", \"v_proj\"],\n    init_weights=True,   # Initialize to identity (zeros)\n)\n\n# Create PEFT model\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== BAT Mode ===\n<syntaxhighlight lang=\"python\">\nfrom peft import BoneConfig, get_peft_model\n\n# BAT (Block Affine Transformation) mode\nconfig = BoneConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    init_weights=\"bat\",  # Use multiplicative block transformation\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Block Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_BoneModel",
      "page_title": "huggingface peft BoneModel",
      "page_type": "Implementation",
      "overview": "Model class that applies Bone (Block Affine) by wrapping Linear layers with Householder reflection-based transformations.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|Bone|https://arxiv.org/abs/2409.15371]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Householder_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nModel class that applies Bone (Block Affine) by wrapping Linear layers with Householder reflection-based transformations.\n\n=== Description ===\n\nBoneModel extends BaseTuner to apply Bone/BAT adaptation to transformer models. It creates BoneLinear layers for Linear target modules. Only torch.nn.Linear layers are supported. Note: Bone is deprecated and will be removed in PEFT v0.19.0.\n\n=== Usage ===\n\nUse BoneModel for Householder reflection fine-tuning. Created automatically via get_peft_model with BoneConfig. For new projects, use MissModel instead.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/bone/model.py src/peft/tuners/bone/model.py]\n* '''Lines:''' 1-127\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass BoneModel(BaseTuner):\n    \"\"\"\n    Creates Bone model (deprecated, use MissModel).\n\n    Args:\n        model: Base transformer model\n        config: BoneConfig\n        adapter_name: Name for the adapter\n\n    Attributes:\n        prefix: \"bone_\"\n        tuner_layer_cls: BoneLayer\n    \"\"\"\n    prefix: str = \"bone_\"\n    tuner_layer_cls = BoneLayer\n\n    def _create_and_replace(self, bone_config, adapter_name, target, ...):\n        \"\"\"Create or update Bone layers.\"\"\"\n\n    @staticmethod\n    def _create_new_module(bone_config, adapter_name, target, **kwargs):\n        \"\"\"Create BoneLinear module.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.bone import BoneModel  # Deprecated\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || nn.Module || Yes || Base transformer model\n|-\n| config || BoneConfig || Yes || Bone configuration\n|-\n| adapter_name || str || No || Adapter name (default: \"default\")\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| BoneModel || nn.Module || Model with Bone layers\n|}\n\n== Usage Examples ==\n\n=== Stable Diffusion with Bone ===\n<syntaxhighlight lang=\"python\">\nfrom diffusers import StableDiffusionPipeline\nfrom peft import BoneModel, BoneConfig\n\nconfig_te = BoneConfig(\n    r=8,\n    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    init_weights=True,\n)\n\nconfig_unet = BoneConfig(\n    r=8,\n    target_modules=[\n        \"proj_in\", \"proj_out\", \"to_k\", \"to_q\", \"to_v\", \"to_out.0\",\n        \"ff.net.0.proj\", \"ff.net.2\",\n    ],\n    init_weights=True,\n)\n\nmodel = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\nmodel.text_encoder = BoneModel(model.text_encoder, config_te, \"default\")\nmodel.unet = BoneModel(model.unet, config_unet, \"default\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Householder Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "Bone",
          "url": "https://arxiv.org/abs/2409.15371"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_C3AConfig",
      "page_title": "huggingface peft C3AConfig",
      "page_type": "Implementation",
      "overview": "Configuration class for C3A (Circulant Convolution Adaptation) that stores parameters for FFT-based block circulant fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Circulant_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration class for C3A (Circulant Convolution Adaptation) that stores parameters for FFT-based block circulant fine-tuning.\n\n=== Description ===\n\nC3AConfig stores configuration for circulant convolution-based adaptation. The block_size parameter must divide both input and output dimensions of target layers. Larger block sizes reduce parameter count. The init_weights parameter supports gaussian, kaiming_uniform, and xavier_uniform (default) initialization. FFT operations require float32.\n\n=== Usage ===\n\nUse C3AConfig for FFT-based circulant adaptation. Choose block_size as the GCD of all target layer dimensions for best compatibility. Use block_size_pattern for layer-specific block sizes.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/c3a/config.py src/peft/tuners/c3a/config.py]\n* '''Lines:''' 1-138\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass C3AConfig(PeftConfig):\n    \"\"\"\n    Configuration for C3A (Circulant Convolution Adaptation).\n\n    Args:\n        block_size: Block size (must divide in/out dimensions)\n        target_modules: Modules to apply C3A to\n        bias: Bias handling ('none', 'all', 'c3a_only')\n        block_size_pattern: Per-layer block size overrides\n        init_weights: Initialization method\n    \"\"\"\n    block_size: int = 256\n    target_modules: Optional[Union[list[str], str]] = None\n    bias: str = \"none\"\n    block_size_pattern: Optional[dict] = field(default_factory=dict)\n    init_weights: Optional[Union[bool, Literal[\"gaussian\", \"kaiming_uniform\", \"xavier_uniform\"]]] = \"xavier_uniform\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import C3AConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| block_size || int || No || Block size (default: 256)\n|-\n| target_modules || list[str] || No || Modules to adapt\n|-\n| init_weights || str || No || Initialization method (default: xavier_uniform)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| C3AConfig || dataclass || Configuration for get_peft_model\n|}\n\n== Usage Examples ==\n\n=== Basic C3A Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import C3AConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = C3AConfig(\n    block_size=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    init_weights=\"xavier_uniform\",\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Per-Layer Block Size ===\n<syntaxhighlight lang=\"python\">\nfrom peft import C3AConfig\n\nconfig = C3AConfig(\n    block_size=256,  # Default\n    block_size_pattern={\n        \"model.layers.0.self_attn.k_proj\": 1280,  # Override for specific layer\n    },\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Circulant Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_C3ALayer",
      "page_title": "huggingface peft C3ALayer",
      "page_type": "Implementation",
      "overview": "Layer implementation for C3A that applies block circulant convolution via FFT for parameter-efficient adaptation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Circulant_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nLayer implementation for C3A that applies block circulant convolution via FFT for parameter-efficient adaptation.\n\n=== Description ===\n\nC3ALayer and C3ALinear implement circulant convolution adaptation. The trainable parameter c3a_kernel has shape [out_features/block_size, in_features/block_size, block_size]. Forward pass applies BlockCircularConvolution (via FFT) and divides by input size. Delta weight is computed via get_circulant_fast which constructs the full circulant matrix. FFT operations use float32.\n\n=== Usage ===\n\nUse C3A layers for FFT-based circulant adaptation. Layers support merge/unmerge and multi-adapter inference. Block size must divide both input and output dimensions.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/c3a/layer.py src/peft/tuners/c3a/layer.py]\n* '''Lines:''' 1-203\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass C3ALayer(BaseTunerLayer):\n    \"\"\"Base C3A layer class.\"\"\"\n    adapter_layer_names = (\"c3a_kernel\",)\n\n    def get_delta_weight(self, adapter) -> torch.Tensor:\n        \"\"\"Get circulant matrix from kernel.\"\"\"\n\n    def update_layer(self, adapter_name, block_size, init_weights, ...):\n        \"\"\"Create c3a_kernel parameter.\"\"\"\n\nclass C3ALinear(nn.Module, C3ALayer):\n    \"\"\"C3A implementation for Linear layers.\"\"\"\n    def forward(self, x: torch.Tensor, ...) -> torch.Tensor:\n        \"\"\"Apply block circulant convolution.\"\"\"\n\n    def merge(self, safe_merge: bool = False, adapter_names=None):\n        \"\"\"Merge circulant matrix into weights.\"\"\"\n\n    def unmerge(self):\n        \"\"\"Remove merged adaptation.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.c3a import C3ALayer, C3ALinear\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Linear || Yes || Base linear layer\n|-\n| adapter_name || str || Yes || Adapter name\n|-\n| block_size || int || Yes || Circulant block size\n|-\n| init_weights || str/bool || Yes || Initialization method\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Base output + circulant convolution\n|-\n| get_delta_weight() || torch.Tensor || Full circulant matrix\n|}\n\n== Usage Examples ==\n\n=== C3A Forward Pass ===\n<syntaxhighlight lang=\"python\">\n# C3A forward computation:\n# 1. Get base layer output\nresult = self.base_layer(x)\n\n# 2. Apply block circulant convolution (in float32)\nx = x.to(torch.float32)\nfor adapter in self.active_adapters:\n    kernel = self.c3a_kernel[adapter]\n    x = BlockCircularConvolution.apply(x, kernel) / x.size(-1)\n    result += x.to(result.dtype)\n</syntaxhighlight>\n\n=== Kernel Shape ===\n<syntaxhighlight lang=\"python\">\n# c3a_kernel shape:\n# [out_features // block_size, in_features // block_size, block_size]\n\n# Example: Linear(4096, 4096) with block_size=256\n# kernel shape: [16, 16, 256]\n# Total params: 16 * 16 * 256 = 65,536\n</syntaxhighlight>\n\n=== Initialization Options ===\n<syntaxhighlight lang=\"python\">\n# True: zeros (no-op initialization)\n# \"xavier_uniform\": Xavier uniform (default)\n# \"kaiming_uniform\": Kaiming uniform\n# \"gaussian\": Normal distribution\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Circulant Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_C3AModel",
      "page_title": "huggingface peft C3AModel",
      "page_type": "Implementation",
      "overview": "Model class that applies C3A (Circulant Convolution Adaptation) by wrapping Linear layers with FFT-based block circulant transformations.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|C3A|https://arxiv.org/abs/2407.19342]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Circulant_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nModel class that applies C3A (Circulant Convolution Adaptation) by wrapping Linear layers with FFT-based block circulant transformations.\n\n=== Description ===\n\nC3AModel extends BaseTuner to apply circulant convolution adaptation to transformer models. It creates C3ALinear layers for Linear target modules. Supports per-layer block_size overrides via block_size_pattern using regex matching. Only torch.nn.Linear layers are supported.\n\n=== Usage ===\n\nUse C3AModel for FFT-based circulant adaptation. Created automatically via get_peft_model with C3AConfig. Set block_size_pattern to override block sizes for specific layers.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/c3a/model.py src/peft/tuners/c3a/model.py]\n* '''Lines:''' 1-102\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass C3AModel(BaseTuner):\n    \"\"\"\n    Creates C3A model from pretrained transformer.\n\n    Args:\n        model: Base transformer model\n        config: C3AConfig\n        adapter_name: Name for the adapter\n\n    Attributes:\n        prefix: \"c3a_\"\n        tuner_layer_cls: C3ALayer\n    \"\"\"\n    prefix: str = \"c3a_\"\n    tuner_layer_cls = C3ALayer\n\n    def _create_and_replace(self, c3a_config, adapter_name, target, ...):\n        \"\"\"Create or update C3A layers with per-layer block_size.\"\"\"\n\n    @staticmethod\n    def _create_new_module(c3a_config, adapter_name, target, **kwargs):\n        \"\"\"Create C3ALinear module.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.c3a import C3AModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || nn.Module || Yes || Base transformer model\n|-\n| config || C3AConfig || Yes || C3A configuration\n|-\n| adapter_name || str || No || Adapter name (default: \"default\")\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| C3AModel || nn.Module || Model with C3A layers\n|}\n\n== Usage Examples ==\n\n=== Basic C3A Model ===\n<syntaxhighlight lang=\"python\">\nfrom peft import C3AConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = C3AConfig(\n    block_size=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Per-Layer Block Size ===\n<syntaxhighlight lang=\"python\">\nfrom peft import C3AConfig, get_peft_model\n\n# Use different block sizes for different layers\nconfig = C3AConfig(\n    block_size=256,\n    block_size_pattern={\n        r\"layers\\.0\\..*\\.k_proj\": 128,  # Regex matching\n        \"layers.1.self_attn.k_proj\": 64,  # Exact match\n    },\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Circulant Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "C3A",
          "url": "https://arxiv.org/abs/2407.19342"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_CPTConfig",
      "page_title": "huggingface peft CPTConfig",
      "page_type": "Implementation",
      "overview": "Configuration class for CPT (Context-aware Prompt Tuning) that stores parameters for context-aware soft prompt fine-tuning with token type masks and projection settings.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|CPT|https://arxiv.org/abs/2410.17222]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Prompt_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration class for CPT (Context-aware Prompt Tuning) that stores parameters for context-aware soft prompt fine-tuning with token type masks and projection settings.\n\n=== Description ===\n\nCPTConfig extends PromptLearningConfig for context-aware prompt tuning. It supports token type masks (cpt_tokens_type_mask), weighted loss with decay (opt_weighted_loss_type, opt_loss_decay_factor), and projection settings (opt_projection_epsilon). CPT is specifically designed for causal language models.\n\n=== Usage ===\n\nUse CPTConfig for context-aware prompt tuning. Only supports task_type=CAUSAL_LM. Configure token IDs, masks, and projection parameters for context-aware adaptation.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/cpt/config.py src/peft/tuners/cpt/config.py]\n* '''Lines:''' 1-100\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass CPTConfig(PromptLearningConfig):\n    \"\"\"\n    Configuration for CPT (Context-aware Prompt Tuning).\n\n    Args:\n        cpt_token_ids: Token IDs for CPT prompts\n        cpt_mask: Mask applied to CPT tokens\n        cpt_tokens_type_mask: Type mask for each token\n        opt_weighted_loss_type: Weighted loss type ('none', 'decay')\n        opt_loss_decay_factor: Decay factor for loss weighting\n        opt_projection_epsilon: Epsilon for input projection\n        tokenizer_name_or_path: Tokenizer for initialization\n    \"\"\"\n    cpt_token_ids: Optional[list[int]] = None\n    cpt_mask: Optional[list[int]] = None\n    cpt_tokens_type_mask: Optional[list[int]] = None\n    opt_weighted_loss_type: Optional[Literal[\"none\", \"decay\"]] = \"none\"\n    opt_loss_decay_factor: Optional[float] = 1.0\n    opt_projection_epsilon: Optional[float] = 0.1\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import CPTConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| cpt_token_ids || list[int] || No || Token IDs for prompts\n|-\n| cpt_tokens_type_mask || list[int] || No || Token type mask\n|-\n| opt_weighted_loss_type || str || No || Loss weighting ('none', 'decay')\n|-\n| task_type || TaskType || Yes || Must be CAUSAL_LM\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| CPTConfig || dataclass || Configuration for get_peft_model\n|}\n\n== Usage Examples ==\n\n=== Basic CPT Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import CPTConfig, get_peft_model, TaskType\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = CPTConfig(\n    task_type=TaskType.CAUSAL_LM,  # Required\n    cpt_token_ids=[1, 2, 3, 4, 5],\n    cpt_tokens_type_mask=[1, 1, 1, 1, 1],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== CPT with Weighted Loss ===\n<syntaxhighlight lang=\"python\">\nfrom peft import CPTConfig, TaskType\n\nconfig = CPTConfig(\n    task_type=TaskType.CAUSAL_LM,\n    cpt_token_ids=[1, 2, 3, 4, 5],\n    opt_weighted_loss_type=\"decay\",\n    opt_loss_decay_factor=0.9,\n    opt_projection_epsilon=0.1,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Prompt Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "CPT",
          "url": "https://arxiv.org/abs/2410.17222"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_CorDA",
      "page_title": "huggingface peft CorDA",
      "page_type": "Implementation",
      "overview": "Context-oriented Decomposition Adaptation that initializes LoRA weights using covariance-weighted SVD decomposition of the original weights for improved fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|CorDA|https://arxiv.org/abs/2406.05223]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::SVD_Initialization]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nContext-oriented Decomposition Adaptation that initializes LoRA weights using covariance-weighted SVD decomposition of the original weights for improved fine-tuning.\n\n=== Description ===\n\nCorDA (Context-oriented Decomposition Adaptation) improves LoRA initialization by considering the input covariance distribution. The preprocessing computes a covariance matrix from model activations on sample data, then performs SVD on the covariance-weighted weight matrix. This produces eigenvectors aligned with the data distribution, enabling better knowledge preservation. CorDA supports two modes: IPM (Important Principal Modes) keeps top singular vectors, while KPM (Knowledge Principal Modes) keeps bottom vectors for knowledge retention.\n\n=== Usage ===\n\nUse CorDA when you want better LoRA initialization that preserves important model knowledge. CorDA requires a preprocessing step where you run the model on representative data to collect covariance statistics. The resulting eigenvectors are cached and used to initialize LoRA weights. CorDA is particularly effective for domain adaptation tasks.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/corda.py src/peft/tuners/lora/corda.py]\n* '''Lines:''' 1-361\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass CordaEigens:\n    \"\"\"Eigendecomposition results for CorDA.\"\"\"\n    S_WC: torch.Tensor  # Singular values\n    U_WC: torch.Tensor  # Left singular vectors\n    V_WC: torch.Tensor  # Right singular vectors (covariance-weighted)\n\ndef preprocess_corda(\n    model: nn.Module,\n    lora_config: LoraConfig,\n    run_model: Optional[Callable[[], None]] = None,\n    hooked_model: Optional[nn.Module] = None,\n):\n    \"\"\"\n    Build CorDA eigenvectors from model and data.\n\n    Args:\n        model: Model to preprocess\n        lora_config: Config with corda_config settings\n        run_model: Callback to run model on sample data\n        hooked_model: Optional separate model for hooks\n    \"\"\"\n\ndef calib_cov_distribution(model, config, run_model, hooked_model, covariance_file):\n    \"\"\"Collect covariance matrices via forward hooks.\"\"\"\n\ndef collect_eigens(model, config, verbose):\n    \"\"\"Compute SVD eigenvectors for each target layer.\"\"\"\n\ndef collect_eigens_for_layer(linear, config) -> CordaEigens:\n    \"\"\"Compute covariance-weighted SVD for a single layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.corda import preprocess_corda, CordaEigens\nfrom peft import LoraConfig, CordaConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || nn.Module || Yes || Model to preprocess for CorDA\n|-\n| lora_config || LoraConfig || Yes || Config with corda_config settings\n|-\n| run_model || Callable || Yes* || Callback to collect covariance (if not cached)\n|-\n| cache_file || str || No || Path to save/load CorDA eigenvectors\n|-\n| covariance_file || str || No || Path to save/load covariance matrices\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| module.eigens || CordaEigens || Eigendecomposition stored on each target module\n|-\n| U_WC || torch.Tensor || Left singular vectors [out_features, rank]\n|-\n| V_WC || torch.Tensor || Covariance-weighted right vectors [in_features, rank]\n|}\n\n== Usage Examples ==\n\n=== Basic CorDA Preprocessing ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoraConfig, get_peft_model, CordaConfig\nfrom peft.tuners.lora.corda import preprocess_corda\n\n# Configure CorDA\ncorda_config = CordaConfig(\n    corda_method=\"ipm\",         # Important Principal Modes\n    cache_file=\"./corda_cache.pt\",\n    covariance_file=\"./corda_cov.pt\",\n)\n\nlora_config = LoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    init_lora_weights=\"corda\",\n    corda_config=corda_config,\n)\n\n# Define data collection callback\ndef run_model():\n    for batch in dataloader:\n        with torch.no_grad():\n            model(**batch)\n\n# Preprocess model with CorDA\npreprocess_corda(model, lora_config, run_model=run_model)\n\n# Create PEFT model with CorDA-initialized weights\npeft_model = get_peft_model(model, lora_config)\n</syntaxhighlight>\n\n=== CorDA with KPM Mode ===\n<syntaxhighlight lang=\"python\">\nfrom peft import CordaConfig, LoraConfig\n\n# Use Knowledge Principal Modes for knowledge preservation\ncorda_config = CordaConfig(\n    corda_method=\"kpm\",         # Keep bottom singular vectors\n    cache_file=\"./kpm_cache.pt\",\n)\n\nlora_config = LoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    init_lora_weights=\"corda\",\n    corda_config=corda_config,\n)\n</syntaxhighlight>\n\n=== Using Cached CorDA ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.corda import preprocess_corda\n\n# If cache exists, preprocess_corda loads from file\n# No need to run model again\ncorda_config = CordaConfig(\n    cache_file=\"./existing_corda_cache.pt\",\n)\n\npreprocess_corda(model, lora_config)  # Loads from cache\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "SVD Initialization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "CorDA",
          "url": "https://arxiv.org/abs/2406.05223"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_FourierFTConfig",
      "page_title": "huggingface peft FourierFTConfig",
      "page_type": "Implementation",
      "overview": "Configuration class for FourierFT that stores parameters for Discrete Fourier Transform-based parameter-efficient fine-tuning in the frequency domain.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Fourier_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration class for FourierFT that stores parameters for Discrete Fourier Transform-based parameter-efficient fine-tuning in the frequency domain.\n\n=== Description ===\n\nFourierFTConfig stores configuration for Fourier-based adaptation. The n_frequency parameter controls how many spectral components are learned (must be \u2264 d^2 for d\u00d7d weights). The scaling parameter is analogous to LoRA's lora_alpha. For similar quality to LoRA r=8, use n_frequency=1000 (about 16x fewer parameters).\n\n=== Usage ===\n\nUse FourierFTConfig for frequency-domain adaptation with extreme parameter efficiency. FourierFT requires about 10-16x fewer parameters than LoRA for similar performance. Use higher n_frequency for better accuracy at cost of memory.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/fourierft/config.py src/peft/tuners/fourierft/config.py]\n* '''Lines:''' 1-207\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass FourierFTConfig(PeftConfig):\n    \"\"\"\n    Configuration for FourierFT (Fourier Fine-Tuning).\n\n    Args:\n        n_frequency: Number of learnable frequencies (1-d^2)\n        scaling: Scaling factor (like lora_alpha)\n        random_loc_seed: Seed for spectral entry locations\n        target_modules: Modules to apply FourierFT to\n        fan_in_fan_out: True for Conv1D layers\n        bias: Bias handling ('none', 'all', 'fourier_only')\n        init_weights: True for zeros, False for normal distribution\n        n_frequency_pattern: Per-layer n_frequency overrides\n    \"\"\"\n    n_frequency: int = 1000\n    scaling: float = 150.0\n    random_loc_seed: Optional[int] = 777\n    target_modules: Optional[Union[list[str], str]] = None\n    fan_in_fan_out: bool = False\n    bias: str = \"none\"\n    init_weights: bool = False\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import FourierFTConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| n_frequency || int || No || Number of frequencies (default: 1000)\n|-\n| scaling || float || No || Scaling factor (default: 150.0)\n|-\n| target_modules || list[str] || No || Modules to adapt\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| FourierFTConfig || dataclass || Configuration for get_peft_model\n|}\n\n== Usage Examples ==\n\n=== Basic FourierFT Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import FourierFTConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = FourierFTConfig(\n    n_frequency=1000,\n    scaling=300.0,           # Recommended for LLaMA\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Recommended Settings by Task ===\n<syntaxhighlight lang=\"python\">\nfrom peft import FourierFTConfig\n\n# NLU tasks (RoBERTa)\nconfig_nlu = FourierFTConfig(\n    n_frequency=1000,\n    scaling=150.0,\n    target_modules=[\"query\", \"value\"],\n)\n\n# Instruction tuning (LLaMA)\nconfig_llm = FourierFTConfig(\n    n_frequency=1000,\n    scaling=300.0,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\n# Image classification (ViT)\nconfig_vit = FourierFTConfig(\n    n_frequency=3000,\n    scaling=300.0,\n    target_modules=[\"query\", \"value\"],\n)\n</syntaxhighlight>\n\n=== Parameter Comparison ===\n<syntaxhighlight lang=\"python\">\n# LoRA r=8 on layer d=4096:\n# Params = 2 * d * r = 2 * 4096 * 8 = 65,536\n\n# FourierFT n_frequency=1000:\n# Params = 1000 (per layer)\n# About 65x fewer parameters!\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Fourier Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_FourierFTLayer",
      "page_title": "huggingface peft FourierFTLayer",
      "page_type": "Implementation",
      "overview": "Layer implementation for FourierFT that learns sparse spectral components and applies inverse FFT to compute weight deltas.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Fourier_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nLayer implementation for FourierFT that learns sparse spectral components and applies inverse FFT to compute weight deltas.\n\n=== Description ===\n\nFourierFTLayer and FourierFTLinear implement frequency-domain adaptation. The trainable parameter fourierft_spectrum has n_frequency elements representing sparse Fourier coefficients. Random indices (controlled by random_loc_seed) select which spectral entries are learned. Delta weight is computed via inverse FFT: ifft2(sparse_spectrum) * scaling.\n\n=== Usage ===\n\nUse FourierFT layers for extreme parameter efficiency. The sparse spectrum approach requires far fewer parameters than LoRA for similar quality. Supports merge/unmerge and multi-adapter inference.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/fourierft/layer.py src/peft/tuners/fourierft/layer.py]\n* '''Lines:''' 1-194\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass FourierFTLayer(BaseTunerLayer):\n    \"\"\"Base FourierFT layer class.\"\"\"\n    adapter_layer_names = (\"fourierft_spectrum\",)\n\n    def update_layer(self, adapter_name, n_frequency, scaling, init_weights, ...):\n        \"\"\"Create spectrum parameter and random indices.\"\"\"\n\n    def get_delta_weight(self, adapter) -> torch.Tensor:\n        \"\"\"Compute delta via ifft2(sparse_spectrum) * scaling.\"\"\"\n\nclass FourierFTLinear(nn.Module, FourierFTLayer):\n    \"\"\"FourierFT implementation for Linear layers.\"\"\"\n    def forward(self, x: torch.Tensor, ...) -> torch.Tensor:\n        \"\"\"Apply Fourier-domain adaptation.\"\"\"\n\n    def merge(self, safe_merge: bool = False, adapter_names=None):\n        \"\"\"Merge delta weight into base weights.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.fourierft import FourierFTLayer, FourierFTLinear\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Linear || Yes || Base linear layer\n|-\n| adapter_name || str || Yes || Adapter name\n|-\n| n_frequency || int || No || Number of spectral components\n|-\n| scaling || float || No || Output scaling factor\n|-\n| random_loc_seed || int || No || Seed for index selection\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Base output + Fourier adaptation\n|-\n| get_delta_weight() || torch.Tensor || ifft2(sparse_spectrum) * scaling\n|}\n\n== Usage Examples ==\n\n=== FourierFT Forward Pass ===\n<syntaxhighlight lang=\"python\">\n# FourierFT forward computation:\n# 1. Get base layer output\nresult = self.base_layer(x)\n\n# 2. Compute delta weight from sparse spectrum\ndelta_w = self.get_delta_weight(adapter)\n# delta_w = ifft2(sparse_spectrum) * scaling\n\n# 3. Add Fourier adaptation\nresult = result + F.linear(x, delta_w)\n</syntaxhighlight>\n\n=== Delta Weight Computation ===\n<syntaxhighlight lang=\"python\">\ndef get_delta_weight(self, adapter):\n    spectrum = self.fourierft_spectrum[adapter]  # [n_frequency]\n    indices = self.indices[adapter]  # Random spectral locations\n\n    # Create sparse spectrum\n    dense_spectrum = torch.zeros(out_features, in_features)\n    dense_spectrum[indices[0], indices[1]] = spectrum\n\n    # Inverse FFT to get delta weight\n    delta_weight = torch.fft.ifft2(dense_spectrum).real\n    return delta_weight * self.fourierft_scaling[adapter]\n</syntaxhighlight>\n\n=== Initialization ===\n<syntaxhighlight lang=\"python\">\n# init_weights=True: zeros (no-op)\n# init_weights=False: standard normal (default)\n\n# Random indices are deterministic based on random_loc_seed\n# Same seed = same spectral locations = reproducible\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Fourier Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_FourierFTModel",
      "page_title": "huggingface peft FourierFTModel",
      "page_type": "Implementation",
      "overview": "FourierFTModel creates a Fourier Fine-Tuning adapted model from a pretrained transformers model by applying frequency-domain transformations to specific layers.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Parameter-Efficient Fine-Tuning]], [[domain::Fourier Transform]], [[domain::Model Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\nFourierFTModel creates a Fourier Fine-Tuning adapted model from a pretrained transformers model by applying frequency-domain transformations to specific layers.\n\n=== Description ===\nFourierFTModel is a parameter-efficient fine-tuning (PEFT) method that adapts pretrained models using Fourier frequency transformations. The method modifies selected layers by injecting learnable frequency components, allowing efficient adaptation with minimal trainable parameters. It extends the BaseTuner class and specifically targets linear layers (torch.nn.Linear and Conv1D) for transformation. The approach is based on the research described in https://huggingface.co/papers/2405.03003.\n\nThe model supports adapter-based architecture, allowing multiple adapters with different configurations. It uses pattern-based targeting to apply different frequency parameters to different layers based on regex patterns.\n\n=== Usage ===\nUse FourierFTModel when you need to fine-tune large language models with minimal parameter overhead using frequency-domain techniques. It's particularly useful when:\n* You want to adapt pretrained models efficiently\n* You need control over the number of frequency components per layer\n* You want to experiment with Fourier-based adaptation methods\n* Memory constraints limit full fine-tuning\n\n== Code Reference ==\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/fourierft/model.py src/peft/tuners/fourierft/model.py]\n* '''Lines:''' 31-129\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass FourierFTModel(BaseTuner):\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        config: FourierFTConfig,\n        adapter_name: str = \"default\",\n        low_cpu_mem_usage: bool = False\n    ):\n        \"\"\"\n        Args:\n            model: The model to be adapted\n            config: The configuration of the FourierFT model\n            adapter_name: The name of the adapter, defaults to \"default\"\n            low_cpu_mem_usage: Create empty adapter weights on meta device\n        \"\"\"\n\n    def _create_and_replace(\n        self,\n        fourierft_config,\n        adapter_name,\n        target,\n        target_name,\n        parent,\n        current_key,\n        **optional_kwargs,\n    ):\n        \"\"\"Create and replace target modules with FourierFT layers\"\"\"\n\n    @staticmethod\n    def _create_new_module(fourierft_config, adapter_name, target, **kwargs):\n        \"\"\"Create new FourierFT module based on target type\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import FourierFTModel, FourierFTConfig\nfrom peft import get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n=== Input Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description !! Default\n|-\n| model || torch.nn.Module || The pretrained model to be adapted || Required\n|-\n| config || FourierFTConfig || Configuration object with FourierFT parameters || Required\n|-\n| adapter_name || str || Name identifier for the adapter || \"default\"\n|-\n| low_cpu_mem_usage || bool || Whether to create empty weights on meta device || False\n|}\n\n=== Configuration Parameters (FourierFTConfig) ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| n_frequency || int || Number of frequency components\n|-\n| n_frequency_pattern || dict || Pattern-based frequency configuration for different layers\n|-\n| scaling || float || Scaling factor for frequency components\n|-\n| random_loc_seed || int || Random seed for frequency location initialization\n|-\n| fan_in_fan_out || bool || Whether target layer stores weights as (fan_in, fan_out)\n|-\n| init_weights || bool || Whether to initialize FourierFT weights\n|}\n\n=== Output ===\n{| class=\"wikitable\"\n! Return Type !! Description\n|-\n| torch.nn.Module || The adapted model with FourierFT layers injected\n|}\n\n=== Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| prefix || str || \"fourierft_\" - prefix for FourierFT parameters\n|-\n| tuner_layer_cls || type || FourierFTLayer class\n|-\n| target_module_mapping || dict || Mapping of model architectures to default target modules\n|}\n\n== Usage Examples ==\n=== Basic Usage ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import FourierFTConfig, get_peft_model\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Configure FourierFT\nconfig = FourierFTConfig(\n    n_frequency=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    scaling=1.0,\n    init_weights=True\n)\n\n# Create FourierFT model\nmodel = get_peft_model(base_model, config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Advanced Configuration with Pattern-Based Frequencies ===\n<syntaxhighlight lang=\"python\">\nfrom peft import FourierFTConfig, get_peft_model\n\n# Configure different frequencies for different layer patterns\nconfig = FourierFTConfig(\n    n_frequency=32,  # default\n    n_frequency_pattern={\n        \"q_proj\": 64,  # use 64 frequencies for query projections\n        \"v_proj\": 64,  # use 64 frequencies for value projections\n        \"mlp\": 16      # use 16 frequencies for MLP layers\n    },\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"mlp\"],\n    scaling=2.0,\n    random_loc_seed=42\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Working with Conv1D Layers ===\n<syntaxhighlight lang=\"python\">\n# For models like GPT-2 that use Conv1D\nconfig = FourierFTConfig(\n    n_frequency=32,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    fan_in_fan_out=True,  # Required for Conv1D layers\n    init_weights=True\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\n# Add first adapter\nconfig1 = FourierFTConfig(n_frequency=32, target_modules=[\"q_proj\", \"v_proj\"])\nmodel = get_peft_model(base_model, config1, adapter_name=\"adapter1\")\n\n# Add second adapter\nconfig2 = FourierFTConfig(n_frequency=64, target_modules=[\"q_proj\", \"v_proj\"])\nmodel.add_adapter(\"adapter2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"adapter1\")\n# ... training/inference\nmodel.set_adapter(\"adapter2\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Parameter-Efficient Fine-Tuning",
        "Fourier Transform",
        "Model Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_GraLoRAConfig",
      "page_title": "huggingface peft GraLoRAConfig",
      "page_type": "Implementation",
      "overview": "GraloraConfig is the configuration class for GraLoRA (Gradient Low-Rank Adaptation), a parameter-efficient fine-tuning method that extends LoRA with block-structured low-rank decomposition for improved expressivity.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Parameter-Efficient Fine-Tuning]], [[domain::Low-Rank Adaptation]], [[domain::Configuration]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\nGraloraConfig is the configuration class for GraLoRA (Gradient Low-Rank Adaptation), a parameter-efficient fine-tuning method that extends LoRA with block-structured low-rank decomposition for improved expressivity.\n\n=== Description ===\nGraloraConfig stores the configuration parameters for GraLoRA models, which partition the low-rank matrices into multiple subblocks determined by the gralora_k parameter. This block structure increases the expressivity of the adapter by a factor of gralora_k while maintaining the same parameter count as standard LoRA with the same rank. The configuration also supports Hybrid GraLoRA, which combines GraLoRA with vanilla LoRA when hybrid_r > 0.\n\nThe configuration is a dataclass that extends PeftConfig and sets the peft_type to GRALORA. It includes validation logic to ensure that the rank r is divisible by gralora_k, which is required for proper subblock partitioning.\n\n=== Usage ===\nUse GraloraConfig when you need to:\n* Configure GraLoRA models with block-structured low-rank adaptation\n* Fine-tune models with improved expressivity compared to standard LoRA\n* Experiment with hybrid approaches combining GraLoRA and vanilla LoRA\n* Control the number of subblocks and their rank distribution\n* Set up parameter-efficient fine-tuning with specific target modules\n\n== Code Reference ==\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/gralora/config.py src/peft/tuners/gralora/config.py]\n* '''Lines:''' 22-183\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass GraloraConfig(PeftConfig):\n    r: int = 32\n    hybrid_r: int = 0\n    target_modules: Optional[Union[list[str], str]] = None\n    alpha: int = 64\n    gralora_dropout: float = 0.0\n    gralora_k: int = 2\n    fan_in_fan_out: bool = False\n    bias: str = \"none\"\n    modules_to_save: Optional[list[str]] = None\n    init_weights: bool = True\n    layers_to_transform: Optional[Union[list[int], int]] = None\n    layers_pattern: Optional[str] = None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import GraloraConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n=== Configuration Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| r || int || 32 || GraLoRA rank - determines the rank of each subblock (must be divisible by gralora_k)\n|-\n| hybrid_r || int || 0 || Rank allocated to vanilla LoRA in Hybrid GraLoRA (enables hybrid mode when > 0)\n|-\n| target_modules || Union[list[str], str] || None || Module names or regex to target (e.g., ['q', 'v'] or 'all-linear')\n|-\n| alpha || int || 64 || Scaling factor for GraLoRA adapter (scale = alpha / (r + hybrid_r))\n|-\n| gralora_dropout || float || 0.0 || Dropout probability for the GraLoRA adapter\n|-\n| gralora_k || int || 2 || Number of subblocks (2 for rank \u226432, 4 for rank \u226564 recommended)\n|-\n| fan_in_fan_out || bool || False || True if layer stores weights as (fan_in, fan_out) like Conv1D\n|-\n| bias || str || \"none\" || Bias type: 'none', 'all', or 'gralora_only'\n|-\n| modules_to_save || list[str] || None || Additional modules to be trainable and saved\n|-\n| init_weights || bool || True || Whether to initialize GraLoRA weights with default initialization\n|-\n| layers_to_transform || Union[list[int], int] || None || Specific layer indices to transform\n|-\n| layers_pattern || str || None || Layer pattern name for layers_to_transform (e.g., 'layers', 'h')\n|}\n\n=== Validation Rules ===\n{| class=\"wikitable\"\n! Rule !! Description\n|-\n| r % gralora_k == 0 || Rank must be divisible by gralora_k for valid subblock partitioning\n|-\n| target_modules conversion || Converts list to set for efficient lookup\n|}\n\n=== Output ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| peft_type || PeftType || Set to PeftType.GRALORA after initialization\n|}\n\n== Usage Examples ==\n=== Basic GraLoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import GraloraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Basic configuration with default settings\nconfig = GraloraConfig(\n    r=32,  # Must be divisible by gralora_k\n    gralora_k=2,  # 2 subblocks\n    target_modules=[\"q_proj\", \"v_proj\"],\n    alpha=64\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Higher Rank Configuration ===\n<syntaxhighlight lang=\"python\">\n# For higher ranks, use gralora_k=4\nconfig = GraloraConfig(\n    r=64,  # Higher rank\n    gralora_k=4,  # 4 subblocks for better expressivity\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    alpha=128,\n    gralora_dropout=0.1\n)\n</syntaxhighlight>\n\n=== Hybrid GraLoRA Configuration ===\n<syntaxhighlight lang=\"python\">\n# Combine GraLoRA with vanilla LoRA\nconfig = GraloraConfig(\n    r=32,  # GraLoRA rank\n    hybrid_r=8,  # Additional vanilla LoRA rank\n    gralora_k=2,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    alpha=80  # Scale becomes alpha / (r + hybrid_r) = 80/40 = 2.0\n)\n</syntaxhighlight>\n\n=== Wildcard Target Modules ===\n<syntaxhighlight lang=\"python\">\n# Target all linear layers\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=\"all-linear\",  # Matches all linear layers except output\n    alpha=64\n)\n</syntaxhighlight>\n\n=== Regex Pattern Targeting ===\n<syntaxhighlight lang=\"python\">\n# Use regex to target specific layers\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=r\".*decoder.*(SelfAttention|EncDecAttention).*(q|v)$\",\n    alpha=64\n)\n</syntaxhighlight>\n\n=== Layer-Specific Configuration ===\n<syntaxhighlight lang=\"python\">\n# Only transform specific layers\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=[0, 1, 2, 3],  # Only first 4 layers\n    layers_pattern=\"layers\",\n    alpha=64\n)\n</syntaxhighlight>\n\n=== Configuration with Bias and Dropout ===\n<syntaxhighlight lang=\"python\">\n# Fine-grained control over bias and dropout\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    alpha=64,\n    bias=\"gralora_only\",  # Train only GraLoRA biases\n    gralora_dropout=0.05,\n    modules_to_save=[\"classifier\"]  # Also train classifier\n)\n</syntaxhighlight>\n\n=== Conv1D Layer Configuration ===\n<syntaxhighlight lang=\"python\">\n# For models using Conv1D (like GPT-2)\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    fan_in_fan_out=True,  # Required for Conv1D\n    alpha=64\n)\n</syntaxhighlight>\n\n== Technical Details ==\n=== GraLoRA Mathematics ===\nThe GraLoRA adapter partitions the low-rank matrices into k subblocks:\n* Total rank: r\n* Number of subblocks: gralora_k\n* Rank per subblock: r / gralora_k\n* Expressivity multiplier: gralora_k\n* Scaling factor: alpha / (r + hybrid_r)\n\n=== Hybrid GraLoRA ===\nWhen hybrid_r > 0:\n* GraLoRA parameters: r\n* Vanilla LoRA parameters: hybrid_r\n* Total parameter count: r + hybrid_r\n* Combines block-structured and standard low-rank adaptation\n\n== Related Pages ==\n* [[configures::Component:huggingface_peft_GraloraModel]]\n* [[inherits_from::Configuration:huggingface_peft_PeftConfig]]\n* [[related::Configuration:huggingface_peft_LoraConfig]]\n* [[related::Configuration:huggingface_peft_AdaLoraConfig]]\n* [[uses::Enumeration:huggingface_peft_PeftType]]\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Parameter-Efficient Fine-Tuning",
        "Low-Rank Adaptation",
        "Configuration"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "configures",
          "target_type": "Component",
          "target_id": "huggingface_peft_GraloraModel"
        },
        {
          "edge_type": "inherits_from",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_PeftConfig"
        },
        {
          "edge_type": "related",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_LoraConfig"
        },
        {
          "edge_type": "related",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_AdaLoraConfig"
        },
        {
          "edge_type": "uses",
          "target_type": "Enumeration",
          "target_id": "huggingface_peft_PeftType"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_GraLoRALayer",
      "page_title": "huggingface peft GraLoRALayer",
      "page_type": "Implementation",
      "overview": "Gradient-aware Low-Rank Adaptation layer that applies block-wise LoRA with information exchange between blocks and optional hybrid vanilla LoRA component.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Block_LoRA]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nGradient-aware Low-Rank Adaptation layer that applies block-wise LoRA with information exchange between blocks and optional hybrid vanilla LoRA component.\n\n=== Description ===\n\nGraLoRALayer implements a block-wise variant of LoRA where the weight matrix is divided into k blocks along both input and output dimensions. Each block has its own low-rank adaptation matrices, and GraLoRA enables information exchange between blocks through its specialized attention-like computation. It also supports a hybrid mode where a vanilla LoRA component is added for global information capture alongside the block-wise adaptation.\n\n=== Usage ===\n\nUse GraLoRA when you want block-wise adaptation with information sharing between blocks. The method is particularly useful when you want to capture both local (block-wise) and global (hybrid LoRA) patterns in the weight updates. The k parameter controls the number of blocks, and hybrid_r adds a vanilla LoRA component.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/gralora/layer.py src/peft/tuners/gralora/layer.py]\n* '''Lines:''' 1-393\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass GraloraLayer(BaseTunerLayer):\n    \"\"\"\n    Gradient-aware LoRA layer with block-wise adaptation.\n\n    Attributes:\n        gralora_A: Block-wise A matrices (nn.ParameterDict)\n        gralora_B: Block-wise B matrices (nn.ParameterDict)\n        gralora_A_general: Hybrid LoRA A component (nn.ModuleDict)\n        gralora_B_general: Hybrid LoRA B component (nn.ModuleDict)\n        gralora_k: Number of blocks per adapter (dict)\n        hybrid_r: Rank of hybrid LoRA component (dict)\n    \"\"\"\n    adapter_layer_names = (\"gralora_A\", \"gralora_B\", \"gralora_A_general\", \"gralora_B_general\")\n    other_param_names = (\"r\", \"hybrid_r\", \"alpha\", \"scaling\", \"gralora_dropout\")\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        module_name: str,\n        r: int,\n        alpha: int,\n        gralora_dropout: float,\n        gralora_k: int = 2,\n        hybrid_r: int = 0,\n        init_weights: bool = True,\n    ):\n        \"\"\"Update layer with GraLoRA parameters.\"\"\"\n\nclass Linear(nn.Linear, GraloraLayer):\n    \"\"\"GraLoRA implemented in a dense layer.\"\"\"\n    def __init__(\n        self,\n        base_layer: nn.Module,\n        adapter_name: str,\n        module_name: str,\n        r: int = 0,\n        alpha: int = 1,\n        gralora_dropout: float = 0.0,\n        gralora_k: int = 2,\n        hybrid_r: int = 0,\n        fan_in_fan_out: bool = False,\n        init_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize GraLoRA Linear layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.gralora import GraloraLayer, GraLoRAConfig, GraLoRAModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained linear layer to adapt\n|-\n| adapter_name || str || Yes || Name identifier for the adapter\n|-\n| module_name || str || Yes || Name of the module being adapted\n|-\n| r || int || Yes || Rank for block-wise LoRA (divided among k blocks)\n|-\n| alpha || int || No || Scaling factor (default: 1)\n|-\n| gralora_k || int || No || Number of blocks (default: 2)\n|-\n| hybrid_r || int || No || Rank for vanilla LoRA component (default: 0)\n|-\n| gralora_dropout || float || No || Dropout probability (default: 0.0)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input transformed by base layer + block-wise LoRA + hybrid LoRA\n|-\n| get_delta_weight() || torch.Tensor || Combined delta weight from blocks and hybrid component\n|}\n\n== Usage Examples ==\n\n=== Basic GraLoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import GraLoRAConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure GraLoRA\nconfig = GraLoRAConfig(\n    r=32,                  # Total rank (divided among k blocks)\n    gralora_k=4,           # Number of blocks\n    alpha=64,              # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],\n    gralora_dropout=0.05,\n)\n\n# Create PEFT model\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== GraLoRA with Hybrid Component ===\n<syntaxhighlight lang=\"python\">\nfrom peft import GraLoRAConfig, get_peft_model\n\n# Add hybrid vanilla LoRA for global patterns\nconfig = GraLoRAConfig(\n    r=32,                  # Block-wise rank\n    hybrid_r=16,           # Additional vanilla LoRA rank\n    gralora_k=4,           # 4 blocks\n    alpha=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n# Effective total rank: 32 (blocks) + 16 (hybrid) = 48\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Block LoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_GraLoRAModel",
      "page_title": "huggingface peft GraLoRAModel",
      "page_type": "Implementation",
      "overview": "GraloraModel creates a Vector-based Random Matrix Adaptation (GraLoRA) model from a pretrained transformers model, implementing block-structured low-rank adaptation for improved parameter efficiency and expressivity.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Parameter-Efficient Fine-Tuning]], [[domain::Low-Rank Adaptation]], [[domain::Model Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\nGraloraModel creates a Vector-based Random Matrix Adaptation (GraLoRA) model from a pretrained transformers model, implementing block-structured low-rank adaptation for improved parameter efficiency and expressivity.\n\n=== Description ===\nGraloraModel is a parameter-efficient fine-tuning method that extends LoRA by partitioning low-rank matrices into multiple subblocks. This block structure multiplies the expressivity by the number of subblocks (gralora_k) while maintaining the same parameter count as standard LoRA. The model supports both pure GraLoRA and Hybrid GraLoRA (combining GraLoRA with vanilla LoRA).\n\nThe implementation extends BaseTuner and creates GraloraLayer instances to wrap target modules (Linear and Conv1D layers). It uses the same target module mapping as LoRA, making it compatible with architectures that support LoRA fine-tuning.\n\n=== Usage ===\nUse GraloraModel when you need:\n* More expressive low-rank adaptation than standard LoRA\n* Parameter-efficient fine-tuning with block-structured decomposition\n* Hybrid approaches combining different adaptation strategies\n* Compatible replacement for LoRA with improved performance\n* Fine-tuning large language models with limited memory\n\n== Code Reference ==\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/gralora/model.py src/peft/tuners/gralora/model.py]\n* '''Lines:''' 30-143\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass GraloraModel(BaseTuner):\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        config: GraloraConfig,\n        adapter_name: str = \"default\"\n    ):\n        \"\"\"\n        Args:\n            model: The pretrained model to be adapted\n            config: The configuration of the GraLoRA model\n            adapter_name: The name of the adapter, defaults to \"default\"\n        \"\"\"\n\n    def _create_and_replace(\n        self,\n        gralora_config,\n        adapter_name,\n        target,\n        target_name,\n        parent,\n        current_key,\n        **optional_kwargs,\n    ):\n        \"\"\"Create and replace target modules with GraLoRA layers\"\"\"\n\n    @staticmethod\n    def _create_new_module(gralora_config, adapter_name, target, module_name, **kwargs):\n        \"\"\"Create new GraLoRA module based on target type\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import GraloraConfig, get_peft_model\n# or directly\nfrom peft import GraloraModel\n</syntaxhighlight>\n\n== I/O Contract ==\n=== Input Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description !! Default\n|-\n| model || torch.nn.Module || The pretrained transformers model to be adapted || Required\n|-\n| config || GraloraConfig || Configuration object with GraLoRA parameters || Required\n|-\n| adapter_name || str || Name identifier for the adapter || \"default\"\n|}\n\n=== Configuration Parameters (via GraloraConfig) ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| r || int || GraLoRA rank (must be divisible by gralora_k)\n|-\n| hybrid_r || int || Vanilla LoRA rank for hybrid mode\n|-\n| alpha || int || Scaling factor\n|-\n| gralora_dropout || float || Dropout probability\n|-\n| gralora_k || int || Number of subblocks\n|-\n| target_modules || Union[list[str], str] || Modules to target for adaptation\n|-\n| fan_in_fan_out || bool || Whether layer stores weights as (fan_in, fan_out)\n|-\n| init_weights || bool || Whether to initialize weights\n|}\n\n=== Output ===\n{| class=\"wikitable\"\n! Return Type !! Description\n|-\n| torch.nn.Module || The adapted model with GraLoRA layers injected\n|}\n\n=== Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| prefix || str || \"gralora_\" - prefix for GraLoRA parameters\n|-\n| tuner_layer_cls || type || GraloraLayer class\n|-\n| target_module_mapping || dict || Mapping of model architectures to default target modules (uses LoRA mapping)\n|}\n\n== Usage Examples ==\n=== Basic GraLoRA Model ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import GraloraConfig, get_peft_model\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Configure GraLoRA\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    alpha=64\n)\n\n# Create GraLoRA model\nmodel = get_peft_model(base_model, config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: xxx || all params: xxx || trainable%: x.xx%\n</syntaxhighlight>\n\n=== Training with GraLoRA ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom peft import GraloraConfig, get_peft_model\n\n# Setup\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n\n# Apply GraLoRA\nconfig = GraloraConfig(\n    r=64,\n    gralora_k=4,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    alpha=128,\n    gralora_dropout=0.1\n)\nmodel = get_peft_model(model, config)\n\n# Training\ntraining_args = TrainingArguments(\n    output_dir=\"./gralora_model\",\n    per_device_train_batch_size=8,\n    learning_rate=3e-4\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== Hybrid GraLoRA Model ===\n<syntaxhighlight lang=\"python\">\n# Combine GraLoRA with vanilla LoRA\nconfig = GraloraConfig(\n    r=32,  # GraLoRA rank\n    hybrid_r=8,  # Vanilla LoRA rank\n    gralora_k=2,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    alpha=80\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\n# Create model with first adapter\nconfig1 = GraloraConfig(r=32, gralora_k=2, target_modules=[\"q_proj\", \"v_proj\"])\nmodel = get_peft_model(base_model, config1, adapter_name=\"task1\")\n\n# Add second adapter\nconfig2 = GraloraConfig(r=64, gralora_k=4, target_modules=[\"q_proj\", \"v_proj\"])\nmodel.add_adapter(\"task2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"task1\")\noutput1 = model(**inputs)\n\nmodel.set_adapter(\"task2\")\noutput2 = model(**inputs)\n</syntaxhighlight>\n\n=== Full Example with Inference ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import GraloraConfig, get_peft_model, PeftModel\n\n# Training phase\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    alpha=64\n)\nmodel = get_peft_model(base_model, config)\n\n# ... training code ...\nmodel.save_pretrained(\"./gralora_model\")\n\n# Inference phase\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nmodel = PeftModel.from_pretrained(base_model, \"./gralora_model\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\ninputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(outputs[0]))\n</syntaxhighlight>\n\n=== Target All Linear Layers ===\n<syntaxhighlight lang=\"python\">\n# Apply GraLoRA to all linear layers\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=\"all-linear\",\n    alpha=64\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Conv1D Layer Support (GPT-2) ===\n<syntaxhighlight lang=\"python\">\n# For models using Conv1D like GPT-2\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\nconfig = GraloraConfig(\n    r=32,\n    gralora_k=2,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    fan_in_fan_out=True,  # Required for Conv1D\n    alpha=64\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n== Technical Details ==\n=== Supported Layer Types ===\n* torch.nn.Linear\n* transformers.pytorch_utils.Conv1D\n\n=== Block Structure ===\nGraLoRA partitions the rank r into gralora_k subblocks:\n* Subblock rank: r / gralora_k\n* Total expressivity: multiplied by gralora_k\n* Parameter count: same as LoRA with rank r\n\n=== Hybrid Mode ===\nWhen hybrid_r > 0:\n* GraLoRA parameters: r\n* Standard LoRA parameters: hybrid_r\n* Total trainable rank: r + hybrid_r\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Parameter-Efficient Fine-Tuning",
        "Low-Rank Adaptation",
        "Model Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_HRAConfig",
      "page_title": "huggingface peft HRAConfig",
      "page_type": "Implementation",
      "overview": "HRAConfig is the configuration class for HRA (Householder Reflection Adaptation), a parameter-efficient fine-tuning method that uses Householder reflections to create orthogonal transformations for model adaptation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|Householder Reflection Adaptation|https://huggingface.co/papers/2405.17484]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Parameter-Efficient Fine-Tuning]], [[domain::Orthogonal Transformation]], [[domain::Configuration]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\nHRAConfig is the configuration class for HRA (Householder Reflection Adaptation), a parameter-efficient fine-tuning method that uses Householder reflections to create orthogonal transformations for model adaptation.\n\n=== Description ===\nHRAConfig stores configuration parameters for Householder Reflection Adaptation models. HRA uses Householder reflections to construct orthogonal transformations that adapt pretrained models efficiently. The method maintains orthogonality through optional Gram-Schmidt orthogonalization (apply_GS), which can improve stability and performance.\n\nThe configuration is a dataclass that extends PeftConfig and includes validation logic to ensure layer patterns and transformations are properly specified. It supports targeting specific modules, excluding others, and applying transformations to selected layers only.\n\n=== Usage ===\nUse HRAConfig when you need to:\n* Configure models with orthogonal transformation-based adaptation\n* Apply Householder reflections for parameter-efficient fine-tuning\n* Control orthogonalization behavior with Gram-Schmidt\n* Target specific layers or modules for adaptation\n* Fine-tune vision and language models with orthogonal constraints\n\n== Code Reference ==\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/hra/config.py src/peft/tuners/hra/config.py]\n* '''Lines:''' 24-134\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass HRAConfig(PeftConfig):\n    r: int = 8\n    apply_GS: bool = False\n    target_modules: Optional[Union[list[str], str]] = None\n    exclude_modules: Optional[Union[list[str], str]] = None\n    init_weights: bool = True\n    layers_to_transform: Optional[Union[list[int], int]] = None\n    layers_pattern: Optional[Union[list[str], str]] = None\n    bias: str = \"none\"\n    modules_to_save: Optional[list[str]] = None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import HRAConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n=== Configuration Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| r || int || 8 || Rank of HRA across different layers (best set to even number)\n|-\n| apply_GS || bool || False || Whether to apply Gram-Schmidt orthogonalization\n|-\n| target_modules || Union[list[str], str] || None || Module names or regex to target (e.g., ['q', 'v'] or 'all-linear')\n|-\n| exclude_modules || Union[list[str], str] || None || Module names or regex to exclude from HRA\n|-\n| init_weights || bool || True || Whether to initialize HRA weights with default initialization\n|-\n| layers_to_transform || Union[list[int], int] || None || Specific layer indices to transform\n|-\n| layers_pattern || Union[list[str], str] || None || Layer pattern name (e.g., 'layers', 'h') for layers_to_transform\n|-\n| bias || str || \"none\" || Bias type: 'none', 'all', or 'hra_only'\n|-\n| modules_to_save || list[str] || None || Additional modules to be trainable and saved\n|}\n\n=== Validation Rules ===\n{| class=\"wikitable\"\n! Rule !! Description\n|-\n| Rank recommendation || r should be set to an even number for default initialization method to work\n|-\n| Regex incompatibility || Cannot use layers_to_transform when target_modules is a regex string\n|-\n| Pattern dependency || layers_pattern can only be used when target_modules is not a regex string\n|-\n| Pattern requirement || When layers_pattern is specified, layers_to_transform must also be specified\n|-\n| Set conversion || Converts list target_modules and exclude_modules to sets for efficient lookup\n|}\n\n=== Output ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| peft_type || PeftType || Set to PeftType.HRA after initialization\n|}\n\n== Usage Examples ==\n=== Basic HRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import HRAConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Basic configuration\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    apply_GS=False\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Configuration with Gram-Schmidt Orthogonalization ===\n<syntaxhighlight lang=\"python\">\n# Enable Gram-Schmidt for better orthogonality\nconfig = HRAConfig(\n    r=16,\n    apply_GS=True,  # Apply Gram-Schmidt orthogonalization\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    init_weights=True\n)\n</syntaxhighlight>\n\n=== Excluding Specific Modules ===\n<syntaxhighlight lang=\"python\">\n# Target all linear layers but exclude specific ones\nconfig = HRAConfig(\n    r=8,\n    target_modules=\"all-linear\",\n    exclude_modules=[\"lm_head\", \"classifier\"],\n    apply_GS=True\n)\n</syntaxhighlight>\n\n=== Layer-Specific Transformation ===\n<syntaxhighlight lang=\"python\">\n# Only transform first 4 layers\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=[0, 1, 2, 3],\n    layers_pattern=\"layers\",  # or \"h\" for GPT-2\n    apply_GS=True\n)\n</syntaxhighlight>\n\n=== Regex Pattern Targeting ===\n<syntaxhighlight lang=\"python\">\n# Use regex to target attention layers\nconfig = HRAConfig(\n    r=8,\n    target_modules=r\".*decoder.*(SelfAttention|EncDecAttention).*(q|v)$\",\n    apply_GS=True\n)\n</syntaxhighlight>\n\n=== Configuration for Vision Models (Stable Diffusion) ===\n<syntaxhighlight lang=\"python\">\n# HRA for text encoder\nconfig_te = HRAConfig(\n    r=8,\n    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    init_weights=True,\n    apply_GS=True\n)\n\n# HRA for UNet\nconfig_unet = HRAConfig(\n    r=8,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    init_weights=True,\n    apply_GS=True\n)\n</syntaxhighlight>\n\n=== Configuration with Bias Training ===\n<syntaxhighlight lang=\"python\">\n# Train HRA biases only\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    bias=\"hra_only\",  # Only train HRA adapter biases\n    apply_GS=True\n)\n\n# Train all biases\nconfig_all = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    bias=\"all\",  # Train all biases\n    apply_GS=True\n)\n</syntaxhighlight>\n\n=== Save Additional Modules ===\n<syntaxhighlight lang=\"python\">\n# For sequence classification tasks\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    modules_to_save=[\"classifier\"],  # Also train classifier head\n    apply_GS=True\n)\n</syntaxhighlight>\n\n=== Higher Rank Configuration ===\n<syntaxhighlight lang=\"python\">\n# Higher rank for more capacity (use even numbers)\nconfig = HRAConfig(\n    r=16,  # Even number recommended\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    apply_GS=True,\n    init_weights=True\n)\n</syntaxhighlight>\n\n=== Transform Single Layer ===\n<syntaxhighlight lang=\"python\">\n# Transform only a specific layer\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=5,  # Single integer for one layer\n    layers_pattern=\"layers\",\n    apply_GS=True\n)\n</syntaxhighlight>\n\n== Technical Details ==\n=== Householder Reflections ===\nHRA uses Householder reflections to construct orthogonal transformations:\n* Each reflection is defined by a vector of dimension r\n* Multiple reflections are composed to create the full transformation\n* Orthogonality is preserved through the reflection structure\n\n=== Gram-Schmidt Orthogonalization ===\nWhen apply_GS=True:\n* Applies Gram-Schmidt process to maintain orthogonality\n* Can improve numerical stability\n* May provide better performance in some cases\n\n=== Rank Considerations ===\n* Best to use even values for r for default initialization\n* Higher r provides more capacity but increases parameters\n* Typical values: 8, 16, 32\n\n=== Supported Layers ===\n* torch.nn.Linear\n* torch.nn.Conv2d\n\n== Related Pages ==\n* [[configures::Component:huggingface_peft_HRAModel]]\n* [[inherits_from::Configuration:huggingface_peft_PeftConfig]]\n* [[uses::Enumeration:huggingface_peft_PeftType]]\n* [[related::Configuration:huggingface_peft_LoraConfig]]\n* [[related::Configuration:huggingface_peft_AdaLoraConfig]]\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n* [[based_on::Paper:Householder_Reflection_Adaptation]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Parameter-Efficient Fine-Tuning",
        "Orthogonal Transformation",
        "Configuration"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "Householder Reflection Adaptation",
          "url": "https://huggingface.co/papers/2405.17484"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "configures",
          "target_type": "Component",
          "target_id": "huggingface_peft_HRAModel"
        },
        {
          "edge_type": "inherits_from",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_PeftConfig"
        },
        {
          "edge_type": "uses",
          "target_type": "Enumeration",
          "target_id": "huggingface_peft_PeftType"
        },
        {
          "edge_type": "related",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_LoraConfig"
        },
        {
          "edge_type": "related",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_AdaLoraConfig"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        },
        {
          "edge_type": "based_on",
          "target_type": "Paper",
          "target_id": "Householder_Reflection_Adaptation"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_HRALayer",
      "page_title": "huggingface peft HRALayer",
      "page_type": "Implementation",
      "overview": "Householder Reflection Adaptation layer that applies orthogonal transformations using Householder reflections with optional Gram-Schmidt orthogonalization.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Householder_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nHouseholder Reflection Adaptation layer that applies orthogonal transformations using Householder reflections with optional Gram-Schmidt orthogonalization.\n\n=== Description ===\n\nHRALayer implements adaptation using Householder reflections, which are a numerically stable way to parameterize orthogonal transformations. The layer learns a set of vectors that define Householder reflections, and the product of these reflections forms an orthogonal matrix that transforms the base weights. Optionally, Gram-Schmidt orthogonalization can be applied to ensure the reflection vectors remain orthogonal during training.\n\n=== Usage ===\n\nUse HRA when you want orthogonal fine-tuning with Householder parameterization. This method is particularly stable numerically and guarantees orthogonality. Enable Gram-Schmidt (apply_GS=True) when you want the reflection vectors to be explicitly orthogonalized, which can improve stability but adds computational cost.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/hra/layer.py src/peft/tuners/hra/layer.py]\n* '''Lines:''' 1-462\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass HRALayer(BaseTunerLayer):\n    \"\"\"\n    Householder Reflection Adaptation layer.\n\n    Attributes:\n        hra_u: Householder vectors (nn.ParameterDict)\n        hra_r: Number of reflections per adapter (dict)\n        hra_apply_GS: Whether to apply Gram-Schmidt (dict)\n    \"\"\"\n    adapter_layer_names = (\"hra_u\",)\n    other_param_names = (\"hra_r\", \"hra_apply_GS\")\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        r: int,\n        apply_GS: bool,\n        init_weights: bool,\n        inference_mode: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Create HRA adapter with r Householder reflections.\"\"\"\n\n    def get_delta_weight(self, adapter_name: str, reverse: bool = False) -> torch.Tensor:\n        \"\"\"Compute orthogonal transformation from Householder vectors.\"\"\"\n\nclass HRALinear(nn.Module, HRALayer):\n    \"\"\"HRA implemented in a dense layer.\"\"\"\n    def __init__(\n        self,\n        base_layer: nn.Module,\n        adapter_name: str,\n        r: int = 0,\n        apply_GS: bool = False,\n        init_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize HRA Linear layer.\"\"\"\n\nclass HRAConv2d(nn.Module, HRALayer):\n    \"\"\"HRA implemented in Conv2d layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.hra import HRALayer, HRAConfig, HRAModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained layer to adapt (Linear or Conv2d)\n|-\n| adapter_name || str || Yes || Name identifier for the adapter\n|-\n| r || int || Yes || Number of Householder reflections\n|-\n| apply_GS || bool || No || Apply Gram-Schmidt orthogonalization (default: False)\n|-\n| init_weights || bool || No || Use symmetric initialization if r is even (default: True)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input transformed by orthogonal Householder transformation\n|-\n| get_delta_weight() || torch.Tensor || Orthogonal matrix from product of Householder reflections\n|}\n\n== Usage Examples ==\n\n=== Basic HRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import HRAConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure HRA\nconfig = HRAConfig(\n    r=8,                   # Number of Householder reflections\n    target_modules=[\"q_proj\", \"v_proj\"],\n    apply_GS=False,        # No Gram-Schmidt\n    init_weights=True,     # Symmetric initialization\n)\n\n# Create PEFT model\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== HRA with Gram-Schmidt ===\n<syntaxhighlight lang=\"python\">\nfrom peft import HRAConfig, get_peft_model\n\n# Use Gram-Schmidt for guaranteed orthogonality\nconfig = HRAConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    apply_GS=True,         # Apply Gram-Schmidt orthogonalization\n)\n\nmodel = get_peft_model(model, config)\n# More stable but computationally more expensive\n</syntaxhighlight>\n\n=== HRA for Vision Models ===\n<syntaxhighlight lang=\"python\">\nfrom peft import HRAConfig, get_peft_model\nfrom transformers import AutoModelForImageClassification\n\n# HRA works well for vision models via Conv2d support\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\"\n)\n\nconfig = HRAConfig(\n    r=4,                   # Fewer reflections for efficiency\n    target_modules=[\"query\", \"value\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Householder Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_HRAModel",
      "page_title": "huggingface peft HRAModel",
      "page_type": "Implementation",
      "overview": "HRAModel creates a Householder Reflection Adaptation (HRA) model from a pretrained model, using orthogonal transformations based on Householder reflections for parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|Householder Reflection Adaptation|https://huggingface.co/papers/2405.17484]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Computer Vision]], [[domain::PEFT]], [[domain::Parameter-Efficient Fine-Tuning]], [[domain::Orthogonal Transformation]], [[domain::Model Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\nHRAModel creates a Householder Reflection Adaptation (HRA) model from a pretrained model, using orthogonal transformations based on Householder reflections for parameter-efficient fine-tuning.\n\n=== Description ===\nHRAModel implements Householder Reflection Adaptation, a PEFT method that uses Householder reflections to construct orthogonal transformations for model adaptation. Unlike methods based on low-rank decomposition (like LoRA), HRA maintains orthogonality through reflection-based transformations, which can provide different inductive biases and potentially better performance for certain tasks.\n\nThe implementation extends BaseTuner and supports both Linear and Conv2d layers, making it applicable to both language models and vision models (e.g., Stable Diffusion). The model supports optional Gram-Schmidt orthogonalization for improved numerical stability.\n\n=== Usage ===\nUse HRAModel when you need:\n* Orthogonal transformation-based model adaptation\n* Parameter-efficient fine-tuning with different inductive biases than LoRA\n* Fine-tuning of both NLP and vision models (Linear and Conv2d support)\n* Better preservation of model properties through orthogonal constraints\n* Stable adaptation through optional Gram-Schmidt orthogonalization\n\n== Code Reference ==\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/hra/model.py src/peft/tuners/hra/model.py]\n* '''Lines:''' 24-132\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass HRAModel(BaseTuner):\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        config: HRAConfig,\n        adapter_name: str = \"default\",\n        low_cpu_mem_usage: bool = False\n    ):\n        \"\"\"\n        Args:\n            model: The model to which adapter tuner layers will be attached\n            config: The configuration of the HRA model\n            adapter_name: The name of the adapter, defaults to \"default\"\n            low_cpu_mem_usage: Create empty adapter weights on meta device\n        \"\"\"\n\n    def _create_and_replace(\n        self,\n        hra_config,\n        adapter_name,\n        target,\n        target_name,\n        parent,\n        current_key,\n        **optional_kwargs,\n    ):\n        \"\"\"Create and replace target modules with HRA layers\"\"\"\n\n    @staticmethod\n    def _create_new_module(hra_config, adapter_name, target, **kwargs):\n        \"\"\"Create new HRA module based on target type\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import HRAModel, HRAConfig\nfrom peft import get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n=== Input Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description !! Default\n|-\n| model || torch.nn.Module || The model to be adapted (language or vision) || Required\n|-\n| config || HRAConfig || Configuration object with HRA parameters || Required\n|-\n| adapter_name || str || Name identifier for the adapter || \"default\"\n|-\n| low_cpu_mem_usage || bool || Whether to create empty weights on meta device || False\n|}\n\n=== Configuration Parameters (via HRAConfig) ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| r || int || Rank of HRA (number of Householder reflections)\n|-\n| apply_GS || bool || Whether to apply Gram-Schmidt orthogonalization\n|-\n| target_modules || Union[list[str], str] || Modules to target for adaptation\n|-\n| exclude_modules || Union[list[str], str] || Modules to exclude from adaptation\n|-\n| init_weights || bool || Whether to initialize HRA weights\n|-\n| layers_to_transform || Union[list[int], int] || Specific layer indices to transform\n|-\n| layers_pattern || Union[list[str], str] || Layer pattern name for targeting\n|}\n\n=== Output ===\n{| class=\"wikitable\"\n! Return Type !! Description\n|-\n| torch.nn.Module || The adapted model with HRA layers injected\n|}\n\n=== Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| prefix || str || \"hra_\" - prefix for HRA parameters\n|-\n| tuner_layer_cls || type || HRALayer class\n|-\n| target_module_mapping || dict || Mapping of model architectures to default target modules\n|}\n\n== Usage Examples ==\n=== Basic Language Model Fine-tuning ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import HRAConfig, get_peft_model\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Configure HRA\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    apply_GS=True\n)\n\n# Create HRA model\nmodel = get_peft_model(base_model, config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Stable Diffusion Fine-tuning ===\n<syntaxhighlight lang=\"python\">\nfrom diffusers import StableDiffusionPipeline\nfrom peft import HRAModel, HRAConfig\n\n# Configuration for text encoder\nconfig_te = HRAConfig(\n    r=8,\n    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    init_weights=True,\n    apply_GS=True\n)\n\n# Configuration for UNet\nconfig_unet = HRAConfig(\n    r=8,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    init_weights=True,\n    apply_GS=True\n)\n\n# Load model\nmodel = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n\n# Apply HRA\nmodel.text_encoder = HRAModel(model.text_encoder, config_te, \"default\")\nmodel.unet = HRAModel(model.unet, config_unet, \"default\")\n</syntaxhighlight>\n\n=== Training with HRA ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer\nfrom peft import HRAConfig, get_peft_model\n\n# Setup model\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Apply HRA with Gram-Schmidt\nconfig = HRAConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    apply_GS=True,\n    init_weights=True\n)\nmodel = get_peft_model(model, config)\n\n# Training\ntraining_args = TrainingArguments(\n    output_dir=\"./hra_model\",\n    per_device_train_batch_size=8,\n    learning_rate=3e-4,\n    num_train_epochs=3\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset\n)\n\ntrainer.train()\n\n# Save\nmodel.save_pretrained(\"./hra_model\")\n</syntaxhighlight>\n\n=== Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\n# Create model with first adapter\nconfig1 = HRAConfig(r=8, target_modules=[\"q_proj\", \"v_proj\"], apply_GS=True)\nmodel = get_peft_model(base_model, config1, adapter_name=\"task1\")\n\n# Add second adapter\nconfig2 = HRAConfig(r=16, target_modules=[\"q_proj\", \"v_proj\"], apply_GS=True)\nmodel.add_adapter(\"task2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"task1\")\noutput1 = model(**inputs)\n\nmodel.set_adapter(\"task2\")\noutput2 = model(**inputs)\n</syntaxhighlight>\n\n=== Excluding Specific Modules ===\n<syntaxhighlight lang=\"python\">\n# Target all linear layers except specific ones\nconfig = HRAConfig(\n    r=8,\n    target_modules=\"all-linear\",\n    exclude_modules=[\"lm_head\"],\n    apply_GS=True\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Layer-Specific Fine-tuning ===\n<syntaxhighlight lang=\"python\">\n# Only transform first 4 layers\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=[0, 1, 2, 3],\n    layers_pattern=\"layers\",\n    apply_GS=True\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Loading Pretrained HRA Model ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Load HRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"./hra_model\")\n\n# Inference\ninputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_length=50)\n</syntaxhighlight>\n\n=== Conv2d Support for Vision Models ===\n<syntaxhighlight lang=\"python\">\nfrom torchvision.models import resnet50\nfrom peft import HRAConfig, get_peft_model\n\n# Load vision model\nmodel = resnet50(pretrained=True)\n\n# Configure HRA for Conv2d layers\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"conv1\", \"conv2\"],  # Conv2d layers\n    apply_GS=True,\n    init_weights=True\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Low Memory Loading ===\n<syntaxhighlight lang=\"python\">\n# Create model with low CPU memory usage\nconfig = HRAConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    apply_GS=True\n)\n\nmodel = HRAModel(\n    base_model,\n    config,\n    adapter_name=\"default\",\n    low_cpu_mem_usage=True  # Speeds up loading\n)\n</syntaxhighlight>\n\n== Technical Details ==\n=== Householder Reflections ===\nHRA constructs orthogonal transformations using Householder reflections:\n* Each reflection is defined by a vector v of dimension d\n* Reflection formula: H = I - 2(vv^T)/(v^Tv)\n* Multiple reflections are composed: H_1 * H_2 * ... * H_r\n* Maintains orthogonality by construction\n\n=== Gram-Schmidt Orthogonalization ===\nWhen apply_GS=True:\n* Applies Gram-Schmidt process to reflection vectors\n* Improves numerical stability\n* Ensures strict orthogonality during training\n\n=== Supported Layer Types ===\n* '''torch.nn.Linear''' - For language models and transformers\n* '''torch.nn.Conv2d''' - For vision models and CNNs\n\n=== Parameter Count ===\nFor a layer with dimension d and rank r:\n* Number of parameters: r * d\n* Comparable to LoRA but with orthogonal constraints\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "Computer Vision",
        "PEFT",
        "Parameter-Efficient Fine-Tuning",
        "Orthogonal Transformation",
        "Model Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "Householder Reflection Adaptation",
          "url": "https://huggingface.co/papers/2405.17484"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_IA3Config",
      "page_title": "huggingface peft IA3Config",
      "page_type": "Implementation",
      "overview": "IA3Config is the configuration class for (IA)\u00b3 (Infused Adapter by Inhibiting and Amplifying Inner Activations), a parameter-efficient fine-tuning method that learns element-wise scaling vectors for model adaptation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Parameter-Efficient Fine-Tuning]], [[domain::Configuration]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\nIA3Config is the configuration class for (IA)\u00b3 (Infused Adapter by Inhibiting and Amplifying Inner Activations), a parameter-efficient fine-tuning method that learns element-wise scaling vectors for model adaptation.\n\n=== Description ===\nIA3Config stores the configuration parameters for (IA)\u00b3 models, which adapt pretrained models by learning scaling vectors that are multiplied element-wise with layer activations. Unlike LoRA which adds trainable rank decomposition matrices, (IA)\u00b3 multiplies learned vectors to either inputs (for feedforward layers) or outputs (for attention layers), resulting in even fewer trainable parameters.\n\nThe configuration is a dataclass that extends PeftConfig and includes validation logic to ensure feedforward_modules is a subset of target_modules. It distinguishes between attention and feedforward layers to apply scaling at the appropriate point in the computation.\n\n=== Usage ===\nUse IA3Config when you need:\n* Extremely parameter-efficient fine-tuning (fewer parameters than LoRA)\n* Element-wise scaling adaptation for model customization\n* Different treatment for attention vs feedforward layers\n* Fast adaptation with minimal memory overhead\n* Compatible alternative to full fine-tuning or LoRA\n\n== Code Reference ==\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/ia3/config.py src/peft/tuners/ia3/config.py]\n* '''Lines:''' 24-113\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass IA3Config(PeftConfig):\n    target_modules: Optional[Union[list[str], str]] = None\n    exclude_modules: Optional[Union[list[str], str]] = None\n    feedforward_modules: Optional[Union[list[str], str]] = None\n    fan_in_fan_out: bool = False\n    modules_to_save: Optional[list[str]] = None\n    init_ia3_weights: bool = True\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import IA3Config\n</syntaxhighlight>\n\n== I/O Contract ==\n=== Configuration Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| target_modules || Union[list[str], str] || None || Module names or regex to apply (IA)\u00b3 to (e.g., ['q', 'v'] or 'all-linear')\n|-\n| exclude_modules || Union[list[str], str] || None || Module names or regex to exclude from (IA)\u00b3\n|-\n| feedforward_modules || Union[list[str], str] || None || Modules to treat as feedforward (scale inputs instead of outputs)\n|-\n| fan_in_fan_out || bool || False || True if layer stores weights as (fan_in, fan_out) like Conv1D\n|-\n| modules_to_save || list[str] || None || Additional modules to be trainable and saved\n|-\n| init_ia3_weights || bool || True || Whether to initialize (IA)\u00b3 scaling vectors\n|}\n\n=== Validation Rules ===\n{| class=\"wikitable\"\n! Rule !! Description\n|-\n| Feedforward subset || feedforward_modules must be a subset of target_modules\n|-\n| Set conversion || Converts list target_modules, exclude_modules, and feedforward_modules to sets\n|-\n| Initialization recommendation || init_ia3_weights=False is discouraged\n|}\n\n=== Output ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| peft_type || PeftType || Set to PeftType.IA3 after initialization\n|}\n\n== Usage Examples ==\n=== Basic IA3 Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import IA3Config, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Basic configuration\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"v_proj\"],\n    feedforward_modules=[],  # Treat as attention layers (scale outputs)\n    init_ia3_weights=True\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nmodel = get_peft_model(base_model, config)\n\n# Very few trainable parameters\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== With Feedforward Modules ===\n<syntaxhighlight lang=\"python\">\n# Distinguish between attention and feedforward layers\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"fc1\", \"fc2\"],\n    feedforward_modules=[\"fc1\", \"fc2\"],  # Scale inputs for these\n    init_ia3_weights=True\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Wildcard Target All Linear ===\n<syntaxhighlight lang=\"python\">\n# Target all linear layers except output\nconfig = IA3Config(\n    target_modules=\"all-linear\",\n    feedforward_modules=[\"mlp.fc1\", \"mlp.fc2\"],\n    init_ia3_weights=True\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Regex Pattern Targeting ===\n<syntaxhighlight lang=\"python\">\n# Use regex to target specific layers\nconfig = IA3Config(\n    target_modules=r\".*decoder.*(SelfAttention|EncDecAttention).*(q|v)$\",\n    feedforward_modules=r\".*decoder.*fc2$\",\n    init_ia3_weights=True\n)\n</syntaxhighlight>\n\n=== Excluding Specific Modules ===\n<syntaxhighlight lang=\"python\">\n# Target many layers but exclude some\nconfig = IA3Config(\n    target_modules=\"all-linear\",\n    exclude_modules=[\"lm_head\", \"embed_tokens\"],\n    feedforward_modules=[\"fc1\", \"fc2\"],\n    init_ia3_weights=True\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Conv1D Layer Configuration (GPT-2) ===\n<syntaxhighlight lang=\"python\">\n# For models using Conv1D like GPT-2\nconfig = IA3Config(\n    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n    feedforward_modules=[\"c_fc\"],\n    fan_in_fan_out=True,  # Required for Conv1D\n    init_ia3_weights=True\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Sequence Classification ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForSequenceClassification\n\n# Fine-tune for classification\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"v_proj\"],\n    feedforward_modules=[\"fc2\"],\n    modules_to_save=[\"classifier\"],  # Also train classifier head\n    init_ia3_weights=True\n)\n\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=2\n)\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Minimal Configuration ===\n<syntaxhighlight lang=\"python\">\n# Absolute minimal parameter count\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"v_proj\"],  # Only attention query and value\n    init_ia3_weights=True\n)\n\nmodel = get_peft_model(base_model, config)\n# Extremely few trainable parameters\n</syntaxhighlight>\n\n=== Token Classification ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForTokenClassification\n\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"v_proj\"],\n    feedforward_modules=[\"fc2\"],\n    modules_to_save=[\"classifier\"],\n    init_ia3_weights=True\n)\n\nbase_model = AutoModelForTokenClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=9  # e.g., NER tags\n)\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Large Language Model Configuration ===\n<syntaxhighlight lang=\"python\">\n# For Llama-style models\nconfig = IA3Config(\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\"\n    ],\n    feedforward_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"],\n    init_ia3_weights=True\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Quantized Model Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training\n\n# Load quantized model\nbnb_config = BitsAndBytesConfig(load_in_8bit=True)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-6.7b\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel = prepare_model_for_kbit_training(base_model)\n\n# IA3 works great with quantization\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"v_proj\"],\n    feedforward_modules=[\"fc2\"],\n    init_ia3_weights=True\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n== Technical Details ==\n=== (IA)\u00b3 Scaling Mechanism ===\n* '''Attention layers''' (not in feedforward_modules):\n  - Scaling applied to outputs\n  - output = base_layer(input) * ia3_vector\n\n* '''Feedforward layers''' (in feedforward_modules):\n  - Scaling applied to inputs\n  - output = base_layer(input * ia3_vector)\n\n=== Parameter Count ===\nFor a layer with output dimension d:\n* (IA)\u00b3 parameters: d (a single scaling vector)\n* Much fewer than LoRA: d << 2 * d * r\n\n=== Initialization ===\nWhen init_ia3_weights=True:\n* Scaling vectors initialized to ones\n* Model initially behaves like the base model\n* Discouraging to set to False\n\n=== Target Module Specification ===\n* '''List''': Exact match or suffix match\n* '''String''': Regex pattern matching\n* '''\"all-linear\"''': All Linear/Conv1D except output layer\n\n== Related Pages ==\n* [[configures::Component:huggingface_peft_IA3Model]]\n* [[inherits_from::Configuration:huggingface_peft_PeftConfig]]\n* [[uses::Enumeration:huggingface_peft_PeftType]]\n* [[related::Configuration:huggingface_peft_LoraConfig]]\n* [[related::Configuration:huggingface_peft_AdaLoraConfig]]\n* [[related::Component:huggingface_peft_IA3Quantized]]\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n* [[implements::Technique:IA3_Adaptation]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Parameter-Efficient Fine-Tuning",
        "Configuration"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "configures",
          "target_type": "Component",
          "target_id": "huggingface_peft_IA3Model"
        },
        {
          "edge_type": "inherits_from",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_PeftConfig"
        },
        {
          "edge_type": "uses",
          "target_type": "Enumeration",
          "target_id": "huggingface_peft_PeftType"
        },
        {
          "edge_type": "related",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_LoraConfig"
        },
        {
          "edge_type": "related",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_AdaLoraConfig"
        },
        {
          "edge_type": "related",
          "target_type": "Component",
          "target_id": "huggingface_peft_IA3Quantized"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        },
        {
          "edge_type": "implements",
          "target_type": "Technique",
          "target_id": "IA3_Adaptation"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_IA3Layer",
      "page_title": "huggingface peft IA3Layer",
      "page_type": "Implementation",
      "overview": "Infused Adapter by Inhibiting and Amplifying Inner Activations layer that applies learned scaling vectors to activations for ultra-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|IA3|https://arxiv.org/abs/2205.05638]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Activation_Scaling]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nInfused Adapter by Inhibiting and Amplifying Inner Activations layer that applies learned scaling vectors to activations for ultra-efficient fine-tuning.\n\n=== Description ===\n\nIA3Layer implements the (IA)^3 method which introduces learned vectors that rescale key, value, and feedforward activations. Unlike LoRA which adds low-rank matrices, IA3 simply multiplies activations by a learned vector, making it extremely parameter-efficient (typically 10x fewer parameters than LoRA). The method distinguishes between feedforward layers (scale inputs) and attention layers (scale outputs).\n\n=== Usage ===\n\nUse IA3 when you need the most parameter-efficient adaptation method. IA3 is ideal when memory is extremely constrained or when you want to store many adapters with minimal overhead. Note that IA3's unmerge operation is approximate due to the multiplicative nature of the adaptation.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/ia3/layer.py src/peft/tuners/ia3/layer.py]\n* '''Lines:''' 1-331\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass IA3Layer(BaseTunerLayer):\n    \"\"\"\n    (IA)^3 layer for activation scaling.\n\n    Attributes:\n        ia3_l: Learned scaling vectors (nn.ParameterDict)\n        is_feedforward: Whether this is a feedforward layer (bool)\n    \"\"\"\n    adapter_layer_names = (\"ia3_l\",)\n\n    def __init__(\n        self,\n        base_layer: nn.Module,\n        is_feedforward: bool,\n        **kwargs\n    ) -> None:\n        \"\"\"Initialize IA3 layer.\"\"\"\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        init_ia3_weights: bool,\n        inference_mode: bool = False,\n        **kwargs\n    ):\n        \"\"\"Create IA3 scaling vector for adapter.\"\"\"\n\nclass Linear(nn.Module, IA3Layer):\n    \"\"\"IA3 implemented in a dense layer.\"\"\"\n    def __init__(\n        self,\n        base_layer: nn.Module,\n        adapter_name: str,\n        fan_in_fan_out: bool = False,\n        is_feedforward: bool = False,\n        is_target_conv_1d_layer: bool = False,\n        init_ia3_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize IA3 Linear layer.\"\"\"\n\nclass Conv2d(IA3Layer):\n    \"\"\"IA3 implemented in a 2D convolutional layer.\"\"\"\n\nclass Conv3d(IA3Layer):\n    \"\"\"IA3 implemented in a 3D convolutional layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.ia3 import IA3Layer, IA3Config, IA3Model\nfrom peft import IA3Config, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained layer to adapt\n|-\n| adapter_name || str || Yes || Name identifier for the adapter\n|-\n| is_feedforward || bool || Yes || True for FFN layers (scale input), False for attention (scale output)\n|-\n| fan_in_fan_out || bool || No || True if layer stores weights as (fan_in, fan_out)\n|-\n| init_ia3_weights || bool || No || Initialize vectors to ones (default: True)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Activation scaled by learned vector\n|-\n| ia3_l || nn.Parameter || Shape (1, in_features) for FFN or (out_features, 1) for attention\n|}\n\n== Usage Examples ==\n\n=== Basic IA3 Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import IA3Config, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure IA3\nconfig = IA3Config(\n    target_modules=[\"k_proj\", \"v_proj\", \"down_proj\"],\n    feedforward_modules=[\"down_proj\"],  # Specify feedforward layers\n    init_ia3_weights=True,\n)\n\n# Create PEFT model\nmodel = get_peft_model(model, config)\nprint(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n# IA3 typically has ~10x fewer parameters than LoRA\n</syntaxhighlight>\n\n=== IA3 for T5 ===\n<syntaxhighlight lang=\"python\">\nfrom peft import IA3Config, get_peft_model\nfrom transformers import AutoModelForSeq2SeqLM\n\n# IA3 was originally designed for T5-style models\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n\nconfig = IA3Config(\n    target_modules=[\"k\", \"v\", \"wo\"],\n    feedforward_modules=[\"wo\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Merging IA3 (Approximate) ===\n<syntaxhighlight lang=\"python\">\n# Note: IA3 unmerge is approximate due to multiplicative scaling\nmodel.merge_adapter()\n\n# For exact reproduction, keep adapters separate\n# IA3 is best used without merging\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Activation Scaling"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "IA3",
          "url": "https://arxiv.org/abs/2205.05638"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_IA3Model",
      "page_title": "huggingface peft IA3Model",
      "page_type": "Implementation",
      "overview": "Model class that creates IA3 adapters from a pretrained transformer, managing activation scaling vectors across key, value, and feedforward modules.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|IA3|https://arxiv.org/abs/2205.05638]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Activation_Scaling]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nModel class that creates IA3 adapters from a pretrained transformer, managing activation scaling vectors across key, value, and feedforward modules.\n\n=== Description ===\n\nIA3Model extends BaseTuner to implement the Infused Adapter by Inhibiting and Amplifying Inner Activations method. The model manages learned scaling vectors for attention (key/value) and feedforward layers. It distinguishes between feedforward and attention layers, applying different scaling strategies. The model supports 4-bit and 8-bit quantization via bitsandbytes integration.\n\n=== Usage ===\n\nUse IA3Model when you need the most parameter-efficient adapter method with minimal memory footprint. IA3 is ideal for deploying many adapters simultaneously since each adapter only adds vectors (not matrices), making it extremely lightweight compared to LoRA.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/ia3/model.py src/peft/tuners/ia3/model.py]\n* '''Lines:''' 1-316\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass IA3Model(BaseTuner):\n    \"\"\"\n    Creates IA3 model from a pretrained transformers model.\n\n    Args:\n        model: The model to be adapted (PreTrainedModel)\n        config: The configuration of the IA3 model (IA3Config)\n        adapter_name: The name of the adapter (default: \"default\")\n        low_cpu_mem_usage: Create empty adapter weights on meta device\n\n    Attributes:\n        prefix: \"ia3_\" - prefix for IA3 parameters\n        tuner_layer_cls: IA3Layer class\n    \"\"\"\n    prefix = \"ia3_\"\n    tuner_layer_cls = IA3Layer\n\n    @staticmethod\n    def _create_new_module(ia3_config, adapter_name, target, **kwargs):\n        \"\"\"Create new IA3 module for the target layer.\"\"\"\n\n    def add_weighted_adapter(\n        self,\n        adapters: list[str],\n        weights: list[float],\n        adapter_name: str,\n    ) -> None:\n        \"\"\"Merge adapters with given weights into new adapter.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import IA3Model, IA3Config, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || The pretrained model to adapt\n|-\n| config || IA3Config || Yes || Configuration specifying target and feedforward modules\n|-\n| adapter_name || str || No || Name for the adapter (default: \"default\")\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward() || ModelOutput || Model output with IA3 scaling applied\n|-\n| add_weighted_adapter() || None || Creates new merged adapter from existing adapters\n|}\n\n== Usage Examples ==\n\n=== Basic IA3 Model ===\n<syntaxhighlight lang=\"python\">\nfrom peft import IA3Config, IA3Model, get_peft_model\nfrom transformers import AutoModelForSeq2SeqLM\n\n# Load base model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n# Configure IA3\nconfig = IA3Config(\n    target_modules=[\"k\", \"v\", \"wo\"],\n    feedforward_modules=[\"wo\"],  # Specify which are feedforward\n    init_ia3_weights=True,\n)\n\n# Create IA3 model\nmodel = get_peft_model(model, config)\n\n# Check parameter count\nmodel.print_trainable_parameters()\n# IA3 typically has ~10x fewer parameters than LoRA\n</syntaxhighlight>\n\n=== IA3 with Model Type Auto-Detection ===\n<syntaxhighlight lang=\"python\">\nfrom peft import IA3Config, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# For supported models, target/feedforward modules are auto-detected\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\nconfig = IA3Config(\n    # target_modules and feedforward_modules auto-detected for gpt2\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Weighted Adapter Merging ===\n<syntaxhighlight lang=\"python\">\n# Train multiple IA3 adapters for different tasks\n# ...training code...\n\n# Merge adapters with custom weights\nmodel.add_weighted_adapter(\n    adapters=[\"task1\", \"task2\"],\n    weights=[0.7, 0.3],\n    adapter_name=\"merged_tasks\",\n)\n\n# Activate the merged adapter\nmodel.set_adapter(\"merged_tasks\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Activation Scaling"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "IA3",
          "url": "https://arxiv.org/abs/2205.05638"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_IA3Quantized",
      "page_title": "huggingface peft IA3Quantized",
      "page_type": "Implementation",
      "overview": "IA3 quantized layer implementations (Linear8bitLt and Linear4bit) provide parameter-efficient fine-tuning with bitsandbytes quantization support, enabling (IA)\u00b3 adaptation on quantized models for reduced memory footprint.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Parameter-Efficient Fine-Tuning]], [[domain::Quantization]], [[domain::Model Compression]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\nIA3 quantized layer implementations (Linear8bitLt and Linear4bit) provide parameter-efficient fine-tuning with bitsandbytes quantization support, enabling (IA)\u00b3 adaptation on quantized models for reduced memory footprint.\n\n=== Description ===\nThis module provides two quantized implementations of (IA)\u00b3 (Infused Adapter by Inhibiting and Amplifying Inner Activations) layers that work with bitsandbytes quantization:\n\n* '''Linear8bitLt''' - (IA)\u00b3 for 8-bit quantized linear layers\n* '''Linear4bit''' - (IA)\u00b3 for 4-bit quantized linear layers\n\nBoth implementations extend the IA3Layer base class and wrap quantized base layers from bitsandbytes. They apply learned scaling vectors to either inputs (for feedforward layers) or outputs (for attention layers), enabling efficient adaptation of quantized models with minimal memory overhead. The quantized base layer weights remain frozen during training.\n\n=== Usage ===\nUse these quantized IA3 layers when you need:\n* Fine-tuning large models with limited GPU memory\n* (IA)\u00b3 adaptation combined with quantization for maximum efficiency\n* Training on consumer hardware with memory constraints\n* Inference optimization while maintaining adaptation capability\n* Reduced memory footprint during both training and inference\n\n== Code Reference ==\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/ia3/bnb.py src/peft/tuners/ia3/bnb.py]\n* '''Lines:''' 26-130\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass Linear8bitLt(torch.nn.Module, IA3Layer):\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        is_feedforward: bool,\n        init_ia3_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            base_layer: 8-bit quantized linear layer to wrap\n            adapter_name: Name of the adapter\n            is_feedforward: Whether this is a feedforward layer\n            init_ia3_weights: Whether to initialize IA3 weights\n        \"\"\"\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        \"\"\"Apply IA3 scaling to quantized layer\"\"\"\n\n\nclass Linear4bit(torch.nn.Module, IA3Layer):\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        is_feedforward: bool,\n        init_ia3_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            base_layer: 4-bit quantized linear layer to wrap\n            adapter_name: Name of the adapter\n            is_feedforward: Whether this is a feedforward layer\n            init_ia3_weights: Whether to initialize IA3 weights\n        \"\"\"\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        \"\"\"Apply IA3 scaling to quantized layer\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# These classes are used internally by PEFT\n# Users typically access them through the high-level API\nfrom peft import IA3Config, get_peft_model, prepare_model_for_kbit_training\n</syntaxhighlight>\n\n== I/O Contract ==\n=== Input Parameters (Constructor) ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description !! Default\n|-\n| base_layer || torch.nn.Module || The quantized linear layer to wrap || Required\n|-\n| adapter_name || str || Name identifier for the adapter || Required\n|-\n| is_feedforward || bool || Whether layer is feedforward (scales input) or attention (scales output) || Required\n|-\n| init_ia3_weights || bool || Whether to initialize IA3 scaling vectors || True\n|-\n| **kwargs || dict || Additional keyword arguments || -\n|}\n\n=== Forward Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| x || torch.Tensor || Input tensor to the layer\n|-\n| *args || tuple || Additional positional arguments passed to base layer\n|-\n| **kwargs || dict || Additional keyword arguments passed to base layer\n|}\n\n=== Output ===\n{| class=\"wikitable\"\n! Return Type !! Description\n|-\n| torch.Tensor || Output tensor with IA3 scaling applied\n|}\n\n=== Attributes (Inherited from IA3Layer) ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| ia3_l || torch.nn.ParameterDict || Dictionary of learned scaling vectors per adapter\n|-\n| is_feedforward || bool || Whether to scale inputs (True) or outputs (False)\n|-\n| active_adapters || list || List of currently active adapter names\n|-\n| disable_adapters || bool || Flag to disable all adapters\n|}\n\n== Usage Examples ==\n=== Basic Quantized IA3 Model (8-bit) ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import IA3Config, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# Load model with 8-bit quantization\nbnb_config = BitsAndBytesConfig(load_in_8bit=True)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-6.7b\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure IA3\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"v_proj\"],\n    feedforward_modules=[\"fc2\"],  # Apply to inputs for feedforward\n    init_ia3_weights=True\n)\n\n# Apply IA3 (automatically uses Linear8bitLt for quantized layers)\nmodel = get_peft_model(model, config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n# Output shows very few trainable parameters despite large model\n</syntaxhighlight>\n\n=== 4-bit Quantized IA3 Model ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import IA3Config, get_peft_model, prepare_model_for_kbit_training\n\n# Load model with 4-bit quantization (even more memory efficient)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Prepare and apply IA3\nmodel = prepare_model_for_kbit_training(base_model)\n\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    feedforward_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"],\n    init_ia3_weights=True\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Training with Quantized IA3 ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer\nfrom peft import IA3Config, get_peft_model, prepare_model_for_kbit_training\n\n# Setup\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-6.7b\",\n    load_in_8bit=True,\n    device_map=\"auto\"\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# Apply IA3\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"v_proj\"],\n    feedforward_modules=[\"fc2\"],\n    init_ia3_weights=True\n)\nmodel = get_peft_model(model, config)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./ia3_quantized\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=3e-4,\n    fp16=True,\n    logging_steps=10\n)\n\n# Train\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n\n# Save only the IA3 adapters (base model stays quantized)\nmodel.save_pretrained(\"./ia3_quantized_adapter\")\n</syntaxhighlight>\n\n=== Inference with Quantized IA3 ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Load quantized base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-6.7b\",\n    load_in_8bit=True,\n    device_map=\"auto\"\n)\n\n# Load IA3 adapter\nmodel = PeftModel.from_pretrained(base_model, \"./ia3_quantized_adapter\")\n\n# Inference\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\ninputs = tokenizer(\"The future of AI is\", return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</syntaxhighlight>\n\n=== Multiple Quantized Adapters ===\n<syntaxhighlight lang=\"python\">\n# Load base quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-6.7b\",\n    load_in_8bit=True,\n    device_map=\"auto\"\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# Add first adapter\nconfig1 = IA3Config(target_modules=[\"q_proj\", \"v_proj\"])\nmodel = get_peft_model(model, config1, adapter_name=\"task1\")\n\n# Add second adapter\nconfig2 = IA3Config(target_modules=[\"q_proj\", \"v_proj\"])\nmodel.add_adapter(\"task2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"task1\")\n# ... inference for task1 ...\n\nmodel.set_adapter(\"task2\")\n# ... inference for task2 ...\n</syntaxhighlight>\n\n=== Memory-Efficient Large Model Fine-tuning ===\n<syntaxhighlight lang=\"python\">\n# Fine-tune 13B model on a single GPU\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-13b-hf\",\n    load_in_4bit=True,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\n# Minimal IA3 configuration\nconfig = IA3Config(\n    target_modules=[\"q_proj\", \"v_proj\"],\n    feedforward_modules=[\"down_proj\"]\n)\n\nmodel = get_peft_model(model, config)\n\n# Can now train 13B model on 24GB GPU\n</syntaxhighlight>\n\n== Technical Details ==\n=== Quantization Support ===\n* '''8-bit (Linear8bitLt)''': Uses bitsandbytes LLM.int8() quantization\n* '''4-bit (Linear4bit)''': Uses bitsandbytes 4-bit NormalFloat quantization\n\n=== Memory Efficiency ===\n* Base weights remain frozen and quantized\n* Only IA3 scaling vectors are trained (FP32/FP16)\n* Typical memory reduction: 75% (4-bit) or 50% (8-bit) vs FP16\n\n=== Forward Pass Behavior ===\nFor feedforward layers (is_feedforward=True):\n1. Scale input: x_scaled = x * ia3_scaling\n2. Apply quantized layer: output = base_layer(x_scaled)\n\nFor attention layers (is_feedforward=False):\n1. Apply quantized layer: output = base_layer(x)\n2. Scale output: output_scaled = output * ia3_scaling\n\n=== Type Conversion ===\nBoth implementations handle automatic type conversion:\n* Convert to FP32 when not in autocast mode\n* Convert back to expected dtype after computation\n* Special handling for 4-bit to support older PyTorch versions\n\n=== Dependencies ===\n* Requires bitsandbytes library\n* Check availability with is_bnb_available() and is_bnb_4bit_available()\n\n== Related Pages ==\n* [[uses::Component:huggingface_peft_IA3Layer]]\n* [[configured_by::Configuration:huggingface_peft_IA3Config]]\n* [[related::Component:huggingface_peft_IA3Model]]\n* [[requires::Library:bitsandbytes]]\n* [[related::Component:huggingface_peft_LoRA_Quantized]]\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n* [[implements::Technique:IA3_with_Quantization]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Parameter-Efficient Fine-Tuning",
        "Quantization",
        "Model Compression"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses",
          "target_type": "Component",
          "target_id": "huggingface_peft_IA3Layer"
        },
        {
          "edge_type": "configured_by",
          "target_type": "Configuration",
          "target_id": "huggingface_peft_IA3Config"
        },
        {
          "edge_type": "related",
          "target_type": "Component",
          "target_id": "huggingface_peft_IA3Model"
        },
        {
          "edge_type": "requires",
          "target_type": "Library",
          "target_id": "bitsandbytes"
        },
        {
          "edge_type": "related",
          "target_type": "Component",
          "target_id": "huggingface_peft_LoRA_Quantized"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        },
        {
          "edge_type": "implements",
          "target_type": "Technique",
          "target_id": "IA3_with_Quantization"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_IncrementalPCA",
      "page_title": "huggingface peft IncrementalPCA",
      "page_type": "Implementation",
      "overview": "GPU-accelerated Incremental Principal Component Analysis for processing large datasets in batches, used internally by PEFT methods like CorDA for covariance matrix decomposition.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Dimensionality_Reduction]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nGPU-accelerated Incremental Principal Component Analysis for processing large datasets in batches, used internally by PEFT methods like CorDA for covariance matrix decomposition.\n\n=== Description ===\n\nIncrementalPCA implements batch-wise PCA using PyTorch with GPU acceleration. It processes data incrementally, maintaining running mean/variance statistics and updating principal components via SVD after each batch. Supports both full SVD (torch.linalg.svd) and low-rank approximation (torch.svd_lowrank) for speed. The implementation adapts scikit-learn's IncrementalPCA to PyTorch tensors with CUDA support.\n\n=== Usage ===\n\nUse IncrementalPCA when computing principal components on large datasets that don't fit in memory, or when GPU acceleration is needed. It's used internally by CorDA for weight initialization. The partial_fit method allows streaming data through the model.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/utils/incremental_pca.py src/peft/utils/incremental_pca.py]\n* '''Lines:''' 1-339\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass IncrementalPCA:\n    \"\"\"\n    GPU-accelerated Incremental Principal Components Analysis.\n\n    Args:\n        n_components: Number of components to keep\n        copy: If False, overwrite input data\n        batch_size: Samples per batch for fit()\n        svd_driver: cuSOLVER method (gesvd, gesvdj, gesvda)\n        lowrank: Use torch.svd_lowrank for speed\n        lowrank_q: Approximation parameter (default: n_components * 2)\n        lowrank_niter: Subspace iterations for lowrank\n        lowrank_seed: Seed for reproducible lowrank results\n\n    Attributes:\n        components_: Principal axes [n_components, n_features]\n        singular_values_: Singular values\n        mean_: Per-feature mean\n        var_: Per-feature variance\n        explained_variance_: Variance explained by each component\n        explained_variance_ratio_: Percentage of variance explained\n        n_samples_seen_: Total samples processed\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: Optional[int] = None,\n        copy: Optional[bool] = True,\n        batch_size: Optional[int] = None,\n        svd_driver: Optional[str] = None,\n        lowrank: bool = False,\n        lowrank_q: Optional[int] = None,\n        lowrank_niter: int = 4,\n        lowrank_seed: Optional[int] = None,\n    ): ...\n\n    def fit(self, X, check_input=True):\n        \"\"\"Fit model using batched processing.\"\"\"\n\n    def partial_fit(self, X, check_input=True):\n        \"\"\"Incrementally fit model with a single batch.\"\"\"\n\n    def transform(self, X) -> torch.Tensor:\n        \"\"\"Project data onto principal components.\"\"\"\n\n    @staticmethod\n    def gen_batches(n: int, batch_size: int, min_batch_size: int = 0):\n        \"\"\"Generate batch slices for processing.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.utils.incremental_pca import IncrementalPCA\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| n_components || int || No || Number of principal components to keep\n|-\n| X || torch.Tensor || Yes || Input data [n_samples, n_features]\n|-\n| batch_size || int || No || Samples per batch (default: 5 * n_features)\n|-\n| lowrank || bool || No || Use low-rank SVD approximation\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| components_ || torch.Tensor || Principal axes [n_components, n_features]\n|-\n| transform() || torch.Tensor || Projected data [n_samples, n_components]\n|-\n| explained_variance_ratio_ || torch.Tensor || Variance explained per component\n|}\n\n== Usage Examples ==\n\n=== Basic Incremental PCA ===\n<syntaxhighlight lang=\"python\">\nfrom peft.utils.incremental_pca import IncrementalPCA\nimport torch\n\n# Create PCA with 64 components\nipca = IncrementalPCA(n_components=64)\n\n# Fit on large dataset in batches\ndata = torch.randn(10000, 4096).cuda()\nipca.fit(data, batch_size=1000)\n\n# Transform new data\nnew_data = torch.randn(100, 4096).cuda()\nprojected = ipca.transform(new_data)\n# Shape: [100, 64]\n</syntaxhighlight>\n\n=== Streaming Partial Fit ===\n<syntaxhighlight lang=\"python\">\nfrom peft.utils.incremental_pca import IncrementalPCA\nimport torch\n\nipca = IncrementalPCA(n_components=32)\n\n# Process data stream incrementally\nfor batch in data_loader:\n    ipca.partial_fit(batch.cuda())\n\n# Access learned components\nprint(f\"Components shape: {ipca.components_.shape}\")\nprint(f\"Variance explained: {ipca.explained_variance_ratio_.sum():.2%}\")\n</syntaxhighlight>\n\n=== Low-Rank SVD for Speed ===\n<syntaxhighlight lang=\"python\">\nfrom peft.utils.incremental_pca import IncrementalPCA\n\n# Use low-rank SVD for faster computation\nipca = IncrementalPCA(\n    n_components=64,\n    lowrank=True,           # Use torch.svd_lowrank\n    lowrank_q=128,          # Approximation rank\n    lowrank_niter=4,        # Subspace iterations\n    lowrank_seed=42,        # Reproducible results\n)\n\nipca.fit(large_data.cuda())\n</syntaxhighlight>\n\n=== Used by CorDA Initialization ===\n<syntaxhighlight lang=\"python\">\n# IncrementalPCA is used internally by CorDA\n# to compute covariance-weighted SVD initialization\nfrom peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    init_lora_weights=\"corda\",\n    corda_config={\n        \"covariance_path\": \"./covariances/\",\n        # Uses IncrementalPCA for PCA decomposition\n    },\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Dimensionality Reduction"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_LNTuningConfig",
      "page_title": "huggingface peft LNTuningConfig",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>LNTuningConfig</code> class is a configuration dataclass for Layer Normalization (LN) Tuning in PEFT. This configuration stores all parameters needed to set up a Layer Normalization tuning model, which is a parameter-efficient fine-tuning method that focuses on tuning only the LayerNorm layers of a pretrained transformer model while keeping other parameters frozen. LN Tuning is based on the paper detailed at https://huggingface.co/papers/2312.11420. This method provides an efficient way to adapt large language models with minimal trainable parameters by selectively updating LayerNorm layers. The configuration inherits from <code>PeftConfig</code> and sets the PEFT type to <code>PeftType.LN_TUNING</code> during initialization.",
      "content": "{{Implementation\n|domain=NLP,PEFT,Parameter-Efficient Fine-Tuning,Layer Normalization Tuning\n|link=https://github.com/huggingface/peft\n}}\n\n== Overview ==\n\n=== Description ===\n\nThe <code>LNTuningConfig</code> class is a configuration dataclass for Layer Normalization (LN) Tuning in PEFT. This configuration stores all parameters needed to set up a Layer Normalization tuning model, which is a parameter-efficient fine-tuning method that focuses on tuning only the LayerNorm layers of a pretrained transformer model while keeping other parameters frozen.\n\nLN Tuning is based on the paper detailed at https://huggingface.co/papers/2312.11420. This method provides an efficient way to adapt large language models with minimal trainable parameters by selectively updating LayerNorm layers.\n\nThe configuration inherits from <code>PeftConfig</code> and sets the PEFT type to <code>PeftType.LN_TUNING</code> during initialization.\n\n=== Usage ===\n\nLNTuningConfig is used to define which modules in a model should be targeted for Layer Normalization tuning. It supports flexible module targeting through regex patterns and can exclude specific modules if needed. The configuration is passed to the PEFT library when creating an adapted model with LN Tuning enabled.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' huggingface/peft\n* '''File Path:''' <code>src/peft/tuners/ln_tuning/config.py</code>\n* '''Lines:''' 23-71\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass LNTuningConfig(PeftConfig):\n    target_modules: Optional[Union[list[str], str]] = None\n    exclude_modules: Optional[Union[list[str], str]] = None\n    modules_to_save: Optional[Union[list[str], str]] = None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.ln_tuning.config import LNTuningConfig\n# or\nfrom peft import LNTuningConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| <code>target_modules</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || List of module names or regex expression of the module names to replace with LNTuning. For example, '.*decoder.*' or '.*encoder.*'. If not specified, modules will be chosen according to the model architecture. If the architecture is not known, an error will be raised.\n|-\n| <code>exclude_modules</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || The names of the modules to not apply the adapter. When passing a string, a regex match will be performed. When passing a list of strings, either an exact match will be performed or it is checked if the name of the module ends with any of the passed strings.\n|-\n| <code>modules_to_save</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || List of modules to be set as trainable and saved in the final checkpoint. For example, in Sequence Classification or Token Classification tasks, the final layer 'classifier/score' are randomly initialized and need to be trainable and saved.\n|}\n\n=== Outputs ===\n\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| <code>peft_type</code> || <code>PeftType</code> || Automatically set to <code>PeftType.LN_TUNING</code> in <code>__post_init__</code> method\n|-\n| <code>target_modules</code> || <code>Optional[Union[list[str], str]]</code> || Configured target modules for LN tuning\n|-\n| <code>exclude_modules</code> || <code>Optional[Union[list[str], str]]</code> || Configured modules to exclude from LN tuning\n|-\n| <code>modules_to_save</code> || <code>Optional[Union[list[str], str]]</code> || Configured modules to save in checkpoint\n|}\n\n== Usage Examples ==\n\n=== Example 1: Basic LN Tuning Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LNTuningConfig, get_peft_model, TaskType\nfrom transformers import AutoModelForCausalLM\n\n# Create LN Tuning configuration\nconfig = LNTuningConfig(\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Load and adapt model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Example 2: LN Tuning with Specific Target Modules ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LNTuningConfig, get_peft_model, TaskType\nfrom transformers import AutoModelForSequenceClassification\n\n# Configure with specific target modules\nconfig = LNTuningConfig(\n    task_type=TaskType.SEQ_CLS,\n    target_modules=[\".*decoder.*\"],  # Target decoder LayerNorm layers only\n    modules_to_save=[\"classifier\"],  # Save classifier layer\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Example 3: LN Tuning with Exclusions ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LNTuningConfig, get_peft_model, TaskType\n\n# Configure with exclusions\nconfig = LNTuningConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\".*layer_norm.*\"],\n    exclude_modules=[\".*embeddings.*\"],  # Exclude embedding LayerNorms\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n== Related Pages ==\n\n* [[huggingface_peft_LNTuningLayer|LNTuningLayer]] - The layer implementation for LN Tuning\n* [[huggingface_peft_LNTuningModel|LNTuningModel]] - The model class that applies LN Tuning\n* [[huggingface_peft_PeftConfig|PeftConfig]] - Base configuration class\n* [[huggingface_peft_LoRAConfig|LoRAConfig]] - Alternative PEFT configuration for LoRA\n* [[huggingface_peft_get_peft_model|get_peft_model]] - Function to create PEFT models\n\n[[Category:PEFT]]\n[[Category:Configuration]]\n[[Category:Layer Normalization]]\n[[Category:Parameter-Efficient Fine-Tuning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LNTuningLayer",
      "page_title": "huggingface peft LNTuningLayer",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>LNTuningLayer</code> class is a PyTorch module that implements the Layer Normalization tuning adapter layer. It wraps a base layer from the model and creates deep copies of it for each adapter, allowing for efficient adapter switching and merging without modifying the original layer weights. LNTuningLayer inherits from both <code>nn.Module</code> and <code>BaseTunerLayer</code>, providing the core functionality for managing multiple adapters, enabling/disabling adapters, and handling forward passes with adapter-specific logic. Key features include: * Deep copy mechanism for preserving original layer weights * Support for adapter merging and unmerging (swapping base layer with adapter layer) * Single active adapter constraint (only one adapter can be active at inference time) * Adapter enable/disable functionality with gradient control",
      "content": "{{Implementation\n|domain=NLP,PEFT,Parameter-Efficient Fine-Tuning,Layer Normalization Tuning,Neural Networks\n|link=https://github.com/huggingface/peft\n}}\n\n== Overview ==\n\n=== Description ===\n\nThe <code>LNTuningLayer</code> class is a PyTorch module that implements the Layer Normalization tuning adapter layer. It wraps a base layer from the model and creates deep copies of it for each adapter, allowing for efficient adapter switching and merging without modifying the original layer weights.\n\nLNTuningLayer inherits from both <code>nn.Module</code> and <code>BaseTunerLayer</code>, providing the core functionality for managing multiple adapters, enabling/disabling adapters, and handling forward passes with adapter-specific logic.\n\nKey features include:\n* Deep copy mechanism for preserving original layer weights\n* Support for adapter merging and unmerging (swapping base layer with adapter layer)\n* Single active adapter constraint (only one adapter can be active at inference time)\n* Adapter enable/disable functionality with gradient control\n\n=== Usage ===\n\nLNTuningLayer is typically not instantiated directly by users but is created internally by the LNTuningModel when wrapping target modules. It manages the lifecycle of adapter layers and handles the forward pass routing based on whether adapters are enabled, disabled, or merged.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' huggingface/peft\n* '''File Path:''' <code>src/peft/tuners/ln_tuning/layer.py</code>\n* '''Lines:''' 25-124\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass LNTuningLayer(nn.Module, BaseTunerLayer):\n    def __init__(self, base_layer: nn.Module, adapter_name: str)\n\n    def update_layer(self, layer: nn.Module, adapter_name: str, inference_mode: bool = False, **kwargs)\n\n    def enable_adapters(self, enabled: bool) -> None\n\n    def merge(self, adapter_names: Optional[list[str]] = None, safe_merge: bool = False)\n\n    def unmerge(self)\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.ln_tuning.layer import LNTuningLayer\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n\n'''Constructor Parameters:'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| <code>base_layer</code> || <code>nn.Module</code> || Required || The original layer to wrap with LN tuning adapter\n|-\n| <code>adapter_name</code> || <code>str</code> || Required || Name identifier for the adapter\n|}\n\n'''Method: update_layer'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| <code>layer</code> || <code>nn.Module</code> || Required || The layer to add as a new adapter\n|-\n| <code>adapter_name</code> || <code>str</code> || Required || Name identifier for the new adapter\n|-\n| <code>inference_mode</code> || <code>bool</code> || <code>False</code> || Whether to set adapter in inference mode\n|}\n\n'''Method: enable_adapters'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>enabled</code> || <code>bool</code> || True to enable adapters, False to disable adapters\n|}\n\n'''Method: merge'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| <code>adapter_names</code> || <code>Optional[list[str]]</code> || <code>None</code> || List of adapter names to merge (only one allowed)\n|-\n| <code>safe_merge</code> || <code>bool</code> || <code>False</code> || Safe merge flag (not used in LN tuning)\n|}\n\n'''Method: forward'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>x</code> || <code>torch.Tensor</code> || Input tensor to the layer\n|-\n| <code>*args</code> || <code>Any</code> || Additional positional arguments passed to the layer\n|-\n| <code>**kwargs</code> || <code>Any</code> || Additional keyword arguments passed to the layer\n|}\n\n=== Outputs ===\n\n'''Attributes:'''\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| <code>base_layer</code> || <code>nn.Module</code> || The wrapped base layer\n|-\n| <code>ln_tuning_layers</code> || <code>nn.ModuleDict</code> || Dictionary of adapter layers indexed by adapter name\n|-\n| <code>merged_adapters</code> || <code>list</code> || List of currently merged adapter names\n|-\n| <code>in_features</code> || <code>int</code> || Input feature dimension of the base layer\n|-\n| <code>out_features</code> || <code>int</code> || Output feature dimension of the base layer\n|}\n\n'''Method Returns:'''\n{| class=\"wikitable\"\n! Method !! Return Type !! Description\n|-\n| <code>forward</code> || <code>torch.Tensor</code> || Output tensor from the layer (base or adapter)\n|-\n| <code>__repr__</code> || <code>str</code> || String representation prefixed with \"ln_tuning.\"\n|}\n\n== Usage Examples ==\n\n=== Example 1: Basic Layer Wrapping (Internal) ===\n<syntaxhighlight lang=\"python\">\nimport torch.nn as nn\nfrom peft.tuners.ln_tuning.layer import LNTuningLayer\n\n# Typically done internally by LNTuningModel\nbase_layer = nn.LayerNorm(768)\nln_layer = LNTuningLayer(base_layer, adapter_name=\"default\")\n\n# Forward pass with adapter\noutput = ln_layer(input_tensor)\n</syntaxhighlight>\n\n=== Example 2: Adapter Merging and Unmerging ===\n<syntaxhighlight lang=\"python\">\n# Merge adapter (swap base layer with adapter layer)\nln_layer.merge(adapter_names=[\"default\"])\nprint(ln_layer.merged)  # True\n\n# Forward pass uses merged adapter as base layer\noutput = ln_layer(input_tensor)\n\n# Unmerge adapter (restore original base layer)\nln_layer.unmerge()\nprint(ln_layer.merged)  # False\n</syntaxhighlight>\n\n=== Example 3: Enabling and Disabling Adapters ===\n<syntaxhighlight lang=\"python\">\n# Enable adapters (default state)\nln_layer.enable_adapters(enabled=True)\n\n# Forward pass with adapter\noutput_with_adapter = ln_layer(input_tensor)\n\n# Disable adapters (use base layer only)\nln_layer.enable_adapters(enabled=False)\n\n# Forward pass without adapter\noutput_without_adapter = ln_layer(input_tensor)\n</syntaxhighlight>\n\n=== Example 4: Multi-Adapter Management ===\n<syntaxhighlight lang=\"python\">\nimport torch.nn as nn\nfrom copy import deepcopy\n\n# Add second adapter\nsecond_layer = deepcopy(base_layer)\nln_layer.update_layer(second_layer, adapter_name=\"adapter2\")\n\n# Set active adapter\nln_layer.set_adapter(\"adapter2\")\n\n# Forward pass uses \"adapter2\"\noutput = ln_layer(input_tensor)\n\n# Note: Only one adapter can be active at inference time\n# Attempting to use multiple active adapters will raise ValueError\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Adapter Merging Mechanism ===\nLN Tuning uses a unique merging approach where the base layer and adapter layer are swapped:\n* '''Merge:''' <code>base_layer</code> and <code>ln_tuning_layers[adapter_name]</code> swap positions\n* '''Unmerge:''' They swap back to original positions\n* This allows seamless switching without weight modifications\n\n=== Constraints ===\n* '''Single Adapter Limitation:''' Only one adapter can be merged at a time\n* '''Single Active Adapter:''' Only one adapter can be active during inference\n* These constraints ensure deterministic behavior and prevent conflicts\n\n=== Forward Pass Logic ===\n<syntaxhighlight lang=\"python\">\nif disable_adapters:\n    if merged: unmerge()\n    return base_layer(x)\nelif merged or no_active_adapters:\n    return base_layer(x)\nelse:\n    return active_adapter_layer(x)\n</syntaxhighlight>\n\n== Related Pages ==\n\n* [[huggingface_peft_LNTuningConfig|LNTuningConfig]] - Configuration for LN Tuning\n* [[huggingface_peft_LNTuningModel|LNTuningModel]] - Model class that creates LNTuningLayer instances\n* [[huggingface_peft_BaseTunerLayer|BaseTunerLayer]] - Base class for tuner layers\n* [[huggingface_peft_LoraLayer|LoraLayer]] - Alternative adapter layer implementation\n* [[huggingface_peft_AdapterLayer|AdapterLayer]] - Another adapter layer implementation\n\n[[Category:PEFT]]\n[[Category:Layer]]\n[[Category:Layer Normalization]]\n[[Category:Adapter]]\n[[Category:PyTorch]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LNTuningModel",
      "page_title": "huggingface peft LNTuningModel",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>LNTuningModel</code> class creates LayerNorm tuning from a pretrained transformer model. This class is part of the PEFT library and implements the Layer Normalization tuning method described in detail at https://huggingface.co/papers/2312.11420. LNTuningModel inherits from <code>BaseTuner</code> and provides a parameter-efficient fine-tuning approach by creating trainable copies of LayerNorm layers while keeping the rest of the model frozen. This method significantly reduces the number of trainable parameters while maintaining model performance. Key features include: * Automatic target module identification based on model architecture * Support for multiple adapters * Adapter creation and replacement mechanism * Specialized unloading and merging capabilities * Integration with the broader PEFT ecosystem",
      "content": "{{Implementation\n|domain=NLP,PEFT,Parameter-Efficient Fine-Tuning,Layer Normalization Tuning,Transformers\n|link=https://github.com/huggingface/peft\n}}\n\n== Overview ==\n\n=== Description ===\n\nThe <code>LNTuningModel</code> class creates LayerNorm tuning from a pretrained transformer model. This class is part of the PEFT library and implements the Layer Normalization tuning method described in detail at https://huggingface.co/papers/2312.11420.\n\nLNTuningModel inherits from <code>BaseTuner</code> and provides a parameter-efficient fine-tuning approach by creating trainable copies of LayerNorm layers while keeping the rest of the model frozen. This method significantly reduces the number of trainable parameters while maintaining model performance.\n\nKey features include:\n* Automatic target module identification based on model architecture\n* Support for multiple adapters\n* Adapter creation and replacement mechanism\n* Specialized unloading and merging capabilities\n* Integration with the broader PEFT ecosystem\n\n=== Usage ===\n\nLNTuningModel is typically instantiated through the <code>get_peft_model</code> function with a LNTuningConfig. Users don't usually instantiate this class directly. The model automatically identifies and wraps appropriate LayerNorm modules in the base model with LNTuningLayer instances.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' huggingface/peft\n* '''File Path:''' <code>src/peft/tuners/ln_tuning/model.py</code>\n* '''Lines:''' 28-133\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass LNTuningModel(BaseTuner):\n    prefix: str = \"ln_tuning_\"\n    tuner_layer_cls = LNTuningLayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_LNTUNING_TARGET_MODULES_MAPPING\n\n    def _create_and_replace(\n        self,\n        peft_config: PeftConfig,\n        adapter_name: str,\n        target: Module,\n        target_name: str,\n        parent: Module,\n        current_key: str,\n    ) -> None\n\n    def _create_new_module(\n        self,\n        peft_config: PeftConfig,\n        target: Module,\n        adapter_name: str,\n    ) -> Module\n\n    def _unloading_checks(self, adapter_names: Optional[list[str]])\n\n    def _unload_and_optionally_merge(\n        self,\n        merge=True,\n        progressbar: bool = False,\n        safe_merge: bool = False,\n        adapter_names: Optional[list[str]] = None,\n    )\n\n    def _cast_adapter_dtype(self, adapter_name: str, autocast_adapter_dtype: bool = True) -> None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.ln_tuning.model import LNTuningModel\n# or use via get_peft_model\nfrom peft import get_peft_model, LNTuningConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n\n'''Constructor (via get_peft_model):'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>model</code> || <code>torch.nn.Module</code> || The pretrained model to be adapted with LN tuning\n|-\n| <code>config</code> || <code>LNTuningConfig</code> || The configuration for LN tuning\n|-\n| <code>adapter_name</code> || <code>str</code> || The name of the adapter (default: \"default\")\n|-\n| <code>low_cpu_mem_usage</code> || <code>bool</code> || No effect on LN tuning; exists for API consistency\n|}\n\n'''Method: _create_and_replace'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>peft_config</code> || <code>PeftConfig</code> || Configuration for the PEFT method\n|-\n| <code>adapter_name</code> || <code>str</code> || Name of the adapter to create\n|-\n| <code>target</code> || <code>Module</code> || Target module to replace\n|-\n| <code>target_name</code> || <code>str</code> || Name of the target module\n|-\n| <code>parent</code> || <code>Module</code> || Parent module containing the target\n|-\n| <code>current_key</code> || <code>str</code> || Key path to the current module\n|}\n\n'''Method: _unload_and_optionally_merge'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| <code>merge</code> || <code>bool</code> || <code>True</code> || Whether to merge adapters before unloading\n|-\n| <code>progressbar</code> || <code>bool</code> || <code>False</code> || Whether to show a progress bar\n|-\n| <code>safe_merge</code> || <code>bool</code> || <code>False</code> || Safe merge flag (not used in LN tuning)\n|-\n| <code>adapter_names</code> || <code>Optional[list[str]]</code> || <code>None</code> || Specific adapters to unload\n|}\n\n=== Outputs ===\n\n'''Attributes:'''\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| <code>model</code> || <code>PreTrainedModel</code> || The adapted model with LN tuning layers\n|-\n| <code>peft_config</code> || <code>LNTuningConfig</code> || The configuration used for adaptation\n|-\n| <code>prefix</code> || <code>str</code> || Prefix for adapter parameters (\"ln_tuning_\")\n|-\n| <code>tuner_layer_cls</code> || <code>type</code> || The layer class used (LNTuningLayer)\n|}\n\n'''Method Returns:'''\n{| class=\"wikitable\"\n! Method !! Return Type !! Description\n|-\n| <code>_create_new_module</code> || <code>Module</code> || New LNTuningLayer wrapping the target\n|-\n| <code>_unload_and_optionally_merge</code> || <code>torch.nn.Module</code> || The base model with adapters unloaded\n|}\n\n== Usage Examples ==\n\n=== Example 1: Basic LN Tuning Model Creation ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, TaskType, LNTuningConfig\n\n# Create configuration\npeft_config = LNTuningConfig(\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Apply LN tuning\nmodel = get_peft_model(model, peft_config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: X || all params: Y || trainable%: Z\n</syntaxhighlight>\n\n=== Example 2: LN Tuning with Custom Target Modules ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import get_peft_model, LNTuningConfig, TaskType\n\n# Configure with specific targets\nconfig = LNTuningConfig(\n    task_type=TaskType.SEQ_CLS,\n    target_modules=[\".*layer_norm.*\"],  # Target all LayerNorm modules\n    modules_to_save=[\"classifier\"],  # Save classifier weights\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=2\n)\n\n# Apply LN tuning\nmodel = get_peft_model(model, config)\n\n# Train the model\n# ... training loop ...\n</syntaxhighlight>\n\n=== Example 3: Multi-Adapter LN Tuning ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LNTuningConfig, TaskType\n\n# Create base model with first adapter\nconfig1 = LNTuningConfig(\n    task_type=TaskType.CAUSAL_LM,\n)\nmodel = get_peft_model(base_model, config1, adapter_name=\"adapter1\")\n\n# Add second adapter\nconfig2 = LNTuningConfig(\n    task_type=TaskType.CAUSAL_LM,\n)\nmodel.add_adapter(\"adapter2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"adapter1\")\noutput1 = model(input_ids)\n\nmodel.set_adapter(\"adapter2\")\noutput2 = model(input_ids)\n</syntaxhighlight>\n\n=== Example 4: Unloading and Merging Adapters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LNTuningConfig, TaskType\n\n# Create and train LN tuning model\nconfig = LNTuningConfig(task_type=TaskType.CAUSAL_LM)\nmodel = get_peft_model(base_model, config)\n\n# ... training ...\n\n# Merge and unload adapter\nmodel = model.merge_and_unload()\n\n# Now model is the base model with adapter merged\n# Can be saved as a standard model\nmodel.save_pretrained(\"./merged_model\")\n</syntaxhighlight>\n\n=== Example 5: LN Tuning for Text Classification ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom peft import get_peft_model, LNTuningConfig, TaskType\nimport torch\n\n# Load model and tokenizer\nmodel_name = \"bert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Apply LN tuning\nconfig = LNTuningConfig(\n    task_type=TaskType.SEQ_CLS,\n    modules_to_save=[\"classifier\"],\n)\nmodel = get_peft_model(model, config)\n\n# Training\nmodel.train()\ninputs = tokenizer(\"This is a test\", return_tensors=\"pt\")\noutputs = model(**inputs, labels=torch.tensor([1]))\nloss = outputs.loss\n\nprint(f\"Loss: {loss.item()}\")\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Target Module Mapping ===\nLNTuningModel uses <code>TRANSFORMERS_MODELS_TO_LNTUNING_TARGET_MODULES_MAPPING</code> to automatically identify appropriate LayerNorm modules for different model architectures. This mapping ensures the correct modules are targeted without manual specification.\n\n=== Adapter Creation Process ===\n<syntaxhighlight lang=\"python\">\n# Simplified flow:\n1. Identify target modules based on config and architecture\n2. For each target module:\n   - Create LNTuningLayer wrapping the original module\n   - Replace original module with LNTuningLayer\n   - Set gradient requirements based on adapter state\n</syntaxhighlight>\n\n=== Unloading Mechanism ===\nThe <code>_unload_and_optionally_merge</code> method:\n* Checks for <code>modules_to_save</code> conflicts with multiple adapters\n* Optionally merges adapters before unloading\n* Extracts base layers from LNTuningLayer wrappers\n* Returns the original model structure\n\n=== Dtype Handling ===\nThe <code>_cast_adapter_dtype</code> method is overridden to do nothing, as LN Tuning creates copies of original layers rather than adding new adapter parameters. This prevents unwanted dtype conversions.\n\n== Related Pages ==\n\n* [[huggingface_peft_LNTuningConfig|LNTuningConfig]] - Configuration for LN Tuning\n* [[huggingface_peft_LNTuningLayer|LNTuningLayer]] - Layer implementation used by this model\n* [[huggingface_peft_BaseTuner|BaseTuner]] - Base class for PEFT tuners\n* [[huggingface_peft_get_peft_model|get_peft_model]] - Function to create PEFT models\n* [[huggingface_peft_LoraModel|LoraModel]] - Alternative PEFT model using LoRA\n* [[huggingface_peft_PeftModel|PeftModel]] - Base PEFT model wrapper\n\n[[Category:PEFT]]\n[[Category:Model]]\n[[Category:Layer Normalization]]\n[[Category:Parameter-Efficient Fine-Tuning]]\n[[Category:Transformers]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LoHaConfig",
      "page_title": "huggingface peft LoHaConfig",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>LoHaConfig</code> class is a configuration dataclass for Low-Rank Hadamard Product (LoHa) adaptation in PEFT. LoHa is a parameter-efficient fine-tuning method that uses Hadamard products and low-rank decomposition to adapt pretrained models with minimal trainable parameters. LoHaConfig inherits from <code>LycorisConfig</code> and provides comprehensive configuration options for LoHa adapters, including rank settings, dropout probabilities, target module specification, and layer-specific parameter patterns. The method is partially described in https://huggingface.co/papers/2108.06098 and is particularly effective for both text and image models. Key configuration parameters include: * Rank (r) and alpha for scaling * Rank and module dropout for regularization * Effective Conv2d decomposition for convolutional layers * Fine-grained control over target modules and layers * Support for rank and alpha patterns per layer",
      "content": "{{Implementation\n|domain=NLP,Computer Vision,PEFT,Parameter-Efficient Fine-Tuning,LoHa,Low-Rank Adaptation\n|link=https://github.com/huggingface/peft\n}}\n\n== Overview ==\n\n=== Description ===\n\nThe <code>LoHaConfig</code> class is a configuration dataclass for Low-Rank Hadamard Product (LoHa) adaptation in PEFT. LoHa is a parameter-efficient fine-tuning method that uses Hadamard products and low-rank decomposition to adapt pretrained models with minimal trainable parameters.\n\nLoHaConfig inherits from <code>LycorisConfig</code> and provides comprehensive configuration options for LoHa adapters, including rank settings, dropout probabilities, target module specification, and layer-specific parameter patterns. The method is partially described in https://huggingface.co/papers/2108.06098 and is particularly effective for both text and image models.\n\nKey configuration parameters include:\n* Rank (r) and alpha for scaling\n* Rank and module dropout for regularization\n* Effective Conv2d decomposition for convolutional layers\n* Fine-grained control over target modules and layers\n* Support for rank and alpha patterns per layer\n\n=== Usage ===\n\nLoHaConfig is used to configure LoHa adapters before applying them to a model via the PEFT library. It allows users to specify which modules to adapt, how to decompose them, and what hyperparameters to use for training. The configuration is particularly useful for diffusion models, vision transformers, and language models.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' huggingface/peft\n* '''File Path:''' <code>src/peft/tuners/loha/config.py</code>\n* '''Lines:''' 23-144\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass LoHaConfig(LycorisConfig):\n    r: int = 8\n    alpha: int = 8\n    rank_dropout: float = 0.0\n    module_dropout: float = 0.0\n    use_effective_conv2d: bool = False\n    target_modules: Optional[Union[list[str], str]] = None\n    exclude_modules: Optional[Union[list[str], str]] = None\n    init_weights: bool = True\n    layers_to_transform: Optional[Union[list[int], int]] = None\n    layers_pattern: Optional[Union[list[str], str]] = None\n    modules_to_save: Optional[list[str]] = None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.loha.config import LoHaConfig\n# or\nfrom peft import LoHaConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| <code>r</code> || <code>int</code> || <code>8</code> || LoHa rank - controls the dimension of the low-rank decomposition\n|-\n| <code>alpha</code> || <code>int</code> || <code>8</code> || The alpha parameter for LoHa scaling\n|-\n| <code>rank_dropout</code> || <code>float</code> || <code>0.0</code> || The dropout probability for rank dimension during training\n|-\n| <code>module_dropout</code> || <code>float</code> || <code>0.0</code> || The dropout probability for disabling LoHa modules during training\n|-\n| <code>use_effective_conv2d</code> || <code>bool</code> || <code>False</code> || Use parameter effective decomposition for Conv2d (and Conv1d) with ksize > 1 (\"Proposition 3\" from FedPara paper)\n|-\n| <code>target_modules</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || List of module names or regex expression of module names to replace with LoHa. Can be 'all-linear' to target all linear layers except output\n|-\n| <code>exclude_modules</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || List of module names or regex expression to exclude from LoHa adaptation\n|-\n| <code>init_weights</code> || <code>bool</code> || <code>True</code> || Whether to initialize the weights of the LoHa layers with their default initialization\n|-\n| <code>layers_to_transform</code> || <code>Optional[Union[list[int], int]]</code> || <code>None</code> || The layer indices to transform. If specified, only these layers will be adapted\n|-\n| <code>layers_pattern</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || The layer pattern name (e.g., 'layers' or 'h'), used with layers_to_transform\n|-\n| <code>modules_to_save</code> || <code>Optional[list[str]]</code> || <code>None</code> || List of modules apart from LoHa layers to be set as trainable and saved in the final checkpoint\n|}\n\n'''Inherited from LycorisConfig:'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>rank_pattern</code> || <code>dict</code> || Mapping from layer names/regex to ranks different from default r\n|-\n| <code>alpha_pattern</code> || <code>dict</code> || Mapping from layer names/regex to alphas different from default alpha\n|}\n\n=== Outputs ===\n\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| <code>peft_type</code> || <code>PeftType</code> || Automatically set to <code>PeftType.LOHA</code> in <code>__post_init__</code>\n|-\n| <code>target_modules</code> || <code>Optional[Union[set[str], str]]</code> || Converted to set if originally a list\n|-\n| <code>exclude_modules</code> || <code>Optional[Union[set[str], str]]</code> || Converted to set if originally a list\n|}\n\n== Usage Examples ==\n\n=== Example 1: Basic LoHa Configuration for Text Models ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoHaConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Create LoHa configuration\nconfig = LoHaConfig(\n    r=8,\n    alpha=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Apply to model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Example 2: LoHa for Stable Diffusion (UNet) ===\n<syntaxhighlight lang=\"python\">\nfrom diffusers import StableDiffusionPipeline\nfrom peft import LoHaConfig, LoHaModel\n\n# Configuration for UNet with effective conv2d\nconfig_unet = LoHaConfig(\n    r=8,\n    alpha=32,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n    use_effective_conv2d=True,  # Important for Conv2d layers\n)\n\nmodel = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\nmodel.unet = LoHaModel(model.unet, config_unet, \"default\")\n</syntaxhighlight>\n\n=== Example 3: LoHa with Dropout and Layer-Specific Ranks ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoHaConfig, get_peft_model\n\nconfig = LoHaConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\".*attention.*(q|k|v)_proj\"],  # Regex pattern\n    rank_dropout=0.1,  # 10% rank dropout\n    module_dropout=0.05,  # 5% module dropout\n    # Layer-specific rank overrides\n    rank_pattern={\n        \"^model.layers.0.*\": 16,  # Higher rank for first layer\n        \"^model.layers.31.*\": 4,  # Lower rank for last layer\n    },\n    # Layer-specific alpha overrides\n    alpha_pattern={\n        \"^model.layers.0.*\": 32,\n    },\n    init_weights=True,\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Example 4: LoHa with Selective Layer Transformation ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoHaConfig, get_peft_model\n\n# Only adapt specific layers\nconfig = LoHaConfig(\n    r=8,\n    alpha=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=[0, 1, 2, 3],  # Only first 4 layers\n    layers_pattern=\"layers\",  # Pattern name in model architecture\n    modules_to_save=[\"lm_head\"],  # Also train lm_head\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Example 5: LoHa with All-Linear Targeting ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoHaConfig, get_peft_model, TaskType\n\n# Target all linear layers automatically\nconfig = LoHaConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=16,\n    alpha=32,\n    target_modules=\"all-linear\",  # Targets all linear layers except output\n    exclude_modules=[\"lm_head\"],  # Explicitly exclude specific modules\n    rank_dropout=0.1,\n    init_weights=True,\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Post-Initialization Processing ===\nThe <code>__post_init__</code> method performs:\n* Sets <code>peft_type</code> to <code>PeftType.LOHA</code>\n* Converts <code>target_modules</code> list to set for efficient lookup\n* Converts <code>exclude_modules</code> list to set\n* Validates that <code>layers_pattern</code> is only used with <code>layers_to_transform</code>\n\n=== Effective Conv2d Decomposition ===\nWhen <code>use_effective_conv2d=True</code>, LoHa applies \"Proposition 3\" from the FedPara paper for efficient decomposition of convolutional layers with kernel size > 1. This is particularly useful for vision models.\n\n=== Rank and Alpha Patterns ===\nThe pattern system allows fine-grained control:\n* Uses regex matching to identify layers\n* Overrides default r and alpha values per layer\n* Enables architecture-specific optimization\n\n== Related Pages ==\n\n* [[huggingface_peft_LoHaModel|LoHaModel]] - Model class that applies LoHa adapters\n* [[huggingface_peft_LoHaLayer|LoHaLayer]] - Layer implementation for LoHa\n* [[huggingface_peft_LycorisConfig|LycorisConfig]] - Base configuration class for Lycoris methods\n* [[huggingface_peft_LoKrConfig|LoKrConfig]] - Configuration for LoKr (related method)\n* [[huggingface_peft_LoRAConfig|LoRAConfig]] - Configuration for LoRA (alternative method)\n* [[huggingface_peft_get_peft_model|get_peft_model]] - Function to create PEFT models\n\n[[Category:PEFT]]\n[[Category:Configuration]]\n[[Category:LoHa]]\n[[Category:Low-Rank Adaptation]]\n[[Category:Parameter-Efficient Fine-Tuning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LoHaLayer",
      "page_title": "huggingface peft LoHaLayer",
      "page_type": "Implementation",
      "overview": "Low-rank Hadamard product adaptation layer that decomposes weight updates using Hadamard (element-wise) products of low-rank matrices for efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|LyCORIS|https://github.com/KohakuBlueleaf/LyCORIS]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::LyCORIS]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nLow-rank Hadamard product adaptation layer that decomposes weight updates using Hadamard (element-wise) products of low-rank matrices for efficient fine-tuning.\n\n=== Description ===\n\nLoHaLayer implements adaptation using the Hadamard product of two low-rank decompositions. Instead of the additive B*A decomposition used in LoRA, LoHa uses (W1a @ W1b) * (W2a @ W2b), where * denotes element-wise multiplication. For convolutional layers, it uses additional tensor decomposition (t1, t2) for more efficient parameterization. This approach can capture different patterns than LoRA while maintaining similar parameter efficiency.\n\n=== Usage ===\n\nUse LoHa when standard LoRA doesn't capture the adaptation patterns well, particularly for diffusion models and image generation tasks. LoHa is part of the LyCORIS family of adapters and is well-suited for Stable Diffusion fine-tuning. It supports rank dropout and module dropout for regularization.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/loha/layer.py src/peft/tuners/loha/layer.py]\n* '''Lines:''' 1-445\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass LoHaLayer(nn.Module, LycorisLayer):\n    \"\"\"\n    Low-rank Hadamard product layer.\n\n    Attributes:\n        hada_w1_a, hada_w1_b: First pair of low-rank matrices\n        hada_w2_a, hada_w2_b: Second pair of low-rank matrices\n        hada_t1, hada_t2: Tensor decomposition for Conv layers\n    \"\"\"\n    adapter_layer_names = (\n        \"hada_w1_a\", \"hada_w1_b\", \"hada_w2_a\", \"hada_w2_b\",\n        \"hada_t1\", \"hada_t2\"\n    )\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        r: int,\n        alpha: float,\n        rank_dropout: float,\n        module_dropout: float,\n        init_weights: bool,\n        use_effective_conv2d: bool = False,\n        inference_mode: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Create LoHa adapter parameters.\"\"\"\n\n    def get_delta_weight(self, adapter_name: str) -> torch.Tensor:\n        \"\"\"Compute Hadamard product of low-rank decompositions.\"\"\"\n\nclass Linear(LoHaLayer):\n    \"\"\"LoHa implemented in Linear layer.\"\"\"\n\nclass Conv2d(LoHaLayer):\n    \"\"\"LoHa implemented in Conv2d layer.\"\"\"\n\nclass Conv1d(LoHaLayer):\n    \"\"\"LoHa implemented in Conv1d layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.loha import LoHaLayer, LoHaConfig, LoHaModel\nfrom peft import LoHaConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained layer (Linear, Conv2d, or Conv1d)\n|-\n| adapter_name || str || No || Name for the adapter (default: \"default\")\n|-\n| r || int || Yes || Rank for low-rank decomposition\n|-\n| alpha || float || No || Scaling factor\n|-\n| rank_dropout || float || No || Dropout probability for rank dimension\n|-\n| module_dropout || float || No || Probability of disabling adapter during training\n|-\n| use_effective_conv2d || bool || No || Use tensor decomposition for Conv2d\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input + Hadamard product adaptation\n|-\n| get_delta_weight() || torch.Tensor || (W1a @ W1b) * (W2a @ W2b) scaled\n|}\n\n== Usage Examples ==\n\n=== Basic LoHa for Diffusion Models ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoHaConfig, get_peft_model\nfrom diffusers import UNet2DConditionModel\n\n# Load Stable Diffusion UNet\nunet = UNet2DConditionModel.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    subfolder=\"unet\"\n)\n\n# Configure LoHa (popular for SD fine-tuning)\nconfig = LoHaConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"to_q\", \"to_v\", \"to_k\", \"to_out.0\"],\n    rank_dropout=0.1,\n    module_dropout=0.0,\n)\n\n# Create PEFT model\nunet = get_peft_model(unet, config)\n</syntaxhighlight>\n\n=== LoHa with Effective Conv2d ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoHaConfig, get_peft_model\n\n# Use tensor decomposition for convolutions\nconfig = LoHaConfig(\n    r=16,\n    alpha=32,\n    target_modules=[\"conv1\", \"conv2\"],\n    use_effective_conv2d=True,  # More efficient for larger kernels\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== LoHa with Regularization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoHaConfig, get_peft_model\n\n# Add dropout regularization\nconfig = LoHaConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"to_q\", \"to_v\"],\n    rank_dropout=0.1,      # Drop rank dimensions\n    module_dropout=0.05,   # Occasionally skip entire adapter\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "LyCORIS"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "LyCORIS",
          "url": "https://github.com/KohakuBlueleaf/LyCORIS"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_LoHaModel",
      "page_title": "huggingface peft LoHaModel",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>LoHaModel</code> class creates Low-Rank Hadamard Product (LoHa) models from pretrained models. This class implements a parameter-efficient fine-tuning method that uses Hadamard products combined with low-rank matrix decomposition to adapt neural networks with minimal trainable parameters. LoHaModel inherits from <code>LycorisTuner</code> and supports various layer types including Linear, Conv1d, and Conv2d. The method is partially described in https://huggingface.co/papers/2108.06098, and the current implementation heavily borrows from the LyCORIS repository (https://github.com/KohakuBlueleaf/LyCORIS). Key features include: * Support for Linear, Conv1d, and Conv2d layers * Automatic target module identification based on model architecture * Layer-specific rank and alpha configuration through patterns * Efficient decomposition for convolutional layers * Compatible with diffusion models, vision transformers, and language models",
      "content": "{{Implementation\n|domain=NLP,Computer Vision,PEFT,Parameter-Efficient Fine-Tuning,LoHa,Low-Rank Adaptation,Diffusion Models\n|link=https://github.com/huggingface/peft\n}}\n\n== Overview ==\n\n=== Description ===\n\nThe <code>LoHaModel</code> class creates Low-Rank Hadamard Product (LoHa) models from pretrained models. This class implements a parameter-efficient fine-tuning method that uses Hadamard products combined with low-rank matrix decomposition to adapt neural networks with minimal trainable parameters.\n\nLoHaModel inherits from <code>LycorisTuner</code> and supports various layer types including Linear, Conv1d, and Conv2d. The method is partially described in https://huggingface.co/papers/2108.06098, and the current implementation heavily borrows from the LyCORIS repository (https://github.com/KohakuBlueleaf/LyCORIS).\n\nKey features include:\n* Support for Linear, Conv1d, and Conv2d layers\n* Automatic target module identification based on model architecture\n* Layer-specific rank and alpha configuration through patterns\n* Efficient decomposition for convolutional layers\n* Compatible with diffusion models, vision transformers, and language models\n\n=== Usage ===\n\nLoHaModel is typically instantiated through the PEFT library's standard interface or directly for more control. It's particularly popular for fine-tuning Stable Diffusion models and other large vision or language models where parameter efficiency is crucial.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' huggingface/peft\n* '''File Path:''' <code>src/peft/tuners/loha/model.py</code>\n* '''Lines:''' 27-117\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass LoHaModel(LycorisTuner):\n    prefix: str = \"hada_\"\n    tuner_layer_cls = LoHaLayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_LOHA_TARGET_MODULES_MAPPING\n    layers_mapping: dict[type[torch.nn.Module], type[LoHaLayer]] = {\n        torch.nn.Conv2d: Conv2d,\n        torch.nn.Conv1d: Conv1d,\n        torch.nn.Linear: Linear,\n    }\n\n    def _create_and_replace(\n        self,\n        config: LycorisConfig,\n        adapter_name: str,\n        target: Union[LoHaLayer, nn.Module],\n        target_name: str,\n        parent: nn.Module,\n        current_key: str,\n    ) -> None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.loha.model import LoHaModel\n# or\nfrom peft import LoHaModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n\n'''Constructor:'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>model</code> || <code>torch.nn.Module</code> || The model to which the adapter tuner layers will be attached\n|-\n| <code>config</code> || <code>LoHaConfig</code> || The configuration of the LoHa model\n|-\n| <code>adapter_name</code> || <code>str</code> || The name of the adapter (default: \"default\")\n|-\n| <code>low_cpu_mem_usage</code> || <code>bool</code> || Create empty adapter weights on meta device for faster loading (default: False)\n|}\n\n'''Method: _create_and_replace'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>config</code> || <code>LycorisConfig</code> || Configuration for the adapter\n|-\n| <code>adapter_name</code> || <code>str</code> || Name of the adapter to create\n|-\n| <code>target</code> || <code>Union[LoHaLayer, nn.Module]</code> || Target module to replace or update\n|-\n| <code>target_name</code> || <code>str</code> || Name of the target module\n|-\n| <code>parent</code> || <code>nn.Module</code> || Parent module containing the target\n|-\n| <code>current_key</code> || <code>str</code> || Key path to the current module\n|}\n\n=== Outputs ===\n\n'''Attributes:'''\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| <code>model</code> || <code>torch.nn.Module</code> || The adapted model with LoHa layers\n|-\n| <code>peft_config</code> || <code>LoHaConfig</code> || The configuration used for adaptation\n|-\n| <code>prefix</code> || <code>str</code> || Parameter prefix for LoHa adapters (\"hada_\")\n|-\n| <code>tuner_layer_cls</code> || <code>type</code> || The base layer class (LoHaLayer)\n|-\n| <code>layers_mapping</code> || <code>dict</code> || Mapping from PyTorch layer types to LoHa layer types\n|}\n\n'''Returns:'''\n{| class=\"wikitable\"\n! Type !! Description\n|-\n| <code>torch.nn.Module</code> || The LoHa-adapted model ready for training or inference\n|}\n\n== Usage Examples ==\n\n=== Example 1: Basic LoHa Model for Language Models ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import LoHaModel, LoHaConfig\n\n# Configure LoHa\nconfig = LoHaConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Load and adapt model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = LoHaModel(model, config, \"default\")\n\n# Check trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable: {trainable_params} / {total_params} ({100*trainable_params/total_params:.2f}%)\")\n</syntaxhighlight>\n\n=== Example 2: LoHa for Stable Diffusion Text Encoder and UNet ===\n<syntaxhighlight lang=\"python\">\nfrom diffusers import StableDiffusionPipeline\nfrom peft import LoHaModel, LoHaConfig\n\n# Configuration for text encoder\nconfig_te = LoHaConfig(\n    r=8,\n    alpha=32,\n    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Configuration for UNet with effective conv2d\nconfig_unet = LoHaConfig(\n    r=8,\n    alpha=32,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n    use_effective_conv2d=True,  # Important for conv layers\n)\n\n# Load pipeline and adapt\nmodel = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\nmodel.text_encoder = LoHaModel(model.text_encoder, config_te, \"default\")\nmodel.unet = LoHaModel(model.unet, config_unet, \"default\")\n\n# Now ready for fine-tuning\n# ... training loop ...\n</syntaxhighlight>\n\n=== Example 3: LoHa with Layer-Specific Ranks ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModel\nfrom peft import LoHaModel, LoHaConfig\n\n# Configure with rank patterns\nconfig = LoHaConfig(\n    r=8,\n    alpha=8,\n    target_modules=[\"query\", \"key\", \"value\"],\n    rank_pattern={\n        # Lower layers get higher rank\n        \"^model.encoder.layer.[0-5].*\": 16,\n        # Middle layers get default rank (8)\n        # Upper layers get lower rank\n        \"^model.encoder.layer.(1[0-9]|2[0-9]|3[0-9]).*\": 4,\n    },\n    alpha_pattern={\n        \"^model.encoder.layer.[0-5].*\": 32,\n    },\n    init_weights=True,\n)\n\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\nmodel = LoHaModel(model, config, \"default\")\n</syntaxhighlight>\n\n=== Example 4: Multi-Adapter LoHa Model ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoHaModel, LoHaConfig\n\n# Create base LoHa model\nconfig1 = LoHaConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\nmodel = LoHaModel(base_model, config1, adapter_name=\"task1\")\n\n# Add second adapter for different task\nconfig2 = LoHaConfig(\n    r=16,\n    alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\"],\n)\n# Note: Adding adapters typically done through PeftModel interface\n# This example shows the conceptual structure\n\n# Switch between adapters during inference\n# model.set_adapter(\"task1\")\n# output1 = model(input_ids)\n\n# model.set_adapter(\"task2\")\n# output2 = model(input_ids)\n</syntaxhighlight>\n\n=== Example 5: LoHa with Dropout for Regularization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import LoHaModel, LoHaConfig\n\nconfig = LoHaConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"q\", \"k\", \"v\", \"o\"],\n    rank_dropout=0.1,  # 10% rank dropout during training\n    module_dropout=0.05,  # 5% chance to skip entire module\n    init_weights=True,\n    use_effective_conv2d=False,\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\nmodel = LoHaModel(model, config, \"default\")\n\n# Train with dropout enabled\nmodel.train()\n# ... training loop ...\n\n# Inference without dropout\nmodel.eval()\n# ... generate outputs ...\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Layer Mapping System ===\nLoHaModel uses a <code>layers_mapping</code> dictionary to map PyTorch layer types to corresponding LoHa implementations:\n<syntaxhighlight lang=\"python\">\nlayers_mapping = {\n    torch.nn.Conv2d: Conv2d,  # LoHa Conv2d layer\n    torch.nn.Conv1d: Conv1d,  # LoHa Conv1d layer\n    torch.nn.Linear: Linear,  # LoHa Linear layer\n}\n</syntaxhighlight>\n\nThis allows LoHa to automatically adapt different layer types with appropriate implementations.\n\n=== Adapter Creation Process ===\nThe <code>_create_and_replace</code> method:\n1. Extracts rank and alpha from patterns or uses defaults\n2. Checks if target is already a LoHaLayer\n   - If yes: updates the layer with new adapter\n   - If no: creates new LoHaLayer wrapping the target\n3. Replaces the original module in the parent\n\n<syntaxhighlight lang=\"python\">\n# Simplified flow:\nr_key = get_pattern_key(config.rank_pattern.keys(), current_key)\nalpha_key = get_pattern_key(config.alpha_pattern.keys(), current_key)\nkwargs[\"r\"] = config.rank_pattern.get(r_key, config.r)\nkwargs[\"alpha\"] = config.alpha_pattern.get(alpha_key, config.alpha)\n\nif isinstance(target, LoHaLayer):\n    target.update_layer(adapter_name, **kwargs)\nelse:\n    new_module = self._create_new_module(config, adapter_name, target, **kwargs)\n    self._replace_module(parent, target_name, new_module, target)\n</syntaxhighlight>\n\n=== Target Module Resolution ===\nLoHaModel uses <code>TRANSFORMERS_MODELS_TO_LOHA_TARGET_MODULES_MAPPING</code> to automatically identify appropriate modules for different model architectures, eliminating the need for manual specification in many cases.\n\n=== Parameter Prefix ===\nAll LoHa parameters use the prefix \"hada_\" (Hadamard), making them easily identifiable in the model's state dict.\n\n== Related Pages ==\n\n* [[huggingface_peft_LoHaConfig|LoHaConfig]] - Configuration for LoHa models\n* [[huggingface_peft_LoHaLayer|LoHaLayer]] - Base layer class for LoHa adapters\n* [[huggingface_peft_LycorisTuner|LycorisTuner]] - Base class for Lycoris-based tuners\n* [[huggingface_peft_LoKrModel|LoKrModel]] - Related model using Kronecker products\n* [[huggingface_peft_LoraModel|LoraModel]] - Alternative low-rank adaptation method\n* [[huggingface_peft_get_peft_model|get_peft_model]] - Standard function to create PEFT models\n\n[[Category:PEFT]]\n[[Category:Model]]\n[[Category:LoHa]]\n[[Category:Low-Rank Adaptation]]\n[[Category:Parameter-Efficient Fine-Tuning]]\n[[Category:Diffusion Models]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LoKrConfig",
      "page_title": "huggingface peft LoKrConfig",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>LoKrConfig</code> class is a configuration dataclass for Low-Rank Kronecker Product (LoKr) adaptation in PEFT. LoKr is an advanced parameter-efficient fine-tuning method that uses Kronecker product decomposition to create extremely efficient low-rank adaptations of neural network layers. LoKrConfig inherits from <code>LycorisConfig</code> and provides comprehensive configuration options for LoKr adapters. The method combines ideas from multiple papers including https://huggingface.co/papers/2108.06098 and https://huggingface.co/papers/2309.14859, offering sophisticated control over the decomposition strategy. Key features include: * Kronecker product-based low-rank decomposition * Optional decomposition of both matrices in the Kronecker product * Configurable decomposition factor * Rank dropout with optional scaling * Effective Conv2d decomposition for convolutional layers * Layer-specific rank and alpha patterns * Multiple weight initialization strategies including \"lycoris\" style",
      "content": "{{Implementation\n|domain=NLP,Computer Vision,PEFT,Parameter-Efficient Fine-Tuning,LoKr,Low-Rank Adaptation,Kronecker Product\n|link=https://github.com/huggingface/peft\n}}\n\n== Overview ==\n\n=== Description ===\n\nThe <code>LoKrConfig</code> class is a configuration dataclass for Low-Rank Kronecker Product (LoKr) adaptation in PEFT. LoKr is an advanced parameter-efficient fine-tuning method that uses Kronecker product decomposition to create extremely efficient low-rank adaptations of neural network layers.\n\nLoKrConfig inherits from <code>LycorisConfig</code> and provides comprehensive configuration options for LoKr adapters. The method combines ideas from multiple papers including https://huggingface.co/papers/2108.06098 and https://huggingface.co/papers/2309.14859, offering sophisticated control over the decomposition strategy.\n\nKey features include:\n* Kronecker product-based low-rank decomposition\n* Optional decomposition of both matrices in the Kronecker product\n* Configurable decomposition factor\n* Rank dropout with optional scaling\n* Effective Conv2d decomposition for convolutional layers\n* Layer-specific rank and alpha patterns\n* Multiple weight initialization strategies including \"lycoris\" style\n\n=== Usage ===\n\nLoKrConfig is used to configure LoKr adapters before applying them to models via the PEFT library. It's particularly effective for models where extreme parameter efficiency is required while maintaining performance. The Kronecker product decomposition often achieves better compression ratios than standard low-rank methods.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' huggingface/peft\n* '''File Path:''' <code>src/peft/tuners/lokr/config.py</code>\n* '''Lines:''' 23-156\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass LoKrConfig(LycorisConfig):\n    r: int = 8\n    alpha: int = 8\n    rank_dropout: float = 0.0\n    module_dropout: float = 0.0\n    use_effective_conv2d: bool = False\n    decompose_both: bool = False\n    decompose_factor: int = -1\n    rank_dropout_scale: bool = False\n    target_modules: Optional[Union[list[str], str]] = None\n    exclude_modules: Optional[Union[list[str], str]] = None\n    init_weights: Union[bool, Literal[\"lycoris\"]] = True\n    layers_to_transform: Optional[Union[list[int], int]] = None\n    layers_pattern: Optional[Union[list[str], str]] = None\n    modules_to_save: Optional[list[str]] = None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lokr.config import LoKrConfig\n# or\nfrom peft import LoKrConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| <code>r</code> || <code>int</code> || <code>8</code> || LoKr rank - controls the dimension of the low-rank decomposition\n|-\n| <code>alpha</code> || <code>int</code> || <code>8</code> || The alpha parameter for LoKr scaling\n|-\n| <code>rank_dropout</code> || <code>float</code> || <code>0.0</code> || The dropout probability for rank dimension during training\n|-\n| <code>module_dropout</code> || <code>float</code> || <code>0.0</code> || The dropout probability for disabling LoKr modules during training\n|-\n| <code>use_effective_conv2d</code> || <code>bool</code> || <code>False</code> || Use parameter effective decomposition for Conv2d (and Conv1d) with ksize > 1 (\"Proposition 3\" from FedPara paper)\n|-\n| <code>decompose_both</code> || <code>bool</code> || <code>False</code> || Perform rank decomposition of left Kronecker product matrix (more parameter efficient)\n|-\n| <code>decompose_factor</code> || <code>int</code> || <code>-1</code> || Kronecker product decomposition factor. -1 means automatic selection\n|-\n| <code>rank_dropout_scale</code> || <code>bool</code> || <code>False</code> || Whether to scale the rank dropout while training\n|-\n| <code>target_modules</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || List of module names or regex expression to replace with LoKr. Can be 'all-linear' for all linear layers except output\n|-\n| <code>exclude_modules</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || List of module names or regex expression to exclude from LoKr adaptation\n|-\n| <code>init_weights</code> || <code>Union[bool, Literal[\"lycoris\"]]</code> || <code>True</code> || Whether to initialize weights. Can be True, False, or \"lycoris\" for LyCORIS-style initialization\n|-\n| <code>layers_to_transform</code> || <code>Optional[Union[list[int], int]]</code> || <code>None</code> || The layer indices to transform. If specified, only these layers will be adapted\n|-\n| <code>layers_pattern</code> || <code>Optional[Union[list[str], str]]</code> || <code>None</code> || The layer pattern name (e.g., 'layers' or 'h'), used with layers_to_transform\n|-\n| <code>modules_to_save</code> || <code>Optional[list[str]]</code> || <code>None</code> || List of modules apart from LoKr layers to be set as trainable and saved\n|}\n\n'''Inherited from LycorisConfig:'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>rank_pattern</code> || <code>dict</code> || Mapping from layer names/regex to ranks different from default r\n|-\n| <code>alpha_pattern</code> || <code>dict</code> || Mapping from layer names/regex to alphas different from default alpha\n|}\n\n=== Outputs ===\n\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| <code>peft_type</code> || <code>PeftType</code> || Automatically set to <code>PeftType.LOKR</code> in <code>__post_init__</code>\n|-\n| <code>target_modules</code> || <code>Optional[Union[set[str], str]]</code> || Converted to set if originally a list\n|-\n| <code>exclude_modules</code> || <code>Optional[Union[set[str], str]]</code> || Converted to set if originally a list\n|}\n\n== Usage Examples ==\n\n=== Example 1: Basic LoKr Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Create LoKr configuration\nconfig = LoKrConfig(\n    r=8,\n    alpha=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Apply to model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Example 2: LoKr with Both Matrices Decomposed ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrConfig, get_peft_model\n\n# More aggressive compression with decompose_both\nconfig = LoKrConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    decompose_both=True,  # Decompose both Kronecker matrices\n    decompose_factor=4,  # Specific decomposition factor\n    rank_dropout=0.0,\n    init_weights=True,\n)\n\nmodel = get_peft_model(base_model, config)\n\n# This configuration uses even fewer parameters than standard LoKr\nprint(\"Highly compressed LoKr model ready\")\n</syntaxhighlight>\n\n=== Example 3: LoKr for Stable Diffusion with Effective Conv2d ===\n<syntaxhighlight lang=\"python\">\nfrom diffusers import StableDiffusionPipeline\nfrom peft import LoKrModel, LoKrConfig\n\n# Configuration for text encoder\nconfig_te = LoKrConfig(\n    r=8,\n    alpha=32,\n    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Configuration for UNet with effective conv2d\nconfig_unet = LoKrConfig(\n    r=8,\n    alpha=32,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n    use_effective_conv2d=True,  # Important for Conv2d layers\n    decompose_both=True,  # Maximum compression\n)\n\nmodel = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\nmodel.text_encoder = LoKrModel(model.text_encoder, config_te, \"default\")\nmodel.unet = LoKrModel(model.unet, config_unet, \"default\")\n</syntaxhighlight>\n\n=== Example 4: LoKr with LyCORIS-Style Initialization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrConfig, get_peft_model\n\n# Use LyCORIS initialization strategy\nconfig = LoKrConfig(\n    r=16,\n    alpha=32,\n    target_modules=[\".*attention.*(q|k|v)_proj\"],  # Regex pattern\n    init_weights=\"lycoris\",  # Use LyCORIS-style initialization\n    decompose_both=False,\n    rank_dropout=0.1,\n    rank_dropout_scale=True,  # Scale dropout during training\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Example 5: LoKr with Layer-Specific Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrConfig, get_peft_model\n\n# Configure with rank patterns and selective layers\nconfig = LoKrConfig(\n    r=8,\n    alpha=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    # Only adapt specific layers\n    layers_to_transform=[0, 1, 2, 3, 28, 29, 30, 31],  # First 4 and last 4 layers\n    layers_pattern=\"layers\",\n    # Layer-specific rank overrides\n    rank_pattern={\n        \"^model.layers.[0-3].*\": 16,  # Higher rank for first layers\n        \"^model.layers.(28|29|30|31).*\": 16,  # Higher rank for last layers\n    },\n    alpha_pattern={\n        \"^model.layers.[0-3].*\": 32,\n        \"^model.layers.(28|29|30|31).*\": 32,\n    },\n    decompose_both=True,\n    decompose_factor=-1,  # Auto-select factor\n    modules_to_save=[\"lm_head\"],\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Example 6: LoKr with Dropout and Regularization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrConfig, get_peft_model\n\nconfig = LoKrConfig(\n    r=8,\n    alpha=16,\n    target_modules=\"all-linear\",  # Target all linear layers\n    exclude_modules=[\"lm_head\"],  # Exclude output layer\n    rank_dropout=0.1,  # 10% rank dropout\n    module_dropout=0.05,  # 5% module dropout\n    rank_dropout_scale=True,  # Scale activations during dropout\n    init_weights=True,\n    decompose_both=False,\n)\n\nmodel = get_peft_model(base_model, config)\n\n# Train with regularization\nmodel.train()\n# ... training loop ...\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Kronecker Product Decomposition ===\nLoKr uses Kronecker product factorization: <code>W \u2248 A \u2297 B</code> where:\n* <code>W</code> is the original weight matrix\n* <code>A</code> and <code>B</code> are smaller matrices\n* <code>\u2297</code> denotes the Kronecker product\n\nWhen <code>decompose_both=True</code>, both A and B are further decomposed into low-rank matrices, achieving even higher compression.\n\n=== Decomposition Factor ===\nThe <code>decompose_factor</code> parameter controls how the matrices are split:\n* <code>-1</code> (default): Automatic selection based on dimensions\n* Positive integer: Specific factor for decomposition\n* Affects the balance between the two Kronecker factors\n\n=== Rank Dropout Scaling ===\nWhen <code>rank_dropout_scale=True</code>, the implementation scales the remaining activations to maintain expected values during dropout, similar to standard dropout behavior. This can improve training stability.\n\n=== Post-Initialization Processing ===\nThe <code>__post_init__</code> method:\n* Sets <code>peft_type</code> to <code>PeftType.LOKR</code>\n* Converts <code>target_modules</code> and <code>exclude_modules</code> to sets\n* Validates <code>layers_pattern</code> is only used with <code>layers_to_transform</code>\n\n=== Initialization Modes ===\n* <code>True</code>: Standard PyTorch initialization\n* <code>False</code>: No initialization (not recommended)\n* <code>\"lycoris\"</code>: Specialized initialization from LyCORIS repository\n\n== Related Pages ==\n\n* [[huggingface_peft_LoKrModel|LoKrModel]] - Model class that applies LoKr adapters\n* [[huggingface_peft_LoKrLayer|LoKrLayer]] - Layer implementation for LoKr\n* [[huggingface_peft_LycorisConfig|LycorisConfig]] - Base configuration class for Lycoris methods\n* [[huggingface_peft_LoHaConfig|LoHaConfig]] - Configuration for LoHa (related method)\n* [[huggingface_peft_LoRAConfig|LoRAConfig]] - Configuration for standard LoRA\n* [[huggingface_peft_get_peft_model|get_peft_model]] - Function to create PEFT models\n\n[[Category:PEFT]]\n[[Category:Configuration]]\n[[Category:LoKr]]\n[[Category:Kronecker Product]]\n[[Category:Low-Rank Adaptation]]\n[[Category:Parameter-Efficient Fine-Tuning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LoKrLayer",
      "page_title": "huggingface peft LoKrLayer",
      "page_type": "Implementation",
      "overview": "Low-rank Kronecker product adaptation layer that decomposes weight updates using Kronecker products of smaller matrices for highly parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|LyCORIS|https://github.com/KohakuBlueleaf/LyCORIS]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::LyCORIS]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nLow-rank Kronecker product adaptation layer that decomposes weight updates using Kronecker products of smaller matrices for highly parameter-efficient fine-tuning.\n\n=== Description ===\n\nLoKrLayer implements adaptation using the Kronecker product of two matrices. The weight update is decomposed as W = kron(W1, W2) where W1 and W2 can themselves be low-rank decompositions (W1 = W1a @ W1b). This allows representing large weight updates with very few parameters since kron(A, B) expands dimensions multiplicatively. The factorization helper automatically determines optimal matrix shapes based on the decompose_factor parameter.\n\n=== Usage ===\n\nUse LoKr for extremely parameter-efficient adaptation, especially for large weight matrices. LoKr is part of the LyCORIS family and excels when the weight update has a Kronecker structure. It's particularly effective for Stable Diffusion and vision models. The decompose_both option allows further reduction by low-rank decomposing both Kronecker factors.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/lokr/layer.py src/peft/tuners/lokr/layer.py]\n* '''Lines:''' 1-512\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass LoKrLayer(nn.Module, LycorisLayer):\n    \"\"\"\n    Low-rank Kronecker product layer.\n\n    Attributes:\n        lokr_w1, lokr_w1_a, lokr_w1_b: First Kronecker factor (full or decomposed)\n        lokr_w2, lokr_w2_a, lokr_w2_b: Second Kronecker factor\n        lokr_t2: Tensor decomposition for effective Conv2d\n    \"\"\"\n    adapter_layer_names = (\n        \"lokr_w1\", \"lokr_w1_a\", \"lokr_w1_b\",\n        \"lokr_w2\", \"lokr_w2_a\", \"lokr_w2_b\", \"lokr_t2\"\n    )\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        r: int,\n        alpha: float,\n        rank_dropout: float,\n        module_dropout: float,\n        init_weights: bool,\n        use_effective_conv2d: bool,\n        decompose_both: bool,\n        decompose_factor: int,\n        inference_mode: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Create LoKr adapter parameters.\"\"\"\n\n    def get_delta_weight(self, adapter_name: str) -> torch.Tensor:\n        \"\"\"Compute Kronecker product of factors.\"\"\"\n\ndef factorization(dimension: int, factor: int = -1) -> tuple[int, int]:\n    \"\"\"Find optimal factorization of dimension.\"\"\"\n\nclass Linear(LoKrLayer):\n    \"\"\"LoKr implemented in Linear layer.\"\"\"\n\nclass Conv2d(LoKrLayer):\n    \"\"\"LoKr implemented in Conv2d layer.\"\"\"\n\nclass Conv1d(LoKrLayer):\n    \"\"\"LoKr implemented in Conv1d layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lokr import LoKrLayer, LoKrConfig, LoKrModel\nfrom peft import LoKrConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained layer (Linear, Conv2d, or Conv1d)\n|-\n| adapter_name || str || No || Name for the adapter (default: \"default\")\n|-\n| r || int || Yes || Rank for low-rank decomposition of Kronecker factors\n|-\n| alpha || float || No || Scaling factor\n|-\n| decompose_both || bool || No || Apply low-rank decomposition to both factors\n|-\n| decompose_factor || int || No || Target factor for dimension factorization (-1 for auto)\n|-\n| use_effective_conv2d || bool || No || Use tensor decomposition for Conv2d\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input + Kronecker product adaptation\n|-\n| get_delta_weight() || torch.Tensor || kron(W1, W2) * scaling\n|}\n\n== Usage Examples ==\n\n=== Basic LoKr Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrConfig, get_peft_model\nfrom diffusers import UNet2DConditionModel\n\n# Load model\nunet = UNet2DConditionModel.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    subfolder=\"unet\"\n)\n\n# Configure LoKr\nconfig = LoKrConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"to_q\", \"to_v\", \"to_k\", \"to_out.0\"],\n    decompose_both=False,  # Only decompose second factor\n    decompose_factor=-1,   # Auto-detect best factorization\n)\n\nunet = get_peft_model(unet, config)\n</syntaxhighlight>\n\n=== LoKr with Full Decomposition ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrConfig, get_peft_model\n\n# Maximum parameter efficiency with decompose_both\nconfig = LoKrConfig(\n    r=4,\n    alpha=8,\n    target_modules=[\"to_q\", \"to_v\"],\n    decompose_both=True,   # Decompose both Kronecker factors\n    decompose_factor=8,    # Target factor for factorization\n)\n\nmodel = get_peft_model(model, config)\n# Even fewer parameters than standard LoKr\n</syntaxhighlight>\n\n=== LoKr for Vision Transformers ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrConfig, get_peft_model\nfrom transformers import AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\"\n)\n\nconfig = LoKrConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"query\", \"value\"],\n    rank_dropout=0.1,\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "LyCORIS"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "LyCORIS",
          "url": "https://github.com/KohakuBlueleaf/LyCORIS"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_LoKrModel",
      "page_title": "huggingface peft LoKrModel",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>LoKrModel</code> class creates Low-Rank Kronecker Product (LoKr) models from pretrained models. This class implements an advanced parameter-efficient fine-tuning method that uses Kronecker product decomposition to achieve extreme parameter efficiency while maintaining model performance. LoKrModel inherits from <code>LycorisTuner</code> and supports various layer types including Linear, Conv1d, and Conv2d. The method is described in multiple papers including https://huggingface.co/papers/2108.06098 and https://huggingface.co/papers/2309.14859. The current implementation heavily borrows from the LyCORIS repository (https://github.com/KohakuBlueleaf/LyCORIS). Key features include: * Kronecker product-based decomposition for extreme compression * Support for Linear, Conv1d, and Conv2d layers * Optional decomposition of both Kronecker matrices for higher compression * Automatic target module identification for common architectures * Layer-specific rank and alpha configuration through patterns * Rank dropout with optional scaling * Compatible with language models, vision models, and diffusion models",
      "content": "{{Implementation\n|domain=NLP,Computer Vision,PEFT,Parameter-Efficient Fine-Tuning,LoKr,Low-Rank Adaptation,Kronecker Product,Diffusion Models\n|link=https://github.com/huggingface/peft\n}}\n\n== Overview ==\n\n=== Description ===\n\nThe <code>LoKrModel</code> class creates Low-Rank Kronecker Product (LoKr) models from pretrained models. This class implements an advanced parameter-efficient fine-tuning method that uses Kronecker product decomposition to achieve extreme parameter efficiency while maintaining model performance.\n\nLoKrModel inherits from <code>LycorisTuner</code> and supports various layer types including Linear, Conv1d, and Conv2d. The method is described in multiple papers including https://huggingface.co/papers/2108.06098 and https://huggingface.co/papers/2309.14859. The current implementation heavily borrows from the LyCORIS repository (https://github.com/KohakuBlueleaf/LyCORIS).\n\nKey features include:\n* Kronecker product-based decomposition for extreme compression\n* Support for Linear, Conv1d, and Conv2d layers\n* Optional decomposition of both Kronecker matrices for higher compression\n* Automatic target module identification for common architectures\n* Layer-specific rank and alpha configuration through patterns\n* Rank dropout with optional scaling\n* Compatible with language models, vision models, and diffusion models\n\n=== Usage ===\n\nLoKrModel is typically instantiated through the PEFT library's interface or directly for fine-grained control. It's particularly effective when extreme parameter efficiency is needed, often achieving better compression ratios than standard LoRA or LoHa while maintaining comparable performance.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' huggingface/peft\n* '''File Path:''' <code>src/peft/tuners/lokr/model.py</code>\n* '''Lines:''' 27-119\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass LoKrModel(LycorisTuner):\n    prefix: str = \"lokr_\"\n    tuner_layer_cls = LoKrLayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_LOKR_TARGET_MODULES_MAPPING\n    layers_mapping: dict[type[torch.nn.Module], type[LoKrLayer]] = {\n        torch.nn.Conv2d: Conv2d,\n        torch.nn.Conv1d: Conv1d,\n        torch.nn.Linear: Linear,\n    }\n\n    def _create_and_replace(\n        self,\n        config: LycorisConfig,\n        adapter_name: str,\n        target: Union[LoKrLayer, nn.Module],\n        target_name: str,\n        parent: nn.Module,\n        current_key: str,\n    ) -> None\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lokr.model import LoKrModel\n# or\nfrom peft import LoKrModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n\n'''Constructor:'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>model</code> || <code>torch.nn.Module</code> || The model to which the adapter tuner layers will be attached\n|-\n| <code>config</code> || <code>LoKrConfig</code> || The configuration of the LoKr model\n|-\n| <code>adapter_name</code> || <code>str</code> || The name of the adapter (default: \"default\")\n|-\n| <code>low_cpu_mem_usage</code> || <code>bool</code> || Create empty adapter weights on meta device for faster loading (default: False)\n|}\n\n'''Method: _create_and_replace'''\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| <code>config</code> || <code>LycorisConfig</code> || Configuration for the adapter\n|-\n| <code>adapter_name</code> || <code>str</code> || Name of the adapter to create\n|-\n| <code>target</code> || <code>Union[LoKrLayer, nn.Module]</code> || Target module to replace or update\n|-\n| <code>target_name</code> || <code>str</code> || Name of the target module\n|-\n| <code>parent</code> || <code>nn.Module</code> || Parent module containing the target\n|-\n| <code>current_key</code> || <code>str</code> || Key path to the current module\n|}\n\n=== Outputs ===\n\n'''Attributes:'''\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| <code>model</code> || <code>torch.nn.Module</code> || The adapted model with LoKr layers\n|-\n| <code>peft_config</code> || <code>LoKrConfig</code> || The configuration used for adaptation\n|-\n| <code>prefix</code> || <code>str</code> || Parameter prefix for LoKr adapters (\"lokr_\")\n|-\n| <code>tuner_layer_cls</code> || <code>type</code> || The base layer class (LoKrLayer)\n|-\n| <code>layers_mapping</code> || <code>dict</code> || Mapping from PyTorch layer types to LoKr layer types\n|}\n\n'''Returns:'''\n{| class=\"wikitable\"\n! Type !! Description\n|-\n| <code>torch.nn.Module</code> || The LoKr-adapted model ready for training or inference\n|}\n\n== Usage Examples ==\n\n=== Example 1: Basic LoKr Model for Language Models ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import LoKrModel, LoKrConfig\n\n# Configure LoKr\nconfig = LoKrConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Load and adapt model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\nmodel = LoKrModel(model, config, \"default\")\n\n# Check trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable: {trainable_params} / {total_params} ({100*trainable_params/total_params:.2f}%)\")\n</syntaxhighlight>\n\n=== Example 2: LoKr for Stable Diffusion with Maximum Compression ===\n<syntaxhighlight lang=\"python\">\nfrom diffusers import StableDiffusionPipeline\nfrom peft import LoKrModel, LoKrConfig\n\n# Configuration for text encoder\nconfig_te = LoKrConfig(\n    r=8,\n    alpha=32,\n    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Configuration for UNet with maximum compression\nconfig_unet = LoKrConfig(\n    r=8,\n    alpha=32,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    rank_dropout=0.0,\n    module_dropout=0.0,\n    init_weights=True,\n    use_effective_conv2d=True,  # Efficient conv decomposition\n    decompose_both=True,  # Decompose both Kronecker matrices\n    decompose_factor=-1,  # Auto-select factor\n)\n\n# Load and adapt pipeline\nmodel = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\nmodel.text_encoder = LoKrModel(model.text_encoder, config_te, \"default\")\nmodel.unet = LoKrModel(model.unet, config_unet, \"default\")\n\nprint(\"Ultra-compressed LoKr model ready for fine-tuning\")\n</syntaxhighlight>\n\n=== Example 3: LoKr with Layer-Specific Ranks and Scaling ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModel\nfrom peft import LoKrModel, LoKrConfig\n\n# Configure with rank patterns and dropout scaling\nconfig = LoKrConfig(\n    r=8,\n    alpha=8,\n    target_modules=[\"query\", \"key\", \"value\"],\n    rank_dropout=0.1,\n    rank_dropout_scale=True,  # Scale dropout for stability\n    # Layer-specific ranks\n    rank_pattern={\n        \"^model.encoder.layer.[0-5].*\": 16,  # Higher rank for early layers\n        \"^model.encoder.layer.[6-9].*\": 8,   # Default rank for middle\n        \"^model.encoder.layer.1[0-1].*\": 4,  # Lower rank for late layers\n    },\n    alpha_pattern={\n        \"^model.encoder.layer.[0-5].*\": 32,\n    },\n    decompose_both=True,\n    init_weights=True,\n)\n\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\nmodel = LoKrModel(model, config, \"default\")\n</syntaxhighlight>\n\n=== Example 4: LoKr with LyCORIS Initialization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import LoKrModel, LoKrConfig\n\n# Use LyCORIS-style initialization for better convergence\nconfig = LoKrConfig(\n    r=16,\n    alpha=32,\n    target_modules=[\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n    init_weights=\"lycoris\",  # LyCORIS initialization\n    decompose_both=True,\n    decompose_factor=4,  # Specific decomposition factor\n    rank_dropout=0.1,\n    module_dropout=0.05,\n    rank_dropout_scale=True,\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\nmodel = LoKrModel(model, config, \"default\")\n\nprint(\"LoKr model with LyCORIS initialization ready\")\n</syntaxhighlight>\n\n=== Example 5: LoKr for Vision Transformer ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import ViTForImageClassification\nfrom peft import LoKrModel, LoKrConfig\n\n# Configure LoKr for ViT\nconfig = LoKrConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"query\", \"key\", \"value\"],\n    rank_dropout=0.1,\n    module_dropout=0.05,\n    decompose_both=True,\n    # Only adapt certain layers\n    layers_to_transform=[0, 1, 2, 9, 10, 11],  # First and last few layers\n    layers_pattern=\"encoder.layer\",\n    init_weights=True,\n)\n\nmodel = ViTForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\",\n    num_labels=10\n)\nmodel = LoKrModel(model, config, \"default\")\n\n# Train on custom dataset\n# ... training loop ...\n</syntaxhighlight>\n\n=== Example 6: Multi-Task LoKr with Different Configs ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoKrModel, LoKrConfig\n\n# Create base LoKr model for task 1\nconfig1 = LoKrConfig(\n    r=8,\n    alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    decompose_both=False,\n)\n# Conceptual: First adapter\nmodel = LoKrModel(base_model, config1, adapter_name=\"task1\")\n\n# Configuration for task 2 with different settings\nconfig2 = LoKrConfig(\n    r=16,\n    alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\"],\n    decompose_both=True,  # More compression for task 2\n    decompose_factor=4,\n)\n\n# In practice, multi-adapter management is done through PeftModel\n# This shows the conceptual structure\nprint(\"Multi-task LoKr model configured\")\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Kronecker Product Decomposition ===\nLoKr decomposes weight matrices using Kronecker products:\n<syntaxhighlight lang=\"python\">\nW \u2248 A \u2297 B\n\nWhere:\n- W: Original weight matrix (m \u00d7 n)\n- A: First Kronecker factor (m1 \u00d7 n1)\n- B: Second Kronecker factor (m2 \u00d7 n2)\n- m = m1 * m2, n = n1 * n2\n</syntaxhighlight>\n\nWhen <code>decompose_both=True</code>:\n<syntaxhighlight lang=\"python\">\nA \u2248 U_A @ V_A.T  (low-rank decomposition)\nB \u2248 U_B @ V_B.T  (low-rank decomposition)\n\nW \u2248 (U_A @ V_A.T) \u2297 (U_B @ V_B.T)\n</syntaxhighlight>\n\nThis achieves extreme compression with minimal accuracy loss.\n\n=== Layer Mapping System ===\nLoKrModel uses a <code>layers_mapping</code> dictionary:\n<syntaxhighlight lang=\"python\">\nlayers_mapping = {\n    torch.nn.Conv2d: Conv2d,  # LoKr Conv2d layer\n    torch.nn.Conv1d: Conv1d,  # LoKr Conv1d layer\n    torch.nn.Linear: Linear,  # LoKr Linear layer\n}\n</syntaxhighlight>\n\nEach LoKr layer type implements the Kronecker product decomposition appropriate for its layer type.\n\n=== Adapter Creation Process ===\nThe <code>_create_and_replace</code> method:\n1. Extracts rank and alpha from patterns or uses defaults\n2. Adds <code>rank_dropout_scale</code> to kwargs\n3. Checks if target is already a LoKrLayer\n   - If yes: updates the layer with new adapter\n   - If no: creates new LoKrLayer wrapping the target\n4. Replaces the original module in the parent\n\n<syntaxhighlight lang=\"python\">\n# Simplified implementation:\nr_key = get_pattern_key(config.rank_pattern.keys(), current_key)\nalpha_key = get_pattern_key(config.alpha_pattern.keys(), current_key)\nkwargs = config.to_dict()\nkwargs[\"r\"] = config.rank_pattern.get(r_key, config.r)\nkwargs[\"alpha\"] = config.alpha_pattern.get(alpha_key, config.alpha)\nkwargs[\"rank_dropout_scale\"] = config.rank_dropout_scale\n\nif isinstance(target, LoKrLayer):\n    target.update_layer(adapter_name, **kwargs)\nelse:\n    new_module = self._create_new_module(config, adapter_name, target, **kwargs)\n    self._replace_module(parent, target_name, new_module, target)\n</syntaxhighlight>\n\n=== Parameter Efficiency Comparison ===\nFor a matrix of size m\u00d7n with rank r:\n* '''Standard LoRA:''' 2 * r * (m + n) parameters\n* '''LoHa:''' 4 * r * sqrt(m) * sqrt(n) parameters (assuming square-like)\n* '''LoKr (basic):''' 2 * sqrt(m*n) * (m1 + n1) parameters\n* '''LoKr (decompose_both):''' Even fewer parameters with nested decomposition\n\nLoKr often achieves 50-70% of LoRA's parameter count with similar performance.\n\n=== Target Module Resolution ===\nUses <code>TRANSFORMERS_MODELS_TO_LOKR_TARGET_MODULES_MAPPING</code> for automatic module identification across different architectures.\n\n=== Parameter Prefix ===\nAll LoKr parameters use the prefix \"lokr_\" for easy identification in state dicts.\n\n== Related Pages ==\n\n* [[huggingface_peft_LoKrConfig|LoKrConfig]] - Configuration for LoKr models\n* [[huggingface_peft_LoKrLayer|LoKrLayer]] - Base layer class for LoKr adapters\n* [[huggingface_peft_LycorisTuner|LycorisTuner]] - Base class for Lycoris-based tuners\n* [[huggingface_peft_LoHaModel|LoHaModel]] - Related model using Hadamard products\n* [[huggingface_peft_LoraModel|LoraModel]] - Standard LoRA implementation\n* [[huggingface_peft_get_peft_model|get_peft_model]] - Standard function to create PEFT models\n\n[[Category:PEFT]]\n[[Category:Model]]\n[[Category:LoKr]]\n[[Category:Kronecker Product]]\n[[Category:Low-Rank Adaptation]]\n[[Category:Parameter-Efficient Fine-Tuning]]\n[[Category:Diffusion Models]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LoraAQLM",
      "page_title": "LoRA AQLM Implementation",
      "page_type": "Implementation",
      "overview": "=== Description === The AQLM LoRA implementation provides support for applying Low-Rank Adaptation (LoRA) to AQLM (Additive Quantization of Language Models) quantized linear layers. AQLM is a quantization method that compresses model weights while maintaining performance. This module enables fine-tuning of AQLM-quantized models using LoRA adapters without requiring full weight updates. The implementation consists of two main components: * '''AqlmLoraLinear''': A specialized LoRA layer for AQLM quantized linear layers * '''dispatch_aqlm''': A dispatcher function that creates AQLM LoRA layers when appropriate Key features: * Compatible with AQLM quantized models * Prevents merging of adapters (not supported for quantized layers) * Automatic dtype casting for compatibility with quantized weights * Support for multiple active adapters * DoRA (Weight-Decomposed Low-Rank Adaptation) is not yet supported",
      "content": "= LoRA AQLM Implementation =\n\n== Knowledge Sources ==\n* '''Repository:''' [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Source File:''' src/peft/tuners/lora/aqlm.py\n\n== Domains ==\n* [[Natural Language Processing]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Quantization]]\n* [[Low-Rank Adaptation]]\n\n== Overview ==\n\n=== Description ===\nThe AQLM LoRA implementation provides support for applying Low-Rank Adaptation (LoRA) to AQLM (Additive Quantization of Language Models) quantized linear layers. AQLM is a quantization method that compresses model weights while maintaining performance. This module enables fine-tuning of AQLM-quantized models using LoRA adapters without requiring full weight updates.\n\nThe implementation consists of two main components:\n* '''AqlmLoraLinear''': A specialized LoRA layer for AQLM quantized linear layers\n* '''dispatch_aqlm''': A dispatcher function that creates AQLM LoRA layers when appropriate\n\nKey features:\n* Compatible with AQLM quantized models\n* Prevents merging of adapters (not supported for quantized layers)\n* Automatic dtype casting for compatibility with quantized weights\n* Support for multiple active adapters\n* DoRA (Weight-Decomposed Low-Rank Adaptation) is not yet supported\n\n=== Usage ===\nThis module is typically used internally by PEFT when applying LoRA to models that have been quantized with AQLM. The dispatcher function automatically detects AQLM quantized layers and applies the appropriate LoRA wrapper.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/lora/aqlm.py</code>\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\nclass AqlmLoraLinear(torch.nn.Module, LoraLayer):\n    def __init__(\n        self,\n        base_layer,\n        adapter_name: str,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        init_lora_weights: bool = True,\n        use_rslora: bool = False,\n        use_dora: bool = False,\n        lora_bias: bool = False,\n        **kwargs,\n    )\n</syntaxhighlight>\n\n=== Dispatcher Function ===\n<syntaxhighlight lang=\"python\">\ndef dispatch_aqlm(\n    target: torch.nn.Module,\n    adapter_name: str,\n    **kwargs: Any,\n) -> Optional[torch.nn.Module]\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.aqlm import AqlmLoraLinear, dispatch_aqlm\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== AqlmLoraLinear Constructor Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || torch.nn.Module || Required || The AQLM quantized base layer to wrap\n|-\n| adapter_name || str || Required || Name of the adapter to create\n|-\n| r || int || 0 || Rank of the LoRA decomposition\n|-\n| lora_alpha || int || 1 || Scaling factor for LoRA updates\n|-\n| lora_dropout || float || 0.0 || Dropout probability for LoRA layers\n|-\n| init_lora_weights || bool || True || Whether to initialize LoRA weights\n|-\n| use_rslora || bool || False || Whether to use rank-stabilized LoRA\n|-\n| use_dora || bool || False || Whether to use DoRA (raises error if True)\n|-\n| lora_bias || bool || False || Whether to include bias in LoRA layers\n|}\n\n=== Forward Method ===\n{| class=\"wikitable\"\n! Input !! Type !! Description\n|-\n| x || torch.Tensor || Input tensor to the layer\n|}\n\n{| class=\"wikitable\"\n! Output !! Type !! Description\n|-\n| result || torch.Tensor || Output tensor with LoRA adaptations applied\n|}\n\n=== dispatch_aqlm Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The module to potentially wrap\n|-\n| adapter_name || str || Name of the adapter\n|-\n| **kwargs || Any || Additional arguments passed to AqlmLoraLinear\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || AqlmLoraLinear if target is AQLM quantized, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Internal Dispatcher Usage ===\nThe dispatcher is typically called internally by PEFT's layer injection mechanism:\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.aqlm import dispatch_aqlm\nimport torch.nn as nn\n\n# Assuming model has AQLM quantized layers\ntarget_layer = model.layer  # An AQLM QuantizedLinear layer\n\n# Dispatcher automatically wraps if appropriate\nnew_layer = dispatch_aqlm(\n    target=target_layer,\n    adapter_name=\"default\",\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1\n)\n\nif new_layer is not None:\n    # Successfully created AQLM LoRA layer\n    model.layer = new_layer\n</syntaxhighlight>\n\n=== Using with PEFT LoraConfig ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LoraConfig\nfrom transformers import AutoModelForCausalLM\n\n# Load AQLM quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model_name\",\n    quantization_config={\"method\": \"aqlm\"}\n)\n\n# Configure LoRA - PEFT will automatically use AQLM dispatcher\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    use_dora=False  # DoRA not supported for AQLM\n)\n\n# PEFT automatically applies AqlmLoraLinear to AQLM layers\nmodel = get_peft_model(model, lora_config)\n</syntaxhighlight>\n\n=== Forward Pass Behavior ===\n<syntaxhighlight lang=\"python\">\nimport torch\n\n# Assuming aqlm_lora_layer is an instance of AqlmLoraLinear\ninput_tensor = torch.randn(batch_size, seq_len, hidden_size)\n\n# Forward pass applies base layer + LoRA adaptations\noutput = aqlm_lora_layer(input_tensor)\n\n# The forward method:\n# 1. Passes input through AQLM base layer\n# 2. For each active adapter:\n#    - Applies dropout\n#    - Passes through lora_A\n#    - Passes through lora_B\n#    - Scales by scaling factor\n#    - Adds to base output\n</syntaxhighlight>\n\n== Related Pages ==\n* [[LoRA Layer]]\n* [[PEFT Configuration]]\n* [[Quantization Methods]]\n* [[AQLM Quantization]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Model Compression]]\n* [[Low-Rank Adaptation]]\n* [[Rank-Stabilized LoRA]]\n\n== Notes ==\n* DoRA (Weight-Decomposed Low-Rank Adaptation) is not currently supported for AQLM layers\n* Merging of adapters is not supported for AQLM quantized layers\n* The implementation handles dtype conversion automatically to ensure compatibility with quantized weights\n* Multiple adapters can be active simultaneously\n* The forward pass does not support merged adapters due to quantization constraints\n\n== References ==\n* AQLM Library: https://github.com/Vahe1994/AQLM\n* LoRA Paper: https://arxiv.org/abs/2106.09685\n* PEFT Documentation: https://huggingface.co/docs/peft\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LoraConfig_for_qlora",
      "page_title": "huggingface peft LoraConfig for qlora",
      "page_type": "Implementation",
      "overview": "Concrete tool for creating LoraConfig optimized for QLoRA training on quantized models.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::File|config.py|src/peft/tuners/lora/config.py]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Quantization]], [[domain::Memory_Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for creating LoraConfig optimized for QLoRA training on quantized models.\n\n=== Description ===\n\nThis is the same `LoraConfig` class used for standard LoRA, but with parameters tuned for quantized training. The key recommendation is using `target_modules=\"all-linear\"` to adapt all linear layers, maximizing the benefit of QLoRA's memory efficiency.\n\n=== Usage ===\n\nCreate LoraConfig with QLoRA-optimized parameters before calling `get_peft_model()` on a quantized model.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''File:''' `src/peft/tuners/lora/config.py`\n* '''Lines:''' L47-300\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass LoraConfig(PeftConfig):\n    r: int = 8\n    lora_alpha: int = 8\n    target_modules: Optional[Union[list[str], str]] = None\n    lora_dropout: float = 0.0\n    bias: Literal[\"none\", \"all\", \"lora_only\"] = \"none\"\n    task_type: Optional[TaskType] = None\n    # ... additional parameters\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoraConfig, TaskType\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== QLoRA Config for Causal LM ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoraConfig, TaskType\n\nqlora_config = LoraConfig(\n    r=16,  # Slightly higher rank for QLoRA\n    lora_alpha=32,\n    target_modules=\"all-linear\",  # Adapt all linear layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n</syntaxhighlight>\n\n=== QLoRA with Specific Modules ===\n<syntaxhighlight lang=\"python\">\nqlora_config = LoraConfig(\n    r=64,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.1,\n    task_type=TaskType.CAUSAL_LM,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_QLoRA_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Fine Tuning",
        "Quantization",
        "Memory Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "File",
          "title": "config.py",
          "url": "src/peft/tuners/lora/config.py"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_QLoRA_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_LoraConfig_init",
      "page_title": "huggingface peft LoraConfig init",
      "page_type": "Implementation",
      "overview": "Concrete tool for configuring Low-Rank Adaptation parameters for standard LoRA fine-tuning workflows.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft/conceptual_guides/lora]]\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Fine_Tuning]], [[domain::Configuration]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for configuring Low-Rank Adaptation parameters for standard LoRA fine-tuning workflows.\n\n=== Description ===\n\n`LoraConfig` is a dataclass that defines all hyperparameters for LoRA adaptation. It specifies which modules to adapt, the rank of the low-rank matrices, scaling factors, and initialization strategies. This configuration is passed to `get_peft_model()` to inject adapter layers into the base model.\n\n=== Usage ===\n\nUse this after loading your base model and before calling `get_peft_model()`. Configure `r` (rank) based on your task complexity - lower ranks (4-8) for simple tasks, higher (16-64) for complex ones. Use `target_modules=\"all-linear\"` to adapt all linear layers, or specify explicit module names for fine-grained control.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/tuners/lora/config.py\n* '''Lines:''' L321-879\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass LoraConfig(PeftConfig):\n    \"\"\"\n    Configuration class for LoRA adaptation.\n\n    Args:\n        r: LoRA attention dimension (rank). Default: 8\n        lora_alpha: Scaling factor for LoRA. Default: 8\n        target_modules: Modules to apply LoRA. Can be list or \"all-linear\"\n        lora_dropout: Dropout probability for LoRA layers. Default: 0.0\n        bias: Bias type - \"none\", \"all\", or \"lora_only\". Default: \"none\"\n        task_type: Task type for model (CAUSAL_LM, SEQ_CLS, etc.)\n        use_rslora: Use Rank-Stabilized LoRA scaling. Default: False\n        use_dora: Enable Weight-Decomposed LoRA (DoRA). Default: False\n        init_lora_weights: Weight initialization strategy. Default: True\n        modules_to_save: Additional modules to train. Default: None\n    \"\"\"\n    r: int = field(default=8, metadata={\"help\": \"Lora attention dimension\"})\n    target_modules: Optional[Union[list[str], str]] = field(default=None)\n    lora_alpha: int = field(default=8, metadata={\"help\": \"Lora alpha\"})\n    lora_dropout: float = field(default=0.0, metadata={\"help\": \"Lora dropout\"})\n    fan_in_fan_out: bool = field(default=False)\n    bias: Literal[\"none\", \"all\", \"lora_only\"] = field(default=\"none\")\n    use_rslora: bool = field(default=False)\n    modules_to_save: Optional[list[str]] = field(default=None)\n    init_lora_weights: bool | Literal[\"gaussian\", \"eva\", \"olora\", \"pissa\", \"corda\", \"loftq\"] = field(default=True)\n    use_dora: bool = field(default=False)\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoraConfig, TaskType\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| r || int || No || LoRA rank dimension. Higher = more capacity but more parameters. Default: 8\n|-\n| lora_alpha || int || No || Scaling factor. Effective scale is alpha/r. Default: 8\n|-\n| target_modules || list[str] or str || No || Modules to adapt. \"all-linear\" for all linear layers, or explicit names\n|-\n| lora_dropout || float || No || Dropout probability (0.0-0.1 typical). Default: 0.0\n|-\n| bias || str || No || Bias handling: \"none\", \"all\", or \"lora_only\". Default: \"none\"\n|-\n| task_type || TaskType || No || Model task type for proper output layer handling\n|-\n| use_rslora || bool || No || Enable Rank-Stabilized LoRA (better at low ranks). Default: False\n|-\n| use_dora || bool || No || Enable DoRA (weight decomposition). Default: False\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| config || LoraConfig || Configuration object ready for get_peft_model()\n|}\n\n== Usage Examples ==\n\n=== Standard LoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoraConfig, TaskType\n\n# Standard LoRA config for causal LM fine-tuning\nconfig = LoraConfig(\n    r=16,                          # Rank dimension\n    lora_alpha=32,                 # Scaling factor (alpha/r = 2)\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Attention layers\n    lora_dropout=0.05,             # Small dropout for regularization\n    bias=\"none\",                   # Don't train biases\n    task_type=TaskType.CAUSAL_LM,  # For decoder-only models\n)\n</syntaxhighlight>\n\n=== All-Linear with DoRA ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoraConfig, TaskType\n\n# DoRA config targeting all linear layers\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=\"all-linear\",   # Adapt all linear layers\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n    use_dora=True,                 # Enable DoRA for better performance\n)\n</syntaxhighlight>\n\n=== With Rank-Stabilized LoRA ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoraConfig, TaskType\n\n# RSLoRA for low-rank configurations\nconfig = LoraConfig(\n    r=4,                           # Very low rank\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    use_rslora=True,               # Rank-stabilized scaling: alpha/sqrt(r)\n    task_type=TaskType.CAUSAL_LM,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_LoRA_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n\n=== Uses Heuristic ===\n* [[uses_heuristic::Heuristic:huggingface_peft_LoRA_Rank_Selection]]\n",
      "domains": [
        "NLP",
        "Fine Tuning",
        "Configuration"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft/conceptual_guides/lora"
        },
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_LoRA_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "huggingface_peft_LoRA_Rank_Selection"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_LoraIntelFP8",
      "page_title": "LoRA Intel FP8 Implementation",
      "page_type": "Implementation",
      "overview": "=== Description === The Intel FP8 LoRA implementation provides support for applying Low-Rank Adaptation (LoRA) to models quantized with Intel Neural Compressor (INC) FP8 quantization. This module enables parameter-efficient fine-tuning of FP8-quantized models while maintaining the memory and computational benefits of quantization. The implementation consists of two main components: * '''IncLoraLinear''': A specialized LoRA layer that extends the standard Linear LoRA layer for INC FP8 quantized layers * '''dispatch_inc''': A dispatcher function that creates INC LoRA layers when appropriate Key features: * Compatible with Intel Neural Compressor FP8 quantization * Inherits standard LoRA Linear functionality * Explicitly disables merge/unmerge operations (not yet implemented for INC layers) * Optimized for Intel Habana hardware * Tested through the Optimum-Habana test suite Limitations: * Merging adapters into base weights is not yet implemented * Unmerging adapters from base weights is not yet implemented",
      "content": "= LoRA Intel FP8 Implementation =\n\n== Knowledge Sources ==\n* '''Repository:''' [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Source File:''' src/peft/tuners/lora/inc.py\n* '''Tests:''' [https://github.com/huggingface/optimum-habana Optimum-Habana Repository]\n\n== Domains ==\n* [[Natural Language Processing]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Quantization]]\n* [[Low-Rank Adaptation]]\n* [[Intel Neural Compressor]]\n\n== Overview ==\n\n=== Description ===\nThe Intel FP8 LoRA implementation provides support for applying Low-Rank Adaptation (LoRA) to models quantized with Intel Neural Compressor (INC) FP8 quantization. This module enables parameter-efficient fine-tuning of FP8-quantized models while maintaining the memory and computational benefits of quantization.\n\nThe implementation consists of two main components:\n* '''IncLoraLinear''': A specialized LoRA layer that extends the standard Linear LoRA layer for INC FP8 quantized layers\n* '''dispatch_inc''': A dispatcher function that creates INC LoRA layers when appropriate\n\nKey features:\n* Compatible with Intel Neural Compressor FP8 quantization\n* Inherits standard LoRA Linear functionality\n* Explicitly disables merge/unmerge operations (not yet implemented for INC layers)\n* Optimized for Intel Habana hardware\n* Tested through the Optimum-Habana test suite\n\nLimitations:\n* Merging adapters into base weights is not yet implemented\n* Unmerging adapters from base weights is not yet implemented\n\n=== Usage ===\nThis module is automatically used by PEFT when applying LoRA to models that have been quantized with Intel Neural Compressor's FP8 quantization method. The dispatcher function detects INC PatchedLinear layers and applies the appropriate LoRA wrapper.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/lora/inc.py</code>\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\nclass IncLoraLinear(Linear):\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        **kwargs,\n    )\n</syntaxhighlight>\n\n=== Dispatcher Function ===\n<syntaxhighlight lang=\"python\">\ndef dispatch_inc(\n    target: torch.nn.Module,\n    adapter_name: str,\n    **kwargs\n) -> Optional[torch.nn.Module]\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.inc import IncLoraLinear, dispatch_inc\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== IncLoraLinear Constructor Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || torch.nn.Module || Required || The INC FP8 quantized base layer to wrap\n|-\n| adapter_name || str || Required || Name of the adapter to create\n|-\n| **kwargs || Any || - || Additional arguments passed to parent Linear class (r, lora_alpha, lora_dropout, etc.)\n|}\n\n=== merge Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| safe_merge || bool || False || If True, checks for NaNs before merging\n|-\n| adapter_names || Optional[list[str]] || None || List of adapter names to merge\n|}\n\n'''Behavior:''' Raises NotImplementedError - merging not yet supported for INC layers.\n\n=== unmerge Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| (none) || - || - || No parameters\n|}\n\n'''Behavior:''' Raises NotImplementedError - unmerging not yet supported for INC layers.\n\n=== dispatch_inc Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The module to potentially wrap\n|-\n| adapter_name || str || Name of the adapter\n|-\n| **kwargs || Any || Additional arguments passed to IncLoraLinear\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || IncLoraLinear if target is INC PatchedLinear, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Using with PEFT and INC Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LoraConfig\nfrom transformers import AutoModelForCausalLM\nfrom neural_compressor.torch import quantize\n\n# Load and quantize model with Intel Neural Compressor\nmodel = AutoModelForCausalLM.from_pretrained(\"model_name\")\n\n# Apply FP8 quantization using INC\n# (Example - actual quantization config may vary)\nquantized_model = quantize(model, quant_config=fp8_config)\n\n# Configure LoRA - PEFT will automatically use INC dispatcher\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# PEFT automatically applies IncLoraLinear to INC layers\npeft_model = get_peft_model(quantized_model, lora_config)\n\n# Train the model with LoRA adapters\n# Only LoRA parameters will be updated\n</syntaxhighlight>\n\n=== Internal Dispatcher Usage ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.inc import dispatch_inc\nfrom neural_compressor.torch.algorithms.fp8_quant._quant_common.helper_modules import PatchedLinear\n\n# Assuming target_layer is an INC PatchedLinear layer\ntarget_layer = model.some_layer  # PatchedLinear instance\n\n# Dispatcher automatically wraps if appropriate\nnew_layer = dispatch_inc(\n    target=target_layer,\n    adapter_name=\"default\",\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1\n)\n\nif new_layer is not None:\n    # Successfully created INC LoRA layer\n    model.some_layer = new_layer\n</syntaxhighlight>\n\n=== Attempting Merge (Will Fail) ===\n<syntaxhighlight lang=\"python\">\n# Note: This will raise NotImplementedError\n\n# Assuming inc_lora_layer is an instance of IncLoraLinear\ntry:\n    inc_lora_layer.merge(safe_merge=False, adapter_names=[\"default\"])\nexcept NotImplementedError as e:\n    print(f\"Merge not supported: {e}\")\n    # Output: \"Merging LoRA with INC layers is not yet implemented\"\n\ntry:\n    inc_lora_layer.unmerge()\nexcept NotImplementedError as e:\n    print(f\"Unmerge not supported: {e}\")\n    # Output: \"Unmerging LoRA from INC layers is not yet implemented\"\n</syntaxhighlight>\n\n=== Training with INC LoRA ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import TrainingArguments, Trainer\n\n# Model with INC LoRA applied\npeft_model = get_peft_model(quantized_model, lora_config)\n\n# Standard training setup\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=3,\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\n# Train only the LoRA parameters\ntrainer.train()\n\n# Save only the LoRA adapters (not the full model)\npeft_model.save_pretrained(\"./lora_adapters\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[LoRA Layer]]\n* [[LoRA Linear]]\n* [[PEFT Configuration]]\n* [[Intel Neural Compressor]]\n* [[FP8 Quantization]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Model Quantization]]\n* [[Low-Rank Adaptation]]\n* [[Optimum Habana]]\n\n== Notes ==\n* PEFT tests for INC are maintained in the Optimum-Habana repository\n* LLM tests: https://github.com/huggingface/optimum-habana/blob/main/tests/test_peft_inference.py\n* Diffusers tests: https://github.com/huggingface/optimum-habana/blob/main/tests/test_diffusers.py\n* The implementation inherits all standard LoRA Linear functionality except merge/unmerge\n* Merge and unmerge operations will be implemented in future versions\n* This implementation is optimized for Intel Habana accelerators\n* The IncLoraLinear class extends the standard Linear LoRA layer with INC-specific handling\n\n== References ==\n* Intel Neural Compressor: https://github.com/intel/neural-compressor\n* Optimum Habana: https://github.com/huggingface/optimum-habana\n* LoRA Paper: https://arxiv.org/abs/2106.09685\n* PEFT Documentation: https://huggingface.co/docs/peft\n* FP8 Quantization Overview: https://arxiv.org/abs/2209.05433\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LoraParallelLinear",
      "page_title": "huggingface peft LoraParallelLinear",
      "page_type": "Implementation",
      "overview": "LoRA layer implementation for Megatron-LM tensor parallel linear layers, handling row and column parallelism with appropriate LoRA matrix splitting.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|Megatron-LM|https://github.com/NVIDIA/Megatron-LM]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Tensor_Parallelism]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nLoRA layer implementation for Megatron-LM tensor parallel linear layers, handling row and column parallelism with appropriate LoRA matrix splitting.\n\n=== Description ===\n\nLoraParallelLinear implements LoRA for Megatron-LM's tensor parallel layers. For RowParallelLinear (where inputs are split across devices), lora_A is implemented as a parallel row layer while lora_B remains a standard linear layer. For ColumnParallelLinear (where outputs are split), lora_A is standard while lora_B is a parallel column layer. This ensures input/output shapes remain consistent with the base parallel layer while adding low-rank adaptation.\n\n=== Usage ===\n\nUse LoraParallelLinear when fine-tuning models that use Megatron-LM tensor parallelism. The layer is automatically dispatched via `dispatch_megatron` when target layers are `RowParallelLinear` or `ColumnParallelLinear`. Requires Megatron configuration for parallel settings.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/tp_layer.py src/peft/tuners/lora/tp_layer.py]\n* '''Lines:''' 1-351\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass LoraParallelLinear(nn.Module, LoraLayer):\n    \"\"\"\n    LoRA for Megatron tensor parallel linear layers.\n\n    For RowParallelLinear: lora_A is row-parallel, lora_B is standard\n    For ColumnParallelLinear: lora_A is standard, lora_B is column-parallel\n    \"\"\"\n\n    def __init__(\n        self,\n        base_layer,\n        adapter_name: str,\n        backend,  # megatron_core.tensor_parallel\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        init_lora_weights: Union[bool, str] = True,\n        use_rslora: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize with Megatron backend and config.\"\"\"\n\n    def update_layer(\n        self,\n        adapter_name,\n        r,\n        lora_alpha,\n        lora_dropout,\n        init_lora_weights,\n        use_rslora,\n        init_method,\n        input_is_parallel,\n        gather_output,\n        **parallel_linear_kwargs,\n    ):\n        \"\"\"Create parallel LoRA layers based on base layer type.\"\"\"\n\n    def forward(self, x: torch.Tensor, *args, **kwargs):\n        \"\"\"Forward with parallel LoRA computation.\"\"\"\n\ndef dispatch_megatron(\n    target: torch.nn.Module,\n    adapter_name: str,\n    lora_config,\n    **kwargs,\n) -> Optional[torch.nn.Module]:\n    \"\"\"Dispatch LoRA for Megatron parallel layers.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.tp_layer import LoraParallelLinear, dispatch_megatron\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || Megatron RowParallelLinear or ColumnParallelLinear\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| backend || module || Yes || megatron_core.tensor_parallel module\n|-\n| megatron_config || dict/TransformerConfig || Yes || Megatron configuration\n|-\n| r || int || Yes || LoRA rank\n|-\n| lora_alpha || int || No || Scaling factor\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || tuple[Tensor, Tensor] || (result, bias) matching Megatron signature\n|-\n| get_delta_weight() || torch.Tensor || LoRA delta weight B @ A * scaling\n|}\n\n== Usage Examples ==\n\n=== LoRA with Megatron Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import LoraConfig, get_peft_model\n\n# Configure LoRA with Megatron settings\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"query_key_value\", \"dense\"],\n    megatron_config={\n        \"tensor_model_parallel_size\": 4,\n        \"sequence_parallel\": True,\n        # ... other Megatron config\n    },\n    megatron_core=\"megatron.core\",  # Import path\n)\n\n# Apply to Megatron model\nmodel = get_peft_model(megatron_model, config)\n</syntaxhighlight>\n\n=== Manual Dispatch ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.tp_layer import dispatch_megatron\n\n# Dispatch automatically checks layer type\nnew_module = dispatch_megatron(\n    target=target_layer,\n    adapter_name=\"default\",\n    lora_config=lora_config,\n    megatron_config=megatron_config,\n)\n\nif new_module is not None:\n    # Replace target with LoRA-wrapped module\n    parent.target_attr = new_module\n</syntaxhighlight>\n\n=== Forward Pass Handling ===\n<syntaxhighlight lang=\"python\">\n# LoraParallelLinear returns (result, bias) tuple\n# matching Megatron's parallel layer signature\nresult, bias = lora_parallel_layer(x)\n\n# For RowParallelLinear:\n#   lora_A splits input across devices\n#   lora_B gathers output\n# For ColumnParallelLinear:\n#   lora_A operates on full input\n#   lora_B splits output across devices\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Tensor Parallelism"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "Megatron-LM",
          "url": "https://github.com/NVIDIA/Megatron-LM"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_LoraTorchAO",
      "page_title": "LoRA TorchAO Implementation",
      "page_type": "Implementation",
      "overview": "=== Description === The TorchAO LoRA implementation provides support for applying Low-Rank Adaptation (LoRA) to models quantized with PyTorch's torchao quantization library. TorchAO provides efficient quantization methods for PyTorch models, and this module enables parameter-efficient fine-tuning while maintaining the benefits of quantization. The implementation consists of two main components: * '''TorchaoLoraLinear''': A specialized LoRA layer for torchao quantized linear layers * '''dispatch_torchao''': A dispatcher function that creates TorchAO LoRA layers when appropriate Key features: * Compatible with torchao AffineQuantizedTensor and LinearActivationQuantizedTensor * Support for merge and unmerge operations with automatic dequantization/requantization * Safe merge option with NaN detection * Currently supports int8 weights (int4 support planned) * Automatic dtype checking and validation * LoRA bias is not yet supported The implementation handles the complexity of working with quantized weights by: 1. Dequantizing weights when merging/unmerging 2. Applying LoRA delta weights 3. Requantizing back to the original quantization format",
      "content": "= LoRA TorchAO Implementation =\n\n== Knowledge Sources ==\n* '''Repository:''' [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Source File:''' src/peft/tuners/lora/torchao.py\n\n== Domains ==\n* [[Natural Language Processing]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Quantization]]\n* [[Low-Rank Adaptation]]\n* [[PyTorch]]\n\n== Overview ==\n\n=== Description ===\nThe TorchAO LoRA implementation provides support for applying Low-Rank Adaptation (LoRA) to models quantized with PyTorch's torchao quantization library. TorchAO provides efficient quantization methods for PyTorch models, and this module enables parameter-efficient fine-tuning while maintaining the benefits of quantization.\n\nThe implementation consists of two main components:\n* '''TorchaoLoraLinear''': A specialized LoRA layer for torchao quantized linear layers\n* '''dispatch_torchao''': A dispatcher function that creates TorchAO LoRA layers when appropriate\n\nKey features:\n* Compatible with torchao AffineQuantizedTensor and LinearActivationQuantizedTensor\n* Support for merge and unmerge operations with automatic dequantization/requantization\n* Safe merge option with NaN detection\n* Currently supports int8 weights (int4 support planned)\n* Automatic dtype checking and validation\n* LoRA bias is not yet supported\n\nThe implementation handles the complexity of working with quantized weights by:\n1. Dequantizing weights when merging/unmerging\n2. Applying LoRA delta weights\n3. Requantizing back to the original quantization format\n\n=== Usage ===\nThis module is automatically used by PEFT when applying LoRA to models that have been quantized with torchao. The dispatcher function detects torchao quantized tensors and applies the appropriate LoRA wrapper.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/lora/torchao.py</code>\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\nclass TorchaoLoraLinear(Linear):\n    def __init__(\n        self,\n        *args,\n        get_apply_tensor_subclass,\n        **kwargs\n    )\n</syntaxhighlight>\n\n=== Dispatcher Function ===\n<syntaxhighlight lang=\"python\">\ndef dispatch_torchao(\n    target: torch.nn.Module,\n    adapter_name: str,\n    lora_config: LoraConfig,\n    **kwargs: Any,\n) -> Optional[torch.nn.Module]\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.torchao import TorchaoLoraLinear, dispatch_torchao\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== TorchaoLoraLinear Constructor Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| *args || Any || - || Positional arguments passed to parent Linear class\n|-\n| get_apply_tensor_subclass || callable || Required || Function to get the quantization subclass for requantization\n|-\n| **kwargs || Any || - || Keyword arguments passed to parent Linear class (r, lora_alpha, lora_dropout, etc.)\n|}\n\n'''Note:''' Raises ValueError if lora_bias is set to True.\n\n=== merge Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| safe_merge || bool || False || If True, checks for NaNs before merging\n|-\n| adapter_names || Optional[list[str]] || None || List of adapter names to merge; None merges all active adapters\n|}\n\n'''Behavior:'''\n* Dequantizes base weights\n* Adds LoRA delta weights\n* Checks for NaNs if safe_merge=True\n* Requantizes weights using torchao\n* Raises NotImplementedError if dequantization is not supported\n\n=== unmerge Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| (none) || - || - || No parameters\n|}\n\n'''Behavior:'''\n* Dequantizes base weights\n* Subtracts LoRA delta weights\n* Requantizes weights using torchao\n* Raises NotImplementedError if dequantization is not supported\n\n=== dispatch_torchao Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The module to potentially wrap\n|-\n| adapter_name || str || Name of the adapter\n|-\n| lora_config || LoraConfig || LoRA configuration object\n|-\n| **kwargs || Any || Additional arguments passed to TorchaoLoraLinear\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || TorchaoLoraLinear if target uses torchao quantization, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Using with PEFT and TorchAO Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LoraConfig\nfrom transformers import AutoModelForCausalLM\nfrom torchao.quantization import quantize_, int8_weight_only\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\"model_name\")\n\n# Apply int8 quantization using torchao\nquantize_(model, int8_weight_only())\n\n# Configure LoRA - PEFT will automatically use TorchAO dispatcher\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",  # lora_bias not supported yet\n    task_type=\"CAUSAL_LM\"\n)\n\n# PEFT automatically applies TorchaoLoraLinear to quantized layers\npeft_model = get_peft_model(model, lora_config)\n\n# Train with LoRA adapters\ntrainer.train(peft_model)\n</syntaxhighlight>\n\n=== Merging and Unmerging Adapters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n\n# Load model with LoRA adapters\npeft_model = PeftModel.from_pretrained(base_model, \"path/to/adapters\")\n\n# Merge adapters into base weights (with safety check)\npeft_model.merge_adapter(safe_merge=True)\n\n# Model now runs without adapter overhead\noutput = peft_model(input_ids)\n\n# Unmerge to restore original weights + adapters\npeft_model.unmerge_adapter()\n</syntaxhighlight>\n\n=== Safe Merge with NaN Detection ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import get_peft_model, LoraConfig\n\n# Setup model with TorchAO quantization and LoRA\n# ... (quantization and PEFT setup)\n\n# Attempt safe merge - will check for NaNs\ntry:\n    peft_model.merge_adapter(safe_merge=True, adapter_names=[\"default\"])\n    print(\"Merge successful!\")\nexcept ValueError as e:\n    print(f\"Merge failed due to NaNs: {e}\")\n    # The adapter may be broken, investigate training process\n</syntaxhighlight>\n\n=== Internal Dispatcher Usage ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lora.torchao import dispatch_torchao\nfrom peft.tuners.lora.config import LoraConfig\nfrom torchao.dtypes import AffineQuantizedTensor\n\n# Check if layer uses torchao quantization\ntarget_layer = model.some_layer\n\n# Create LoRA config\nlora_config = LoraConfig(r=8, lora_alpha=16)\n\n# Dispatcher automatically wraps if appropriate\nnew_layer = dispatch_torchao(\n    target=target_layer,\n    adapter_name=\"default\",\n    lora_config=lora_config,\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1\n)\n\nif new_layer is not None:\n    # Successfully created TorchAO LoRA layer\n    model.some_layer = new_layer\n    print(\"Applied TorchAO LoRA wrapper\")\n</syntaxhighlight>\n\n=== Handling Unsupported Quantization Types ===\n<syntaxhighlight lang=\"python\">\nfrom torchao.quantization import int4_weight_only\n\n# Currently only int8 is fully supported\ntry:\n    quantize_(model, int4_weight_only())\n    peft_model = get_peft_model(model, lora_config)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Output: \"TorchaoLoraLinear only supports int8 weights for now.\"\n\n# Use int8 instead\nquantize_(model, int8_weight_only())\npeft_model = get_peft_model(model, lora_config)  # Works!\n</syntaxhighlight>\n\n=== Checking Quantization Status ===\n<syntaxhighlight lang=\"python\">\n# The implementation checks dtype automatically\nfrom torchao.dtypes import AffineQuantizedTensor\n\n# Check if a layer is quantized with torchao\nlayer = model.layer\n\nif isinstance(layer.weight, AffineQuantizedTensor):\n    print(\"Layer is quantized with torchao\")\n\n    # Check dtype support (int8 vs int4)\n    if hasattr(layer.weight, 'tensor_impl'):\n        # torchao 0.7.0+\n        dtype = layer.weight.tensor_impl.data.dtype\n    else:\n        # torchao < 0.7.0\n        dtype = layer.weight.layout_tensor.data.dtype\n\n    print(f\"Quantization dtype: {dtype}\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[LoRA Layer]]\n* [[LoRA Linear]]\n* [[PEFT Configuration]]\n* [[TorchAO Quantization]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Model Quantization]]\n* [[Low-Rank Adaptation]]\n* [[Int8 Quantization]]\n* [[Weight Quantization]]\n\n== Notes ==\n* '''Supported Quantization Types:''' Currently only int8 weights are fully supported; int4 support is planned\n* '''LoRA Bias:''' Not yet supported - will raise ValueError if enabled\n* '''Merge/Unmerge:''' Fully implemented with automatic dequantization/requantization\n* '''Safe Merge:''' Optional NaN detection to catch broken adapters\n* '''TorchAO Versions:''' Code handles both torchao 0.7.0+ and earlier versions\n* '''Memory Efficiency:''' Merge operation requires temporary dequantization, using additional memory\n* '''Quantization Preservation:''' After merge/unmerge, weights remain in quantized format\n\n== Implementation Details ==\n\n=== Dtype Checking ===\nThe implementation checks for int8 dtype support in the constructor:\n* For torchao >= 0.7.0: Checks <code>weight.tensor_impl.data.dtype</code>\n* For torchao < 0.7.0: Checks <code>weight.layout_tensor.data.dtype</code>\n\n=== Merge/Unmerge Process ===\n1. Dequantize base weights using <code>weight.dequantize()</code>\n2. Apply delta weights (add for merge, subtract for unmerge)\n3. Delete old quantized weight\n4. Assign new float weight\n5. Requantize using <code>quantize_(base_layer, get_apply_tensor_subclass())</code>\n\n=== Error Handling ===\n* Raises <code>NotImplementedError</code> if dequantization is not supported\n* Raises <code>ValueError</code> if safe_merge detects NaNs\n* Raises <code>ValueError</code> if lora_bias is enabled\n* Raises <code>ValueError</code> if dtype is not int8\n\n== References ==\n* TorchAO Repository: https://github.com/pytorch/ao\n* LoRA Paper: https://arxiv.org/abs/2106.09685\n* PEFT Documentation: https://huggingface.co/docs/peft\n* PyTorch Quantization: https://pytorch.org/docs/stable/quantization.html\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_LyCORISUtils",
      "page_title": "LyCORIS Utilities",
      "page_type": "Implementation",
      "overview": "=== Description === The LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) utilities module provides base classes and utilities for implementing LyCORIS-style adapters in PEFT. LyCORIS extends the LoRA concept with additional techniques for parameter-efficient fine-tuning. This module contains foundational components for LyCORIS-style adapters: * '''LycorisConfig''': Configuration dataclass for LyCORIS adapters with rank and alpha patterns * '''LycorisLayer''': Base layer class for LyCORIS-style adapter implementations * '''LycorisTuner''': Base tuner class for creating and managing LyCORIS adapters Key features: * Pattern-based rank and alpha configuration for fine-grained control * Support for rank and module dropout * Merge and unmerge capabilities * Scale operations for adapter weights * Safe merge with NaN detection * Abstract interface for implementing custom LyCORIS variants The module provides a flexible framework that can be extended to implement various LyCORIS methods such as LoHa, LoKr, and other adaptations.",
      "content": "= LyCORIS Utilities =\n\n== Knowledge Sources ==\n* '''Repository:''' [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Source File:''' src/peft/tuners/lycoris_utils.py\n\n== Domains ==\n* [[Natural Language Processing]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Low-Rank Adaptation]]\n* [[Model Tuning]]\n\n== Overview ==\n\n=== Description ===\nThe LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) utilities module provides base classes and utilities for implementing LyCORIS-style adapters in PEFT. LyCORIS extends the LoRA concept with additional techniques for parameter-efficient fine-tuning.\n\nThis module contains foundational components for LyCORIS-style adapters:\n* '''LycorisConfig''': Configuration dataclass for LyCORIS adapters with rank and alpha patterns\n* '''LycorisLayer''': Base layer class for LyCORIS-style adapter implementations\n* '''LycorisTuner''': Base tuner class for creating and managing LyCORIS adapters\n\nKey features:\n* Pattern-based rank and alpha configuration for fine-grained control\n* Support for rank and module dropout\n* Merge and unmerge capabilities\n* Scale operations for adapter weights\n* Safe merge with NaN detection\n* Abstract interface for implementing custom LyCORIS variants\n\nThe module provides a flexible framework that can be extended to implement various LyCORIS methods such as LoHa, LoKr, and other adaptations.\n\n=== Usage ===\nThis module is primarily used as a base for implementing specific LyCORIS adapter types. It is not typically instantiated directly but rather extended by concrete implementations like LoHa or LoKr.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/lycoris_utils.py</code>\n\n=== LycorisConfig Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass LycorisConfig(PeftConfig):\n    rank_pattern: Optional[dict] = field(default_factory=dict)\n    alpha_pattern: Optional[dict] = field(default_factory=dict)\n</syntaxhighlight>\n\n=== LycorisLayer Signature ===\n<syntaxhighlight lang=\"python\">\nclass LycorisLayer(BaseTunerLayer):\n    other_param_names = (\"r\", \"alpha\", \"scaling\", \"rank_dropout\", \"module_dropout\")\n\n    def __init__(self, base_layer: nn.Module) -> None\n</syntaxhighlight>\n\n=== LycorisTuner Signature ===\n<syntaxhighlight lang=\"python\">\nclass LycorisTuner(BaseTuner):\n    prefix: str\n    tuner_layer_cls = LycorisLayer\n    layers_mapping: dict[type[torch.nn.Module], type[LycorisLayer]]\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lycoris_utils import LycorisConfig, LycorisLayer, LycorisTuner\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== LycorisConfig Fields ===\n{| class=\"wikitable\"\n! Field !! Type !! Default !! Description\n|-\n| rank_pattern || Optional[dict] || {} || Mapping from layer names/regex to ranks different from default\n|-\n| alpha_pattern || Optional[dict] || {} || Mapping from layer names/regex to alphas different from default\n|}\n\n=== LycorisLayer Constructor ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| base_layer || nn.Module || The base layer to wrap with LyCORIS adapter\n|}\n\n=== LycorisLayer Key Methods ===\n\n==== merge ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| safe_merge || bool || False || Check for NaNs before merging\n|-\n| adapter_names || Optional[list[str]] || None || Adapters to merge; None merges all active\n|}\n\n==== unmerge ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| (none) || - || - || Unmerges all merged adapters\n|}\n\n==== set_scale ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| adapter || str || Name of the adapter\n|-\n| scale || float || Scale factor to apply\n|}\n\n==== scale_layer ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| scale || float || Scale factor to multiply existing scaling\n|}\n\n==== unscale_layer ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| scale || Optional[float] || None || Scale to divide by; None resets to default\n|}\n\n=== LycorisTuner Methods ===\n\n==== _create_new_module (classmethod) ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| config || LycorisConfig || Configuration for the adapter\n|-\n| adapter_name || str || Name of the adapter\n|-\n| target || nn.Module || Target module to adapt\n|-\n| **kwargs || Any || Additional arguments\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || LycorisLayer || New LyCORIS layer wrapping the target\n|}\n\n=== Abstract Methods (Must be Implemented by Subclasses) ===\n\n==== LycorisLayer Abstract Methods ====\n* <code>_available_adapters</code> (property): Returns set of available adapter names\n* <code>create_adapter_parameters(adapter_name, r, **kwargs)</code>: Creates adapter parameter tensors\n* <code>_get_delta_activations(adapter_name, x, *args, **kwargs)</code>: Computes activations to add to base output\n* <code>get_delta_weight(adapter_name)</code>: Computes weight delta for merging\n* <code>reset_adapter_parameters(adapter_name)</code>: Resets adapter parameters\n* <code>update_layer(adapter_name, r, alpha, **kwargs)</code>: Updates or creates adapter in layer\n\n==== LycorisTuner Abstract Methods ====\n* <code>_create_and_replace(config, adapter_name, target, target_name, parent, current_key)</code>: Creates and replaces module with LyCORIS version\n\n== Usage Examples ==\n\n=== Defining Custom LyCORIS Config ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lycoris_utils import LycorisConfig\nfrom peft.utils import PeftType\n\n@dataclass\nclass CustomLycorisConfig(LycorisConfig):\n    \"\"\"Custom LyCORIS configuration\"\"\"\n    custom_param: float = field(default=1.0, metadata={\"help\": \"Custom parameter\"})\n\n    def __post_init__(self):\n        super().__post_init__()\n        self.peft_type = PeftType.CUSTOM_LYCORIS\n\n# Use with pattern-based configuration\nconfig = CustomLycorisConfig(\n    r=8,\n    alpha=16,\n    rank_pattern={\n        \"^model.decoder.layers.0.encoder_attn.k_proj\": 16,\n        \"^model.decoder.layers.0.encoder_attn.q_proj\": 16,\n    },\n    alpha_pattern={\n        \"^model.decoder.layers.0\": 32,\n    },\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n</syntaxhighlight>\n\n=== Implementing Custom LyCORIS Layer ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lycoris_utils import LycorisLayer\nimport torch.nn as nn\nimport torch\n\nclass CustomLycorisLinear(LycorisLayer):\n    adapter_layer_names = (\"custom_A\", \"custom_B\")\n\n    def __init__(self, base_layer: nn.Module, adapter_name: str, r: int = 8,\n                 alpha: float = 16, **kwargs):\n        super().__init__(base_layer)\n        self.custom_A = nn.ModuleDict({})\n        self.custom_B = nn.ModuleDict({})\n        self.update_layer(adapter_name, r, alpha, **kwargs)\n\n    @property\n    def _available_adapters(self) -> set[str]:\n        return set(self.custom_A.keys())\n\n    def create_adapter_parameters(self, adapter_name: str, r: int, **kwargs):\n        self.custom_A[adapter_name] = nn.Linear(self.in_features, r, bias=False)\n        self.custom_B[adapter_name] = nn.Linear(r, self.out_features, bias=False)\n\n    def _get_delta_activations(self, adapter_name: str, x: torch.Tensor,\n                               *args, **kwargs) -> torch.Tensor:\n        custom_A = self.custom_A[adapter_name]\n        custom_B = self.custom_B[adapter_name]\n        scaling = self.scaling[adapter_name]\n        return custom_B(custom_A(x)) * scaling\n\n    def get_delta_weight(self, adapter_name: str) -> torch.Tensor:\n        custom_A = self.custom_A[adapter_name]\n        custom_B = self.custom_B[adapter_name]\n        return custom_B.weight @ custom_A.weight\n\n    def reset_adapter_parameters(self, adapter_name: str):\n        nn.init.kaiming_uniform_(self.custom_A[adapter_name].weight)\n        nn.init.zeros_(self.custom_B[adapter_name].weight)\n\n    def update_layer(self, adapter_name: str, r: int, alpha: float, **kwargs):\n        if adapter_name not in self.custom_A:\n            self.r[adapter_name] = r\n            self.alpha[adapter_name] = alpha\n            self.scaling[adapter_name] = alpha / r\n            self.create_adapter_parameters(adapter_name, r, **kwargs)\n            self.reset_adapter_parameters(adapter_name)\n</syntaxhighlight>\n\n=== Using Rank and Alpha Patterns ===\n<syntaxhighlight lang=\"python\">\n# Configure different ranks for different layers\nconfig = LycorisConfig(\n    r=8,  # Default rank\n    alpha=16,  # Default alpha\n    rank_pattern={\n        # Attention layers get higher rank\n        \".*attn.*\": 16,\n        # MLP layers get lower rank\n        \".*mlp.*\": 4,\n        # Specific layer gets custom rank\n        \"^model.layer.0.attention.self.query\": 32,\n    },\n    alpha_pattern={\n        # Scale attention layers more\n        \".*attn.*\": 32,\n    },\n)\n\n# Pattern matching uses regex, so you can be very specific\n# or very general based on your needs\n</syntaxhighlight>\n\n=== Merge and Unmerge Operations ===\n<syntaxhighlight lang=\"python\">\n# Assuming lycoris_layer is a LycorisLayer instance\n\n# Merge adapter into base weights\nlycoris_layer.merge(safe_merge=False, adapter_names=[\"default\"])\n\n# Now the base layer includes adapter weights\n# Forward pass will be faster but adapters can't be disabled\n\n# Check if merged\nif lycoris_layer.merged:\n    print(\"Adapters are merged\")\n\n# Unmerge to separate adapters from base weights\nlycoris_layer.unmerge()\n\n# Adapters can now be enabled/disabled independently\n</syntaxhighlight>\n\n=== Safe Merge with NaN Detection ===\n<syntaxhighlight lang=\"python\">\ntry:\n    # Attempt merge with safety check\n    lycoris_layer.merge(safe_merge=True, adapter_names=[\"adapter1\", \"adapter2\"])\n    print(\"Merge successful!\")\nexcept ValueError as e:\n    print(f\"Merge failed: {e}\")\n    # One of the adapters has NaN values\n    # Investigate the adapter or training process\n</syntaxhighlight>\n\n=== Scaling Operations ===\n<syntaxhighlight lang=\"python\">\n# Set scale for specific adapter\nlycoris_layer.set_scale(\"default\", scale=2.0)\n# New scaling = 2.0 * alpha / r\n\n# Scale all active adapters\nlycoris_layer.scale_layer(scale=1.5)\n# Multiplies existing scaling by 1.5\n\n# Reset scaling to default\nlycoris_layer.unscale_layer(scale=None)\n# Resets to alpha / r\n\n# Unscale by specific factor\nlycoris_layer.unscale_layer(scale=1.5)\n# Divides existing scaling by 1.5\n</syntaxhighlight>\n\n=== Implementing Custom LyCORIS Tuner ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.lycoris_utils import LycorisTuner\nimport torch.nn as nn\n\nclass CustomLycorisTuner(LycorisTuner):\n    prefix: str = \"custom_lycoris_\"\n\n    # Map layer types to custom implementations\n    layers_mapping = {\n        nn.Linear: CustomLycorisLinear,\n        # Add more mappings as needed\n    }\n\n    def _create_and_replace(self, config, adapter_name, target,\n                           target_name, parent, current_key):\n        # Create new module using base class method\n        new_module = self._create_new_module(\n            config, adapter_name, target,\n            r=config.r,\n            alpha=config.alpha,\n        )\n\n        # Replace old module with new one\n        if adapter_name not in self.active_adapters:\n            new_module.requires_grad_(False)\n\n        self._replace_module(parent, target_name, new_module, target)\n\n# Use the custom tuner\nmodel = CustomLycorisTuner(base_model, config, adapter_name=\"default\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[LoRA]]\n* [[LoHa]]\n* [[LoKr]]\n* [[PEFT Configuration]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Low-Rank Adaptation]]\n* [[Adapter Methods]]\n* [[Model Fine-Tuning]]\n\n== Notes ==\n* LyCORIS adapters can be more expressive than standard LoRA for certain tasks\n* Pattern-based configuration allows fine-grained control over adapter parameters per layer\n* The merge operation modifies base weights in-place and cannot be reversed without unmerge\n* Safe merge checks for NaN values which can indicate training instability\n* Rank and module dropout are supported for regularization\n* The framework supports multiple active adapters simultaneously\n* Subclasses must implement all abstract methods for the framework to function\n\n== Advanced Features ==\n\n=== Empty Weight Initialization ===\nThe <code>_init_empty_weights</code> method enables fast initialization:\n<syntaxhighlight lang=\"python\">\n# Instead of initializing on CPU then moving to device\n# Initialize directly on target device without materializing weights\nlayer._init_empty_weights(\n    nn.Linear,\n    in_features=768,\n    out_features=768,\n    device=\"cuda\"\n)\n</syntaxhighlight>\n\n=== Cast Input Dtype ===\nThe <code>cast_input_dtype_enabled</code> flag controls automatic dtype casting:\n<syntaxhighlight lang=\"python\">\n# Enable/disable automatic input dtype casting\nlycoris_layer.cast_input_dtype_enabled = True  # Default\n# When enabled, inputs are cast to match weight dtype\n</syntaxhighlight>\n\n=== Rank and Module Dropout ===\n<syntaxhighlight lang=\"python\">\n# Rank dropout: randomly drops rank dimensions during training\n# Module dropout: randomly drops entire adapter modules\nconfig = LycorisConfig(\n    r=16,\n    rank_dropout=0.1,  # Drop 10% of rank dimensions\n    module_dropout=0.05,  # Drop adapter with 5% probability\n)\n</syntaxhighlight>\n\n== References ==\n* LyCORIS Repository: https://github.com/KohakuBlueleaf/LyCORIS\n* LoRA Paper: https://arxiv.org/abs/2106.09685\n* PEFT Documentation: https://huggingface.co/docs/peft\n* Stable Diffusion Fine-tuning: https://huggingface.co/docs/diffusers\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_MissConfig",
      "page_title": "MiSS Configuration",
      "page_type": "Implementation",
      "overview": "=== Description === The MiSS (Memory-efficient Incremental Singular Space) configuration class defines the settings for applying MiSS adapters to neural network models. MiSS is a parameter-efficient fine-tuning method based on Householder reflections, providing an alternative to LoRA with different trade-offs between parameter efficiency and model expressiveness. Key features of MiSS: * '''Dual-rank decomposition''': Separate ranks for in_features (r) and out_features (mini_r) dimensions * '''Multiple initialization modes''': balance (default/efficient), bat (nonlinear updates), mini (smaller rank) * '''Pattern-based targeting''': Flexible module selection with regex support * '''Layer-specific application''': Optional layer indices for selective adaptation * '''Dropout support''': Regularization through miss_dropout The configuration supports three initialization variants: 1. '''balance''' (default=True): Most efficient and general method 2. '''bat''': Enables nonlinear updates across different shards 3. '''mini''': Smaller rank variant for fewer trainable parameters",
      "content": "= MiSS Configuration =\n\n== Knowledge Sources ==\n* '''Repository:''' [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Source File:''' src/peft/tuners/miss/config.py\n* '''Paper:''' [https://huggingface.co/papers/2409.15371 MiSS: Householder Reflection Adaptation]\n\n== Domains ==\n* [[Natural Language Processing]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Model Configuration]]\n* [[Householder Reflection]]\n\n== Overview ==\n\n=== Description ===\nThe MiSS (Memory-efficient Incremental Singular Space) configuration class defines the settings for applying MiSS adapters to neural network models. MiSS is a parameter-efficient fine-tuning method based on Householder reflections, providing an alternative to LoRA with different trade-offs between parameter efficiency and model expressiveness.\n\nKey features of MiSS:\n* '''Dual-rank decomposition''': Separate ranks for in_features (r) and out_features (mini_r) dimensions\n* '''Multiple initialization modes''': balance (default/efficient), bat (nonlinear updates), mini (smaller rank)\n* '''Pattern-based targeting''': Flexible module selection with regex support\n* '''Layer-specific application''': Optional layer indices for selective adaptation\n* '''Dropout support''': Regularization through miss_dropout\n\nThe configuration supports three initialization variants:\n1. '''balance''' (default=True): Most efficient and general method\n2. '''bat''': Enables nonlinear updates across different shards\n3. '''mini''': Smaller rank variant for fewer trainable parameters\n\n=== Usage ===\nMissConfig is used to configure MiSS adapters when fine-tuning models with PEFT. It inherits from PeftConfig and provides MiSS-specific parameters for controlling the adapter behavior.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/miss/config.py</code>\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass MissConfig(PeftConfig):\n    r: int = field(default=64)\n    miss_dropout: float = field(default=0.0)\n    mini_r: int = field(default=1)\n    target_modules: Optional[Union[list[str], str]] = field(default=None)\n    exclude_modules: Optional[Union[list[str], str]] = field(default=None)\n    init_weights: bool | Literal[\"bat\", \"mini\"] = field(default=True)\n    layers_to_transform: Optional[Union[list[int], int]] = field(default=None)\n    layers_pattern: Optional[str] = field(default=None)\n    bias: str = field(default=\"none\")\n    modules_to_save: Optional[list[str]] = field(default=None)\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.miss.config import MissConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Configuration Fields ===\n{| class=\"wikitable\"\n! Field !! Type !! Default !! Description\n|-\n| r || int || 64 || Rank for low-rank decomposition along in_features dimension (should be even)\n|-\n| miss_dropout || float || 0.0 || Dropout probability for MiSS layers\n|-\n| mini_r || int || 1 || Rank for decomposition along out_features dimension (out_features should be divisible by mini_r)\n|-\n| target_modules || Optional[Union[list[str], str]] || None || Module names or regex to apply adapter; \"all-linear\" for all linear layers\n|-\n| exclude_modules || Optional[Union[list[str], str]] || None || Module names or regex to exclude from adaptation\n|-\n| init_weights || bool &#124; Literal[\"bat\", \"mini\"] || True || Initialization mode: True=balance, \"bat\"=nonlinear updates, \"mini\"=smaller rank\n|-\n| layers_to_transform || Optional[Union[list[int], int]] || None || Layer indices to transform; None transforms all\n|-\n| layers_pattern || Optional[str] || None || Layer pattern name (used with layers_to_transform)\n|-\n| bias || str || \"none\" || Bias type: \"none\", \"all\", or \"MiSS_only\"\n|-\n| modules_to_save || Optional[list[str]] || None || Additional modules to train and save (e.g., classifier heads)\n|}\n\n=== Validation Rules ===\nThe <code>__post_init__</code> method enforces:\n* Sets <code>peft_type = PeftType.MISS</code>\n* Converts list target_modules and exclude_modules to sets\n* Raises ValueError if target_modules is str and layers_to_transform is not None\n* Raises ValueError if target_modules is str and layers_pattern is not None\n\n== Usage Examples ==\n\n=== Basic MiSS Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Create MiSS config with default settings\nconfig = MissConfig(\n    r=64,  # Rank along in_features\n    target_modules=[\"c_attn\", \"c_proj\"],  # Target attention layers\n    init_weights=True,  # Use balanced initialization\n    bias=\"none\",\n)\n\n# Apply MiSS to model\npeft_model = get_peft_model(model, config)\nprint(f\"Trainable parameters: {peft_model.num_parameters(only_trainable=True)}\")\n</syntaxhighlight>\n\n=== Mini Variant for Maximum Efficiency ===\n<syntaxhighlight lang=\"python\">\n# Mini variant uses smaller rank for fewer parameters\nconfig = MissConfig(\n    r=32,  # Reduced rank\n    mini_r=4,  # Smaller out_features rank\n    miss_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    init_weights=\"mini\",  # Mini initialization\n)\n\n# Important: Ensure out_features % mini_r == 0\n# For a layer with out_features=768, mini_r should divide 768\n# Valid: 1, 2, 3, 4, 6, 8, 12, 16, 24, 32, etc.\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Bat Variant for Nonlinear Updates ===\n<syntaxhighlight lang=\"python\">\n# Bat mode enables nonlinear updates across shards\nconfig = MissConfig(\n    r=64,\n    target_modules=[\"attn.c_attn\"],\n    init_weights=\"bat\",  # Bat initialization\n    miss_dropout=0.05,\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Using Regex Patterns for Target Modules ===\n<syntaxhighlight lang=\"python\">\n# Use regex to target modules\nconfig = MissConfig(\n    r=64,\n    # Match all query and value projections in any layer\n    target_modules=r\".*\\.(q_proj|v_proj)$\",\n    # Exclude specific layers\n    exclude_modules=[\"lm_head\", \"embed_tokens\"],\n    init_weights=True,\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Target All Linear Layers ===\n<syntaxhighlight lang=\"python\">\n# Apply MiSS to all linear layers except output\nconfig = MissConfig(\n    r=32,\n    mini_r=2,\n    target_modules=\"all-linear\",\n    init_weights=True,\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Layer-Specific Configuration ===\n<syntaxhighlight lang=\"python\">\n# Apply MiSS only to specific layers\nconfig = MissConfig(\n    r=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    layers_to_transform=[0, 1, 2],  # Only first 3 layers\n    layers_pattern=\"layers\",  # Pattern for layer indexing\n    init_weights=True,\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Configuration for Sequence Classification ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=2\n)\n\n# Configure MiSS with classifier head training\nconfig = MissConfig(\n    r=16,\n    mini_r=2,\n    target_modules=[\"query\", \"value\"],\n    modules_to_save=[\"classifier\"],  # Also train the classifier\n    bias=\"MiSS_only\",  # Add bias only to MiSS layers\n    init_weights=True,\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Configuration with Dropout ===\n<syntaxhighlight lang=\"python\">\n# Add dropout for regularization\nconfig = MissConfig(\n    r=64,\n    mini_r=4,\n    miss_dropout=0.1,  # 10% dropout\n    target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\"],\n    init_weights=True,\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Complete Configuration Example ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissConfig, get_peft_model, TaskType\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = MissConfig(\n    # Core MiSS parameters\n    r=64,  # In-features rank (use even number)\n    mini_r=4,  # Out-features rank (divisor of out_features)\n    miss_dropout=0.05,\n\n    # Target configuration\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    exclude_modules=[\"lm_head\"],\n\n    # Initialization\n    init_weights=True,  # Balanced/efficient initialization\n\n    # Bias configuration\n    bias=\"none\",\n\n    # Task type\n    task_type=TaskType.CAUSAL_LM,\n)\n\npeft_model = get_peft_model(model, config)\n\n# Print configuration\nprint(config)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[MiSS Model]]\n* [[PEFT Configuration]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Householder Reflection]]\n* [[LoRA Configuration]]\n* [[Model Adapter Methods]]\n* [[Low-Rank Decomposition]]\n\n== Notes ==\n\n=== Important Constraints ===\n* '''Even rank (r)''': It's best to set r to an even number; otherwise, the default initialization may not work\n* '''mini_r divisibility''': out_features should be divisible by mini_r for mini initialization\n* '''Regex restrictions''': When target_modules is a regex string:\n** layers_to_transform must be None\n** layers_pattern must be None\n\n=== Initialization Modes ===\n* '''True (balance)''': Default efficient method, suitable for most use cases\n* '''\"bat\"''': Enables nonlinear updates across shards, may provide better expressiveness\n* '''\"mini\"''': Smallest parameter count, recommended to keep out_features % mini_r == 0\n\n=== Bias Options ===\n* '''\"none\"''': No bias parameters (most efficient)\n* '''\"all\"''': Add bias to all adapted layers\n* '''\"MiSS_only\"''': Add bias only to MiSS adapter layers\n\n=== Target Module Selection ===\n* '''Explicit list''': [\"q_proj\", \"v_proj\"] matches exact names\n* '''Regex string''': \".*attn.*\" matches all modules containing \"attn\"\n* '''\"all-linear\"''': Matches all nn.Linear layers except output layer\n* '''Exclude modules''': Takes precedence over target_modules\n\n== Advanced Configuration ==\n\n=== Comparing Initialization Modes ===\n<syntaxhighlight lang=\"python\">\n# Balanced (default) - most efficient\nconfig_balance = MissConfig(r=64, mini_r=1, init_weights=True)\n\n# Bat - nonlinear updates\nconfig_bat = MissConfig(r=64, mini_r=1, init_weights=\"bat\")\n\n# Mini - smallest parameters\nconfig_mini = MissConfig(r=32, mini_r=8, init_weights=\"mini\")\n\n# Compare parameter counts\nfor name, cfg in [(\"balance\", config_balance), (\"bat\", config_bat), (\"mini\", config_mini)]:\n    model_copy = get_peft_model(base_model, cfg)\n    params = model_copy.num_parameters(only_trainable=True)\n    print(f\"{name}: {params:,} trainable parameters\")\n</syntaxhighlight>\n\n=== Dynamic Configuration Based on Model ===\n<syntaxhighlight lang=\"python\">\ndef create_miss_config(model, efficiency=\"balanced\"):\n    \"\"\"Create MiSS config based on model architecture\"\"\"\n\n    # Get hidden size to determine ranks\n    hidden_size = model.config.hidden_size\n\n    if efficiency == \"high\":\n        r = 16\n        mini_r = max(1, hidden_size // 128)\n        init_weights = \"mini\"\n    elif efficiency == \"balanced\":\n        r = 64\n        mini_r = 1\n        init_weights = True\n    else:  # \"expressive\"\n        r = 128\n        mini_r = 1\n        init_weights = \"bat\"\n\n    return MissConfig(\n        r=r,\n        mini_r=mini_r,\n        init_weights=init_weights,\n        target_modules=\"all-linear\",\n        miss_dropout=0.05,\n    )\n\nconfig = create_miss_config(model, efficiency=\"balanced\")\n</syntaxhighlight>\n\n== References ==\n* MiSS Paper: https://huggingface.co/papers/2409.15371\n* PEFT Documentation: https://huggingface.co/docs/peft\n* Householder Reflection: https://en.wikipedia.org/wiki/Householder_transformation\n* Parameter-Efficient Fine-Tuning: https://arxiv.org/abs/2110.04366\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_MissLayer",
      "page_title": "huggingface peft MissLayer",
      "page_type": "Implementation",
      "overview": "Mixed-rank Sparse Scaling layer that applies block-wise additive or multiplicative adaptation with minimal parameters for efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|MiSS|https://arxiv.org/abs/2503.01944]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Sparse_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nMixed-rank Sparse Scaling layer that applies block-wise additive or multiplicative adaptation with minimal parameters for efficient fine-tuning.\n\n=== Description ===\n\nMissLayer implements Multiple Initialization Schemes with Sparsity (MiSS) for parameter-efficient adaptation. The layer supports three initialization modes: standard MiSS adds a sparse block matrix to reshaped inputs, BAT (Block Additive Transformation) applies block-diagonal transformations, and mini uses repeated small matrices. The adapter modifies outputs by reshaping inputs into blocks, applying the learned sparse transformation, and summing back to the original shape.\n\n=== Usage ===\n\nUse MiSS when you need extremely parameter-efficient adaptation. MiSS can be more efficient than LoRA for certain architectures due to its block-sparse structure. The BAT variant provides multiplicative adaptation while standard MiSS is additive. The mini variant is useful when output features should share parameters.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/miss/layer.py src/peft/tuners/miss/layer.py]\n* '''Lines:''' 1-394\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass MissLayer(BaseTunerLayer):\n    \"\"\"\n    Mixed-rank Sparse Scaling layer.\n\n    Attributes:\n        miss_block: ParameterDict of sparse adaptation matrices\n        miss_r: Dict of rank values per adapter\n        miss_mini_r: Dict of mini rank values per adapter\n        miss_dropout: ModuleDict of dropout layers\n    \"\"\"\n    adapter_layer_names = (\"miss_block\",)\n    other_param_names = (\"miss_r\", \"miss_dropout\", \"miss_mini_r\")\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        r: int,\n        mini_r: int,\n        miss_dropout: float,\n        init_weights: bool | str,\n        **kwargs,\n    ) -> None:\n        \"\"\"Create MiSS adapter with specified initialization.\"\"\"\n\n    def reset_bat_parameters(self, adapter_name: str, r: int):\n        \"\"\"Initialize BAT block-diagonal parameters.\"\"\"\n\n    def reset_mini_parameters(self, adapter_name: str, r: int, mini_r: int):\n        \"\"\"Initialize mini shared parameters.\"\"\"\n\nclass MissLinear(nn.Module, MissLayer):\n    \"\"\"MiSS implemented in Linear layer.\"\"\"\n\n    def get_delta_weight(self, adapter, orig_weight, re: bool = False):\n        \"\"\"Compute BAT delta weight transformation.\"\"\"\n\n    def get_delta_weight_miss(self, adapter, orig_weight, re: bool = False):\n        \"\"\"Compute standard/mini MiSS delta weight.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.miss import MissLayer, MissConfig, MissModel\nfrom peft import MissConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained Linear layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| r || int || Yes || Rank/block size for adaptation\n|-\n| mini_r || int || No || Mini rank for shared parameters\n|-\n| miss_dropout || float || No || Dropout probability\n|-\n| init_weights || bool/str || Yes || \"bat\", \"mini\", or True for standard\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input + sparse block adaptation\n|-\n| get_delta_weight() || torch.Tensor || Block transformation for merging\n|}\n\n== Usage Examples ==\n\n=== Standard MiSS Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Standard MiSS with additive blocks\nconfig = MissConfig(\n    r=8,                        # Block size\n    target_modules=[\"q_proj\", \"v_proj\"],\n    miss_dropout=0.0,\n    init_weights=True,          # Standard initialization\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== BAT (Block Additive Transformation) ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissConfig, get_peft_model\n\n# BAT provides multiplicative block adaptation\nconfig = MissConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    init_weights=\"bat\",         # Block Additive Transformation\n)\n\nmodel = get_peft_model(model, config)\n# BAT: W' = W @ block + block (block-wise transformation)\n</syntaxhighlight>\n\n=== Mini MiSS with Shared Parameters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissConfig, get_peft_model\n\n# Mini variant shares small matrix across output\nconfig = MissConfig(\n    r=16,\n    mini_r=64,                  # Small shared matrix size\n    target_modules=[\"q_proj\", \"v_proj\"],\n    init_weights=\"mini\",        # Use mini initialization\n)\n\nmodel = get_peft_model(model, config)\n# Even fewer parameters than standard MiSS\n</syntaxhighlight>\n\n=== MiSS Forward Computation ===\n<syntaxhighlight lang=\"python\">\n# For standard MiSS:\n# 1. Reshape input: x -> [batch, seq, in_features//r, r]\n# 2. Sum over blocks: sum(x, dim=-2) -> [batch, seq, r]\n# 3. Apply transformation: summed @ miss_block -> [batch, seq, out_features]\n# 4. Add to base output: result = base(x) + transformed\n\n# For BAT:\n# 1. Reshape weight into r x r blocks\n# 2. Apply learned block transformation\n# 3. Reconstruct transformed weight\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Sparse Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "MiSS",
          "url": "https://arxiv.org/abs/2503.01944"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_MissModel",
      "page_title": "MiSS Model",
      "page_type": "Implementation",
      "overview": "=== Description === The MiSSModel class creates Householder reflection adaptation (MiSS) models from pretrained models. MiSS is a parameter-efficient fine-tuning method that uses Householder reflections to adapt model weights with minimal additional parameters. It provides an alternative to LoRA with different characteristics for memory efficiency and model expressiveness. Key features: * '''Householder reflection-based adaptation''': Uses mathematical properties of Householder matrices for efficient weight updates * '''Dual-rank decomposition''': Separate control over in_features (r) and out_features (mini_r) dimensions * '''Multiple initialization modes''': Supports balance, bat, and mini variants * '''Linear layer support''': Currently supports torch.nn.Linear layers * '''Model architecture mapping''': Automatic target module detection for known architectures * '''Low memory overhead''': Efficient adapter creation with optional meta device initialization The implementation follows the PEFT BaseTuner pattern: * '''MissModel''': Main tuner class for creating and managing MiSS adapters * '''MissLinear''': Layer implementation (imported from .layer module) * '''MissLayer''': Base layer class (imported from .layer module)",
      "content": "= MiSS Model =\n\n== Knowledge Sources ==\n* '''Repository:''' [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Source File:''' src/peft/tuners/miss/model.py\n* '''Paper:''' [https://huggingface.co/papers/2409.15371 MiSS: Householder Reflection Adaptation]\n\n== Domains ==\n* [[Natural Language Processing]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Model Tuning]]\n* [[Householder Reflection]]\n* [[Deep Learning]]\n\n== Overview ==\n\n=== Description ===\nThe MiSSModel class creates Householder reflection adaptation (MiSS) models from pretrained models. MiSS is a parameter-efficient fine-tuning method that uses Householder reflections to adapt model weights with minimal additional parameters. It provides an alternative to LoRA with different characteristics for memory efficiency and model expressiveness.\n\nKey features:\n* '''Householder reflection-based adaptation''': Uses mathematical properties of Householder matrices for efficient weight updates\n* '''Dual-rank decomposition''': Separate control over in_features (r) and out_features (mini_r) dimensions\n* '''Multiple initialization modes''': Supports balance, bat, and mini variants\n* '''Linear layer support''': Currently supports torch.nn.Linear layers\n* '''Model architecture mapping''': Automatic target module detection for known architectures\n* '''Low memory overhead''': Efficient adapter creation with optional meta device initialization\n\nThe implementation follows the PEFT BaseTuner pattern:\n* '''MissModel''': Main tuner class for creating and managing MiSS adapters\n* '''MissLinear''': Layer implementation (imported from .layer module)\n* '''MissLayer''': Base layer class (imported from .layer module)\n\n=== Usage ===\nMissModel is used to apply MiSS adapters to pretrained models for parameter-efficient fine-tuning. It's particularly useful for vision and language models where memory efficiency is important.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/miss/model.py</code>\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\nclass MissModel(BaseTuner):\n    prefix: str = \"miss_\"\n    tuner_layer_cls = MissLayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_MISS_TARGET_MODULES_MAPPING\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        config: MissConfig,\n        adapter_name: str,\n        low_cpu_mem_usage: bool = False,\n    )\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissModel\nfrom peft.tuners.miss import MissConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Constructor Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| model || torch.nn.Module || Required || The model to which adapter tuner layers will be attached\n|-\n| config || MissConfig || Required || Configuration of the MiSS model\n|-\n| adapter_name || str || Required || Name of the adapter\n|-\n| low_cpu_mem_usage || bool || False || Create empty adapter weights on meta device for faster loading\n|}\n\n=== _create_and_replace Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| miss_config || MissConfig || Configuration for MiSS adapter\n|-\n| adapter_name || str || Name of the adapter\n|-\n| target || torch.nn.Module || Target module to adapt\n|-\n| target_name || str || Name of target module\n|-\n| parent || torch.nn.Module || Parent module containing target\n|-\n| current_key || str || Key identifying current module in model\n|-\n| **optional_kwargs || Any || Additional keyword arguments\n|}\n\n=== _create_new_module (static method) ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| miss_config || MissConfig || Configuration for MiSS adapter\n|-\n| adapter_name || str || Name of the adapter\n|-\n| target || torch.nn.Module || Target module to wrap\n|-\n| **kwargs || Any || Additional arguments (r, mini_r, miss_dropout, init_weights, bias)\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || MissLinear || New MiSS layer wrapping the target\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with Language Model ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissModel, MissConfig\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Load pretrained model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Create MiSS configuration\nconfig = MissConfig(\n    r=8,\n    mini_r=1,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    init_weights=True,\n)\n\n# Apply MiSS adapters\nmodel = MissModel(model, config, adapter_name=\"default\")\n\n# Check trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params:,}\")\n</syntaxhighlight>\n\n=== Usage with Stable Diffusion (Example from Docstring) ===\n<syntaxhighlight lang=\"python\">\nfrom diffusers import StableDiffusionPipeline\nfrom peft import MissModel, MissConfig\n\n# Configuration for text encoder\nconfig_te = MissConfig(\n    r=8,\n    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    init_weights=True,\n)\n\n# Configuration for UNet\nconfig_unet = MissConfig(\n    r=8,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    init_weights=True,\n)\n\n# Load Stable Diffusion pipeline\nmodel = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n\n# Apply MiSS to text encoder and UNet\nmodel.text_encoder = MissModel(model.text_encoder, config_te, \"default\")\nmodel.unet = MissModel(model.unet, config_unet, \"default\")\n\n# Now ready for fine-tuning\n</syntaxhighlight>\n\n=== Using with PEFT get_peft_model ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, MissConfig\nfrom transformers import AutoModelForSequenceClassification\n\n# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=3\n)\n\n# Create configuration\nconfig = MissConfig(\n    r=16,\n    mini_r=2,\n    target_modules=[\"query\", \"value\"],\n    modules_to_save=[\"classifier\"],\n    init_weights=True,\n)\n\n# Apply MiSS using PEFT helper\nmodel = get_peft_model(model, config)\n\n# Train the model\ntrainer.train(model)\n</syntaxhighlight>\n\n=== Low Memory Usage Mode ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissModel, MissConfig\n\n# For large models, use low_cpu_mem_usage to speed up loading\nconfig = MissConfig(\n    r=64,\n    mini_r=4,\n    target_modules=\"all-linear\",\n    init_weights=True,\n)\n\n# Weights created on meta device, then moved to target device\nmodel = MissModel(\n    base_model,\n    config,\n    adapter_name=\"default\",\n    low_cpu_mem_usage=True  # Faster loading for large models\n)\n</syntaxhighlight>\n\n=== Adding Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissModel, MissConfig\n\n# Create base model with first adapter\nconfig1 = MissConfig(r=8, target_modules=[\"q_proj\", \"v_proj\"])\nmodel = MissModel(base_model, config1, adapter_name=\"task1\")\n\n# Add second adapter\nconfig2 = MissConfig(r=16, target_modules=[\"q_proj\", \"v_proj\"])\nmodel.add_adapter(\"task2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"task1\")  # Use task1 adapter\noutput1 = model(**inputs)\n\nmodel.set_adapter(\"task2\")  # Use task2 adapter\noutput2 = model(**inputs)\n</syntaxhighlight>\n\n=== Using Different Initialization Modes ===\n<syntaxhighlight lang=\"python\">\n# Balanced initialization (default)\nconfig_balanced = MissConfig(\n    r=64,\n    mini_r=1,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    init_weights=True,  # Most efficient and general\n)\nmodel_balanced = MissModel(base_model, config_balanced, \"balanced\")\n\n# Bat initialization (nonlinear updates)\nconfig_bat = MissConfig(\n    r=64,\n    mini_r=1,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    init_weights=\"bat\",  # Nonlinear updates across shards\n)\nmodel_bat = MissModel(base_model, config_bat, \"bat\")\n\n# Mini initialization (smaller rank)\nconfig_mini = MissConfig(\n    r=32,\n    mini_r=8,  # Ensure out_features % mini_r == 0\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    init_weights=\"mini\",  # Fewer trainable parameters\n)\nmodel_mini = MissModel(base_model, config_mini, \"mini\")\n</syntaxhighlight>\n\n=== Training with MiSS ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import TrainingArguments, Trainer\nfrom peft import get_peft_model, MissConfig\n\n# Setup\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nconfig = MissConfig(\n    r=8,\n    mini_r=1,\n    miss_dropout=0.05,\n    target_modules=[\"c_attn\"],\n    bias=\"none\",\n)\npeft_model = get_peft_model(model, config)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./miss_model\",\n    per_device_train_batch_size=4,\n    learning_rate=3e-4,\n    num_train_epochs=3,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n)\n\n# Train\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\ntrainer.train()\n\n# Save adapter\npeft_model.save_pretrained(\"./miss_adapter\")\n</syntaxhighlight>\n\n=== Loading Saved Adapter ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Load MiSS adapter\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"path/to/miss_adapter\"\n)\n\n# Use for inference\noutput = model.generate(**inputs)\n</syntaxhighlight>\n\n=== Inspecting Adapted Layers ===\n<syntaxhighlight lang=\"python\">\nfrom peft import MissModel, MissConfig\n\nmodel = MissModel(base_model, config, \"default\")\n\n# Check which layers were adapted\nprint(\"Adapted modules:\")\nfor name, module in model.named_modules():\n    if hasattr(module, 'miss_'):\n        print(f\"  {name}\")\n\n# Check specific layer\ntarget_layer = model.model.layer[0].attention.self.query\nif hasattr(target_layer, 'base_layer'):\n    print(f\"Layer has MiSS adapter\")\n    print(f\"  r = {target_layer.r}\")\n    print(f\"  mini_r = {target_layer.mini_r}\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[MiSS Configuration]]\n* [[MiSS Layer]]\n* [[PEFT Configuration]]\n* [[Parameter-Efficient Fine-Tuning]]\n* [[Householder Reflection]]\n* [[LoRA Model]]\n* [[Model Adapter Methods]]\n* [[BaseTuner]]\n\n== Notes ==\n\n=== Supported Layer Types ===\n* Currently only <code>torch.nn.Linear</code> layers are supported\n* Attempting to adapt other layer types will raise a ValueError\n* Future versions may add support for Conv2d and other layer types\n\n=== Adapter Management ===\n* Multiple adapters can be added to the same model\n* Each adapter has its own name and configuration\n* Only one adapter is active at a time (can be switched)\n* Adapters can be merged into base weights or kept separate\n\n=== Memory Efficiency ===\n* <code>low_cpu_mem_usage=True</code> creates adapters on meta device first\n* Useful for very large models to avoid CPU memory spikes\n* Adapters are then materialized on the target device\n\n=== Initialization Modes ===\n* '''Balance''' (default): Most efficient, suitable for most tasks\n* '''Bat''': Enables nonlinear updates, potentially more expressive\n* '''Mini''': Fewest parameters, requires out_features % mini_r == 0\n\n=== Target Module Mapping ===\nThe <code>TRANSFORMERS_MODELS_TO_MISS_TARGET_MODULES_MAPPING</code> provides default target modules for known architectures:\n* Automatically selects appropriate layers for popular models\n* Can be overridden by specifying <code>target_modules</code> in config\n* Supports models like BERT, GPT-2, LLaMA, etc.\n\n== Implementation Details ==\n\n=== Module Creation Process ===\n1. <code>_create_and_replace</code> is called for each target module\n2. Checks if module is already a MissLayer\n3. If not, calls <code>_create_new_module</code> to create MissLinear\n4. Validates that target is torch.nn.Linear\n5. Replaces original module with MiSS-adapted version\n6. If adapter not active, sets requires_grad to False\n\n=== Layer Update Process ===\nIf target is already a MissLayer:\n1. Calls <code>update_layer</code> on existing layer\n2. Adds new adapter to existing MissLayer\n3. Preserves other adapters already present\n\n=== Error Handling ===\n* Raises ValueError if <code>current_key</code> is None\n* Raises ValueError if target is not torch.nn.Linear\n* Provides helpful error messages for unsupported layer types\n\n== Advanced Usage ==\n\n=== Combining with Other PEFT Methods ===\n<syntaxhighlight lang=\"python\">\n# MiSS can be combined with other PEFT methods\nfrom peft import get_peft_model, MissConfig, LoraConfig\n\n# Apply MiSS to some layers\nmiss_config = MissConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\nmodel = get_peft_model(base_model, miss_config)\n\n# Add LoRA to other layers\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"k_proj\", \"o_proj\"]\n)\nmodel.add_adapter(\"lora_adapter\", lora_config)\n</syntaxhighlight>\n\n=== Custom Target Module Selection ===\n<syntaxhighlight lang=\"python\">\ndef get_attention_modules(model):\n    \"\"\"Extract all attention module names\"\"\"\n    attention_modules = []\n    for name, module in model.named_modules():\n        if \"attn\" in name.lower() and isinstance(module, torch.nn.Linear):\n            attention_modules.append(name)\n    return attention_modules\n\ntarget_modules = get_attention_modules(base_model)\nconfig = MissConfig(r=16, target_modules=target_modules)\nmodel = MissModel(base_model, config, \"custom\")\n</syntaxhighlight>\n\n== References ==\n* MiSS Paper: https://huggingface.co/papers/2409.15371\n* PEFT Documentation: https://huggingface.co/docs/peft\n* Householder Transformation: https://en.wikipedia.org/wiki/Householder_transformation\n* Parameter-Efficient Fine-Tuning Survey: https://arxiv.org/abs/2110.04366\n* Diffusers Documentation: https://huggingface.co/docs/diffusers\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_MultitaskPromptTuningConfig",
      "page_title": "huggingface peft MultitaskPromptTuningConfig",
      "page_type": "Implementation",
      "overview": "",
      "content": "== MultitaskPromptTuningConfig ==\n\n=== Knowledge Sources ===\n* [https://github.com/huggingface/peft PEFT Repository]\n* [https://huggingface.co/papers/2303.02861 Multitask Prompt Tuning Paper]\n\n=== Domains ===\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Prompt_Tuning]]\n[[Category:Multitask_Learning]]\n[[Category:Configuration]]\n\n=== Overview ===\n\n==== Description ====\n'''MultitaskPromptTuningConfig''' is a configuration class for multitask prompt tuning, extending the PromptTuningConfig to support training prompts across multiple tasks. It implements the approach described in the paper [https://huggingface.co/papers/2303.02861 \"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning\"].\n\nThis configuration enables different initialization strategies for prompt tuning parameters, including:\n* Random initialization\n* Text-based initialization\n* Transfer learning from source tasks (average, exact, or shared embeddings only)\n\nThe configuration manages task-specific parameters including the number of tasks, ranks for low-rank decomposition, and paths to pretrained source prompts for transfer learning scenarios.\n\n==== Usage ====\nUsed to configure multitask prompt tuning models when adapting pre-trained language models to multiple related tasks simultaneously. It's particularly useful for parameter-efficient transfer learning where knowledge from source tasks can be leveraged to improve performance on target tasks.\n\n=== Code Reference ===\n\n==== Source Location ====\n<code>src/peft/tuners/multitask_prompt_tuning/config.py</code>\n\n==== Signature ====\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass MultitaskPromptTuningConfig(PromptTuningConfig):\n    prompt_tuning_init: Union[MultitaskPromptTuningInit, str] = field(\n        default=MultitaskPromptTuningInit.RANDOM,\n        metadata={\n            \"help\": (\n                \"How to initialize the prompt tuning parameters. Can be one of TEXT, RANDOM, AVERAGE_SOURCE_TASKS, \"\n                \"EXACT_SOURCE_TASK, ONLY_SOURCE_SHARED.\"\n            ),\n        },\n    )\n    prompt_tuning_init_state_dict_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The path of source state dict. This is required when training the downstream target prompt from \"\n                \"the pretrained source prompt\"\n            ),\n        },\n    )\n    prompt_tuning_init_task: Optional[int] = field(default=0, metadata={\"help\": \"source task id for initialization\"})\n    num_ranks: Optional[int] = field(default=1, metadata={\"help\": \"ranks\"})\n    num_tasks: Optional[int] = field(default=1, metadata={\"help\": \"number of tasks\"})\n</syntaxhighlight>\n\n==== Import ====\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.multitask_prompt_tuning import MultitaskPromptTuningConfig\n</syntaxhighlight>\n\n=== I/O Contract ===\n\n==== Initialization Parameters ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| prompt_tuning_init || Union[MultitaskPromptTuningInit, str] || RANDOM || Initialization method: TEXT, RANDOM, AVERAGE_SOURCE_TASKS, EXACT_SOURCE_TASK, or ONLY_SOURCE_SHARED\n|-\n| prompt_tuning_init_state_dict_path || Optional[str] || None || Path to source state dict for transfer learning\n|-\n| prompt_tuning_init_task || Optional[int] || 0 || Source task ID for initialization when using EXACT_SOURCE_TASK\n|-\n| num_ranks || Optional[int] || 1 || Number of ranks for low-rank decomposition\n|-\n| num_tasks || Optional[int] || 1 || Number of tasks to handle\n|-\n| colspan=\"4\" | ''Inherits all parameters from PromptTuningConfig''\n|}\n\n==== Initialization Enum Values ====\n{| class=\"wikitable\"\n! Value !! Description\n|-\n| TEXT || Initialize prompt with text\n|-\n| RANDOM || Initialize prompt with random matrix\n|-\n| AVERAGE_SOURCE_TASKS || Average the prefix and column matrices from all source tasks\n|-\n| EXACT_SOURCE_TASK || Use prefix and column matrices from a specific source task\n|-\n| ONLY_SOURCE_SHARED || Use only the shared prompt embeddings from source training\n|}\n\n==== Returns ====\nConfiguration object ready to be used with MultitaskPromptEmbedding model.\n\n==== Side Effects ====\n* Sets peft_type to PeftType.MULTITASK_PROMPT_TUNING in __post_init__\n\n=== Usage Examples ===\n\n==== Basic Configuration ====\n<syntaxhighlight lang=\"python\">\nfrom peft import MultitaskPromptTuningConfig\n\n# Create configuration for multitask prompt tuning with random initialization\nconfig = MultitaskPromptTuningConfig(\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    num_tasks=5,\n    num_ranks=4,\n    prompt_tuning_init=\"RANDOM\"\n)\n</syntaxhighlight>\n\n==== Transfer Learning from Source Tasks ====\n<syntaxhighlight lang=\"python\">\nfrom peft import MultitaskPromptTuningConfig\n\n# Initialize from averaged source tasks\nconfig = MultitaskPromptTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=10,\n    num_tasks=3,\n    num_ranks=2,\n    prompt_tuning_init=\"AVERAGE_SOURCE_TASKS\",\n    prompt_tuning_init_state_dict_path=\"path/to/source_model.pt\"\n)\n</syntaxhighlight>\n\n==== Exact Source Task Initialization ====\n<syntaxhighlight lang=\"python\">\nfrom peft import MultitaskPromptTuningConfig\n\n# Initialize from a specific source task\nconfig = MultitaskPromptTuningConfig(\n    task_type=\"SEQ_CLS\",\n    num_virtual_tokens=15,\n    num_tasks=4,\n    num_ranks=3,\n    prompt_tuning_init=\"EXACT_SOURCE_TASK\",\n    prompt_tuning_init_state_dict_path=\"path/to/source_model.pt\",\n    prompt_tuning_init_task=2  # Use task ID 2 from source\n)\n</syntaxhighlight>\n\n=== Related Pages ===\n* [[huggingface_peft_MultitaskPromptTuningModel|MultitaskPromptTuningModel]] - The model implementation using this configuration\n* [[huggingface_peft_PromptTuningConfig|PromptTuningConfig]] - Parent configuration class\n* [[huggingface_peft_PeftConfig|PeftConfig]] - Base PEFT configuration class\n* [[PEFT|Parameter-Efficient Fine-Tuning]]\n* [[Prompt_Tuning|Prompt Tuning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_MultitaskPromptTuningModel",
      "page_title": "huggingface peft MultitaskPromptTuningModel",
      "page_type": "Implementation",
      "overview": "",
      "content": "== MultitaskPromptEmbedding ==\n\n=== Knowledge Sources ===\n* [https://github.com/huggingface/peft PEFT Repository]\n* [https://huggingface.co/papers/2303.02861 Multitask Prompt Tuning Paper]\n* [https://mit-ibm-watson-ai-lab.github.io MIT-IBM Watson AI Lab]\n\n=== Domains ===\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Prompt_Tuning]]\n[[Category:Multitask_Learning]]\n[[Category:Neural_Networks]]\n\n=== Overview ===\n\n==== Description ====\n'''MultitaskPromptEmbedding''' is a PyTorch neural network module that implements multitask prompt tuning embeddings. It extends the PromptEmbedding class to support multiple tasks simultaneously using a low-rank factorization approach.\n\nThe model represents task-specific prompts as the product of two low-rank matrices:\n* '''prefix_task_cols''': Task-specific column matrix (num_tasks \u00d7 total_virtual_tokens \u00d7 num_ranks)\n* '''prefix_task_rows''': Task-specific row matrix (num_tasks \u00d7 num_ranks \u00d7 token_dim)\n\nThis factorization allows efficient parameter sharing across tasks while maintaining task-specific adaptations. The architecture was developed at MIT-IBM Watson Research Lab as described in the paper [https://huggingface.co/papers/2303.02861 \"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning\"].\n\n==== Usage ====\nUsed as the embedding layer for multitask prompt tuning in PEFT models. It generates task-specific virtual token embeddings by combining shared prompt embeddings with task-specific transformations derived from low-rank matrices.\n\n=== Code Reference ===\n\n==== Source Location ====\n<code>src/peft/tuners/multitask_prompt_tuning/model.py</code>\n\n==== Signature ====\n<syntaxhighlight lang=\"python\">\nclass MultitaskPromptEmbedding(PromptEmbedding):\n    def __init__(self, config: MultitaskPromptTuningConfig, word_embeddings):\n        \"\"\"\n        Initialize multitask prompt embedding.\n\n        Args:\n            config: MultitaskPromptTuningConfig object\n            word_embeddings: Base model word embeddings\n        \"\"\"\n\n    def forward(self, indices, task_ids):\n        \"\"\"\n        Generate task-specific prompt embeddings.\n\n        Args:\n            indices: Token indices for prompt\n            task_ids: Task identifiers for batch samples\n\n        Returns:\n            Prompt embeddings modulated by task-specific factors\n        \"\"\"\n</syntaxhighlight>\n\n==== Import ====\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.multitask_prompt_tuning import MultitaskPromptEmbedding\n</syntaxhighlight>\n\n=== I/O Contract ===\n\n==== Constructor Parameters ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| config || MultitaskPromptTuningConfig || Configuration object specifying model parameters\n|-\n| word_embeddings || torch.nn.Embedding || Base model's word embedding layer\n|}\n\n==== Forward Method ====\n\n===== Inputs =====\n{| class=\"wikitable\"\n! Parameter !! Type !! Shape !! Description\n|-\n| indices || torch.Tensor || (batch_size, num_virtual_tokens) || Indices for prompt token embeddings\n|-\n| task_ids || torch.Tensor || (batch_size,) || Task identifiers for each sample in the batch\n|}\n\n===== Returns =====\n{| class=\"wikitable\"\n! Type !! Shape !! Description\n|-\n| torch.Tensor || (batch_size, num_virtual_tokens, token_dim) || Task-specific prompt embeddings\n|}\n\n==== Model Attributes ====\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| num_tasks || int || Number of tasks supported\n|-\n| num_ranks || int || Rank of the low-rank factorization\n|-\n| num_virtual_tokens || int || Number of virtual tokens per task\n|-\n| num_transformer_submodules || int || Number of transformer submodules (1 for encoder-only, 2 for encoder-decoder)\n|-\n| token_dim || int || Dimension of token embeddings\n|-\n| prefix_task_cols || torch.nn.Parameter || Task-specific column matrix (learned)\n|-\n| prefix_task_rows || torch.nn.Parameter || Task-specific row matrix (learned)\n|-\n| embedding || torch.nn.Embedding || Shared prompt embedding layer (inherited)\n|}\n\n==== Side Effects ====\n* Loads pretrained weights from state dict if using transfer learning initialization\n* Creates trainable parameters for task-specific low-rank matrices\n\n==== Exceptions ====\n* '''ValueError''': Raised if task_ids is None in forward pass\n* '''ValueError''': Raised if prompt_tuning_init_state_dict_path is None when using source task initialization\n\n=== Usage Examples ===\n\n==== Basic Usage ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import MultitaskPromptTuningConfig\nfrom peft.tuners.multitask_prompt_tuning import MultitaskPromptEmbedding\n\n# Create configuration\nconfig = MultitaskPromptTuningConfig(\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_tasks=5,\n    num_ranks=4,\n    num_transformer_submodules=2\n)\n\n# Create word embeddings (from base model)\nword_embeddings = torch.nn.Embedding(50000, 768)\n\n# Initialize multitask prompt embedding\nprompt_embedding = MultitaskPromptEmbedding(config, word_embeddings)\n\n# Forward pass\nbatch_size = 8\nindices = torch.arange(20).unsqueeze(0).expand(batch_size, -1)\ntask_ids = torch.randint(0, 5, (batch_size,))\nembeddings = prompt_embedding(indices, task_ids)\n\nprint(embeddings.shape)  # torch.Size([8, 20, 768])\n</syntaxhighlight>\n\n==== Transfer Learning from Source Tasks ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import MultitaskPromptTuningConfig\nfrom peft.tuners.multitask_prompt_tuning import MultitaskPromptEmbedding\n\n# Configuration with averaged source task initialization\nconfig = MultitaskPromptTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=10,\n    token_dim=768,\n    num_tasks=3,\n    num_ranks=2,\n    prompt_tuning_init=\"AVERAGE_SOURCE_TASKS\",\n    prompt_tuning_init_state_dict_path=\"path/to/source_checkpoint.pt\"\n)\n\nword_embeddings = torch.nn.Embedding(50000, 768)\nprompt_embedding = MultitaskPromptEmbedding(config, word_embeddings)\n\n# The model now has weights initialized from averaged source tasks\n</syntaxhighlight>\n\n==== Exact Source Task Initialization ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import MultitaskPromptTuningConfig\nfrom peft.tuners.multitask_prompt_tuning import MultitaskPromptEmbedding\n\n# Initialize from specific source task\nconfig = MultitaskPromptTuningConfig(\n    task_type=\"SEQ_CLS\",\n    num_virtual_tokens=15,\n    token_dim=1024,\n    num_tasks=4,\n    num_ranks=3,\n    prompt_tuning_init=\"EXACT_SOURCE_TASK\",\n    prompt_tuning_init_state_dict_path=\"source_model.safetensors\",\n    prompt_tuning_init_task=1  # Use source task 1\n)\n\nword_embeddings = torch.nn.Embedding(50000, 1024)\nprompt_embedding = MultitaskPromptEmbedding(config, word_embeddings)\n</syntaxhighlight>\n\n==== Understanding the Low-Rank Decomposition ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import MultitaskPromptTuningConfig\nfrom peft.tuners.multitask_prompt_tuning import MultitaskPromptEmbedding\n\nconfig = MultitaskPromptTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_tasks=3,\n    num_ranks=4\n)\n\nword_embeddings = torch.nn.Embedding(50000, 768)\nprompt_embedding = MultitaskPromptEmbedding(config, word_embeddings)\n\n# The model creates low-rank matrices\nprint(f\"Column matrix shape: {prompt_embedding.prefix_task_cols.shape}\")\n# Output: torch.Size([3, 20, 4])  # (num_tasks, total_virtual_tokens, num_ranks)\n\nprint(f\"Row matrix shape: {prompt_embedding.prefix_task_rows.shape}\")\n# Output: torch.Size([3, 4, 768])  # (num_tasks, num_ranks, token_dim)\n\n# During forward pass:\n# task_prompts = matmul(task_cols, task_rows)\n# This creates a (batch_size, total_virtual_tokens, token_dim) matrix\n# that modulates the shared prompt embeddings\n</syntaxhighlight>\n\n=== Related Pages ===\n* [[huggingface_peft_MultitaskPromptTuningConfig|MultitaskPromptTuningConfig]] - Configuration for this model\n* [[huggingface_peft_PromptEmbedding|PromptEmbedding]] - Parent class\n* [[huggingface_peft_PromptEncoder|PromptEncoder]] - Related P-tuning encoder\n* [[PEFT|Parameter-Efficient Fine-Tuning]]\n* [[Multitask_Learning|Multitask Learning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_OFTConfig",
      "page_title": "OFTConfig",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>OFTConfig</code> class is the configuration dataclass for Orthogonal Fine-Tuning (OFT) models in PEFT. It stores all parameters needed to configure and initialize an OFT model, including rank, block size, target modules, dropout, and various optimization options. OFT is a parameter-efficient fine-tuning method that applies orthogonal transformations to model weights, preserving their norm while allowing adaptation to new tasks. This configuration class provides comprehensive control over all aspects of OFT behavior. Key features: * Comprehensive parameter configuration for OFT * Validation of parameter combinations (r and oft_block_size) * Support for both standard and constrained OFT (COFT) variants * Cayley-Neumann parameterization for computational efficiency * Version compatibility checking for backwards compatibility * Module selection via target_modules and exclude_modules",
      "content": "= OFTConfig =\n\n== Knowledge Sources ==\n\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://arxiv.org/abs/2306.07280 OFT: Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n\n== Domains ==\n\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Orthogonal_Fine_Tuning]]\n[[Category:Configuration]]\n\n== Overview ==\n\n=== Description ===\n\nThe <code>OFTConfig</code> class is the configuration dataclass for Orthogonal Fine-Tuning (OFT) models in PEFT. It stores all parameters needed to configure and initialize an OFT model, including rank, block size, target modules, dropout, and various optimization options.\n\nOFT is a parameter-efficient fine-tuning method that applies orthogonal transformations to model weights, preserving their norm while allowing adaptation to new tasks. This configuration class provides comprehensive control over all aspects of OFT behavior.\n\nKey features:\n* Comprehensive parameter configuration for OFT\n* Validation of parameter combinations (r and oft_block_size)\n* Support for both standard and constrained OFT (COFT) variants\n* Cayley-Neumann parameterization for computational efficiency\n* Version compatibility checking for backwards compatibility\n* Module selection via target_modules and exclude_modules\n\n=== Usage ===\n\nThis configuration class is used to initialize OFT models through PEFT's <code>get_peft_model</code> function. It defines which modules to target, the rank/block size of the transformation, and various training-time behaviors.\n\n== Code Reference ==\n\n=== Source Location ===\n\nFile: <code>src/peft/tuners/oft/config.py</code>\n\nRepository: HuggingFace PEFT (Parameter-Efficient Fine-Tuning)\n\n=== Class: OFTConfig ===\n\n==== Signature ====\n\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass OFTConfig(PeftConfig):\n    r: int = 0\n    oft_block_size: int = 32\n    module_dropout: float = 0.0\n    target_modules: Optional[Union[list[str], str]] = None\n    fan_in_fan_out: bool = False\n    bias: Literal[\"none\", \"all\", \"oft_only\"] = \"none\"\n    exclude_modules: Optional[Union[list[str], str]] = None\n    init_weights: bool = True\n    layers_to_transform: Optional[Union[list[int], int]] = None\n    layers_pattern: Optional[Union[list[str], str]] = None\n    modules_to_save: Optional[list[str]] = None\n    coft: bool = False\n    eps: float = 6e-5\n    block_share: bool = False\n    use_cayley_neumann: bool = True\n    num_cayley_neumann_terms: int = 5\n</syntaxhighlight>\n\n==== Import ====\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig\n# or\nfrom peft.tuners.oft.config import OFTConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Configuration Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| r || int || 0 || OFT rank - number of OFT blocks per injected layer\n|-\n| oft_block_size || int || 32 || OFT block size across different layers (r \u00d7 oft_block_size = layer dimension)\n|-\n| module_dropout || float || 0.0 || Multiplicative dropout probability for randomly setting OFT blocks to identity\n|-\n| target_modules || Optional[Union[list[str], str]] || None || Names of modules to apply adapter to (regex or list, 'all-linear' for all linear layers)\n|-\n| fan_in_fan_out || bool || False || Set to True if layer stores weight as (fan_in, fan_out)\n|-\n| bias || Literal[\"none\", \"all\", \"oft_only\"] || \"none\" || Bias type - which biases to train\n|-\n| exclude_modules || Optional[Union[list[str], str]] || None || Names of modules to exclude from adaptation\n|-\n| init_weights || bool || True || Whether to initialize OFT weights\n|-\n| layers_to_transform || Optional[Union[list[int], int]] || None || Specific layer indices to transform\n|-\n| layers_pattern || Optional[Union[list[str], str]] || None || Layer pattern name (e.g., 'layers', 'h') for layers_to_transform\n|-\n| modules_to_save || Optional[list[str]] || None || Modules to set as trainable and save (e.g., classifier heads)\n|-\n| coft || bool || False || Whether to use constrained OFT variant\n|-\n| eps || float || 6e-5 || Control strength for COFT - freedom of rotation (only when coft=True)\n|-\n| block_share || bool || False || Whether to share OFT parameters between blocks\n|-\n| use_cayley_neumann || bool || True || Whether to use Cayley-Neumann formulation for efficiency\n|-\n| num_cayley_neumann_terms || int || 5 || Number of terms in Cayley-Neumann approximation\n|}\n\n=== Validation Rules ===\n\nThe <code>__post_init__</code> method enforces several validation rules:\n\n1. Either <code>r</code> or <code>oft_block_size</code> must be non-zero\n2. Only one of <code>r</code> or <code>oft_block_size</code> can be specified (XOR constraint)\n3. If <code>layers_pattern</code> is specified, <code>layers_to_transform</code> must also be specified\n\n== Usage Examples ==\n\n=== Basic Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Create basic OFT configuration\noft_config = OFTConfig(\n    r=8,  # 8 OFT blocks\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.1,\n)\n\n# Apply to model\nmodel = AutoModelForCausalLM.from_pretrained(\"model-name\")\npeft_model = get_peft_model(model, oft_config)\n</syntaxhighlight>\n\n=== Using Block Size Instead of Rank ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig\n\n# Specify block size instead of rank\n# For a layer with 768 dimensions and block_size=32, r will be 768/32 = 24\noft_config = OFTConfig(\n    oft_block_size=32,  # Block size\n    r=0,  # Must be 0 when using oft_block_size\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)\n</syntaxhighlight>\n\n=== Constrained OFT (COFT) ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig\n\n# Configure constrained OFT for better control\noft_config = OFTConfig(\n    r=16,\n    target_modules=\"all-linear\",  # Target all linear layers\n    coft=True,  # Enable constrained OFT\n    eps=1e-4,   # Control freedom of rotation\n    module_dropout=0.05,\n)\n</syntaxhighlight>\n\n=== Advanced Configuration with Layer Selection ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig\n\n# Only transform specific layers\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    exclude_modules=[\"lm_head\"],  # Exclude output layer\n    layers_to_transform=[0, 1, 2, 3],  # Only first 4 layers\n    layers_pattern=\"layers\",  # Pattern for nn.ModuleList name\n    modules_to_save=[\"classifier\"],  # Save classifier separately\n)\n</syntaxhighlight>\n\n=== Cayley-Neumann Optimization ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig\n\n# Configure Cayley-Neumann parameterization\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    use_cayley_neumann=True,  # Enable for computational efficiency\n    num_cayley_neumann_terms=7,  # More terms = better orthogonality, slower\n    block_share=False,\n)\n</syntaxhighlight>\n\n=== Block Sharing Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig\n\n# Share parameters between blocks for memory efficiency\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    block_share=True,  # Share OFT parameters between blocks\n    module_dropout=0.0,  # Dropout not compatible with block_share=True\n)\n</syntaxhighlight>\n\n=== Diffusion Model Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig\n\n# Configuration for diffusion models (e.g., Stable Diffusion)\nconfig_unet = OFTConfig(\n    r=8,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    module_dropout=0.0,\n    init_weights=True,\n)\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Parameter Initialization ===\n\nAfter validation in <code>__post_init__</code>, the configuration:\n1. Sets <code>peft_type</code> to <code>PeftType.OFT</code>\n2. Converts <code>target_modules</code> and <code>exclude_modules</code> to sets if they are lists\n\n=== Version Compatibility ===\n\nThe <code>check_kwargs</code> class method performs version compatibility checks:\n\n1. Checks for <code>oft_block_size</code> parameter (added in PEFT 0.14.0)\n2. For <code>use_cayley_neumann</code>, checks version >= 0.18.0 (parameterization changed for numerical stability)\n\n=== Mutually Exclusive Parameters ===\n\nThe configuration enforces that exactly one of <code>r</code> or <code>oft_block_size</code> is non-zero:\n\n<syntaxhighlight lang=\"python\">\nif not (self.r != 0) ^ (self.oft_block_size != 0):\n    raise ValueError(\n        f\"You can only specify either r ({self.r}) or \"\n        f\"oft_block_size ({self.oft_block_size}), but not both \"\n        f\"simultaneously, because r x oft_block_size == in_features.\"\n    )\n</syntaxhighlight>\n\n=== Bias Training Options ===\n\nThree options for bias training:\n* <code>\"none\"</code>: No bias training (default)\n* <code>\"all\"</code>: Train all biases in the model\n* <code>\"oft_only\"</code>: Train only biases in OFT-adapted layers\n\n== Related Pages ==\n\n* [[huggingface_peft_OFTModel|OFTModel]] - Main OFT model implementation using this config\n* [[huggingface_peft_OFT_AQLM|OFT AQLM Integration]] - AQLM quantized OFT layers\n* [[huggingface_peft_OFT_AWQ|OFT AWQ Integration]] - AWQ quantized OFT layers\n* [[huggingface_peft_OFT_GPTQ|OFT GPTQ Integration]] - GPTQ quantized OFT layers\n* [[huggingface_peft_OFT_EETQ|OFT EETQ Integration]] - EETQ quantized OFT layers\n* [[huggingface_peft_OFT_HQQ|OFT HQQ Integration]] - HQQ quantized OFT layers\n\n== See Also ==\n\n* OFT Paper: [https://arxiv.org/abs/2306.07280 Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* PEFT Documentation: [https://huggingface.co/docs/peft HuggingFace PEFT Docs]\n* Configuration Guide: [https://huggingface.co/docs/peft/main/en/package_reference/config PEFT Config Reference]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_OFTLayer",
      "page_title": "huggingface peft OFTLayer",
      "page_type": "Implementation",
      "overview": "Orthogonal Fine-Tuning layer that applies block-diagonal orthogonal transformations to model weights using Cayley parameterization for stable, geometry-preserving adaptation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|OFT|https://arxiv.org/abs/2306.07280]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Orthogonal_Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nOrthogonal Fine-Tuning layer that applies block-diagonal orthogonal transformations to model weights using Cayley parameterization for stable, geometry-preserving adaptation.\n\n=== Description ===\n\nOFTLayer implements orthogonal transformations using block-diagonal matrices. The layer learns skew-symmetric matrices that are transformed via Cayley parameterization into orthogonal matrices. This ensures the transformation is always orthogonal, preserving the geometry of the weight space. The layer supports constrained OFT (COFT) for additional regularization, block sharing for parameter reduction, and multiplicative dropout for regularization during training.\n\n=== Usage ===\n\nUse OFT when you want to preserve the geometric properties of pretrained weights during fine-tuning. OFT is particularly effective for vision models and when you need stable training without the risk of catastrophic forgetting. The block_share option allows using a single orthogonal matrix across all blocks for extreme parameter efficiency.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/oft/layer.py src/peft/tuners/oft/layer.py]\n* '''Lines:''' 1-951\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass OFTLayer(BaseTunerLayer):\n    \"\"\"\n    Orthogonal Fine-Tuning layer.\n\n    Attributes:\n        oft_R: OFTRotationModule containing orthogonal parameters\n        r: Number of blocks per adapter\n        oft_block_size: Size of each orthogonal block\n        oft_dropout: Multiplicative dropout layers\n    \"\"\"\n    adapter_layer_names = (\"oft_R\",)\n    other_param_names = (\"r\", \"oft_block_size\", \"oft_dropout\")\n\n    def update_layer(\n        self,\n        adapter_name: str,\n        r: int,\n        oft_block_size: int,\n        module_dropout: float,\n        coft: bool,\n        eps: float,\n        block_share: bool,\n        init_weights: bool,\n        use_cayley_neumann: bool,\n        num_cayley_neumann_terms: int,\n        inference_mode: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Create OFT rotation module.\"\"\"\n\nclass OFTRotationModule(nn.Module):\n    \"\"\"Module containing orthogonal rotation parameters and Cayley transform.\"\"\"\n    def __init__(\n        self,\n        r: int,\n        n_elements: int,\n        block_size: int,\n        in_features: int,\n        coft: bool = False,\n        eps: float = 6e-5,\n        block_share: bool = False,\n        use_cayley_neumann: bool = True,\n        num_cayley_neumann_terms: int = 5,\n    ):\n        \"\"\"Initialize rotation module.\"\"\"\n\n    def get_weight(self) -> torch.Tensor:\n        \"\"\"Compute block-diagonal orthogonal matrix.\"\"\"\n\nclass Linear(nn.Module, OFTLayer):\n    \"\"\"OFT implemented in Linear layer.\"\"\"\n\nclass Conv2d(nn.Module, OFTLayer):\n    \"\"\"OFT implemented in Conv2d layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft import OFTLayer, OFTConfig, OFTModel\nfrom peft import OFTConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained layer (Linear or Conv2d)\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| r || int || Yes* || Number of blocks (set one of r or oft_block_size)\n|-\n| oft_block_size || int || Yes* || Size of each orthogonal block\n|-\n| coft || bool || No || Use constrained OFT (default: False)\n|-\n| eps || float || No || COFT constraint strength (default: 6e-5)\n|-\n| block_share || bool || No || Share rotation across blocks (default: False)\n|-\n| module_dropout || float || No || Multiplicative dropout probability\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input rotated by orthogonal transformation\n|-\n| get_delta_weight() || torch.Tensor || Block-diagonal orthogonal matrix\n|}\n\n== Usage Examples ==\n\n=== Basic OFT Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure OFT with block size\nconfig = OFTConfig(\n    r=8,                   # Number of blocks (alternative to oft_block_size)\n    # oft_block_size=512,  # Or specify block size directly\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.0,\n    coft=False,\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Constrained OFT (COFT) ===\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\n\n# Use COFT for additional regularization\nconfig = OFTConfig(\n    r=4,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    coft=True,             # Enable constrained OFT\n    eps=6e-5,              # Constraint strength (higher = more regularization)\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== OFT with Block Sharing ===\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\n\n# Share orthogonal matrix across all blocks for minimal parameters\nconfig = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    block_share=True,      # Use single rotation for all blocks\n)\n\nmodel = get_peft_model(model, config)\n# Dramatically fewer parameters with shared blocks\n</syntaxhighlight>\n\n=== OFT for Vision Models ===\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\nfrom transformers import AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\"\n)\n\n# OFT works well for vision fine-tuning\nconfig = OFTConfig(\n    r=4,\n    target_modules=[\"query\", \"value\"],\n    module_dropout=0.1,    # Add dropout regularization\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Orthogonal Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "OFT",
          "url": "https://arxiv.org/abs/2306.07280"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_OFTModel",
      "page_title": "OFTModel",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>OFTModel</code> class is the main implementation of Orthogonal Fine-Tuning (OFT) in the PEFT library. It extends <code>BaseTuner</code> to create and manage OFT adapter layers attached to a pre-trained model. OFT is a parameter-efficient fine-tuning method that applies orthogonal transformations to model weights, preserving their norm while enabling task-specific adaptation. The class handles the creation, replacement, and management of OFT layers across different module types and quantization backends. It provides a unified interface for applying OFT to various model architectures and supports multiple quantization methods including GPTQ, AQLM, AWQ, EETQ, HQQ, and bits-and-bytes (BNB). Key features: * Automatic detection and replacement of target modules with OFT layers * Support for multiple quantization backends * Dispatcher system for selecting appropriate OFT implementation per layer type * Configuration-based module targeting (target_modules, exclude_modules) * Layer-specific transformation via layers_to_transform * Merge capability checking for different backends * Integration with HuggingFace Transformers models",
      "content": "= OFTModel =\n\n== Knowledge Sources ==\n\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://arxiv.org/abs/2306.07280 OFT: Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n\n== Domains ==\n\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Orthogonal_Fine_Tuning]]\n[[Category:Model_Architecture]]\n\n== Overview ==\n\n=== Description ===\n\nThe <code>OFTModel</code> class is the main implementation of Orthogonal Fine-Tuning (OFT) in the PEFT library. It extends <code>BaseTuner</code> to create and manage OFT adapter layers attached to a pre-trained model. OFT is a parameter-efficient fine-tuning method that applies orthogonal transformations to model weights, preserving their norm while enabling task-specific adaptation.\n\nThe class handles the creation, replacement, and management of OFT layers across different module types and quantization backends. It provides a unified interface for applying OFT to various model architectures and supports multiple quantization methods including GPTQ, AQLM, AWQ, EETQ, HQQ, and bits-and-bytes (BNB).\n\nKey features:\n* Automatic detection and replacement of target modules with OFT layers\n* Support for multiple quantization backends\n* Dispatcher system for selecting appropriate OFT implementation per layer type\n* Configuration-based module targeting (target_modules, exclude_modules)\n* Layer-specific transformation via layers_to_transform\n* Merge capability checking for different backends\n* Integration with HuggingFace Transformers models\n\n=== Usage ===\n\nThe <code>OFTModel</code> is typically instantiated through PEFT's <code>get_peft_model</code> function, which automatically creates and attaches OFT adapters to a pre-trained model based on the provided <code>OFTConfig</code>.\n\n== Code Reference ==\n\n=== Source Location ===\n\nFile: <code>src/peft/tuners/oft/model.py</code>\n\nRepository: HuggingFace PEFT (Parameter-Efficient Fine-Tuning)\n\n=== Class: OFTModel ===\n\n==== Signature ====\n\n<syntaxhighlight lang=\"python\">\nclass OFTModel(BaseTuner):\n    prefix: str = \"oft_\"\n    tuner_layer_cls = OFTLayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_OFT_TARGET_MODULES_MAPPING\n</syntaxhighlight>\n\n==== Import ====\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTModel, OFTConfig\n# or\nfrom peft.tuners.oft.model import OFTModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Constructor (Implicit via get_peft_model) ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| model || torch.nn.Module || The pretrained model to which adapter layers will be attached\n|-\n| config || OFTConfig || Configuration object for OFT\n|-\n| adapter_name || str || Name of the adapter (default: \"default\")\n|-\n| low_cpu_mem_usage || bool || Create empty adapter weights on meta device (default: False)\n|}\n\n=== Key Methods ===\n\n==== _create_and_replace ====\n\nInternal method to create and replace target modules with OFT layers.\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| oft_config || OFTConfig || OFT configuration\n|-\n| adapter_name || str || Name for the adapter\n|-\n| target || torch.nn.Module || The module to replace\n|-\n| target_name || str || Name of the target module\n|-\n| parent || torch.nn.Module || Parent module containing the target\n|-\n| current_key || str || Full key path to the module\n|}\n\n==== _create_new_module ====\n\nStatic method to create a new OFT module using the dispatcher system.\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| oft_config || OFTConfig || OFT configuration\n|-\n| adapter_name || str || Name for the adapter\n|-\n| target || torch.nn.Module || The module to wrap\n|-\n| **kwargs || Any || Additional keyword arguments\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || torch.nn.Module || The newly created OFT-wrapped module\n|}\n\n==== _check_merge_allowed ====\n\nVerifies that the configuration supports merging.\n\nRaises <code>ValueError</code> if:\n* Model is GPTQ quantized\n* Base model layers are replicated\n\n== Usage Examples ==\n\n=== Basic Usage ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load a pre-trained model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Configure OFT\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"c_attn\"],  # Target attention projection\n    module_dropout=0.1,\n)\n\n# Create OFT model - automatically wraps with OFTModel\npeft_model = get_peft_model(model, oft_config)\n\n# Model now has OFT adapters attached\nprint(peft_model)\n</syntaxhighlight>\n\n=== Diffusion Model Example ===\n\n<syntaxhighlight lang=\"python\">\nfrom diffusers import StableDiffusionPipeline\nfrom peft import OFTModel, OFTConfig\n\n# Configure OFT for text encoder\nconfig_te = OFTConfig(\n    r=8,\n    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Configure OFT for UNet\nconfig_unet = OFTConfig(\n    r=8,\n    target_modules=[\n        \"proj_in\",\n        \"proj_out\",\n        \"to_k\",\n        \"to_q\",\n        \"to_v\",\n        \"to_out.0\",\n        \"ff.net.0.proj\",\n        \"ff.net.2\",\n    ],\n    module_dropout=0.0,\n    init_weights=True,\n)\n\n# Load model\nmodel = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n\n# Apply OFT to both text encoder and UNet\nmodel.text_encoder = OFTModel(model.text_encoder, config_te, \"default\")\nmodel.unet = OFTModel(model.unet, config_unet, \"default\")\n\n# Fine-tune the model\n# ... training code ...\n</syntaxhighlight>\n\n=== With Quantized Models ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import get_peft_model, OFTConfig\n\n# Load model with 8-bit quantization\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n)\n\n# Configure OFT\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.1,\n)\n\n# Apply OFT - automatically uses BNB 8-bit OFT layers\npeft_model = get_peft_model(model, oft_config)\n</syntaxhighlight>\n\n=== Layer-Specific Transformation ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\n\n# Only transform first 4 transformer layers\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=[0, 1, 2, 3],  # First 4 layers only\n    layers_pattern=\"layers\",  # Pattern for nn.ModuleList\n)\n\npeft_model = get_peft_model(model, oft_config)\n</syntaxhighlight>\n\n=== Multiple Adapters ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, OFTConfig\n\n# Add first adapter\noft_config1 = OFTConfig(r=8, target_modules=[\"q_proj\", \"v_proj\"])\npeft_model = get_peft_model(model, oft_config1, adapter_name=\"task1\")\n\n# Add second adapter\noft_config2 = OFTConfig(r=16, target_modules=[\"q_proj\", \"v_proj\"])\npeft_model.add_adapter(\"task2\", oft_config2)\n\n# Switch between adapters\npeft_model.set_adapter(\"task1\")\noutput1 = peft_model(input_ids)\n\npeft_model.set_adapter(\"task2\")\noutput2 = peft_model(input_ids)\n</syntaxhighlight>\n\n=== Checking Merge Capability ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, OFTConfig\n\n# GPTQ model - merging not allowed\ngptq_model = AutoModelForCausalLM.from_pretrained(\n    \"model-name\",\n    quantization_config={\"method\": \"gptq\"}\n)\npeft_model = get_peft_model(gptq_model, oft_config)\n\n# Try to merge\ntry:\n    peft_model.merge_adapter()\nexcept ValueError as e:\n    print(e)  # \"Cannot merge OFT layers when the model is gptq quantized\"\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Dispatcher System ===\n\nThe <code>_create_new_module</code> method uses a dispatcher system to select the appropriate OFT implementation:\n\n1. '''BNB 8-bit''' (if available): <code>dispatch_bnb_8bit</code>\n2. '''BNB 4-bit''' (if available): <code>dispatch_bnb_4bit</code>\n3. '''EETQ''': <code>dispatch_eetq</code>\n4. '''AQLM''': <code>dispatch_aqlm</code>\n5. '''AWQ''': <code>dispatch_awq</code>\n6. '''GPTQ''': <code>dispatch_gptq</code>\n7. '''HQQ''': <code>dispatch_hqq</code>\n8. '''INC''': <code>dispatch_inc</code>\n9. '''Default''' (Linear, Conv2d): <code>dispatch_default</code>\n\nThe first dispatcher that returns a non-None module is used.\n\n=== Quantization Configuration Handling ===\n\nThe class automatically detects and passes quantization configuration to dispatchers:\n\n<syntaxhighlight lang=\"python\">\nquant_methods = [\"gptq\", \"aqlm\", \"awq\"]\nfor quant_method in quant_methods:\n    quantization_config = get_quantization_config(self.model, method=quant_method)\n    if quantization_config is not None:\n        kwargs[f\"{quant_method}_quantization_config\"] = quantization_config\n</syntaxhighlight>\n\n=== Module Replacement Logic ===\n\nWhen creating/replacing modules:\n\n1. Check if target is already an OFTLayer\n2. If not, create new module via dispatcher system\n3. If newly added adapter (not in active_adapters), set requires_grad=False\n4. Replace module in parent\n5. If already OFTLayer, update existing layer with new adapter\n\n=== Merge Restrictions ===\n\nThe <code>_check_merge_allowed</code> method enforces:\n\n1. Call parent class check (general BaseTuner restrictions)\n2. No GPTQ quantization:\n<syntaxhighlight lang=\"python\">\nif getattr(self.model, \"quantization_method\", None) == \"gptq\":\n    raise ValueError(\"Cannot merge OFT layers when the model is gptq quantized\")\n</syntaxhighlight>\n\n3. No layer replication:\n<syntaxhighlight lang=\"python\">\nif self.peft_config.get(\"layer_replication\"):\n    raise ValueError(\"Cannot merge OFT layers when base model layers are replicated\")\n</syntaxhighlight>\n\n=== Target Module Mapping ===\n\nThe class uses <code>TRANSFORMERS_MODELS_TO_OFT_TARGET_MODULES_MAPPING</code> to automatically determine target modules for known architectures when <code>target_modules</code> is not explicitly specified.\n\n=== Low CPU Memory Mode ===\n\nWhen <code>low_cpu_mem_usage=True</code>, adapter weights are created on meta device to speed up loading, particularly useful for large models.\n\n== Related Pages ==\n\n* [[huggingface_peft_OFTConfig|OFTConfig]] - Configuration class for OFT\n* [[huggingface_peft_OFT_AQLM|OFT AQLM Integration]] - AQLM quantized OFT layers\n* [[huggingface_peft_OFT_AWQ|OFT AWQ Integration]] - AWQ quantized OFT layers\n* [[huggingface_peft_OFT_GPTQ|OFT GPTQ Integration]] - GPTQ quantized OFT layers\n* [[huggingface_peft_OFT_EETQ|OFT EETQ Integration]] - EETQ quantized OFT layers\n* [[huggingface_peft_OFT_HQQ|OFT HQQ Integration]] - HQQ quantized OFT layers\n* [[huggingface_peft_OFT_IntelFP8|OFT Intel FP8 Integration]] - Intel Neural Compressor integration\n\n== See Also ==\n\n* OFT Paper: [https://arxiv.org/abs/2306.07280 Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* PEFT Documentation: [https://huggingface.co/docs/peft HuggingFace PEFT Docs]\n* BaseTuner: Base class for all PEFT tuner implementations\n* OFTLayer: Base layer class for OFT implementations\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_OFTQuantized",
      "page_title": "huggingface peft OFTQuantized",
      "page_type": "Implementation",
      "overview": "Quantized OFT layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling orthogonal fine-tuning on quantized models.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|bitsandbytes|https://github.com/bitsandbytes-foundation/bitsandbytes]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nQuantized OFT layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling orthogonal fine-tuning on quantized models.\n\n=== Description ===\n\nLinear8bitLt and Linear4bit implement OFT (Orthogonal Fine-Tuning) for bitsandbytes quantized layers. The layers apply orthogonal transformations to inputs before the quantized linear operation. During merge, weights are dequantized, transformed, and requantized. The implementation handles the conversion between quantized and float formats for both forward pass and weight merging operations.\n\n=== Usage ===\n\nUse OFT quantized layers when fine-tuning quantized models (loaded with load_in_8bit or load_in_4bit). The layers are automatically dispatched via dispatch_bnb_8bit and dispatch_bnb_4bit when target layers are bitsandbytes Linear8bitLt or Linear4bit. Note that merging may introduce rounding errors due to quantization.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/oft/bnb.py src/peft/tuners/oft/bnb.py]\n* '''Lines:''' 1-389\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass Linear8bitLt(torch.nn.Module, OFTLayer):\n    \"\"\"OFT for 8-bit quantized layers.\"\"\"\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        r: int = 8,\n        oft_block_size: int = 0,\n        module_dropout: float = 0.0,\n        coft: bool = False,\n        eps: float = 6e-5,\n        block_share: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize OFT for 8-bit layer.\"\"\"\n\n    def merge(self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None):\n        \"\"\"Merge OFT into dequantized weights, then requantize.\"\"\"\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        \"\"\"Apply OFT rotation then 8-bit linear.\"\"\"\n\nclass Linear4bit(torch.nn.Module, OFTLayer):\n    \"\"\"OFT for 4-bit quantized layers.\"\"\"\n\ndef dispatch_bnb_8bit(target, adapter_name, **kwargs):\n    \"\"\"Dispatch OFT for 8-bit layers.\"\"\"\n\ndef dispatch_bnb_4bit(target, adapter_name, **kwargs):\n    \"\"\"Dispatch OFT for 4-bit layers.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.bnb import Linear8bitLt, Linear4bit\nfrom peft.tuners.oft.bnb import dispatch_bnb_8bit, dispatch_bnb_4bit\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || bnb.nn.Linear8bitLt/Linear4bit || Yes || Quantized base layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| r || int || Yes || Number of OFT blocks\n|-\n| oft_block_size || int || No || Size of each orthogonal block\n|-\n| coft || bool || No || Use constrained OFT\n|-\n| block_share || bool || No || Share rotation across blocks\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || OFT-rotated input through quantized layer\n|-\n| get_delta_weight() || torch.Tensor || Orthogonal rotation matrix\n|}\n\n== Usage Examples ==\n\n=== OFT with 4-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import OFTConfig, get_peft_model\n\n# Load model in 4-bit\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n)\n\n# Apply OFT to quantized model\nconfig = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n# OFT layers automatically use Linear4bit class\n</syntaxhighlight>\n\n=== OFT with 8-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import OFTConfig, get_peft_model\n\n# Load model in 8-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    load_in_8bit=True,\n)\n\nconfig = OFTConfig(\n    r=4,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    coft=True,\n)\n\nmodel = get_peft_model(model, config)\n# OFT layers automatically use Linear8bitLt class\n</syntaxhighlight>\n\n=== Merging Quantized OFT ===\n<syntaxhighlight lang=\"python\">\n# Warning: Merging may introduce rounding errors\nmodel.merge_adapter()\n\n# For inference without adapter overhead:\n# OFT rotation is applied to dequantized weights\n# then requantized back to 4-bit/8-bit\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "bitsandbytes",
          "url": "https://github.com/bitsandbytes-foundation/bitsandbytes"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_OFT_AQLM",
      "page_title": "OFT AQLM Integration",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>AqlmOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for AQLM (Additive Quantization for Large Models) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with AQLM, combining the benefits of both orthogonal fine-tuning and extreme model compression. AQLM is a quantization method that uses additive quantization techniques to achieve extreme compression of large language models. The OFT adapter wraps AQLM's <code>QuantizedLinear</code> layers, allowing fine-tuning while maintaining the quantized model structure. Key features: * Supports AQLM quantized models with OFT adapters * Preserves quantization during fine-tuning * Applies orthogonal transformations before quantized computation * Handles dtype conversions appropriately for autocast scenarios",
      "content": "= OFT AQLM Integration =\n\n== Knowledge Sources ==\n\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://arxiv.org/abs/2306.07280 OFT: Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* [https://arxiv.org/abs/2306.12929 AQLM: Extreme Compression of Large Language Models via Additive Quantization]\n\n== Domains ==\n\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Orthogonal_Fine_Tuning]]\n[[Category:Quantization]]\n[[Category:AQLM]]\n\n== Overview ==\n\n=== Description ===\n\nThe <code>AqlmOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for AQLM (Additive Quantization for Large Models) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with AQLM, combining the benefits of both orthogonal fine-tuning and extreme model compression.\n\nAQLM is a quantization method that uses additive quantization techniques to achieve extreme compression of large language models. The OFT adapter wraps AQLM's <code>QuantizedLinear</code> layers, allowing fine-tuning while maintaining the quantized model structure.\n\nKey features:\n* Supports AQLM quantized models with OFT adapters\n* Preserves quantization during fine-tuning\n* Applies orthogonal transformations before quantized computation\n* Handles dtype conversions appropriately for autocast scenarios\n\n=== Usage ===\n\nThis module is used internally by the PEFT library when applying OFT to AQLM-quantized models. The dispatcher function <code>dispatch_aqlm</code> automatically detects AQLM quantized layers and wraps them with the appropriate OFT adapter.\n\n== Code Reference ==\n\n=== Source Location ===\n\nFile: <code>src/peft/tuners/oft/aqlm.py</code>\n\nRepository: HuggingFace PEFT (Parameter-Efficient Fine-Tuning)\n\n=== Class: AqlmOFTLinear ===\n\n==== Signature ====\n\n<syntaxhighlight lang=\"python\">\nclass AqlmOFTLinear(torch.nn.Module, OFTLayer):\n    def __init__(\n        self,\n        base_layer,\n        adapter_name: str,\n        r: int = 0,\n        oft_block_size: int = 32,\n        module_dropout: float = 0.0,\n        init_weights: bool = True,\n        coft: bool = False,\n        eps: float = 6e-5,\n        block_share: bool = False,\n        fan_in_fan_out: bool = False,\n        use_cayley_neumann: bool = False,\n        num_cayley_neumann_terms: int = 5,\n        **kwargs,\n    )\n</syntaxhighlight>\n\n==== Import ====\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.aqlm import AqlmOFTLinear, dispatch_aqlm\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Constructor Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || Module || required || The AQLM quantized linear layer to wrap\n|-\n| adapter_name || str || required || Name identifier for this adapter\n|-\n| r || int || 0 || OFT rank (number of OFT blocks per injected layer)\n|-\n| oft_block_size || int || 32 || Size of OFT blocks across different layers\n|-\n| module_dropout || float || 0.0 || Multiplicative dropout probability for OFT blocks\n|-\n| init_weights || bool || True || Whether to initialize OFT weights\n|-\n| coft || bool || False || Whether to use constrained OFT variant\n|-\n| eps || float || 6e-5 || Control strength for COFT (only used if coft=True)\n|-\n| block_share || bool || False || Whether to share OFT parameters between blocks\n|-\n| fan_in_fan_out || bool || False || Set to True if layer stores weights as (fan_in, fan_out)\n|-\n| use_cayley_neumann || bool || False || Whether to use Cayley-Neumann formulation\n|-\n| num_cayley_neumann_terms || int || 5 || Number of terms in Cayley-Neumann approximation\n|}\n\n=== Forward Method ===\n\n{| class=\"wikitable\"\n! Input !! Type !! Description\n|-\n| x || torch.Tensor || Input tensor to the layer\n|}\n\n{| class=\"wikitable\"\n! Output !! Type !! Description\n|-\n| result || torch.Tensor || Output tensor after applying OFT transformation and quantized computation\n|}\n\n=== dispatch_aqlm Function ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The target module to potentially wrap\n|-\n| adapter_name || str || Name for the adapter\n|-\n| **kwargs || Any || Additional keyword arguments passed to constructor\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || AqlmOFTLinear instance if target is AQLM quantized, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with PEFT ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load a model quantized with AQLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model-name\",\n    quantization_config={\"method\": \"aqlm\"}\n)\n\n# Configure OFT\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.1,\n    coft=False,\n)\n\n# Apply OFT - automatically uses AqlmOFTLinear for AQLM layers\npeft_model = get_peft_model(model, oft_config)\n\n# Fine-tune the model\n# ... training code ...\n</syntaxhighlight>\n\n=== Explicit Dispatcher Usage ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.aqlm import dispatch_aqlm\nfrom aqlm import QuantizedLinear\n\n# Assume we have an AQLM quantized layer\naqlm_layer = QuantizedLinear(...)\n\n# Dispatch to create OFT adapter\noft_layer = dispatch_aqlm(\n    target=aqlm_layer,\n    adapter_name=\"default\",\n    r=8,\n    oft_block_size=32,\n    module_dropout=0.1\n)\n\n# Use the OFT-wrapped layer\noutput = oft_layer(input_tensor)\n</syntaxhighlight>\n\n=== Advanced Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig\n\n# Configure OFT with advanced options\noft_config = OFTConfig(\n    r=16,\n    oft_block_size=0,  # Will be computed from r\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    module_dropout=0.05,\n    coft=True,  # Use constrained OFT\n    eps=1e-4,\n    block_share=False,\n    use_cayley_neumann=True,\n    num_cayley_neumann_terms=7,\n)\n\n# Apply to AQLM quantized model\npeft_model = get_peft_model(aqlm_quantized_model, oft_config)\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Forward Pass Logic ===\n\nThe forward method implements the following logic:\n\n1. Check if adapters are disabled - if so, bypass OFT and use base layer directly\n2. For each active adapter:\n   * Apply dtype conversion if not in autocast mode\n   * Apply the OFT rotation transformation (oft_R)\n3. Pass transformed input through the AQLM quantized base layer\n4. Convert result back to expected dtype if needed\n\n=== Quantization Weight Handling ===\n\nThe dispatcher updates the <code>qweight</code> attribute to reference the AQLM quantized weights:\n\n<syntaxhighlight lang=\"python\">\ntarget.qweight = target_base_layer.codes\n</syntaxhighlight>\n\nThis ensures proper access to the quantized weight codes used by AQLM.\n\n=== Merging Not Supported ===\n\nUnlike some other OFT implementations, merging OFT weights into AQLM quantized weights is not supported due to the complexity of the quantization scheme.\n\n== Related Pages ==\n\n* [[huggingface_peft_OFTConfig|OFTConfig]] - Configuration class for OFT\n* [[huggingface_peft_OFTModel|OFTModel]] - Main OFT model implementation\n* [[huggingface_peft_OFT_AWQ|OFT AWQ Integration]] - Similar integration for AWQ quantization\n* [[huggingface_peft_OFT_GPTQ|OFT GPTQ Integration]] - Similar integration for GPTQ quantization\n* [[huggingface_peft_OFT_HQQ|OFT HQQ Integration]] - Similar integration for HQQ quantization\n\n== See Also ==\n\n* OFT Paper: [https://arxiv.org/abs/2306.07280 Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* AQLM Paper: [https://arxiv.org/abs/2306.12929 Extreme Compression via Additive Quantization]\n* PEFT Documentation: [https://huggingface.co/docs/peft HuggingFace PEFT Docs]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_OFT_AWQ",
      "page_title": "OFT AWQ Integration",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>AwqOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for AWQ (Activation-aware Weight Quantization) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with AWQ, allowing practitioners to fine-tune compressed models while maintaining their memory efficiency. AWQ is a quantization method that uses activation-aware techniques to achieve high-quality quantization with minimal accuracy loss. The OFT adapter wraps AWQ's <code>WQLinear_GEMM</code> layers, allowing fine-tuning while preserving the quantized structure. Key features: * Supports AWQ quantized models with OFT adapters * Version checking to ensure compatibility (requires autoawq >= 0.2.0) * Preserves quantization during fine-tuning * Applies orthogonal transformations before quantized computation * Handles dtype conversions for autocast scenarios",
      "content": "= OFT AWQ Integration =\n\n== Knowledge Sources ==\n\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://arxiv.org/abs/2306.07280 OFT: Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* [https://arxiv.org/abs/2306.00978 AWQ: Activation-aware Weight Quantization]\n\n== Domains ==\n\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Orthogonal_Fine_Tuning]]\n[[Category:Quantization]]\n[[Category:AWQ]]\n\n== Overview ==\n\n=== Description ===\n\nThe <code>AwqOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for AWQ (Activation-aware Weight Quantization) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with AWQ, allowing practitioners to fine-tune compressed models while maintaining their memory efficiency.\n\nAWQ is a quantization method that uses activation-aware techniques to achieve high-quality quantization with minimal accuracy loss. The OFT adapter wraps AWQ's <code>WQLinear_GEMM</code> layers, allowing fine-tuning while preserving the quantized structure.\n\nKey features:\n* Supports AWQ quantized models with OFT adapters\n* Version checking to ensure compatibility (requires autoawq >= 0.2.0)\n* Preserves quantization during fine-tuning\n* Applies orthogonal transformations before quantized computation\n* Handles dtype conversions for autocast scenarios\n\n=== Usage ===\n\nThis module is used internally by the PEFT library when applying OFT to AWQ-quantized models. The dispatcher function <code>dispatch_awq</code> automatically detects AWQ quantized layers and wraps them with the appropriate OFT adapter.\n\n== Code Reference ==\n\n=== Source Location ===\n\nFile: <code>src/peft/tuners/oft/awq.py</code>\n\nRepository: HuggingFace PEFT (Parameter-Efficient Fine-Tuning)\n\n=== Class: AwqOFTLinear ===\n\n==== Signature ====\n\n<syntaxhighlight lang=\"python\">\nclass AwqOFTLinear(torch.nn.Module, OFTLayer):\n    def __init__(\n        self,\n        base_layer,\n        adapter_name,\n        r: int = 0,\n        oft_block_size: int = 32,\n        module_dropout: float = 0.0,\n        coft: bool = False,\n        eps: float = 6e-5,\n        block_share: bool = False,\n        fan_in_fan_out: bool = False,\n        init_weights: bool = True,\n        use_cayley_neumann: bool = False,\n        num_cayley_neumann_terms: int = 5,\n        **kwargs,\n    )\n</syntaxhighlight>\n\n==== Import ====\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.awq import AwqOFTLinear, dispatch_awq\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Constructor Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || Module || required || The AWQ quantized linear layer to wrap\n|-\n| adapter_name || str || required || Name identifier for this adapter\n|-\n| r || int || 0 || OFT rank (number of OFT blocks per injected layer)\n|-\n| oft_block_size || int || 32 || Size of OFT blocks across different layers\n|-\n| module_dropout || float || 0.0 || Multiplicative dropout probability for OFT blocks\n|-\n| coft || bool || False || Whether to use constrained OFT variant\n|-\n| eps || float || 6e-5 || Control strength for COFT (only used if coft=True)\n|-\n| block_share || bool || False || Whether to share OFT parameters between blocks\n|-\n| fan_in_fan_out || bool || False || Set to True if layer stores weights as (fan_in, fan_out)\n|-\n| init_weights || bool || True || Whether to initialize OFT weights\n|-\n| use_cayley_neumann || bool || False || Whether to use Cayley-Neumann formulation\n|-\n| num_cayley_neumann_terms || int || 5 || Number of terms in Cayley-Neumann approximation\n|}\n\n=== Forward Method ===\n\n{| class=\"wikitable\"\n! Input !! Type !! Description\n|-\n| x || torch.Tensor || Input tensor to the layer\n|}\n\n{| class=\"wikitable\"\n! Output !! Type !! Description\n|-\n| result || torch.Tensor || Output tensor after applying OFT transformation and quantized computation\n|}\n\n=== dispatch_awq Function ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The target module to potentially wrap\n|-\n| adapter_name || str || Name for the adapter\n|-\n| **kwargs || Any || Additional keyword arguments passed to constructor\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || AwqOFTLinear instance if target is AWQ quantized, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with PEFT ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load a model quantized with AWQ\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model-name\",\n    quantization_config={\"method\": \"awq\"}\n)\n\n# Configure OFT\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.1,\n    coft=False,\n)\n\n# Apply OFT - automatically uses AwqOFTLinear for AWQ layers\npeft_model = get_peft_model(model, oft_config)\n\n# Fine-tune the model\n# ... training code ...\n</syntaxhighlight>\n\n=== Version Compatibility Check ===\n\n<syntaxhighlight lang=\"python\">\nimport importlib.metadata as importlib_metadata\nimport packaging.version\n\n# The dispatcher automatically checks version compatibility\n# Minimum required version is 0.2.0\nAUTOAWQ_MINIMUM_VERSION = packaging.version.parse(\"0.2.0\")\nversion_autoawq = packaging.version.parse(\n    importlib_metadata.version(\"autoawq\")\n)\n\nif AUTOAWQ_MINIMUM_VERSION > version_autoawq:\n    raise ImportError(\n        f\"Found incompatible version {version_autoawq}, \"\n        f\"but only versions above {AUTOAWQ_MINIMUM_VERSION} are supported\"\n    )\n</syntaxhighlight>\n\n=== Advanced Configuration with Constrained OFT ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\n\n# Configure OFT with constrained variant\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    module_dropout=0.05,\n    coft=True,  # Use constrained OFT\n    eps=1e-4,   # Control freedom of rotation\n    block_share=False,\n    use_cayley_neumann=True,\n    num_cayley_neumann_terms=7,\n)\n\n# Apply to AWQ quantized model\npeft_model = get_peft_model(awq_quantized_model, oft_config)\n</syntaxhighlight>\n\n=== Explicit Dispatcher Usage ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.awq import dispatch_awq\nfrom awq.modules.linear import WQLinear_GEMM\n\n# Assume we have an AWQ quantized layer\nawq_layer = WQLinear_GEMM(...)\n\n# Dispatch to create OFT adapter\noft_layer = dispatch_awq(\n    target=awq_layer,\n    adapter_name=\"default\",\n    r=8,\n    oft_block_size=32,\n    module_dropout=0.1\n)\n\n# Use the OFT-wrapped layer\noutput = oft_layer(input_tensor)\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Forward Pass Logic ===\n\nThe forward method implements the following logic:\n\n1. Check if adapters are disabled - if so, bypass OFT and use base layer directly\n2. For each active adapter:\n   * Apply dtype conversion if not in autocast mode\n   * Apply the OFT rotation transformation (oft_R)\n   * Convert back to expected dtype if needed\n3. Pass transformed input through the AWQ quantized base layer\n\n=== Quantization Weight Handling ===\n\nThe dispatcher updates the <code>qweight</code> attribute to reference the AWQ quantized weights:\n\n<syntaxhighlight lang=\"python\">\ntarget.qweight = target_base_layer.qweight\n</syntaxhighlight>\n\nThis ensures proper access to the quantized weights used by AWQ.\n\n=== Backward Compatibility ===\n\nThe class maintains both <code>base_layer</code> and <code>quant_linear_module</code> attributes pointing to the same object:\n* <code>base_layer</code> - for consistency with other OFT implementations\n* <code>quant_linear_module</code> - for backward compatibility with older code\n\n=== Version Requirements ===\n\nThe implementation requires <code>autoawq >= 0.2.0</code>. The dispatcher automatically checks this requirement and raises an informative error if an incompatible version is detected.\n\n== Related Pages ==\n\n* [[huggingface_peft_OFTConfig|OFTConfig]] - Configuration class for OFT\n* [[huggingface_peft_OFTModel|OFTModel]] - Main OFT model implementation\n* [[huggingface_peft_OFT_AQLM|OFT AQLM Integration]] - Similar integration for AQLM quantization\n* [[huggingface_peft_OFT_GPTQ|OFT GPTQ Integration]] - Similar integration for GPTQ quantization\n* [[huggingface_peft_OFT_EETQ|OFT EETQ Integration]] - Similar integration for EETQ quantization\n\n== See Also ==\n\n* OFT Paper: [https://arxiv.org/abs/2306.07280 Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* AWQ Paper: [https://arxiv.org/abs/2306.00978 Activation-aware Weight Quantization]\n* PEFT Documentation: [https://huggingface.co/docs/peft HuggingFace PEFT Docs]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_OFT_EETQ",
      "page_title": "OFT EETQ Integration",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>EetqOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for EETQ (Easy & Efficient Quantization for Transformers) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with EETQ, combining efficient quantization with orthogonal adaptation. EETQ is a quantization method designed for ease of use and computational efficiency in transformer models. The OFT adapter wraps EETQ's <code>EetqLinear</code> layers, allowing fine-tuning while maintaining the quantized model structure. Key features: * Supports EETQ quantized models with OFT adapters * Preserves quantization during fine-tuning * Applies orthogonal transformations before quantized computation * Handles dtype conversions appropriately for autocast scenarios * Explicitly disables merging/unmerging operations (not supported for EETQ)",
      "content": "= OFT EETQ Integration =\n\n== Knowledge Sources ==\n\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://arxiv.org/abs/2306.07280 OFT: Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* [https://github.com/NetEase-FuXi/EETQ EETQ: Easy & Efficient Quantization for Transformers]\n\n== Domains ==\n\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Orthogonal_Fine_Tuning]]\n[[Category:Quantization]]\n[[Category:EETQ]]\n\n== Overview ==\n\n=== Description ===\n\nThe <code>EetqOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for EETQ (Easy & Efficient Quantization for Transformers) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with EETQ, combining efficient quantization with orthogonal adaptation.\n\nEETQ is a quantization method designed for ease of use and computational efficiency in transformer models. The OFT adapter wraps EETQ's <code>EetqLinear</code> layers, allowing fine-tuning while maintaining the quantized model structure.\n\nKey features:\n* Supports EETQ quantized models with OFT adapters\n* Preserves quantization during fine-tuning\n* Applies orthogonal transformations before quantized computation\n* Handles dtype conversions appropriately for autocast scenarios\n* Explicitly disables merging/unmerging operations (not supported for EETQ)\n\n=== Usage ===\n\nThis module is used internally by the PEFT library when applying OFT to EETQ-quantized models. The dispatcher function <code>dispatch_eetq</code> automatically detects EETQ quantized layers and wraps them with the appropriate OFT adapter.\n\n== Code Reference ==\n\n=== Source Location ===\n\nFile: <code>src/peft/tuners/oft/eetq.py</code>\n\nRepository: HuggingFace PEFT (Parameter-Efficient Fine-Tuning)\n\n=== Class: EetqOFTLinear ===\n\n==== Signature ====\n\n<syntaxhighlight lang=\"python\">\nclass EetqOFTLinear(torch.nn.Module, OFTLayer):\n    def __init__(\n        self,\n        base_layer,\n        adapter_name,\n        r: int = 0,\n        oft_block_size: int = 0,\n        module_dropout: float = 0.0,\n        init_weights: bool = True,\n        coft: bool = False,\n        eps: float = 6e-5,\n        block_share: bool = False,\n        use_cayley_neumann: bool = False,\n        num_cayley_neumann_terms: int = 5,\n        fan_in_fan_out: bool = False,\n        **kwargs,\n    )\n</syntaxhighlight>\n\n==== Import ====\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.eetq import EetqOFTLinear, dispatch_eetq\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Constructor Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || Module || required || The EETQ quantized linear layer to wrap\n|-\n| adapter_name || str || required || Name identifier for this adapter\n|-\n| r || int || 0 || OFT rank (number of OFT blocks per injected layer)\n|-\n| oft_block_size || int || 0 || Size of OFT blocks across different layers\n|-\n| module_dropout || float || 0.0 || Multiplicative dropout probability for OFT blocks\n|-\n| init_weights || bool || True || Whether to initialize OFT weights\n|-\n| coft || bool || False || Whether to use constrained OFT variant\n|-\n| eps || float || 6e-5 || Control strength for COFT (only used if coft=True)\n|-\n| block_share || bool || False || Whether to share OFT parameters between blocks\n|-\n| use_cayley_neumann || bool || False || Whether to use Cayley-Neumann formulation\n|-\n| num_cayley_neumann_terms || int || 5 || Number of terms in Cayley-Neumann approximation\n|-\n| fan_in_fan_out || bool || False || Set to True if layer stores weights as (fan_in, fan_out)\n|}\n\n=== Forward Method ===\n\n{| class=\"wikitable\"\n! Input !! Type !! Description\n|-\n| x || torch.Tensor || Input tensor to the layer\n|}\n\n{| class=\"wikitable\"\n! Output !! Type !! Description\n|-\n| result || torch.Tensor || Output tensor after applying OFT transformation and quantized computation\n|}\n\n=== dispatch_eetq Function ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The target module to potentially wrap\n|-\n| adapter_name || str || Name for the adapter\n|-\n| **kwargs || Any || Additional keyword arguments passed to constructor\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || EetqOFTLinear instance if target is EETQ quantized, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with PEFT ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load a model quantized with EETQ\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model-name\",\n    quantization_config={\"method\": \"eetq\"}\n)\n\n# Configure OFT\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.1,\n    coft=False,\n)\n\n# Apply OFT - automatically uses EetqOFTLinear for EETQ layers\npeft_model = get_peft_model(model, oft_config)\n\n# Fine-tune the model\n# ... training code ...\n</syntaxhighlight>\n\n=== Advanced Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\n\n# Configure OFT with advanced options\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    module_dropout=0.05,\n    coft=True,  # Use constrained OFT\n    eps=1e-4,\n    block_share=False,\n    use_cayley_neumann=True,\n    num_cayley_neumann_terms=7,\n)\n\n# Apply to EETQ quantized model\npeft_model = get_peft_model(eetq_quantized_model, oft_config)\n</syntaxhighlight>\n\n=== Explicit Dispatcher Usage ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.eetq import dispatch_eetq\nfrom eetq import EetqLinear\n\n# Assume we have an EETQ quantized layer\neetq_layer = EetqLinear(...)\n\n# Dispatch to create OFT adapter\noft_layer = dispatch_eetq(\n    target=eetq_layer,\n    adapter_name=\"default\",\n    r=8,\n    oft_block_size=0,\n    module_dropout=0.1\n)\n\n# Use the OFT-wrapped layer\noutput = oft_layer(input_tensor)\n</syntaxhighlight>\n\n=== Handling Merge Operations ===\n\n<syntaxhighlight lang=\"python\">\n# Note: Merging is not supported for EETQ layers\ntry:\n    peft_model.merge_adapter()\nexcept AttributeError as e:\n    print(e)  # \"Merging LoRA layers is not supported for Eetq layers.\"\n\n# Similarly for unmerge\ntry:\n    peft_model.unmerge_adapter()\nexcept AttributeError as e:\n    print(e)  # \"Unmerging LoRA layers is not supported for Eetq layers.\"\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Forward Pass Logic ===\n\nThe forward method implements the following logic:\n\n1. Check if adapters are disabled - if so, bypass OFT and use base layer directly\n2. For each active adapter:\n   * Apply dtype conversion if not in autocast mode\n   * Apply the OFT rotation transformation (oft_R)\n3. Pass transformed input through the EETQ quantized base layer\n4. Convert result back to expected dtype if needed\n\n=== Merge/Unmerge Not Supported ===\n\nUnlike some other OFT implementations, EETQ does not support merging adapter weights into the base layer:\n\n<syntaxhighlight lang=\"python\">\ndef merge(self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None) -> None:\n    raise AttributeError(\"Merging LoRA layers is not supported for Eetq layers.\")\n\ndef unmerge(self) -> None:\n    raise AttributeError(\"Unmerging LoRA layers is not supported for Eetq layers.\")\n</syntaxhighlight>\n\nThis is because the EETQ quantization format does not allow for easy weight merging without dequantization.\n\n=== Weight and Bias Handling ===\n\nThe dispatcher copies weight and bias references from the base layer:\n\n<syntaxhighlight lang=\"python\">\ntarget.weight = target_base_layer.weight\n\nif hasattr(target, \"bias\"):\n    target.bias = target_base_layer.bias\n</syntaxhighlight>\n\n=== Backward Compatibility ===\n\nThe class maintains both <code>base_layer</code> and <code>quant_linear_module</code> attributes pointing to the same object:\n* <code>base_layer</code> - for consistency with other OFT implementations\n* <code>quant_linear_module</code> - for backward compatibility with older code\n\n== Related Pages ==\n\n* [[huggingface_peft_OFTConfig|OFTConfig]] - Configuration class for OFT\n* [[huggingface_peft_OFTModel|OFTModel]] - Main OFT model implementation\n* [[huggingface_peft_OFT_AQLM|OFT AQLM Integration]] - Similar integration for AQLM quantization\n* [[huggingface_peft_OFT_AWQ|OFT AWQ Integration]] - Similar integration for AWQ quantization\n* [[huggingface_peft_OFT_GPTQ|OFT GPTQ Integration]] - Similar integration for GPTQ quantization\n* [[huggingface_peft_OFT_HQQ|OFT HQQ Integration]] - Similar integration for HQQ quantization\n\n== See Also ==\n\n* OFT Paper: [https://arxiv.org/abs/2306.07280 Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* EETQ Repository: [https://github.com/NetEase-FuXi/EETQ EETQ on GitHub]\n* PEFT Documentation: [https://huggingface.co/docs/peft HuggingFace PEFT Docs]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_OFT_GPTQ",
      "page_title": "OFT GPTQ Integration",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>GPTQOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for GPTQ (Generative Pre-trained Transformer Quantization) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with GPTQ, allowing practitioners to fine-tune 4-bit quantized models while maintaining memory efficiency. GPTQ is a post-training quantization method specifically designed for generative pre-trained transformers, achieving high compression ratios with minimal accuracy loss. The OFT adapter wraps GPTQ quantized layers, allowing fine-tuning while preserving the quantized structure. Key features: * Supports both GPTQModel and AutoGPTQ quantized models * Preserves quantization during fine-tuning * Applies orthogonal transformations before quantized computation * Handles dtype conversions appropriately for autocast scenarios * Note: Merging adapters into GPTQ weights is not supported",
      "content": "= OFT GPTQ Integration =\n\n== Knowledge Sources ==\n\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://arxiv.org/abs/2306.07280 OFT: Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* [https://arxiv.org/abs/2210.17323 GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers]\n\n== Domains ==\n\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Orthogonal_Fine_Tuning]]\n[[Category:Quantization]]\n[[Category:GPTQ]]\n\n== Overview ==\n\n=== Description ===\n\nThe <code>GPTQOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for GPTQ (Generative Pre-trained Transformer Quantization) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with GPTQ, allowing practitioners to fine-tune 4-bit quantized models while maintaining memory efficiency.\n\nGPTQ is a post-training quantization method specifically designed for generative pre-trained transformers, achieving high compression ratios with minimal accuracy loss. The OFT adapter wraps GPTQ quantized layers, allowing fine-tuning while preserving the quantized structure.\n\nKey features:\n* Supports both GPTQModel and AutoGPTQ quantized models\n* Preserves quantization during fine-tuning\n* Applies orthogonal transformations before quantized computation\n* Handles dtype conversions appropriately for autocast scenarios\n* Note: Merging adapters into GPTQ weights is not supported\n\n=== Usage ===\n\nThis module is used internally by the PEFT library when applying OFT to GPTQ-quantized models. The dispatcher function <code>dispatch_gptq</code> automatically detects GPTQ quantized layers and wraps them with the appropriate OFT adapter.\n\n== Code Reference ==\n\n=== Source Location ===\n\nFile: <code>src/peft/tuners/oft/gptq.py</code>\n\nRepository: HuggingFace PEFT (Parameter-Efficient Fine-Tuning)\n\n=== Class: GPTQOFTLinear ===\n\n==== Signature ====\n\n<syntaxhighlight lang=\"python\">\nclass GPTQOFTLinear(torch.nn.Module, OFTLayer):\n    def __init__(\n        self,\n        base_layer,\n        adapter_name: str,\n        r: int = 8,\n        oft_block_size: int = 0,\n        module_dropout: float = 0.0,\n        coft: bool = False,\n        eps: float = 6e-5,\n        block_share: bool = False,\n        use_cayley_neumann: bool = False,\n        num_cayley_neumann_terms: int = 5,\n        fan_in_fan_out: bool = False,\n        init_weights: bool = True,\n        **kwargs,\n    )\n</syntaxhighlight>\n\n==== Import ====\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.gptq import GPTQOFTLinear, dispatch_gptq\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Constructor Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || Module || required || The GPTQ quantized linear layer to wrap\n|-\n| adapter_name || str || required || Name identifier for this adapter\n|-\n| r || int || 8 || OFT rank (number of OFT blocks per injected layer)\n|-\n| oft_block_size || int || 0 || Size of OFT blocks across different layers\n|-\n| module_dropout || float || 0.0 || Multiplicative dropout probability for OFT blocks\n|-\n| coft || bool || False || Whether to use constrained OFT variant\n|-\n| eps || float || 6e-5 || Control strength for COFT (only used if coft=True)\n|-\n| block_share || bool || False || Whether to share OFT parameters between blocks\n|-\n| use_cayley_neumann || bool || False || Whether to use Cayley-Neumann formulation\n|-\n| num_cayley_neumann_terms || int || 5 || Number of terms in Cayley-Neumann approximation\n|-\n| fan_in_fan_out || bool || False || Set to True if layer stores weights as (fan_in, fan_out)\n|-\n| init_weights || bool || True || Whether to initialize OFT weights\n|}\n\n=== Forward Method ===\n\n{| class=\"wikitable\"\n! Input !! Type !! Description\n|-\n| x || torch.Tensor || Input tensor to the layer\n|}\n\n{| class=\"wikitable\"\n! Output !! Type !! Description\n|-\n| result || torch.Tensor || Output tensor after applying OFT transformation and quantized computation\n|}\n\n=== dispatch_gptq Function ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The target module to potentially wrap\n|-\n| adapter_name || str || Name for the adapter\n|-\n| **kwargs || Any || Additional keyword arguments (may include gptq_quantization_config)\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || GPTQOFTLinear instance if target is GPTQ quantized, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with PEFT ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load a model quantized with GPTQ\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model-name\",\n    quantization_config={\"bits\": 4, \"method\": \"gptq\"}\n)\n\n# Configure OFT\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.1,\n    coft=False,\n)\n\n# Apply OFT - automatically uses GPTQOFTLinear for GPTQ layers\npeft_model = get_peft_model(model, oft_config)\n\n# Fine-tune the model\n# ... training code ...\n</syntaxhighlight>\n\n=== Loading Pre-quantized GPTQ Model ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load pre-quantized GPTQ model from HuggingFace Hub\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TheBloke/Llama-2-7B-GPTQ\",\n    device_map=\"auto\",\n    trust_remote_code=False,\n)\n\n# Apply OFT for fine-tuning\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    module_dropout=0.05,\n)\n\npeft_model = get_peft_model(model, oft_config)\n</syntaxhighlight>\n\n=== Advanced Configuration with Constrained OFT ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\n\n# Configure OFT with constrained variant\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    module_dropout=0.05,\n    coft=True,  # Use constrained OFT\n    eps=1e-4,   # Control freedom of rotation\n    block_share=False,\n    use_cayley_neumann=True,\n    num_cayley_neumann_terms=7,\n)\n\n# Apply to GPTQ quantized model\npeft_model = get_peft_model(gptq_quantized_model, oft_config)\n</syntaxhighlight>\n\n=== Explicit Dispatcher Usage ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.gptq import dispatch_gptq\n\n# Assume we have a GPTQ quantized layer\n# Works with both gptqmodel.nn_modules.qlinear.BaseQuantLinear\n# and auto_gptq quantized linear layers\n\n# Dispatch to create OFT adapter\noft_layer = dispatch_gptq(\n    target=gptq_layer,\n    adapter_name=\"default\",\n    r=8,\n    oft_block_size=0,\n    module_dropout=0.1,\n    gptq_quantization_config=quantization_config,\n)\n\n# Use the OFT-wrapped layer\noutput = oft_layer(input_tensor)\n</syntaxhighlight>\n\n=== Important: Merging Not Supported ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load and configure GPTQ model with OFT\nmodel = AutoModelForCausalLM.from_pretrained(\"gptq-model\")\noft_config = OFTConfig(r=8, target_modules=[\"q_proj\", \"v_proj\"])\npeft_model = get_peft_model(model, oft_config)\n\n# Train the model\n# ...\n\n# IMPORTANT: Cannot merge adapters into GPTQ weights\ntry:\n    peft_model.merge_adapter()\nexcept ValueError as e:\n    print(e)  # \"Cannot merge OFT layers when the model is gptq quantized\"\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Forward Pass Logic ===\n\nThe forward method implements the following logic:\n\n1. First computes the result from the quantized base layer (note: this appears twice in the code, likely a bug)\n2. Check if adapters are disabled - if so, return base layer result directly\n3. For each active adapter:\n   * Apply dtype conversion if not in autocast mode\n   * Apply the OFT rotation transformation (oft_R)\n4. Pass transformed input through the GPTQ quantized base layer\n5. Convert result back to expected dtype if needed\n\n=== Multiple Backend Support ===\n\nThe dispatcher supports two GPTQ implementations:\n\n1. '''GPTQModel''' (preferred):\n<syntaxhighlight lang=\"python\">\nfrom gptqmodel.nn_modules.qlinear import BaseQuantLinear\n</syntaxhighlight>\n\n2. '''AutoGPTQ''' (fallback):\n<syntaxhighlight lang=\"python\">\nfrom peft.utils import get_auto_gptq_quant_linear\nquant_linear = get_auto_gptq_quant_linear(cfg)\n</syntaxhighlight>\n\n=== Quantization Weight Handling ===\n\nThe dispatcher updates the <code>qweight</code> attribute to reference the GPTQ quantized weights:\n\n<syntaxhighlight lang=\"python\">\ntarget.qweight = target_base_layer.qweight\n</syntaxhighlight>\n\nThis ensures proper access to the quantized weights used by GPTQ.\n\n=== Backward Compatibility ===\n\nThe class maintains both <code>base_layer</code> and <code>quant_linear_module</code> attributes pointing to the same object:\n* <code>base_layer</code> - for consistency with other OFT implementations\n* <code>quant_linear_module</code> - for backward compatibility with older code\n\n=== Merging Restrictions ===\n\nThe OFTModel class explicitly prevents merging for GPTQ models:\n\n<syntaxhighlight lang=\"python\">\nif getattr(self.model, \"quantization_method\", None) == \"gptq\":\n    raise ValueError(\"Cannot merge OFT layers when the model is gptq quantized\")\n</syntaxhighlight>\n\nThis is because merging would require dequantization and re-quantization, which is not supported.\n\n== Related Pages ==\n\n* [[huggingface_peft_OFTConfig|OFTConfig]] - Configuration class for OFT\n* [[huggingface_peft_OFTModel|OFTModel]] - Main OFT model implementation\n* [[huggingface_peft_OFT_AQLM|OFT AQLM Integration]] - Similar integration for AQLM quantization\n* [[huggingface_peft_OFT_AWQ|OFT AWQ Integration]] - Similar integration for AWQ quantization\n* [[huggingface_peft_OFT_EETQ|OFT EETQ Integration]] - Similar integration for EETQ quantization\n* [[huggingface_peft_OFT_HQQ|OFT HQQ Integration]] - Similar integration for HQQ quantization\n\n== See Also ==\n\n* OFT Paper: [https://arxiv.org/abs/2306.07280 Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* GPTQ Paper: [https://arxiv.org/abs/2210.17323 GPTQ: Accurate Post-Training Quantization]\n* PEFT Documentation: [https://huggingface.co/docs/peft HuggingFace PEFT Docs]\n* GPTQModel: [https://github.com/ModelCloud/GPTQModel GPTQModel on GitHub]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_OFT_HQQ",
      "page_title": "OFT HQQ Integration",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>HqqOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for HQQ (Half-Quadratic Quantization) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with HQQ, offering unique support for merging and unmerging adapters with quantized weights. HQQ is a quantization method that uses half-quadratic optimization for weight quantization. Unlike most other quantization backends, the HQQ integration supports merging OFT adapter weights into the quantized base weights through a dequantize-transform-requantize process. Key features: * Supports HQQ quantized models with OFT adapters * Unique merge/unmerge support for quantized weights * Dequantizes, applies transformation, and requantizes for merging * Preserves quantization configuration during merge operations * Handles dtype conversions appropriately for autocast scenarios",
      "content": "= OFT HQQ Integration =\n\n== Knowledge Sources ==\n\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://arxiv.org/abs/2306.07280 OFT: Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* [https://github.com/mobiusml/hqq Half-Quadratic Quantization (HQQ)]\n\n== Domains ==\n\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Orthogonal_Fine_Tuning]]\n[[Category:Quantization]]\n[[Category:HQQ]]\n\n== Overview ==\n\n=== Description ===\n\nThe <code>HqqOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for HQQ (Half-Quadratic Quantization) quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with HQQ, offering unique support for merging and unmerging adapters with quantized weights.\n\nHQQ is a quantization method that uses half-quadratic optimization for weight quantization. Unlike most other quantization backends, the HQQ integration supports merging OFT adapter weights into the quantized base weights through a dequantize-transform-requantize process.\n\nKey features:\n* Supports HQQ quantized models with OFT adapters\n* Unique merge/unmerge support for quantized weights\n* Dequantizes, applies transformation, and requantizes for merging\n* Preserves quantization configuration during merge operations\n* Handles dtype conversions appropriately for autocast scenarios\n\n=== Usage ===\n\nThis module is used internally by the PEFT library when applying OFT to HQQ-quantized models. The dispatcher function <code>dispatch_hqq</code> automatically detects HQQ quantized layers and wraps them with the appropriate OFT adapter.\n\n== Code Reference ==\n\n=== Source Location ===\n\nFile: <code>src/peft/tuners/oft/hqq.py</code>\n\nRepository: HuggingFace PEFT (Parameter-Efficient Fine-Tuning)\n\n=== Class: HqqOFTLinear ===\n\n==== Signature ====\n\n<syntaxhighlight lang=\"python\">\nclass HqqOFTLinear(torch.nn.Module, OFTLayer):\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        r: int = 8,\n        oft_block_size: int = 0,\n        module_dropout: float = 0.0,\n        init_weights: bool = True,\n        coft: bool = False,\n        eps: float = 6e-5,\n        block_share: bool = False,\n        use_cayley_neumann: bool = False,\n        num_cayley_neumann_terms: int = 5,\n        **kwargs,\n    )\n</syntaxhighlight>\n\n==== Import ====\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.hqq import HqqOFTLinear, dispatch_hqq\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Constructor Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || torch.nn.Module || required || The HQQ quantized linear layer to wrap\n|-\n| adapter_name || str || required || Name identifier for this adapter\n|-\n| r || int || 8 || OFT rank (number of OFT blocks per injected layer)\n|-\n| oft_block_size || int || 0 || Size of OFT blocks across different layers\n|-\n| module_dropout || float || 0.0 || Multiplicative dropout probability for OFT blocks\n|-\n| init_weights || bool || True || Whether to initialize OFT weights\n|-\n| coft || bool || False || Whether to use constrained OFT variant\n|-\n| eps || float || 6e-5 || Control strength for COFT (only used if coft=True)\n|-\n| block_share || bool || False || Whether to share OFT parameters between blocks\n|-\n| use_cayley_neumann || bool || False || Whether to use Cayley-Neumann formulation\n|-\n| num_cayley_neumann_terms || int || 5 || Number of terms in Cayley-Neumann approximation\n|}\n\n=== Forward Method ===\n\n{| class=\"wikitable\"\n! Input !! Type !! Description\n|-\n| x || torch.Tensor || Input tensor to the layer\n|-\n| *args || Any || Additional positional arguments\n|-\n| **kwargs || Any || Additional keyword arguments (including optional adapter_names)\n|}\n\n{| class=\"wikitable\"\n! Output !! Type !! Description\n|-\n| result || torch.Tensor || Output tensor after applying OFT transformation and quantized computation\n|}\n\n=== Merge Method ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| safe_merge || bool || False || If True, check for NaNs before merging\n|-\n| adapter_names || Optional[list[str]] || None || List of adapter names to merge (None = all active)\n|}\n\n=== Unmerge Method ===\n\nNo parameters - unmerges all merged adapters.\n\n=== dispatch_hqq Function ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The target module to potentially wrap\n|-\n| adapter_name || str || Name for the adapter\n|-\n| **kwargs || Any || Additional keyword arguments passed to constructor\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || HqqOFTLinear instance if target is HQQ quantized, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with PEFT ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load a model quantized with HQQ\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model-name\",\n    quantization_config={\"method\": \"hqq\"}\n)\n\n# Configure OFT\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.1,\n    coft=False,\n)\n\n# Apply OFT - automatically uses HqqOFTLinear for HQQ layers\npeft_model = get_peft_model(model, oft_config)\n\n# Fine-tune the model\n# ... training code ...\n</syntaxhighlight>\n\n=== Merging Adapters (Unique to HQQ) ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, OFTConfig\n\n# Load HQQ model and add OFT adapters\npeft_model = get_peft_model(hqq_model, oft_config)\n\n# Train the model\n# ... training code ...\n\n# Merge adapters into base weights (supported for HQQ!)\npeft_model.merge_adapter()\n\n# Model now has adapters merged into quantized weights\n# Can be used for inference without adapter overhead\n\n# Later, can unmerge if needed\npeft_model.unmerge_adapter()\n</syntaxhighlight>\n\n=== Safe Merging with NaN Check ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, OFTConfig\n\npeft_model = get_peft_model(hqq_model, oft_config)\n\n# Train the model\n# ... training code ...\n\n# Safely merge with NaN checking\ntry:\n    peft_model.merge_adapter(safe_merge=True)\n    print(\"Merge successful!\")\nexcept ValueError as e:\n    print(f\"Merge failed: {e}\")\n    # Handle broken adapter\n</syntaxhighlight>\n\n=== Advanced Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\n\n# Configure OFT with advanced options\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    module_dropout=0.05,\n    coft=True,  # Use constrained OFT\n    eps=1e-4,\n    block_share=False,\n    use_cayley_neumann=True,\n    num_cayley_neumann_terms=7,\n)\n\n# Apply to HQQ quantized model\npeft_model = get_peft_model(hqq_quantized_model, oft_config)\n</syntaxhighlight>\n\n=== Selective Adapter Merging ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n\n# Model with multiple adapters\npeft_model = PeftModel(hqq_model, oft_config)\npeft_model.load_adapter(\"adapter1\", adapter_name=\"task1\")\npeft_model.load_adapter(\"adapter2\", adapter_name=\"task2\")\n\n# Merge only specific adapters\npeft_model.merge_adapter(adapter_names=[\"task1\"])\n\n# task1 is now merged, task2 still separate\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Merge Operation ===\n\nThe merge process is unique to HQQ and involves:\n\n1. Dequantize the base weights:\n<syntaxhighlight lang=\"python\">\noutput = layer.dequantize()\n</syntaxhighlight>\n\n2. Get the OFT transformation matrix:\n<syntaxhighlight lang=\"python\">\noft_data = self.get_delta_weight(active_adapter)\n</syntaxhighlight>\n\n3. Apply the transformation:\n<syntaxhighlight lang=\"python\">\noutput = torch.transpose(output, 0, 1)\nw_data = torch.mm(oft_data, output.to(oft_data.dtype))\nw_data = torch.transpose(w_data, 0, 1)\n</syntaxhighlight>\n\n4. Requantize with original configuration:\n<syntaxhighlight lang=\"python\">\nnew_hqq_layer = HQQLinear(None, quant_config, compute_dtype=layer.compute_dtype, device=layer.device)\nnew_hqq_layer.quantize(w_data, **quant_config)\nself.base_layer = new_hqq_layer\n</syntaxhighlight>\n\n=== Unmerge Operation ===\n\nThe unmerge process reverses the transformation:\n\n1. Dequantize current (merged) weights\n2. Apply inverse OFT transformation using <code>oft_data.t()</code> (transpose)\n3. Requantize back to HQQ format\n\n=== Forward Pass Logic ===\n\nThe forward method implements:\n\n1. Check if adapters are disabled:\n   * If merged, unmerge first\n   * Return base layer result directly\n2. If adapters are merged, use base layer directly\n3. Otherwise, for each active adapter:\n   * Apply dtype conversion if not in autocast mode\n   * Apply the OFT rotation transformation (oft_R)\n4. Pass transformed input through HQQ quantized base layer\n5. Convert result back to expected dtype if needed\n\n=== Quantization Configuration Preservation ===\n\nThe implementation carefully preserves the quantization configuration during merge/unmerge:\n\n<syntaxhighlight lang=\"python\">\nquant_config = {\n    **copy.deepcopy(layer.quant_config),\n    \"offload_meta\": layer.offload_meta\n}\n# ... transform ...\nquant_config.pop(\"offload_meta\", None)\nnew_hqq_layer.quantize(w_data, **quant_config)\n</syntaxhighlight>\n\n=== Fan-in/Fan-out ===\n\nThe class explicitly sets <code>fan_in_fan_out = False</code>, meaning it expects standard weight layout.\n\n== Related Pages ==\n\n* [[huggingface_peft_OFTConfig|OFTConfig]] - Configuration class for OFT\n* [[huggingface_peft_OFTModel|OFTModel]] - Main OFT model implementation\n* [[huggingface_peft_OFT_AQLM|OFT AQLM Integration]] - Similar integration for AQLM quantization\n* [[huggingface_peft_OFT_AWQ|OFT AWQ Integration]] - Similar integration for AWQ quantization\n* [[huggingface_peft_OFT_GPTQ|OFT GPTQ Integration]] - Similar integration for GPTQ quantization\n* [[huggingface_peft_OFT_EETQ|OFT EETQ Integration]] - Similar integration for EETQ quantization\n\n== See Also ==\n\n* OFT Paper: [https://arxiv.org/abs/2306.07280 Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* HQQ Repository: [https://github.com/mobiusml/hqq HQQ on GitHub]\n* PEFT Documentation: [https://huggingface.co/docs/peft HuggingFace PEFT Docs]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_OFT_IntelFP8",
      "page_title": "OFT Intel Neural Compressor FP8 Integration",
      "page_type": "Implementation",
      "overview": "=== Description === The <code>IncOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for Intel Neural Compressor (INC) FP8 quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with Intel's Neural Compressor for FP8 precision, particularly for use with Intel Habana Gaudi accelerators. Intel Neural Compressor is a quantization library optimized for Intel hardware, including Habana Gaudi AI accelerators. The FP8 (8-bit floating point) quantization provides a balance between model size, computational efficiency, and accuracy. The OFT adapter wraps INC's <code>PatchedLinear</code> layers, allowing fine-tuning while maintaining the quantized structure. Key features: * Supports Intel Neural Compressor FP8 quantized models with OFT adapters * Optimized for Intel Habana Gaudi hardware * Extends the standard Linear OFT implementation * Note: Merge/unmerge operations not yet implemented * Tests are handled in the Optimum-Habana repository",
      "content": "= OFT Intel Neural Compressor FP8 Integration =\n\n== Knowledge Sources ==\n\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://arxiv.org/abs/2306.07280 OFT: Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* [https://github.com/intel/neural-compressor Intel Neural Compressor]\n* [https://github.com/huggingface/optimum-habana Optimum Habana]\n\n== Domains ==\n\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Orthogonal_Fine_Tuning]]\n[[Category:Quantization]]\n[[Category:Intel]]\n[[Category:FP8]]\n[[Category:Habana]]\n\n== Overview ==\n\n=== Description ===\n\nThe <code>IncOFTLinear</code> class provides an implementation of Orthogonal Fine-Tuning (OFT) for Intel Neural Compressor (INC) FP8 quantized linear layers. This integration enables parameter-efficient fine-tuning of models quantized with Intel's Neural Compressor for FP8 precision, particularly for use with Intel Habana Gaudi accelerators.\n\nIntel Neural Compressor is a quantization library optimized for Intel hardware, including Habana Gaudi AI accelerators. The FP8 (8-bit floating point) quantization provides a balance between model size, computational efficiency, and accuracy. The OFT adapter wraps INC's <code>PatchedLinear</code> layers, allowing fine-tuning while maintaining the quantized structure.\n\nKey features:\n* Supports Intel Neural Compressor FP8 quantized models with OFT adapters\n* Optimized for Intel Habana Gaudi hardware\n* Extends the standard Linear OFT implementation\n* Note: Merge/unmerge operations not yet implemented\n* Tests are handled in the Optimum-Habana repository\n\n=== Usage ===\n\nThis module is used internally by the PEFT library when applying OFT to INC FP8-quantized models. The dispatcher function <code>dispatch_inc</code> automatically detects INC quantized layers and wraps them with the appropriate OFT adapter.\n\n== Code Reference ==\n\n=== Source Location ===\n\nFile: <code>src/peft/tuners/oft/inc.py</code>\n\nRepository: HuggingFace PEFT (Parameter-Efficient Fine-Tuning)\n\n=== Class: IncOFTLinear ===\n\n==== Signature ====\n\n<syntaxhighlight lang=\"python\">\nclass IncOFTLinear(Linear):\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        **kwargs,\n    )\n</syntaxhighlight>\n\n==== Import ====\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.inc import IncOFTLinear, dispatch_inc\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Constructor Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| base_layer || torch.nn.Module || The INC FP8 quantized linear layer (PatchedLinear) to wrap\n|-\n| adapter_name || str || Name identifier for this adapter\n|-\n| **kwargs || Any || Additional keyword arguments passed to parent Linear class\n|}\n\nThe <code>**kwargs</code> are passed to the parent <code>Linear</code> class and include all standard OFT parameters:\n* <code>r</code>: OFT rank\n* <code>oft_block_size</code>: Block size\n* <code>module_dropout</code>: Dropout probability\n* <code>coft</code>: Constrained OFT flag\n* <code>eps</code>: COFT epsilon\n* <code>block_share</code>: Block sharing flag\n* <code>use_cayley_neumann</code>: Cayley-Neumann flag\n* <code>num_cayley_neumann_terms</code>: Number of Cayley-Neumann terms\n* And other parameters from the Linear parent class\n\n=== Forward Method ===\n\nInherited from the parent <code>Linear</code> class.\n\n{| class=\"wikitable\"\n! Input !! Type !! Description\n|-\n| x || torch.Tensor || Input tensor to the layer\n|-\n| *args || Any || Additional positional arguments\n|-\n| **kwargs || Any || Additional keyword arguments\n|}\n\n{| class=\"wikitable\"\n! Output !! Type !! Description\n|-\n| result || torch.Tensor || Output tensor after applying OFT transformation and quantized computation\n|}\n\n=== Merge/Unmerge Methods ===\n\nCurrently raise <code>NotImplementedError</code>:\n\n{| class=\"wikitable\"\n! Method !! Status !! Error Message\n|-\n| merge() || Not Implemented || \"Merging OFT with INC layers is not yet implemented\"\n|-\n| unmerge() || Not Implemented || \"Unmerging OFT from INC layers is not yet implemented\"\n|}\n\n=== dispatch_inc Function ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| target || torch.nn.Module || The target module to potentially wrap\n|-\n| adapter_name || str || Name for the adapter\n|-\n| **kwargs || Any || Additional keyword arguments passed to constructor\n|}\n\n{| class=\"wikitable\"\n! Return !! Type !! Description\n|-\n| new_module || Optional[torch.nn.Module] || IncOFTLinear instance if target is INC quantized, None otherwise\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with PEFT ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load a model quantized with Intel Neural Compressor FP8\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model-name\",\n    quantization_config={\"method\": \"inc\", \"precision\": \"fp8\"}\n)\n\n# Configure OFT\noft_config = OFTConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    module_dropout=0.1,\n    coft=False,\n)\n\n# Apply OFT - automatically uses IncOFTLinear for INC layers\npeft_model = get_peft_model(model, oft_config)\n\n# Fine-tune the model on Intel Habana Gaudi\n# ... training code ...\n</syntaxhighlight>\n\n=== Usage on Intel Habana Gaudi ===\n\n<syntaxhighlight lang=\"python\">\nimport habana_frameworks.torch.core as htcore\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, OFTConfig\n\n# Load model with INC FP8 quantization for Gaudi\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model-name\",\n    device_map=\"hpu\",  # Habana Processing Unit\n)\n\n# Apply FP8 quantization with INC\n# (typically done during model preparation)\n\n# Configure OFT\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    module_dropout=0.05,\n)\n\n# Apply OFT\npeft_model = get_peft_model(model, oft_config)\n\n# Training on Gaudi\n# ... training code ...\n</syntaxhighlight>\n\n=== Advanced Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import OFTConfig, get_peft_model\n\n# Configure OFT with advanced options\noft_config = OFTConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    module_dropout=0.05,\n    coft=True,  # Use constrained OFT\n    eps=1e-4,\n    block_share=False,\n    use_cayley_neumann=True,\n    num_cayley_neumann_terms=7,\n)\n\n# Apply to INC FP8 quantized model\npeft_model = get_peft_model(inc_quantized_model, oft_config)\n</syntaxhighlight>\n\n=== Handling Merge Operations ===\n\n<syntaxhighlight lang=\"python\">\n# Note: Merging is not yet supported for INC layers\ntry:\n    peft_model.merge_adapter()\nexcept NotImplementedError as e:\n    print(e)  # \"Merging OFT with INC layers is not yet implemented\"\n\n# Similarly for unmerge\ntry:\n    peft_model.unmerge_adapter()\nexcept NotImplementedError as e:\n    print(e)  # \"Unmerging OFT from INC layers is not yet implemented\"\n\n# For inference, keep adapters as separate modules\npeft_model.eval()\nwith torch.no_grad():\n    output = peft_model(input_ids)\n</syntaxhighlight>\n\n=== Explicit Dispatcher Usage ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.oft.inc import dispatch_inc\nfrom neural_compressor.torch.algorithms.fp8_quant._quant_common.helper_modules import PatchedLinear\n\n# Assume we have an INC FP8 quantized layer\ninc_layer = PatchedLinear(...)\n\n# Dispatch to create OFT adapter\noft_layer = dispatch_inc(\n    target=inc_layer,\n    adapter_name=\"default\",\n    r=8,\n    oft_block_size=0,\n    module_dropout=0.1\n)\n\n# Use the OFT-wrapped layer\noutput = oft_layer(input_tensor)\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Inheritance Structure ===\n\n<code>IncOFTLinear</code> inherits from the standard <code>Linear</code> OFT layer implementation:\n\n<syntaxhighlight lang=\"python\">\nclass IncOFTLinear(Linear):\n    def __init__(self, base_layer: torch.nn.Module, adapter_name: str, **kwargs):\n        super().__init__(base_layer, adapter_name, **kwargs)\n</syntaxhighlight>\n\nThis means it has all the functionality of the standard OFT Linear layer, with only merge/unmerge explicitly disabled.\n\n=== Forward Pass ===\n\nThe forward pass is inherited from the parent <code>Linear</code> class and implements:\n\n1. Check if adapters are disabled or merged\n2. For each active adapter, apply OFT rotation transformation\n3. Pass transformed input through INC FP8 quantized base layer\n4. Handle dtype conversions as needed\n\n=== PatchedLinear Detection ===\n\nThe dispatcher checks for INC's <code>PatchedLinear</code> module:\n\n<syntaxhighlight lang=\"python\">\nfrom neural_compressor.torch.algorithms.fp8_quant._quant_common.helper_modules import PatchedLinear\n\nif isinstance(target_base_layer, PatchedLinear):\n    new_module = IncOFTLinear(target, adapter_name, **kwargs)\n</syntaxhighlight>\n\n=== Test Location ===\n\nTests for INC integration are not in the main PEFT repository. According to the file header:\n\n<syntaxhighlight lang=\"python\">\n# NOTE: PEFT tests related to INC are handled under Optimum-Habana repository:\n# - LLMs: https://github.com/huggingface/optimum-habana/blob/main/tests/test_peft_inference.py\n# - Diffusers: https://github.com/huggingface/optimum-habana/blob/main/tests/test_diffusers.py\n</syntaxhighlight>\n\n=== Merge/Unmerge Not Implemented ===\n\nBoth merge and unmerge operations raise <code>NotImplementedError</code>:\n\n<syntaxhighlight lang=\"python\">\ndef merge(self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None) -> None:\n    raise NotImplementedError(\"Merging OFT with INC layers is not yet implemented\")\n\ndef unmerge(self) -> None:\n    raise NotImplementedError(\"Unmerging OFT from INC layers is not yet implemented\")\n</syntaxhighlight>\n\nThis is likely due to the complexity of merging with FP8 quantized weights or because the use case on Habana hardware doesn't require merging.\n\n== Related Pages ==\n\n* [[huggingface_peft_OFTConfig|OFTConfig]] - Configuration class for OFT\n* [[huggingface_peft_OFTModel|OFTModel]] - Main OFT model implementation\n* [[huggingface_peft_OFT_AQLM|OFT AQLM Integration]] - Similar integration for AQLM quantization\n* [[huggingface_peft_OFT_AWQ|OFT AWQ Integration]] - Similar integration for AWQ quantization\n* [[huggingface_peft_OFT_GPTQ|OFT GPTQ Integration]] - Similar integration for GPTQ quantization\n* [[huggingface_peft_OFT_EETQ|OFT EETQ Integration]] - Similar integration for EETQ quantization\n* [[huggingface_peft_OFT_HQQ|OFT HQQ Integration]] - Similar integration for HQQ quantization\n\n== See Also ==\n\n* OFT Paper: [https://arxiv.org/abs/2306.07280 Controlling Text-to-Image Diffusion by Orthogonal Finetuning]\n* Intel Neural Compressor: [https://github.com/intel/neural-compressor Intel Neural Compressor on GitHub]\n* Optimum Habana: [https://github.com/huggingface/optimum-habana Optimum Habana on GitHub]\n* PEFT Documentation: [https://huggingface.co/docs/peft HuggingFace PEFT Docs]\n* Intel Habana Gaudi: [https://habana.ai/ Habana AI]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_PeftModel_from_pretrained",
      "page_title": "huggingface peft PeftModel from pretrained",
      "page_type": "Implementation",
      "overview": "Concrete tool for loading a trained PEFT adapter onto a base model for inference or further training.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft/quicktour#load-and-use-a-peft-model]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Model_Loading]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for loading a trained PEFT adapter onto a base model for inference or further training.\n\n=== Description ===\n\n`PeftModel.from_pretrained` is the primary method for loading pre-trained PEFT adapters. It loads the adapter configuration and weights from disk or HuggingFace Hub, injects adapter layers into the provided base model, and restores the trained adapter weights. This enables inference with task-specific adapters.\n\n=== Usage ===\n\nUse this when you have a trained adapter saved and want to use it for inference or continue training. The base model must match the model used during training. For inference, set `is_trainable=False` (default) to freeze adapters.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/peft_model.py\n* '''Lines:''' L388-604\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@classmethod\ndef from_pretrained(\n    cls,\n    model: torch.nn.Module,\n    model_id: Union[str, os.PathLike],\n    adapter_name: str = \"default\",\n    is_trainable: bool = False,\n    config: Optional[PeftConfig] = None,\n    autocast_adapter_dtype: bool = True,\n    ephemeral_gpu_offload: bool = False,\n    low_cpu_mem_usage: bool = False,\n    **kwargs: Any,\n) -> PeftModel:\n    \"\"\"\n    Load a PEFT adapter from a pretrained checkpoint.\n\n    Args:\n        model: Base model to adapt\n        model_id: Adapter path (local or HuggingFace Hub)\n        adapter_name: Name for the adapter. Default: \"default\"\n        is_trainable: Load for training (True) or inference (False)\n        config: Override config (usually auto-loaded)\n        autocast_adapter_dtype: Cast adapter weights for stability\n\n    Returns:\n        PeftModel with loaded adapter\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || Base model matching the adapter's base_model_name_or_path\n|-\n| model_id || str || Yes || HuggingFace Hub ID or local path to adapter\n|-\n| adapter_name || str || No || Name for loaded adapter. Default: \"default\"\n|-\n| is_trainable || bool || No || False for inference, True for training. Default: False\n|-\n| autocast_adapter_dtype || bool || No || Cast float16/bfloat16 to float32. Default: True\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| peft_model || PeftModel || Model with loaded adapter ready for inference/training\n|}\n\n== Usage Examples ==\n\n=== Load for Inference ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\n# 1. Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# 2. Load adapter\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"username/my-lora-adapter\",  # HuggingFace Hub path\n    is_trainable=False,          # Inference mode\n)\n\n# 3. Use for generation\nmodel.eval()\noutputs = model.generate(**inputs)\n</syntaxhighlight>\n\n=== Load from Local Path ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n\n# Load from local directory\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"./trained-adapter\",\n    adapter_name=\"math_adapter\",\n)\n</syntaxhighlight>\n\n=== Continue Training ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n\n# Load for further training\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"./checkpoint-1000\",\n    is_trainable=True,  # Enable gradients\n)\n\n# Continue training\ntrainer.train(resume_from_checkpoint=True)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Adapter_Loading]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Adapter",
        "Model Loading",
        "Inference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft/quicktour#load-and-use-a-peft-model"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Loading"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_PeftModel_save_pretrained",
      "page_title": "huggingface peft PeftModel save pretrained",
      "page_type": "Implementation",
      "overview": "Concrete tool for saving trained LoRA adapter weights and configuration to disk for later loading.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft/quicktour#save-and-load-a-model]]\n|-\n! Domains\n| [[domain::Serialization]], [[domain::Fine_Tuning]], [[domain::Adapter]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for saving trained LoRA adapter weights and configuration to disk for later loading.\n\n=== Description ===\n\n`PeftModel.save_pretrained()` saves only the trained adapter weights and configuration, not the base model. This results in very small checkpoint sizes (typically 10-100MB vs. multiple GB for full models). The saved adapter can be loaded onto any compatible base model using `PeftModel.from_pretrained()`.\n\n=== Usage ===\n\nCall this after training completes to persist your adapter. The function creates `adapter_model.safetensors` (weights) and `adapter_config.json` (configuration). For multi-adapter models, use `selected_adapters` to save specific adapters. Enable `safe_serialization=True` (default) for safetensors format.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/peft_model.py\n* '''Lines:''' L190-386\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef save_pretrained(\n    self,\n    save_directory: str,\n    safe_serialization: bool = True,\n    selected_adapters: Optional[list[str]] = None,\n    save_embedding_layers: Union[str, bool] = \"auto\",\n    is_main_process: bool = True,\n    path_initial_model_for_weight_conversion: Optional[str] = None,\n    **kwargs: Any,\n) -> None:\n    \"\"\"\n    Save adapter weights and configuration.\n\n    Args:\n        save_directory: Output directory path\n        safe_serialization: Use safetensors format (recommended)\n        selected_adapters: List of adapter names to save (None = all)\n        save_embedding_layers: Save modified embeddings (\"auto\" checks config)\n        is_main_process: Only save on main process (for distributed)\n        path_initial_model_for_weight_conversion: For PiSSA/CorDA conversion\n\n    Creates:\n        - adapter_model.safetensors (or adapter_model.bin)\n        - adapter_config.json\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Method on PeftModel, no explicit import needed\n# model.save_pretrained(...)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| save_directory || str || Yes || Output directory path (created if not exists)\n|-\n| safe_serialization || bool || No || Use safetensors format. Default: True\n|-\n| selected_adapters || list[str] || No || Specific adapters to save. Default: all\n|-\n| save_embedding_layers || str or bool || No || Save embeddings. \"auto\" checks target_modules\n|-\n| is_main_process || bool || No || Only save on main process (DDP). Default: True\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| adapter_model.safetensors || File || Adapter weights in safetensors format\n|-\n| adapter_config.json || File || LoraConfig serialized as JSON\n|-\n| README.md || File || Model card (if created)\n|}\n\n== Usage Examples ==\n\n=== Basic Save After Training ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LoraConfig\n\n# After training...\ntrainer.train()\n\n# Save the adapter\nmodel.save_pretrained(\"./my-lora-adapter\")\n\n# Directory contents:\n# ./my-lora-adapter/\n#   adapter_model.safetensors  (~40MB for 7B model with r=16)\n#   adapter_config.json        (~1KB)\n</syntaxhighlight>\n\n=== Save Specific Adapters ===\n<syntaxhighlight lang=\"python\">\n# Model with multiple adapters\nmodel.load_adapter(\"./math-adapter\", adapter_name=\"math\")\nmodel.load_adapter(\"./code-adapter\", adapter_name=\"code\")\n\n# Save only the math adapter\nmodel.save_pretrained(\n    \"./saved-adapters\",\n    selected_adapters=[\"math\"],\n)\n\n# Save all adapters to subdirectories\nmodel.save_pretrained(\n    \"./saved-adapters\",\n    selected_adapters=[\"math\", \"code\"],\n)\n# Creates:\n#   ./saved-adapters/math/adapter_model.safetensors\n#   ./saved-adapters/code/adapter_model.safetensors\n</syntaxhighlight>\n\n=== Push to HuggingFace Hub ===\n<syntaxhighlight lang=\"python\">\n# Save locally then push\nmodel.save_pretrained(\"./my-adapter\")\nmodel.push_to_hub(\"username/my-lora-adapter\")\n\n# Or push directly\nmodel.push_to_hub(\n    \"username/my-lora-adapter\",\n    private=True,\n    commit_message=\"Add trained LoRA adapter\",\n)\n</syntaxhighlight>\n\n=== With PiSSA/CorDA Conversion ===\n<syntaxhighlight lang=\"python\">\n# Convert PiSSA adapter to standard LoRA for compatibility\nmodel.save_pretrained(\n    \"./converted-adapter\",\n    path_initial_model_for_weight_conversion=\"./initial-pissa-adapter\",\n)\n# The saved adapter can now be used without PiSSA-specific loading\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Adapter_Serialization]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Serialization",
        "Fine Tuning",
        "Adapter"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft/quicktour#save-and-load-a-model"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Serialization"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_PolyConfig",
      "page_title": "PolyConfig",
      "page_type": "Implementation",
      "overview": "=== Description === PolyConfig is the configuration class for storing the configuration of a PolyModel. It implements the Polytropon (Poly) and Multi-Head Routing (MHR) parameter-efficient fine-tuning approaches, which use multiple LoRA (Low-Rank Adaptation) modules (\"skills\") that can be combined dynamically for multi-task scenarios. The Poly approach enables efficient multi-task learning by maintaining a pool of LoRA modules that can be routed and combined based on task requirements. Multi-Head Routing (MHR) extends this by allowing splits within each LoRA module for more fine-grained control.",
      "content": "= PolyConfig =\n\n== Knowledge Sources ==\n* Source Repository: https://github.com/huggingface/peft\n* Paper: [https://huggingface.co/papers/2202.13914 Polytropon (Poly)]\n* Paper: [https://huggingface.co/papers/2211.03831 Multi-Head Routing (MHR)]\n\n== Domains ==\n* [[NLP]]\n* [[PEFT]] (Parameter-Efficient Fine-Tuning)\n* [[Multi-Task Learning]]\n* [[LoRA]] (Low-Rank Adaptation)\n* [[Model Adaptation]]\n\n== Overview ==\n\n=== Description ===\nPolyConfig is the configuration class for storing the configuration of a PolyModel. It implements the Polytropon (Poly) and Multi-Head Routing (MHR) parameter-efficient fine-tuning approaches, which use multiple LoRA (Low-Rank Adaptation) modules (\"skills\") that can be combined dynamically for multi-task scenarios.\n\nThe Poly approach enables efficient multi-task learning by maintaining a pool of LoRA modules that can be routed and combined based on task requirements. Multi-Head Routing (MHR) extends this by allowing splits within each LoRA module for more fine-grained control.\n\n=== Usage ===\nPolyConfig is used to configure Poly layers in a model for multi-task learning scenarios. It specifies the number of tasks, skills (LoRA modules), and splits for routing, along with standard LoRA parameters like rank and target modules.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/poly/config.py</code>\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass PolyConfig(PeftConfig):\n    def __init__(\n        self,\n        r: int = 8,\n        target_modules: Optional[Union[list[str], str]] = None,\n        exclude_modules: Optional[Union[list[str], str]] = None,\n        modules_to_save: Optional[list[str]] = None,\n        init_weights: bool = True,\n        poly_type: Literal[\"poly\"] = \"poly\",\n        n_tasks: int = 1,\n        n_skills: int = 4,\n        n_splits: int = 1,\n    )\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.poly.config import PolyConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| r || int || 8 || Attention dimension of each LoRA in Poly\n|-\n| target_modules || Optional[Union[list[str], str]] || None || The names of the modules to apply Poly to (e.g., ['q', 'v'] or regex pattern)\n|-\n| exclude_modules || Optional[Union[list[str], str]] || None || The names of the modules to exclude from Poly (regex match or exact match)\n|-\n| modules_to_save || Optional[list[str]] || None || List of modules apart from Poly layers to be set as trainable and saved\n|-\n| init_weights || bool || True || Whether to initialize the weights of the Poly layers\n|-\n| poly_type || Literal[\"poly\"] || \"poly\" || The variant of the Poly module to use (currently only \"poly\" is supported)\n|-\n| n_tasks || int || 1 || The number of tasks in a multitasking scenario\n|-\n| n_skills || int || 4 || The number of skills (LoRA) in each Poly layer\n|-\n| n_splits || int || 1 || The number of splits within each LoRA (values > 1 indicate Multi-Head Routing)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n! Return Type !! Description\n|-\n| PolyConfig || A configured PolyConfig instance with peft_type set to PeftType.POLY\n|}\n\n=== Post-Initialization ===\nThe <code>__post_init__</code> method:\n* Sets <code>peft_type</code> to <code>PeftType.POLY</code>\n* Converts <code>target_modules</code> to a set if provided as a list\n* Converts <code>exclude_modules</code> to a set if provided as a list\n\n== Usage Examples ==\n\n=== Basic Multi-Task Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PolyConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"base-model-name\")\n\n# Configure Poly for 3 tasks with 4 skills per layer\nconfig = PolyConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    n_tasks=3,\n    n_skills=4,\n    n_splits=1\n)\n\n# Apply Poly to model\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Multi-Head Routing Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PolyConfig, get_peft_model\n\n# Configure Poly with Multi-Head Routing (MHR)\nconfig = PolyConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    n_tasks=5,\n    n_skills=8,\n    n_splits=4,  # Use 4 splits for MHR\n    init_weights=True\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Regex Pattern for Target Modules ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PolyConfig\n\n# Use regex to target specific modules\nconfig = PolyConfig(\n    r=8,\n    target_modules=\".*decoder.*(SelfAttention|EncDecAttention).*(q|v)$\",\n    exclude_modules=[\"classifier\"],\n    n_tasks=10,\n    n_skills=6,\n    modules_to_save=[\"classifier\"]\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_PolyModel|PolyModel]] - The model implementation that uses PolyConfig\n* [[huggingface_peft_PolyRouter|PolyRouter]] - The routing mechanism for Poly layers\n* [[huggingface_peft_PolyLayer|PolyLayer]] - The layer implementation for Poly\n* [[huggingface_peft_LoraConfig|LoraConfig]] - Related LoRA configuration\n* [[PEFT]] - Parameter-Efficient Fine-Tuning overview\n* [[Multi-Task Learning]] - Multi-task learning concepts\n\n== Categories ==\n[[Category:PEFT]]\n[[Category:Configuration]]\n[[Category:Multi-Task Learning]]\n[[Category:LoRA]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_PolyLayer",
      "page_title": "huggingface peft PolyLayer",
      "page_type": "Implementation",
      "overview": "Polytropon adapter layer that uses routable mixtures of low-rank skill modules for multi-task parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|Poly|https://arxiv.org/abs/2307.06069]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Multi_Task_Learning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nPolytropon adapter layer that uses routable mixtures of low-rank skill modules for multi-task parameter-efficient fine-tuning.\n\n=== Description ===\n\nPolyLayer implements the Polytropon method which combines multiple \"skill\" LoRA modules through learned routing. The layer maintains n_skills sets of low-rank matrices (A and B) split across n_splits dimensions. A router module dynamically computes mixing weights based on task IDs or input, combining skills via einsum operations. This allows a single model to handle multiple tasks by routing to different skill combinations.\n\n=== Usage ===\n\nUse Poly for multi-task learning scenarios where different tasks may benefit from different combinations of learned skills. It's particularly effective when tasks have overlapping but distinct requirements. The router can be task-ID based (Poly) or input-based (MoPE) for more fine-grained routing decisions.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/poly/layer.py src/peft/tuners/poly/layer.py]\n* '''Lines:''' 1-166\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass PolyLayer(BaseTunerLayer):\n    \"\"\"\n    Polytropon layer with routable skill modules.\n\n    Attributes:\n        poly_lora_A: ParameterDict of A matrices [n_splits, n_skills, in//n_splits, r]\n        poly_lora_B: ParameterDict of B matrices [n_splits, n_skills, r, out//n_splits]\n        poly_router: ModuleDict of router modules\n        r: Rank for low-rank decomposition\n        n_tasks: Number of tasks\n        n_skills: Number of skill modules\n        n_splits: Number of dimension splits\n    \"\"\"\n    adapter_layer_names = (\"poly_lora_A\", \"poly_lora_B\", \"poly_router\")\n    other_param_names = (\"r\", \"n_tasks\", \"n_skills\", \"n_splits\")\n\n    def update_layer(\n        self,\n        adapter_name,\n        poly_config: PolyConfig,\n        inference_mode: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Create Poly adapter parameters.\"\"\"\n\n    def reset_poly_parameters(self, adapter_name, init_weights):\n        \"\"\"Initialize A/B matrices and router.\"\"\"\n\nclass Linear(nn.Module, PolyLayer):\n    \"\"\"Poly implemented in Linear layer.\"\"\"\n    def forward(\n        self,\n        x: torch.Tensor,\n        *args,\n        task_ids: torch.Tensor = None,\n        **kwargs,\n    ) -> torch.Tensor:\n        \"\"\"Forward with task-based routing.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.poly import PolyLayer, PolyConfig, PolyModel\nfrom peft import PolyConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained Linear layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| r || int || Yes || Rank for low-rank decomposition\n|-\n| n_tasks || int || Yes || Number of tasks for routing\n|-\n| n_skills || int || Yes || Number of skill modules\n|-\n| n_splits || int || No || Number of dimension splits (default: 1)\n|-\n| poly_type || str || No || Router type (\"poly\" or \"mope\")\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Input + routed skill combination\n|-\n| mixing_weights || torch.Tensor || Router output [batch, n_splits, n_skills]\n|}\n\n== Usage Examples ==\n\n=== Basic Poly Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PolyConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Configure Poly for multi-task learning\nconfig = PolyConfig(\n    r=8,\n    n_tasks=4,              # Number of tasks\n    n_skills=8,             # Skill modules to combine\n    n_splits=1,             # Dimension splits\n    target_modules=[\"c_attn\", \"c_proj\"],\n    poly_type=\"poly\",       # Task-based routing\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Poly with Task IDs ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PolyConfig, get_peft_model\n\nconfig = PolyConfig(\n    r=16,\n    n_tasks=3,\n    n_skills=6,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n\n# During training/inference, pass task_ids\ntask_ids = torch.tensor([0, 1, 0, 2])  # Task for each batch item\noutputs = model(input_ids, task_ids=task_ids)\n</syntaxhighlight>\n\n=== MoPE (Input-Based Routing) ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PolyConfig, get_peft_model\n\n# Use input-based routing instead of task IDs\nconfig = PolyConfig(\n    r=8,\n    n_tasks=1,              # Not used with MoPE\n    n_skills=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    poly_type=\"mope\",       # Mixture of Prompt Experts\n)\n\nmodel = get_peft_model(model, config)\n# Routing is determined by input, no task_ids needed\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Multi Task Learning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "Poly",
          "url": "https://arxiv.org/abs/2307.06069"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_PolyModel",
      "page_title": "PolyModel",
      "page_type": "Implementation",
      "overview": "=== Description === PolyModel is a BaseTuner implementation that applies the Polytropon (Poly) parameter-efficient fine-tuning method to neural networks. It manages the creation, replacement, and lifecycle of Poly layers within a model, enabling multi-task learning through dynamic routing of multiple LoRA modules. The model handles task-specific routing by registering forward hooks that inject task IDs into the computation, allowing different combinations of LoRA \"skills\" to be activated for different tasks.",
      "content": "= PolyModel =\n\n== Knowledge Sources ==\n* Source Repository: https://github.com/huggingface/peft\n* Paper: [https://huggingface.co/papers/2202.13914 Polytropon (Poly)]\n* Paper: [https://huggingface.co/papers/2211.03831 Multi-Head Routing (MHR)]\n\n== Domains ==\n* [[NLP]]\n* [[PEFT]] (Parameter-Efficient Fine-Tuning)\n* [[Multi-Task Learning]]\n* [[LoRA]] (Low-Rank Adaptation)\n* [[Model Adaptation]]\n\n== Overview ==\n\n=== Description ===\nPolyModel is a BaseTuner implementation that applies the Polytropon (Poly) parameter-efficient fine-tuning method to neural networks. It manages the creation, replacement, and lifecycle of Poly layers within a model, enabling multi-task learning through dynamic routing of multiple LoRA modules.\n\nThe model handles task-specific routing by registering forward hooks that inject task IDs into the computation, allowing different combinations of LoRA \"skills\" to be activated for different tasks.\n\n=== Usage ===\nPolyModel is typically instantiated through the <code>get_peft_model</code> function with a PolyConfig. It provides <code>forward</code> and <code>generate</code> methods that accept task IDs to enable task-specific routing during inference.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/poly/model.py</code>\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass PolyModel(BaseTuner):\n    prefix: str = \"poly_\"\n    tuner_layer_cls = PolyLayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_POLY_TARGET_MODULES_MAPPING\n\n    def forward(self, *args, task_ids=None, **kwargs)\n    def generate(self, *args, task_ids=None, **kwargs)\n    def _create_and_replace(self, poly_config, adapter_name, target, target_name, parent, **optional_kwargs)\n    @staticmethod\n    def _create_new_module(poly_config, adapter_name, target, **kwargs)\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.poly.model import PolyModel\n# Or use through get_peft_model:\nfrom peft import get_peft_model, PolyConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== forward Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| *args || tuple || - || Positional arguments passed to the base model\n|-\n| task_ids || Optional[torch.Tensor] || None || Tensor of task IDs for routing\n|-\n| **kwargs || dict || - || Keyword arguments passed to the base model\n|}\n\n'''Returns:''' Model outputs with task-specific Poly routing applied\n\n=== generate Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| *args || tuple || - || Positional arguments for generation\n|-\n| task_ids || Optional[torch.Tensor] || None || Tensor of task IDs for routing\n|-\n| **kwargs || dict || - || Keyword arguments for generation\n|}\n\n'''Returns:''' Generated outputs with task-specific Poly routing\n\n=== _create_new_module Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| poly_config || PolyConfig || The Poly configuration\n|-\n| adapter_name || str || Name of the adapter\n|-\n| target || nn.Module || The target module to replace\n|-\n| **kwargs || dict || Optional additional arguments\n|}\n\n'''Returns:''' Linear - A new Poly Linear layer\n\n'''Raises:''' ValueError if target module type is not supported (only <code>torch.nn.Linear</code> is supported)\n\n== Usage Examples ==\n\n=== Basic Multi-Task Inference ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import get_peft_model, PolyConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"base-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"base-model\")\n\n# Configure and apply Poly\nconfig = PolyConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    n_tasks=3,\n    n_skills=4\n)\npeft_model = get_peft_model(model, config)\n\n# Prepare inputs\ninputs = tokenizer(\"Example text\", return_tensors=\"pt\")\n\n# Forward pass with task ID\ntask_ids = torch.tensor([0])  # Task 0\noutputs = peft_model.forward(**inputs, task_ids=task_ids)\n</syntaxhighlight>\n\n=== Text Generation with Task Routing ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import get_peft_model, PolyConfig\n\n# Configure Poly for 5 different tasks\nconfig = PolyConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    n_tasks=5,\n    n_skills=8,\n    n_splits=2\n)\n\npeft_model = get_peft_model(model, config)\npeft_model.eval()\n\n# Generate text for different tasks\nfor task_id in range(5):\n    task_ids = torch.tensor([task_id])\n\n    inputs = tokenizer(f\"Task {task_id} prompt\", return_tensors=\"pt\")\n\n    outputs = peft_model.generate(\n        **inputs,\n        task_ids=task_ids,\n        max_length=50\n    )\n\n    print(f\"Task {task_id}: {tokenizer.decode(outputs[0])}\")\n</syntaxhighlight>\n\n=== Adding Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, PolyConfig\n\n# Create model with first adapter\nconfig1 = PolyConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    n_tasks=3,\n    n_skills=4\n)\n\npeft_model = get_peft_model(model, config1, adapter_name=\"adapter1\")\n\n# Add second adapter\nconfig2 = PolyConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    n_tasks=5,\n    n_skills=6\n)\n\npeft_model.add_adapter(\"adapter2\", config2)\n\n# Switch between adapters\npeft_model.set_adapter(\"adapter1\")\noutputs1 = peft_model.forward(**inputs, task_ids=torch.tensor([0]))\n\npeft_model.set_adapter(\"adapter2\")\noutputs2 = peft_model.forward(**inputs, task_ids=torch.tensor([1]))\n</syntaxhighlight>\n\n=== Context Manager for Pre-Hooks ===\n<syntaxhighlight lang=\"python\">\n# The model internally uses a context manager for pre-hooks\n# This is handled automatically in forward/generate methods\n\n# Manual usage (advanced):\nwith peft_model._manage_pre_hooks(task_ids=torch.tensor([2])):\n    # All forward passes in this context will use task_id=2\n    output = peft_model.model(**inputs)\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Module Replacement ===\nThe <code>_create_and_replace</code> method handles two scenarios:\n# '''Updating existing PolyLayer''': If target is already a PolyLayer, updates it with the new adapter\n# '''Creating new PolyLayer''': Replaces a standard Linear module with a new Poly Linear layer\n\n=== Pre-Hook Management ===\nPolyModel uses forward pre-hooks to inject task IDs:\n* Pre-hooks are registered on all Poly Linear modules\n* Task IDs are passed through kwargs during forward pass\n* Context manager ensures proper cleanup of hooks after execution\n\n=== Supported Target Modules ===\nCurrently only <code>torch.nn.Linear</code> modules are supported for Poly transformation.\n\n== Related Pages ==\n* [[huggingface_peft_PolyConfig|PolyConfig]] - Configuration for PolyModel\n* [[huggingface_peft_PolyRouter|PolyRouter]] - Routing mechanism for Poly\n* [[huggingface_peft_PolyLayer|PolyLayer]] - Layer implementation\n* [[huggingface_peft_BaseTuner|BaseTuner]] - Base class for tuners\n* [[PEFT]] - Parameter-Efficient Fine-Tuning\n* [[Multi-Task Learning]]\n\n== Categories ==\n[[Category:PEFT]]\n[[Category:Model]]\n[[Category:Multi-Task Learning]]\n[[Category:LoRA]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_PolyRouter",
      "page_title": "PolyRouter",
      "page_type": "Implementation",
      "overview": "=== Description === PolyRouter is a neural routing module that dynamically computes mixture weights for combining multiple LoRA \"skills\" in a Poly layer. It maintains learnable logits for each task and produces normalized weights using sigmoid activation and Gumbel-Softmax sampling during training. The router implements task-specific routing by learning a matrix of logits (n_tasks \u00d7 n_skills \u00d7 n_splits) and computing normalized weights based on the task ID. During training, it uses RelaxedBernoulli (Gumbel-Softmax) sampling for differentiable routing; during inference, it uses deterministic sigmoid activation.",
      "content": "= PolyRouter =\n\n== Knowledge Sources ==\n* Source Repository: https://github.com/huggingface/peft\n* Reference Implementation: https://github.com/microsoft/mttl/blob/ce4ca51dbca73be656feb9b3e5233633e3c5dec7/mttl/models/poly.py#L138\n* Paper: [https://huggingface.co/papers/2202.13914 Polytropon (Poly)]\n* Paper: [https://huggingface.co/papers/2211.03831 Multi-Head Routing (MHR)]\n\n== Domains ==\n* [[NLP]]\n* [[PEFT]] (Parameter-Efficient Fine-Tuning)\n* [[Multi-Task Learning]]\n* [[Neural Network Routing]]\n* [[Adaptive Computation]]\n\n== Overview ==\n\n=== Description ===\nPolyRouter is a neural routing module that dynamically computes mixture weights for combining multiple LoRA \"skills\" in a Poly layer. It maintains learnable logits for each task and produces normalized weights using sigmoid activation and Gumbel-Softmax sampling during training.\n\nThe router implements task-specific routing by learning a matrix of logits (n_tasks \u00d7 n_skills \u00d7 n_splits) and computing normalized weights based on the task ID. During training, it uses RelaxedBernoulli (Gumbel-Softmax) sampling for differentiable routing; during inference, it uses deterministic sigmoid activation.\n\n=== Usage ===\nPolyRouter is instantiated automatically when creating Poly layers. It's called during forward passes with task IDs and input tensors to produce routing weights that determine how different LoRA skills are combined.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/poly/router.py</code>\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass PolyRouter(Router):\n    def __init__(self, poly_config: PolyConfig)\n    def reset(self)\n    def forward(self, task_ids: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.poly.router import PolyRouter, get_router\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== __init__ Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| poly_config || PolyConfig || Configuration containing n_tasks, n_skills, n_splits, and poly_type\n|}\n\n'''Creates:''' A PolyRouter with learnable module_logits parameter of shape (n_tasks, n_splits \u00d7 n_skills)\n\n=== forward Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| task_ids || torch.Tensor || Tensor containing task IDs (must not be None)\n|-\n| input_ids || torch.Tensor || Input tensor (currently unused in routing computation)\n|}\n\n'''Returns:''' torch.Tensor - Normalized routing weights of shape (batch_size, n_splits, n_skills)\n\n'''Raises:'''\n* ValueError if task_ids is None\n* ValueError if any task_id >= n_tasks\n\n=== reset Method ===\nReinitializes the module_logits parameter with uniform distribution U(-1e-3, 1e-3).\n\n== Routing Computation ==\n\nThe forward pass computes routing weights through:\n\n# '''Logit Selection''': <code>module_logits = self.module_logits[task_ids]</code>\n# '''Reshape''': Reshape to (batch_size, n_splits, n_skills)\n# '''Stochastic Sampling''' (training): <code>RelaxedBernoulli(temperature=1.0, logits=module_logits).rsample()</code>\n# '''Deterministic''' (inference): <code>torch.sigmoid(module_logits)</code>\n# '''Normalization''': <code>module_weights = module_logits / (module_logits.sum(dim=-1, keepdim=True) + EPS)</code>\n\nWhere EPS = 1e-12 prevents division by zero.\n\n== Usage Examples ==\n\n=== Creating a Router ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.poly.config import PolyConfig\nfrom peft.tuners.poly.router import get_router\nimport torch\n\n# Create configuration\nconfig = PolyConfig(\n    r=8,\n    n_tasks=5,\n    n_skills=8,\n    n_splits=2,\n    poly_type=\"poly\"\n)\n\n# Get router instance\nrouter = get_router(config)\n\nprint(f\"Router logits shape: {router.module_logits.shape}\")\n# Output: torch.Size([5, 16])  # 5 tasks, 16 = 2 splits \u00d7 8 skills\n</syntaxhighlight>\n\n=== Computing Routing Weights ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft.tuners.poly.router import PolyRouter\nfrom peft.tuners.poly.config import PolyConfig\n\n# Initialize router\nconfig = PolyConfig(n_tasks=3, n_skills=4, n_splits=2)\nrouter = PolyRouter(config)\nrouter.eval()  # Set to inference mode\n\n# Prepare task IDs\ntask_ids = torch.tensor([0, 1, 2])  # Batch of 3 samples with different tasks\ninput_ids = torch.randn(3, 128)  # Dummy input\n\n# Compute routing weights\nweights = router.forward(task_ids, input_ids)\n\nprint(f\"Weights shape: {weights.shape}\")\n# Output: torch.Size([3, 2, 4])  # batch=3, splits=2, skills=4\n\nprint(f\"Weights sum per split: {weights.sum(dim=-1)}\")\n# Each split's weights sum to approximately 1.0\n</syntaxhighlight>\n\n=== Training vs Inference Mode ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft.tuners.poly.router import PolyRouter\nfrom peft.tuners.poly.config import PolyConfig\n\nconfig = PolyConfig(n_tasks=2, n_skills=4, n_splits=1)\nrouter = PolyRouter(config)\n\ntask_ids = torch.tensor([0])\ninput_ids = torch.randn(1, 128)\n\n# Training mode: stochastic sampling\nrouter.train()\nweights_train_1 = router.forward(task_ids, input_ids)\nweights_train_2 = router.forward(task_ids, input_ids)\nprint(f\"Training mode - weights differ: {not torch.allclose(weights_train_1, weights_train_2)}\")\n# Output: True (sampling introduces randomness)\n\n# Inference mode: deterministic\nrouter.eval()\nweights_eval_1 = router.forward(task_ids, input_ids)\nweights_eval_2 = router.forward(task_ids, input_ids)\nprint(f\"Eval mode - weights identical: {torch.allclose(weights_eval_1, weights_eval_2)}\")\n# Output: True (deterministic sigmoid)\n</syntaxhighlight>\n\n=== Resetting Router Weights ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft.tuners.poly.router import PolyRouter\nfrom peft.tuners.poly.config import PolyConfig\n\nconfig = PolyConfig(n_tasks=2, n_skills=4, n_splits=1)\nrouter = PolyRouter(config)\n\n# Original logits\noriginal_logits = router.module_logits.clone()\n\n# Modify logits\nrouter.module_logits.data.fill_(10.0)\n\n# Reset to small random values\nrouter.reset()\n\nprint(f\"After reset, logits are small: {router.module_logits.abs().max() < 1e-2}\")\n# Output: True\nprint(f\"Logits range: [{router.module_logits.min():.6f}, {router.module_logits.max():.6f}]\")\n# Output approximately in [-0.001, 0.001]\n</syntaxhighlight>\n\n=== Multi-Head Routing Example ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft.tuners.poly.router import get_router\nfrom peft.tuners.poly.config import PolyConfig\n\n# Configure Multi-Head Routing with 4 splits\nconfig = PolyConfig(\n    n_tasks=10,\n    n_skills=8,\n    n_splits=4  # Multi-Head Routing\n)\n\nrouter = get_router(config)\nrouter.eval()\n\n# Process batch with different tasks\ntask_ids = torch.tensor([0, 2, 5, 9])\ninput_ids = torch.randn(4, 256)\n\nweights = router.forward(task_ids, input_ids)\n\nprint(f\"MHR weights shape: {weights.shape}\")\n# Output: torch.Size([4, 4, 8])  # batch=4, splits=4, skills=8\n\n# Each split has independent routing\nfor split_idx in range(4):\n    print(f\"Split {split_idx} weights sum: {weights[:, split_idx, :].sum(dim=-1)}\")\n    # Each should sum to approximately 1.0\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Gumbel-Softmax Sampling ===\nDuring training, RelaxedBernoulli (Gumbel-Softmax) provides:\n* '''Differentiability''': Gradients flow through sampling operation\n* '''Exploration''': Stochastic routing prevents collapse to single skill\n* '''Temperature''': Fixed at 1.0 for balanced exploration/exploitation\n\n=== Normalization ===\nWeights are normalized by dividing by the sum across skills, ensuring each split's weights sum to 1.0. The epsilon constant (1e-12) prevents numerical issues.\n\n=== Device Handling ===\nTask IDs are automatically moved to the router's device: <code>task_ids = task_ids.to(self.module_logits.device)</code>\n\n== Related Pages ==\n* [[huggingface_peft_PolyConfig|PolyConfig]] - Configuration for Poly routing\n* [[huggingface_peft_PolyModel|PolyModel]] - Model using PolyRouter\n* [[huggingface_peft_PolyLayer|PolyLayer]] - Layer implementation\n* [[Gumbel-Softmax]] - Sampling technique\n* [[Multi-Task Learning]]\n* [[Neural Network Routing]]\n\n== Categories ==\n[[Category:PEFT]]\n[[Category:Routing]]\n[[Category:Multi-Task Learning]]\n[[Category:Neural Networks]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_PrefixEncoder",
      "page_title": "huggingface peft PrefixEncoder",
      "page_type": "Implementation",
      "overview": "",
      "content": "== PrefixEncoder ==\n\n=== Knowledge Sources ===\n* [https://github.com/huggingface/peft PEFT Repository]\n* [https://github.com/THUDM/P-tuning-v2 P-tuning v2 (Original Implementation)]\n* [https://arxiv.org/abs/2101.00190 Prefix-Tuning Paper]\n\n=== Domains ===\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Prompt_Tuning]]\n[[Category:Prefix_Tuning]]\n[[Category:Neural_Networks]]\n\n=== Overview ===\n\n==== Description ====\n'''PrefixEncoder''' is a PyTorch neural network module that encodes prefix embeddings for prefix tuning. It generates continuous vectors that are prepended to the keys and values at each layer of a transformer model, effectively conditioning the model's behavior without modifying its parameters.\n\nThe encoder supports two modes:\n* '''Direct embedding''': Maps virtual token indices directly to prefix embeddings\n* '''Projected embedding''': Uses a two-layer MLP (with tanh activation) to reparameterize the prefix, providing better training stability and optimization\n\nThe architecture is based on the P-tuning v2 implementation and outputs prefix embeddings that contain both key and value vectors for all transformer layers (shape: batch_size \u00d7 num_virtual_tokens \u00d7 2*layers*hidden_dim).\n\n==== Usage ====\nUsed as the prefix generation component in prefix tuning PEFT models. It creates learnable continuous prefixes that are inserted into the attention mechanism at each transformer layer, allowing task-specific adaptation while keeping the base model frozen.\n\n=== Code Reference ===\n\n==== Source Location ====\n<code>src/peft/tuners/prefix_tuning/model.py</code>\n\n==== Signature ====\n<syntaxhighlight lang=\"python\">\nclass PrefixEncoder(torch.nn.Module):\n    r\"\"\"\n    The `torch.nn` model to encode the prefix.\n\n    Args:\n        config ([`PrefixTuningConfig`]): The configuration of the prefix encoder.\n\n    Example:\n\n    ```py\n    >>> from peft import PrefixEncoder, PrefixTuningConfig\n\n    >>> config = PrefixTuningConfig(\n    ...     peft_type=\"PREFIX_TUNING\",\n    ...     task_type=\"SEQ_2_SEQ_LM\",\n    ...     num_virtual_tokens=20,\n    ...     token_dim=768,\n    ...     num_transformer_submodules=1,\n    ...     num_attention_heads=12,\n    ...     num_layers=12,\n    ...     encoder_hidden_size=768,\n    ... )\n    >>> prefix_encoder = PrefixEncoder(config)\n    ```\n\n    **Attributes**:\n        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prefix encoder.\n        - **transform** (`torch.nn.Sequential`) -- The two-layer MLP to transform the prefix embeddings if\n          `prefix_projection` is `True`.\n        - **prefix_projection** (`bool`) -- Whether to project the prefix embeddings.\n\n    Input shape: (`batch_size`, `num_virtual_tokens`)\n\n    Output shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initialize the prefix encoder.\n\n        Args:\n            config: PrefixTuningConfig object\n        \"\"\"\n\n    def forward(self, prefix: torch.Tensor):\n        \"\"\"\n        Generate prefix key-value embeddings for all layers.\n\n        Args:\n            prefix: Prefix token indices\n\n        Returns:\n            Past key-value embeddings for transformer layers\n        \"\"\"\n</syntaxhighlight>\n\n==== Import ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PrefixEncoder, PrefixTuningConfig\n</syntaxhighlight>\n\n=== I/O Contract ===\n\n==== Constructor Parameters ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| config || PrefixTuningConfig || Configuration object specifying encoder architecture\n|}\n\n==== Forward Method ====\n\n===== Inputs =====\n{| class=\"wikitable\"\n! Parameter !! Type !! Shape !! Description\n|-\n| prefix || torch.Tensor || (batch_size, num_virtual_tokens) || Indices for prefix tokens\n|}\n\n===== Returns =====\n{| class=\"wikitable\"\n! Type !! Shape !! Description\n|-\n| torch.Tensor || (batch_size, num_virtual_tokens, 2*num_layers*token_dim) || Past key-value embeddings for all transformer layers\n|}\n\n==== Model Attributes ====\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| prefix_projection || bool || Whether to use MLP projection for prefix embeddings\n|-\n| embedding || torch.nn.Embedding || Embedding layer for prefix tokens\n|-\n| transform || torch.nn.Sequential || Two-layer MLP for reparameterization (only if prefix_projection=True)\n|}\n\n==== Side Effects ====\n* Creates trainable embedding parameters\n* Creates trainable MLP parameters if prefix_projection is enabled\n\n=== Usage Examples ===\n\n==== Basic Prefix Encoder Without Projection ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PrefixEncoder, PrefixTuningConfig\n\n# Configuration without projection\nconfig = PrefixTuningConfig(\n    peft_type=\"PREFIX_TUNING\",\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_layers=12,\n    prefix_projection=False\n)\n\n# Create prefix encoder\nprefix_encoder = PrefixEncoder(config)\n\n# Generate prefix embeddings\nbatch_size = 4\nprefix_indices = torch.arange(20).unsqueeze(0).expand(batch_size, -1)\npast_key_values = prefix_encoder(prefix_indices)\n\n# Output shape: (batch_size, num_virtual_tokens, 2*num_layers*token_dim)\nprint(past_key_values.shape)  # torch.Size([4, 20, 18432])\n# 18432 = 2 * 12 * 768 (keys and values for 12 layers, each 768-dim)\n</syntaxhighlight>\n\n==== Prefix Encoder With MLP Projection ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PrefixEncoder, PrefixTuningConfig\n\n# Configuration with MLP projection (recommended)\nconfig = PrefixTuningConfig(\n    peft_type=\"PREFIX_TUNING\",\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_layers=12,\n    encoder_hidden_size=512,\n    prefix_projection=True\n)\n\n# Create prefix encoder\nprefix_encoder = PrefixEncoder(config)\n\n# Generate prefix embeddings\nbatch_size = 4\nprefix_indices = torch.arange(20).unsqueeze(0).expand(batch_size, -1)\npast_key_values = prefix_encoder(prefix_indices)\n\nprint(past_key_values.shape)  # torch.Size([4, 20, 18432])\n</syntaxhighlight>\n\n==== Encoder-Decoder Model (Seq2Seq) ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PrefixEncoder, PrefixTuningConfig\n\n# Configuration for seq2seq models\nconfig = PrefixTuningConfig(\n    peft_type=\"PREFIX_TUNING\",\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_transformer_submodules=2,  # Encoder + Decoder\n    num_attention_heads=12,\n    num_layers=12,\n    encoder_hidden_size=768,\n    prefix_projection=True\n)\n\nprefix_encoder = PrefixEncoder(config)\n\n# Forward pass\nbatch_size = 4\nprefix_indices = torch.arange(20).unsqueeze(0).expand(batch_size, -1)\npast_key_values = prefix_encoder(prefix_indices)\n\nprint(past_key_values.shape)  # torch.Size([4, 20, 18432])\n</syntaxhighlight>\n\n==== Integration with PEFT Model ====\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, PrefixTuningConfig\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Create prefix tuning configuration\nconfig = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=30,\n    encoder_hidden_size=512,\n    prefix_projection=True\n)\n\n# Apply prefix tuning (PrefixEncoder is created internally)\npeft_model = get_peft_model(model, config)\n\n# Check trainable parameters\npeft_model.print_trainable_parameters()\n# Output shows only prefix parameters are trainable\n</syntaxhighlight>\n\n==== Inspecting Model Architecture ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PrefixEncoder, PrefixTuningConfig\n\n# Configuration with projection\nconfig = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_layers=12,\n    encoder_hidden_size=512,\n    prefix_projection=True\n)\n\nprefix_encoder = PrefixEncoder(config)\n\nprint(\"Prefix projection enabled:\", prefix_encoder.prefix_projection)\nprint(\"\\nEmbedding layer:\", prefix_encoder.embedding)\nprint(\"\\nTransform MLP:\")\nfor i, layer in enumerate(prefix_encoder.transform):\n    print(f\"  Layer {i}: {layer}\")\n\n# Output:\n# Prefix projection enabled: True\n# Embedding layer: Embedding(20, 768)\n# Transform MLP:\n#   Layer 0: Linear(in_features=768, out_features=512, bias=True)\n#   Layer 1: Tanh()\n#   Layer 2: Linear(in_features=512, out_features=18432, bias=True)\n</syntaxhighlight>\n\n==== Understanding the Output Shape ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PrefixEncoder, PrefixTuningConfig\n\nconfig = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=10,\n    token_dim=768,\n    num_layers=6,\n    prefix_projection=False\n)\n\nprefix_encoder = PrefixEncoder(config)\n\n# Forward pass\nbatch_size = 2\nprefix = torch.arange(10).unsqueeze(0).expand(batch_size, -1)\noutput = prefix_encoder(prefix)\n\nprint(f\"Output shape: {output.shape}\")\n# Output: torch.Size([2, 10, 9216])\n\n# Breaking down the output dimension:\nnum_layers = 6\ntoken_dim = 768\noutput_dim = 2 * num_layers * token_dim  # 2 for keys and values\nprint(f\"Output dimension breakdown: 2 \u00d7 {num_layers} \u00d7 {token_dim} = {output_dim}\")\n# Output: Output dimension breakdown: 2 \u00d7 6 \u00d7 768 = 9216\n\n# This output is then reshaped and split to create past_key_values\n# for each of the 6 transformer layers\n</syntaxhighlight>\n\n==== Comparing Direct vs Projected Embeddings ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PrefixEncoder, PrefixTuningConfig\n\n# Direct embedding (no projection)\nconfig_direct = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_layers=12,\n    prefix_projection=False\n)\n\nencoder_direct = PrefixEncoder(config_direct)\nprint(\"Direct embedding size:\", encoder_direct.embedding.weight.shape)\n# Output: torch.Size([20, 18432])\n# Directly embeds to final size\n\n# With MLP projection\nconfig_mlp = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_layers=12,\n    encoder_hidden_size=512,\n    prefix_projection=True\n)\n\nencoder_mlp = PrefixEncoder(config_mlp)\nprint(\"MLP - Embedding size:\", encoder_mlp.embedding.weight.shape)\nprint(\"MLP - First linear:\", encoder_mlp.transform[0])\nprint(\"MLP - Second linear:\", encoder_mlp.transform[2])\n# Output:\n# MLP - Embedding size: torch.Size([20, 768])\n# MLP - First linear: Linear(in_features=768, out_features=512, bias=True)\n# MLP - Second linear: Linear(in_features=512, out_features=18432, bias=True)\n</syntaxhighlight>\n\n=== Related Pages ===\n* [[huggingface_peft_PrefixTuningConfig|PrefixTuningConfig]] - Configuration for this encoder\n* [[huggingface_peft_PromptEncoder|PromptEncoder]] - Related P-tuning encoder\n* [[huggingface_peft_MultitaskPromptTuningModel|MultitaskPromptEmbedding]] - Related multitask prompt embedding\n* [[Prefix_Tuning|Prefix Tuning]]\n* [[PEFT|Parameter-Efficient Fine-Tuning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_PrefixTuningConfig",
      "page_title": "huggingface peft PrefixTuningConfig",
      "page_type": "Implementation",
      "overview": "",
      "content": "== PrefixTuningConfig ==\n\n=== Knowledge Sources ===\n* [https://github.com/huggingface/peft PEFT Repository]\n* [https://arxiv.org/abs/2101.00190 Prefix-Tuning Paper]\n\n=== Domains ===\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Prompt_Tuning]]\n[[Category:Prefix_Tuning]]\n[[Category:Configuration]]\n\n=== Overview ===\n\n==== Description ====\n'''PrefixTuningConfig''' is the configuration class for prefix tuning, a parameter-efficient fine-tuning method that prepends trainable continuous vectors (prefixes) to the hidden states at each transformer layer. This approach, introduced in the paper \"Prefix-Tuning: Optimizing Continuous Prompts for Generation\", enables task-specific adaptation while keeping the base model parameters frozen.\n\nThe configuration manages two key aspects:\n* '''encoder_hidden_size''': The dimensionality of the hidden layer in the optional prefix encoder\n* '''prefix_projection''': Whether to use an MLP to project prefix embeddings (recommended for improved training stability)\n\nWhen prefix_projection is enabled, a two-layer MLP with tanh activation transforms the prefix embeddings, providing better reparameterization and more stable optimization.\n\n==== Usage ====\nUsed to configure prefix tuning when adapting pre-trained language models to downstream tasks. Prefix tuning is particularly effective for generation tasks and provides a flexible alternative to full fine-tuning or adapter methods.\n\n=== Code Reference ===\n\n==== Source Location ====\n<code>src/peft/tuners/prefix_tuning/config.py</code>\n\n==== Signature ====\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass PrefixTuningConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`PrefixEncoder`].\n\n    Args:\n        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n        prefix_projection (`bool`): Whether to project the prefix embeddings.\n    \"\"\"\n\n    encoder_hidden_size: int = field(\n        default=None,\n        metadata={\"help\": \"The hidden size of the encoder\"},\n    )\n    prefix_projection: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to project the prefix tokens\"},\n    )\n</syntaxhighlight>\n\n==== Import ====\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.prefix_tuning import PrefixTuningConfig\n</syntaxhighlight>\n\n=== I/O Contract ===\n\n==== Configuration Parameters ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| encoder_hidden_size || int || None || Hidden size of the MLP encoder used when prefix_projection is True\n|-\n| prefix_projection || bool || False || Whether to use an MLP to reparameterize prefix embeddings\n|-\n| colspan=\"4\" | ''Inherits all parameters from PromptLearningConfig''\n|}\n\n==== Key Inherited Parameters ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| num_virtual_tokens || int || Number of virtual tokens (prefix length)\n|-\n| token_dim || int || Dimension of the token embeddings\n|-\n| num_layers || int || Number of transformer layers\n|-\n| task_type || str/TaskType || Type of task (e.g., \"CAUSAL_LM\", \"SEQ_2_SEQ_LM\")\n|}\n\n==== Returns ====\nConfiguration object ready to be used with PrefixEncoder model.\n\n==== Side Effects ====\n* Sets peft_type to PeftType.PREFIX_TUNING in __post_init__\n\n=== Usage Examples ===\n\n==== Basic Configuration Without Projection ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PrefixTuningConfig\n\n# Simple prefix tuning without MLP projection\nconfig = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=30,\n    token_dim=768,\n    num_layers=12,\n    prefix_projection=False\n)\n</syntaxhighlight>\n\n==== Configuration With Projection (Recommended) ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PrefixTuningConfig\n\n# Prefix tuning with MLP projection for better stability\nconfig = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=30,\n    token_dim=768,\n    num_layers=12,\n    encoder_hidden_size=512,\n    prefix_projection=True\n)\n</syntaxhighlight>\n\n==== Seq2Seq Model Configuration ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PrefixTuningConfig\n\n# Configuration for encoder-decoder models\nconfig = PrefixTuningConfig(\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_transformer_submodules=2,  # Encoder + Decoder\n    num_attention_heads=12,\n    num_layers=12,\n    encoder_hidden_size=768,\n    prefix_projection=True\n)\n</syntaxhighlight>\n\n==== Complete Example with Model ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PrefixTuningConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Create configuration\nconfig = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    encoder_hidden_size=1024,\n    prefix_projection=True\n)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Apply prefix tuning\npeft_model = get_peft_model(model, config)\n\n# Check trainable parameters\npeft_model.print_trainable_parameters()\n# Output shows only prefix parameters are trainable\n</syntaxhighlight>\n\n==== Comparing Projection vs No Projection ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PrefixTuningConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Without projection\nconfig_no_proj = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    prefix_projection=False\n)\nmodel1 = get_peft_model(model, config_no_proj)\n\n# With projection (more parameters but better training stability)\nconfig_with_proj = PrefixTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    encoder_hidden_size=512,\n    prefix_projection=True\n)\nmodel2 = get_peft_model(model, config_with_proj)\n\nprint(\"Without projection:\")\nmodel1.print_trainable_parameters()\n\nprint(\"\\nWith projection:\")\nmodel2.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Related Pages ===\n* [[huggingface_peft_PrefixEncoder|PrefixEncoder]] - The model implementation using this configuration\n* [[huggingface_peft_PromptLearningConfig|PromptLearningConfig]] - Parent configuration class\n* [[huggingface_peft_PromptEncoderConfig|PromptEncoderConfig]] - Related P-tuning configuration\n* [[Prefix_Tuning|Prefix Tuning]]\n* [[PEFT|Parameter-Efficient Fine-Tuning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_PromptEmbedding",
      "page_title": "huggingface peft PromptEmbedding",
      "page_type": "Implementation",
      "overview": "Prompt embedding module that encodes virtual tokens into continuous prompt embeddings for prefix-based parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|Prompt Tuning|https://arxiv.org/abs/2104.08691]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Prompt_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nPrompt embedding module that encodes virtual tokens into continuous prompt embeddings for prefix-based parameter-efficient fine-tuning.\n\n=== Description ===\n\nPromptEmbedding creates learnable virtual token embeddings that are prepended to the input sequence. The module supports three initialization strategies: random initialization, sampling from the model's vocabulary (SAMPLE_VOCAB), or initializing from a text prompt (TEXT). Text initialization tokenizes the provided string and uses those embeddings as starting points, which often leads to faster convergence than random initialization.\n\n=== Usage ===\n\nUse PromptEmbedding for prompt tuning where you want to learn soft prompts instead of modifying model weights. This is the most parameter-efficient method as only the prompt embeddings are trained. TEXT initialization is recommended when you have a good natural language description of the task.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/prompt_tuning/model.py src/peft/tuners/prompt_tuning/model.py]\n* '''Lines:''' 1-106\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass PromptEmbedding(torch.nn.Module):\n    \"\"\"\n    The model to encode virtual tokens into prompt embeddings.\n\n    Args:\n        config: PromptTuningConfig with num_virtual_tokens, token_dim, etc.\n        word_embeddings: The word embeddings of the base transformer model\n\n    Attributes:\n        embedding: torch.nn.Embedding layer for virtual tokens\n\n    Input Shape: (batch_size, total_virtual_tokens)\n    Output Shape: (batch_size, total_virtual_tokens, token_dim)\n    \"\"\"\n\n    def __init__(self, config, word_embeddings):\n        \"\"\"Initialize prompt embedding with optional text/vocab initialization.\"\"\"\n\n    def forward(self, indices):\n        \"\"\"Get embeddings for virtual token indices.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptEmbedding, PromptTuningConfig\nfrom peft.tuners.prompt_tuning import PromptEmbedding\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| config || PromptTuningConfig || Yes || Configuration with num_virtual_tokens, token_dim\n|-\n| word_embeddings || nn.Module || Yes || Base model's word embedding layer\n|-\n| indices || torch.Tensor || Yes || Virtual token indices for forward pass\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| prompt_embeddings || torch.Tensor || Embeddings [batch, num_virtual_tokens, token_dim]\n|}\n\n== Usage Examples ==\n\n=== Basic Prompt Tuning ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Random initialization\nconfig = PromptTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n# Only prompt embeddings are trained\n</syntaxhighlight>\n\n=== Text-Initialized Prompt Tuning ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, PromptTuningInit, get_peft_model\n\n# Initialize prompts from text description\nconfig = PromptTuningConfig(\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    prompt_tuning_init=PromptTuningInit.TEXT,\n    prompt_tuning_init_text=\"Classify the sentiment of this review:\",\n    tokenizer_name_or_path=\"t5-base\",\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Vocabulary-Sampled Initialization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, PromptTuningInit, get_peft_model\n\n# Sample random tokens from vocabulary for initialization\nconfig = PromptTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=10,\n    prompt_tuning_init=PromptTuningInit.SAMPLE_VOCAB,\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Direct PromptEmbedding Usage ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptEmbedding, PromptTuningConfig\n\nconfig = PromptTuningConfig(\n    peft_type=\"PROMPT_TUNING\",\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_transformer_submodules=1,\n    prompt_tuning_init=\"TEXT\",\n    prompt_tuning_init_text=\"Predict sentiment:\",\n    tokenizer_name_or_path=\"t5-base\",\n)\n\n# Create prompt embedding using model's word embeddings\nprompt_embedding = PromptEmbedding(config, t5_model.shared)\n\n# Get embeddings for batch\nindices = torch.arange(20).unsqueeze(0).expand(batch_size, -1)\nembeddings = prompt_embedding(indices)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Prompt Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "Prompt Tuning",
          "url": "https://arxiv.org/abs/2104.08691"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_PromptEncoder",
      "page_title": "huggingface peft PromptEncoder",
      "page_type": "Implementation",
      "overview": "",
      "content": "== PromptEncoder ==\n\n=== Knowledge Sources ===\n* [https://github.com/huggingface/peft PEFT Repository]\n* [https://github.com/NVIDIA/NeMo NeMo Framework (Original Implementation)]\n* [https://arxiv.org/abs/2103.10385 GPT Understands, Too (P-tuning Paper)]\n\n=== Domains ===\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Prompt_Tuning]]\n[[Category:P_Tuning]]\n[[Category:Neural_Networks]]\n\n=== Overview ===\n\n==== Description ====\n'''PromptEncoder''' is a PyTorch neural network module that generates virtual token embeddings for P-tuning. Unlike simple embedding layers, the PromptEncoder uses a trainable neural network (MLP or LSTM) to reparameterize the prompt embeddings, making them more expressive and effective.\n\nThe architecture consists of:\n* An embedding layer that creates initial token representations\n* A reparameterization network (MLP or bidirectional LSTM) that transforms embeddings\n* For LSTM: A bidirectional LSTM followed by a 2-layer MLP\n* For MLP: A 2-layer feed-forward network with ReLU activations\n\nThis approach, based on NVIDIA's NeMo implementation, has been shown to significantly improve prompt-based learning, especially for smaller language models. The encoder transforms fixed-size virtual token indices into continuous embeddings that can be prepended to the input sequence.\n\n==== Usage ====\nUsed as the prompt generation component in P-tuning PEFT models. It creates learnable continuous prompts that guide model behavior without modifying the base model parameters.\n\n=== Code Reference ===\n\n==== Source Location ====\n<code>src/peft/tuners/p_tuning/model.py</code>\n\n==== Signature ====\n<syntaxhighlight lang=\"python\">\nclass PromptEncoder(torch.nn.Module):\n    \"\"\"\n    The prompt encoder network that is used to generate the virtual token embeddings for p-tuning.\n\n    Args:\n        config ([`PromptEncoderConfig`]): The configuration of the prompt encoder.\n\n    Input shape: (`batch_size`, `total_virtual_tokens`)\n\n    Output shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initialize the prompt encoder.\n\n        Args:\n            config: PromptEncoderConfig object\n        \"\"\"\n\n    def forward(self, indices):\n        \"\"\"\n        Generate continuous prompt embeddings.\n\n        Args:\n            indices: Virtual token indices\n\n        Returns:\n            Continuous prompt embeddings\n        \"\"\"\n</syntaxhighlight>\n\n==== Import ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptEncoder, PromptEncoderConfig\n</syntaxhighlight>\n\n=== I/O Contract ===\n\n==== Constructor Parameters ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| config || PromptEncoderConfig || Configuration object specifying encoder architecture\n|}\n\n==== Forward Method ====\n\n===== Inputs =====\n{| class=\"wikitable\"\n! Parameter !! Type !! Shape !! Description\n|-\n| indices || torch.Tensor || (batch_size, total_virtual_tokens) || Indices for virtual token embeddings\n|}\n\n===== Returns =====\n{| class=\"wikitable\"\n! Type !! Shape !! Description\n|-\n| torch.Tensor || (batch_size, total_virtual_tokens, token_dim) || Continuous prompt embeddings\n|}\n\n==== Model Attributes ====\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| token_dim || int || Hidden embedding dimension of the base transformer\n|-\n| input_size || int || Input size to the encoder (equals token_dim)\n|-\n| output_size || int || Output size from the encoder (equals token_dim)\n|-\n| hidden_size || int || Hidden size of the encoder network\n|-\n| total_virtual_tokens || int || Total number of virtual tokens (num_virtual_tokens \u00d7 num_transformer_submodules)\n|-\n| encoder_type || PromptEncoderReparameterizationType || Type of encoder (MLP or LSTM)\n|-\n| embedding || torch.nn.Embedding || Embedding layer for virtual tokens\n|-\n| mlp_head || torch.nn.Sequential || MLP transformation network (always present when not in inference mode)\n|-\n| lstm_head || torch.nn.LSTM || Bidirectional LSTM (only present when encoder_type is LSTM)\n|}\n\n==== Side Effects ====\n* Creates trainable embedding and encoder parameters\n* Warns if encoder_num_layers is specified for MLP (always uses 2 layers)\n\n==== Exceptions ====\n* '''ValueError''': Raised if encoder_reparameterization_type is not MLP or LSTM\n\n=== Usage Examples ===\n\n==== Basic MLP Encoder ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PromptEncoder, PromptEncoderConfig\n\n# Create configuration with MLP encoder\nconfig = PromptEncoderConfig(\n    peft_type=\"P_TUNING\",\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_transformer_submodules=1,\n    num_attention_heads=12,\n    num_layers=12,\n    encoder_reparameterization_type=\"MLP\",\n    encoder_hidden_size=768,\n)\n\n# Create prompt encoder\nprompt_encoder = PromptEncoder(config)\n\n# Generate prompt embeddings\nbatch_size = 4\nindices = torch.arange(20).unsqueeze(0).expand(batch_size, -1)\nembeddings = prompt_encoder(indices)\n\nprint(embeddings.shape)  # torch.Size([4, 20, 768])\n</syntaxhighlight>\n\n==== LSTM Encoder ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PromptEncoder, PromptEncoderConfig\n\n# Create configuration with LSTM encoder\nconfig = PromptEncoderConfig(\n    peft_type=\"P_TUNING\",\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=10,\n    token_dim=1024,\n    num_transformer_submodules=1,\n    encoder_reparameterization_type=\"LSTM\",\n    encoder_hidden_size=512,\n    encoder_num_layers=2,\n    encoder_dropout=0.1,\n)\n\n# Create prompt encoder\nprompt_encoder = PromptEncoder(config)\n\n# Forward pass\nbatch_size = 8\nindices = torch.arange(10).unsqueeze(0).expand(batch_size, -1)\nembeddings = prompt_encoder(indices)\n\nprint(embeddings.shape)  # torch.Size([8, 10, 1024])\n</syntaxhighlight>\n\n==== Integration with PEFT Model ====\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, PromptEncoderConfig\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Create P-tuning configuration\nconfig = PromptEncoderConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=20,\n    encoder_reparameterization_type=\"MLP\",\n    encoder_hidden_size=768\n)\n\n# Apply P-tuning (PromptEncoder is created internally)\npeft_model = get_peft_model(model, config)\n\n# Check trainable parameters\npeft_model.print_trainable_parameters()\n# Output: trainable params: X || all params: Y || trainable%: Z\n</syntaxhighlight>\n\n==== Encoder-Decoder Model (Seq2Seq) ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PromptEncoder, PromptEncoderConfig\n\n# For encoder-decoder models, total_virtual_tokens is doubled\nconfig = PromptEncoderConfig(\n    peft_type=\"P_TUNING\",\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_transformer_submodules=2,  # Encoder + Decoder\n    encoder_reparameterization_type=\"MLP\",\n    encoder_hidden_size=768,\n)\n\nprompt_encoder = PromptEncoder(config)\n\n# total_virtual_tokens = 20 * 2 = 40\nprint(f\"Total virtual tokens: {prompt_encoder.total_virtual_tokens}\")\n# Output: Total virtual tokens: 40\n\nbatch_size = 4\nindices = torch.arange(40).unsqueeze(0).expand(batch_size, -1)\nembeddings = prompt_encoder(indices)\n\nprint(embeddings.shape)  # torch.Size([4, 40, 768])\n</syntaxhighlight>\n\n==== Inspecting Model Architecture ====\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PromptEncoder, PromptEncoderConfig\n\nconfig = PromptEncoderConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=10,\n    token_dim=768,\n    encoder_reparameterization_type=\"MLP\",\n    encoder_hidden_size=1024,\n)\n\nprompt_encoder = PromptEncoder(config)\n\n# Inspect the MLP architecture\nprint(\"Embedding layer:\", prompt_encoder.embedding)\nprint(\"\\nMLP Head:\")\nfor i, layer in enumerate(prompt_encoder.mlp_head):\n    print(f\"  Layer {i}: {layer}\")\n\n# Output shows:\n# Embedding layer: Embedding(10, 768)\n# MLP Head:\n#   Layer 0: Linear(in_features=768, out_features=1024, bias=True)\n#   Layer 1: ReLU()\n#   Layer 2: Linear(in_features=1024, out_features=1024, bias=True)\n#   Layer 3: ReLU()\n#   Layer 4: Linear(in_features=1024, out_features=768, bias=True)\n</syntaxhighlight>\n\n=== Related Pages ===\n* [[huggingface_peft_PromptEncoderConfig|PromptEncoderConfig]] - Configuration for this encoder\n* [[huggingface_peft_PrefixEncoder|PrefixEncoder]] - Related prefix tuning encoder\n* [[huggingface_peft_MultitaskPromptTuningModel|MultitaskPromptEmbedding]] - Related multitask prompt embedding\n* [[P_Tuning|P-Tuning]]\n* [[PEFT|Parameter-Efficient Fine-Tuning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_PromptEncoderConfig",
      "page_title": "huggingface peft PromptEncoderConfig",
      "page_type": "Implementation",
      "overview": "",
      "content": "== PromptEncoderConfig ==\n\n=== Knowledge Sources ===\n* [https://github.com/huggingface/peft PEFT Repository]\n* [https://arxiv.org/abs/2103.10385 GPT Understands, Too (P-tuning Paper)]\n\n=== Domains ===\n[[Category:NLP]]\n[[Category:PEFT]]\n[[Category:Prompt_Tuning]]\n[[Category:P_Tuning]]\n[[Category:Configuration]]\n\n=== Overview ===\n\n==== Description ====\n'''PromptEncoderConfig''' is the configuration class for the PromptEncoder used in P-tuning. P-tuning is a parameter-efficient fine-tuning method that uses a trainable encoder (MLP or LSTM) to generate continuous prompt embeddings, rather than using discrete tokens or simple embedding layers.\n\nThe configuration manages the architecture of the prompt encoder, including:\n* The type of reparameterization (MLP or LSTM)\n* Hidden layer dimensions\n* Number of encoder layers\n* Dropout probability\n\nThis configuration extends PromptLearningConfig and is specifically designed for the P-tuning approach, which showed that using a prompt encoder can significantly improve the effectiveness of prompt-based learning, especially for smaller models.\n\n==== Usage ====\nUsed to configure P-tuning models when adapting pre-trained language models with learnable continuous prompts. The encoder network generates more expressive virtual token embeddings compared to direct embedding approaches.\n\n=== Code Reference ===\n\n==== Source Location ====\n<code>src/peft/tuners/p_tuning/config.py</code>\n\n==== Signature ====\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass PromptEncoderConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`PromptEncoder`].\n\n    Args:\n        encoder_reparameterization_type (Union[[`PromptEncoderReparameterizationType`], `str`]):\n            The type of reparameterization to use.\n        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n        encoder_num_layers (`int`): The number of layers of the prompt encoder.\n        encoder_dropout (`float`): The dropout probability of the prompt encoder.\n    \"\"\"\n\n    encoder_reparameterization_type: Union[str, PromptEncoderReparameterizationType] = field(\n        default=PromptEncoderReparameterizationType.MLP,\n        metadata={\"help\": \"How to reparameterize the prompt encoder\"},\n    )\n    encoder_hidden_size: int = field(\n        default=None,\n        metadata={\"help\": \"The hidden size of the prompt encoder\"},\n    )\n    encoder_num_layers: int = field(\n        default=2,\n        metadata={\"help\": \"The number of layers of the prompt encoder\"},\n    )\n    encoder_dropout: float = field(\n        default=0.0,\n        metadata={\"help\": \"The dropout of the prompt encoder\"},\n    )\n</syntaxhighlight>\n\n==== Import ====\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.p_tuning import PromptEncoderConfig\n</syntaxhighlight>\n\n=== I/O Contract ===\n\n==== Configuration Parameters ====\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| encoder_reparameterization_type || Union[str, PromptEncoderReparameterizationType] || MLP || Type of encoder: \"MLP\" (recommended) or \"LSTM\"\n|-\n| encoder_hidden_size || int || None || Hidden size of the prompt encoder network\n|-\n| encoder_num_layers || int || 2 || Number of layers in the encoder (note: MLP always uses 2 layers regardless)\n|-\n| encoder_dropout || float || 0.0 || Dropout probability for LSTM encoder (not used for MLP)\n|-\n| colspan=\"4\" | ''Inherits all parameters from PromptLearningConfig''\n|}\n\n==== Encoder Type Enum ====\n{| class=\"wikitable\"\n! Value !! Description\n|-\n| MLP || Multi-layer perceptron encoder (recommended, always 2 layers)\n|-\n| LSTM || Bidirectional LSTM encoder with configurable layers\n|}\n\n==== Returns ====\nConfiguration object ready to be used with PromptEncoder model.\n\n==== Side Effects ====\n* Sets peft_type to PeftType.P_TUNING in __post_init__\n\n=== Usage Examples ===\n\n==== Basic MLP Configuration ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptEncoderConfig\n\n# Create P-tuning configuration with MLP encoder\nconfig = PromptEncoderConfig(\n    task_type=\"SEQ_2_SEQ_LM\",\n    num_virtual_tokens=20,\n    token_dim=768,\n    num_transformer_submodules=1,\n    num_attention_heads=12,\n    num_layers=12,\n    encoder_reparameterization_type=\"MLP\",\n    encoder_hidden_size=768\n)\n</syntaxhighlight>\n\n==== LSTM Encoder Configuration ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptEncoderConfig\n\n# Create P-tuning configuration with LSTM encoder\nconfig = PromptEncoderConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=10,\n    token_dim=1024,\n    encoder_reparameterization_type=\"LSTM\",\n    encoder_hidden_size=512,\n    encoder_num_layers=3,\n    encoder_dropout=0.1\n)\n</syntaxhighlight>\n\n==== Using Enum for Encoder Type ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptEncoderConfig\nfrom peft.tuners.p_tuning import PromptEncoderReparameterizationType\n\n# Use enum for type safety\nconfig = PromptEncoderConfig(\n    task_type=\"SEQ_CLS\",\n    num_virtual_tokens=15,\n    token_dim=768,\n    encoder_reparameterization_type=PromptEncoderReparameterizationType.MLP,\n    encoder_hidden_size=1024\n)\n</syntaxhighlight>\n\n==== Complete Configuration for Sequence Classification ====\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptEncoderConfig, get_peft_model\nfrom transformers import AutoModelForSequenceClassification\n\n# Create configuration\nconfig = PromptEncoderConfig(\n    task_type=\"SEQ_CLS\",\n    num_virtual_tokens=20,\n    encoder_reparameterization_type=\"MLP\",\n    encoder_hidden_size=768\n)\n\n# Load base model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=2\n)\n\n# Apply P-tuning\npeft_model = get_peft_model(model, config)\nprint(peft_model.print_trainable_parameters())\n</syntaxhighlight>\n\n=== Related Pages ===\n* [[huggingface_peft_PromptEncoder|PromptEncoder]] - The model implementation using this configuration\n* [[huggingface_peft_PromptLearningConfig|PromptLearningConfig]] - Parent configuration class\n* [[huggingface_peft_PrefixTuningConfig|PrefixTuningConfig]] - Related prefix tuning configuration\n* [[P_Tuning|P-Tuning]]\n* [[PEFT|Parameter-Efficient Fine-Tuning]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_PromptTuningConfig",
      "page_title": "PromptTuningConfig",
      "page_type": "Implementation",
      "overview": "=== Description === PromptTuningConfig is the configuration class for storing the configuration of a PromptEmbedding. It implements prompt tuning, a parameter-efficient fine-tuning technique where continuous, task-specific vectors (soft prompts) are prepended to the input embeddings while keeping the language model frozen. The configuration supports three initialization strategies: TEXT (initialize from text tokens), SAMPLE_VOCAB (randomly sample from vocabulary), and RANDOM (random continuous vectors). This allows flexible initialization based on the use case and available information.",
      "content": "= PromptTuningConfig =\n\n== Knowledge Sources ==\n* Source Repository: https://github.com/huggingface/peft\n* Related: [https://arxiv.org/abs/2104.08691 The Power of Scale for Parameter-Efficient Prompt Tuning]\n\n== Domains ==\n* [[NLP]]\n* [[PEFT]] (Parameter-Efficient Fine-Tuning)\n* [[Prompt Engineering]]\n* [[Soft Prompts]]\n* [[Few-Shot Learning]]\n\n== Overview ==\n\n=== Description ===\nPromptTuningConfig is the configuration class for storing the configuration of a PromptEmbedding. It implements prompt tuning, a parameter-efficient fine-tuning technique where continuous, task-specific vectors (soft prompts) are prepended to the input embeddings while keeping the language model frozen.\n\nThe configuration supports three initialization strategies: TEXT (initialize from text tokens), SAMPLE_VOCAB (randomly sample from vocabulary), and RANDOM (random continuous vectors). This allows flexible initialization based on the use case and available information.\n\n=== Usage ===\nPromptTuningConfig is used to configure prompt tuning for language models. It's typically used with models that support prompt embeddings to enable task-specific adaptation without modifying model weights.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/prompt_tuning/config.py</code>\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass PromptTuningConfig(PromptLearningConfig):\n    def __init__(\n        self,\n        prompt_tuning_init: Union[PromptTuningInit, str] = PromptTuningInit.RANDOM,\n        prompt_tuning_init_text: Optional[str] = None,\n        tokenizer_name_or_path: Optional[str] = None,\n        tokenizer_kwargs: Optional[dict] = None,\n        # Inherited from PromptLearningConfig:\n        # num_virtual_tokens, task_type, inference_mode, etc.\n    )\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.prompt_tuning.config import PromptTuningConfig, PromptTuningInit\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| prompt_tuning_init || Union[PromptTuningInit, str] || PromptTuningInit.RANDOM || Initialization strategy: TEXT, SAMPLE_VOCAB, or RANDOM\n|-\n| prompt_tuning_init_text || Optional[str] || None || Text to initialize prompt embedding (required if init is TEXT)\n|-\n| tokenizer_name_or_path || Optional[str] || None || Name or path of tokenizer (required if init is TEXT)\n|-\n| tokenizer_kwargs || Optional[dict] || None || Keyword arguments for AutoTokenizer.from_pretrained (only used with TEXT init)\n|}\n\n=== PromptTuningInit Enum Values ===\n{| class=\"wikitable\"\n! Value !! Description\n|-\n| TEXT || Initialize prompt embedding with embeddings of provided text tokens\n|-\n| SAMPLE_VOCAB || Initialize with randomly sampled tokens from model's vocabulary\n|-\n| RANDOM || Initialize with random continuous soft tokens (may fall outside embedding manifold)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n! Return Type !! Description\n|-\n| PromptTuningConfig || A configured PromptTuningConfig instance with peft_type set to PeftType.PROMPT_TUNING\n|}\n\n=== Validation ===\nThe <code>__post_init__</code> method performs validation:\n* When <code>prompt_tuning_init</code> is TEXT, <code>tokenizer_name_or_path</code> must be provided\n* When <code>prompt_tuning_init</code> is TEXT, <code>prompt_tuning_init_text</code> must be provided\n* <code>tokenizer_kwargs</code> can only be used when <code>prompt_tuning_init</code> is TEXT\n\n== Usage Examples ==\n\n=== Random Initialization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, PromptTuningInit, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Configure with random initialization (default)\nconfig = PromptTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    prompt_tuning_init=PromptTuningInit.RANDOM,\n    num_virtual_tokens=20\n)\n\n# Apply prompt tuning\npeft_model = get_peft_model(model, config)\nprint(f\"Trainable parameters: {peft_model.num_parameters(only_trainable=True)}\")\n</syntaxhighlight>\n\n=== Text Initialization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, PromptTuningInit, get_peft_model\nfrom transformers import AutoModelForSeq2SeqLM\n\n# Load model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n# Initialize prompt from text\nconfig = PromptTuningConfig(\n    task_type=\"SEQ_2_SEQ_LM\",\n    prompt_tuning_init=PromptTuningInit.TEXT,\n    prompt_tuning_init_text=\"Classify the sentiment of this text as positive or negative:\",\n    tokenizer_name_or_path=\"t5-base\",\n    num_virtual_tokens=20\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Vocabulary Sampling Initialization ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, PromptTuningInit, get_peft_model\n\n# Sample random tokens from vocabulary\nconfig = PromptTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    prompt_tuning_init=PromptTuningInit.SAMPLE_VOCAB,\n    num_virtual_tokens=50,\n    inference_mode=False\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== With Tokenizer Kwargs ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, PromptTuningInit\n\n# Provide additional tokenizer arguments\nconfig = PromptTuningConfig(\n    task_type=\"SEQ_CLS\",\n    prompt_tuning_init=PromptTuningInit.TEXT,\n    prompt_tuning_init_text=\"Question: Is this statement true or false?\",\n    tokenizer_name_or_path=\"bert-base-uncased\",\n    tokenizer_kwargs={\n        \"use_fast\": True,\n        \"add_special_tokens\": True,\n        \"max_length\": 512\n    },\n    num_virtual_tokens=10\n)\n</syntaxhighlight>\n\n=== Multi-Task Setup ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, PromptTuningInit, get_peft_model\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n# Configure different prompts for different tasks\ntask_configs = {\n    \"summarization\": PromptTuningConfig(\n        task_type=\"SEQ_2_SEQ_LM\",\n        prompt_tuning_init=PromptTuningInit.TEXT,\n        prompt_tuning_init_text=\"summarize: \",\n        tokenizer_name_or_path=\"t5-base\",\n        num_virtual_tokens=8\n    ),\n    \"translation\": PromptTuningConfig(\n        task_type=\"SEQ_2_SEQ_LM\",\n        prompt_tuning_init=PromptTuningInit.TEXT,\n        prompt_tuning_init_text=\"translate English to French: \",\n        tokenizer_name_or_path=\"t5-base\",\n        num_virtual_tokens=8\n    )\n}\n\n# Apply first adapter\npeft_model = get_peft_model(model, task_configs[\"summarization\"], adapter_name=\"summarization\")\n\n# Add second adapter\npeft_model.add_adapter(\"translation\", task_configs[\"translation\"])\n</syntaxhighlight>\n\n=== Saving and Loading Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PromptTuningConfig, PromptTuningInit\nimport json\n\n# Create configuration\nconfig = PromptTuningConfig(\n    task_type=\"CAUSAL_LM\",\n    prompt_tuning_init=PromptTuningInit.TEXT,\n    prompt_tuning_init_text=\"Generate a creative story:\",\n    tokenizer_name_or_path=\"gpt2\",\n    num_virtual_tokens=15\n)\n\n# Save configuration\nconfig.save_pretrained(\"./prompt_tuning_config\")\n\n# Load configuration\nloaded_config = PromptTuningConfig.from_pretrained(\"./prompt_tuning_config\")\nprint(f\"Loaded init method: {loaded_config.prompt_tuning_init}\")\n</syntaxhighlight>\n\n== Initialization Strategies ==\n\n=== TEXT Initialization ===\n* '''Pros''': Uses meaningful token embeddings, may converge faster\n* '''Cons''': Requires tokenizer, limited to vocabulary tokens\n* '''Use case''': When you have a good textual description of the task\n\n=== SAMPLE_VOCAB Initialization ===\n* '''Pros''': Uses real vocabulary embeddings, no tokenizer needed for config\n* '''Cons''': Random selection may not be meaningful\n* '''Use case''': When you want embeddings from vocabulary but no specific text\n\n=== RANDOM Initialization ===\n* '''Pros''': No constraints, maximum flexibility\n* '''Cons''': May start outside embedding manifold, potentially slower convergence\n* '''Use case''': When you want the model to learn prompts from scratch\n\n== Related Pages ==\n* [[huggingface_peft_PromptEmbedding|PromptEmbedding]] - The embedding layer using this config\n* [[huggingface_peft_PromptEncoder|PromptEncoder]] - Related prompt encoding approach\n* [[huggingface_peft_PrefixTuningConfig|PrefixTuningConfig]] - Related prefix tuning configuration\n* [[huggingface_peft_MultitaskPromptTuningConfig|MultitaskPromptTuningConfig]] - Multi-task variant\n* [[PEFT]] - Parameter-Efficient Fine-Tuning overview\n* [[Prompt Engineering]]\n\n== Categories ==\n[[Category:PEFT]]\n[[Category:Configuration]]\n[[Category:Prompt Tuning]]\n[[Category:NLP]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_RandLoraConfig",
      "page_title": "RandLoraConfig",
      "page_type": "Implementation",
      "overview": "=== Description === RandLoraConfig is the configuration class for storing the configuration of a RandLoraModel. RandLora (Random Low-Rank Adaptation) is a parameter-efficient fine-tuning method that uses fixed random bases (basis_A and basis_B) instead of learned low-rank matrices, with only diagonal scaling matrices (lambda/gamma) being trainable. Unlike standard LoRA where the rank parameter determines trainable parameters (higher rank = more parameters), RandLora's rank is inversely proportional to trainable parameters: reducing the rank increases trainable parameters. The method supports sparse random bases for potential matmul-free computation.",
      "content": "= RandLoraConfig =\n\n== Knowledge Sources ==\n* Source Repository: https://github.com/huggingface/peft\n* Paper: https://huggingface.co/papers/2502.00987\n* Related: [https://huggingface.co/papers/2406.02528v1 MatMul-Free Computation]\n\n== Domains ==\n* [[NLP]]\n* [[PEFT]] (Parameter-Efficient Fine-Tuning)\n* [[LoRA]] (Low-Rank Adaptation)\n* [[Random Projection]]\n* [[Sparse Networks]]\n\n== Overview ==\n\n=== Description ===\nRandLoraConfig is the configuration class for storing the configuration of a RandLoraModel. RandLora (Random Low-Rank Adaptation) is a parameter-efficient fine-tuning method that uses fixed random bases (basis_A and basis_B) instead of learned low-rank matrices, with only diagonal scaling matrices (lambda/gamma) being trainable.\n\nUnlike standard LoRA where the rank parameter determines trainable parameters (higher rank = more parameters), RandLora's rank is inversely proportional to trainable parameters: reducing the rank increases trainable parameters. The method supports sparse random bases for potential matmul-free computation.\n\n=== Usage ===\nRandLoraConfig is used to configure RandLora layers in a model for parameter-efficient fine-tuning. It provides control over the random basis rank, sparsity, dropout, and scaling parameters.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/randlora/config.py</code>\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass RandLoraConfig(PeftConfig):\n    def __init__(\n        self,\n        r: int = 32,\n        target_modules: Optional[Union[list[str], str]] = None,\n        projection_prng_key: int = 0,\n        save_projection: bool = True,\n        sparse: bool = False,\n        very_sparse: bool = False,\n        randlora_dropout: float = 0.0,\n        randlora_alpha: int = 640,\n        fan_in_fan_out: bool = False,\n        bias: str = \"none\",\n        modules_to_save: Optional[list[str]] = None,\n        init_weights: bool = True,\n        layers_to_transform: Optional[Union[list[int], int]] = None,\n        layers_pattern: Optional[str] = None,\n    )\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.randlora.config import RandLoraConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| r || int || 32 || Random basis rank dimension (inversely proportional to trainable parameters)\n|-\n| target_modules || Optional[Union[list[str], str]] || None || Names of modules to apply RandLora to (only linear layers supported)\n|-\n| projection_prng_key || int || 0 || PRNG init key for basis_A and basis_B initialization\n|-\n| save_projection || bool || True || Whether to save basis_A/basis_B in state dict (increases checkpoint size but ensures reproducibility)\n|-\n| sparse || bool || False || Use sparse ternary bases (-1, 0, 1) with attribution probability 1/6 for -1/1, 2/3 for 0\n|-\n| very_sparse || bool || False || Use highly sparse ternary bases with attribution probability 1/\u221aD for -1/1, (1-2/\u221aD) for 0\n|-\n| randlora_dropout || float || 0.0 || Dropout probability for RandLora layers\n|-\n| randlora_alpha || int || 640 || Scaling coefficient (typically 20 times the rank, can cause instability if too high)\n|-\n| fan_in_fan_out || bool || False || Set to True if layer stores weights as (fan_in, fan_out) like Conv1D in GPT-2\n|-\n| bias || str || \"none\" || Bias type: 'none', 'all', or 'randlora_only'\n|-\n| modules_to_save || Optional[list[str]] || None || Modules apart from RandLora layers to be trainable and saved\n|-\n| init_weights || bool || True || Whether to initialize RandLora layer weights\n|-\n| layers_to_transform || Optional[Union[list[int], int]] || None || Specific layer indexes to transform\n|-\n| layers_pattern || Optional[str] || None || Layer pattern name for layers_to_transform\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n! Return Type !! Description\n|-\n| RandLoraConfig || A configured RandLoraConfig instance with peft_type set to PeftType.RANDLORA\n|}\n\n=== Post-Initialization ===\nThe <code>__post_init__</code> method:\n* Sets <code>peft_type</code> to <code>PeftType.RANDLORA</code>\n* Converts <code>target_modules</code> to a set if provided as a list\n* Warns if <code>save_projection</code> is False about potential reproducibility issues\n\n== Usage Examples ==\n\n=== Basic RandLora Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"base-model-name\")\n\n# Configure RandLora\nconfig = RandLoraConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    randlora_alpha=640,\n    randlora_dropout=0.1\n)\n\n# Apply RandLora\npeft_model = get_peft_model(model, config)\nprint(f\"Trainable params: {peft_model.num_parameters(only_trainable=True)}\")\n</syntaxhighlight>\n\n=== Sparse RandLora ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\n\n# Use sparse ternary bases for reduced overfitting\nconfig = RandLoraConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    sparse=True,  # Enable sparse bases\n    randlora_alpha=640,\n    save_projection=True\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Very Sparse Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig\n\n# Use very sparse bases for maximum regularization\nconfig = RandLoraConfig(\n    r=64,  # Higher rank with very sparse = even more regularization\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    very_sparse=True,  # Highly sparse bases\n    randlora_alpha=1280,  # 20 * 64\n    randlora_dropout=0.05\n)\n\n# Note: very_sparse may decrease performance but reduces overfitting\n</syntaxhighlight>\n\n=== PRNG Key for Reproducibility ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig\n\n# Use consistent PRNG key for reproducible random bases\nconfig = RandLoraConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    projection_prng_key=42,  # Fixed seed\n    save_projection=False,  # Don't save bases, rely on PRNG key\n    randlora_alpha=640\n)\n\n# Warning: save_projection=False may cause issues across different systems\n</syntaxhighlight>\n\n=== Layer-Specific Application ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig\n\n# Apply RandLora only to specific layers\nconfig = RandLoraConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=[0, 1, 2, 3],  # Only first 4 layers\n    layers_pattern=\"model.layers\",  # Pattern to identify layers\n    randlora_alpha=640\n)\n</syntaxhighlight>\n\n=== With Bias Training ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig\n\n# Train biases along with RandLora\nconfig = RandLoraConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    bias=\"randlora_only\",  # Train only RandLora biases\n    randlora_alpha=640,\n    randlora_dropout=0.1\n)\n\n# Alternative: bias=\"all\" trains all biases in the model\n</syntaxhighlight>\n\n=== Conv1D Layers (GPT-2) ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# GPT-2 uses Conv1D layers with fan_in_fan_out weight storage\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\nconfig = RandLoraConfig(\n    r=32,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    fan_in_fan_out=True,  # Important for GPT-2!\n    randlora_alpha=640\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Lower Rank for More Parameters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig\n\n# Counterintuitive: lower rank = MORE trainable parameters in RandLora\nconfig_low_rank = RandLoraConfig(\n    r=8,  # Low rank\n    target_modules=[\"q_proj\", \"v_proj\"],\n    randlora_alpha=160  # 20 * 8\n)\n\nconfig_high_rank = RandLoraConfig(\n    r=64,  # High rank\n    target_modules=[\"q_proj\", \"v_proj\"],\n    randlora_alpha=1280  # 20 * 64\n)\n\n# config_low_rank will have MORE trainable parameters than config_high_rank\n</syntaxhighlight>\n\n=== Full Configuration Example ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\n\nconfig = RandLoraConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    projection_prng_key=12345,\n    save_projection=True,  # Save bases for guaranteed reproducibility\n    sparse=False,\n    very_sparse=False,\n    randlora_dropout=0.1,\n    randlora_alpha=640,  # 20 * 32\n    fan_in_fan_out=False,\n    bias=\"none\",\n    modules_to_save=[\"classifier\"],  # Save classifier layer\n    init_weights=True,\n    layers_to_transform=None,  # Apply to all layers\n    layers_pattern=None\n)\n\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n</syntaxhighlight>\n\n== Key Concepts ==\n\n=== Inverse Rank-Parameter Relationship ===\nUnlike standard LoRA, RandLora's rank is inversely proportional to parameters:\n* '''Lower r''' = Larger diagonal matrices (lambda/gamma) = More trainable parameters\n* '''Higher r''' = Smaller diagonal matrices = Fewer trainable parameters\n\n=== Random Basis Projection ===\n* '''basis_A''' and '''basis_B''' are fixed random matrices\n* Only diagonal scaling matrices are learned\n* PRNG key ensures reproducibility of random bases\n\n=== Sparsity Options ===\n* '''Dense''' (default): Standard random matrices\n* '''Sparse''': Ternary {-1, 0, 1} with P(-1)=P(1)=1/6, P(0)=2/3\n* '''Very Sparse''': Ternary with P(-1)=P(1)=1/\u221aD, P(0)=1-2/\u221aD\n\n=== Scaling Considerations ===\nThe large default alpha (640) can cause numerical instabilities with high learning rates. Consider:\n* Reducing learning rate\n* Reducing randlora_alpha\n* Using gradient clipping\n\n== Related Pages ==\n* [[huggingface_peft_RandLoraModel|RandLoraModel]] - Model implementation\n* [[huggingface_peft_RandLoraLayer|RandLoraLayer]] - Layer implementation\n* [[huggingface_peft_LoraConfig|LoraConfig]] - Standard LoRA configuration\n* [[PEFT]] - Parameter-Efficient Fine-Tuning\n* [[LoRA]] - Low-Rank Adaptation\n\n== Categories ==\n[[Category:PEFT]]\n[[Category:Configuration]]\n[[Category:LoRA]]\n[[Category:Random Projection]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_RandLoraLayer",
      "page_title": "huggingface peft RandLoraLayer",
      "page_type": "Implementation",
      "overview": "Random projection-based LoRA layer that uses shared random bases with per-layer trainable scaling vectors for parameter-efficient adaptation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|RandLoRA|https://arxiv.org/abs/2502.00987]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Random_Projection]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nRandom projection-based LoRA layer that uses shared random bases with per-layer trainable scaling vectors for parameter-efficient adaptation.\n\n=== Description ===\n\nRandLoraLayer implements adaptation using shared random projection matrices (randlora_A and randlora_B) with per-layer trainable parameters (lambda and gamma). The random bases are initialized once and shared across all layers, with lambda scaling the rank dimension and gamma scaling the base dimension. A custom autograd function (UniqueBaseGrad) efficiently computes gradients for the unique shared base. This achieves fewer parameters than LoRA while maintaining expressivity.\n\n=== Usage ===\n\nUse RandLoRA when you want parameter efficiency beyond LoRA by sharing random projection matrices across layers. The method is particularly effective when the projection_prng_key ensures reproducible random matrices. RandLoRA supports sparse random projections for additional efficiency.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/randlora/layer.py src/peft/tuners/randlora/layer.py]\n* '''Lines:''' 1-351\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass UniqueBaseGrad(torch.autograd.Function):\n    \"\"\"Memory-efficient gradient for shared random base.\"\"\"\n    @staticmethod\n    def forward(ctx, randlora_A, randlora_lambda, randlora_gamma):\n        \"\"\"Compute lambda * A * gamma.\"\"\"\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"Gradient for lambda and gamma only (A is frozen).\"\"\"\n\nclass RandLoraLayer(BaseTunerLayer):\n    \"\"\"\n    Random projection LoRA layer.\n\n    Attributes:\n        randlora_lambda: ParameterDict of rank scaling vectors [r, num_bases]\n        randlora_gamma: ParameterDict of base scaling vectors [num_bases, min_dim]\n        randlora_A: BufferDict reference to shared A matrices\n        randlora_B: BufferDict reference to shared B matrices\n        num_bases: Number of bases for full-rank coverage\n    \"\"\"\n    adapter_layer_names = (\"randlora_lambda\", \"randlora_gamma\")\n    other_param_names = (\"randlora_A\", \"randlora_B\")\n\n    def update_layer(\n        self,\n        adapter_name,\n        randlora_A: BufferDict,\n        randlora_B: BufferDict,\n        r,\n        randlora_alpha,\n        randlora_dropout,\n        init_weights,\n        **kwargs,\n    ):\n        \"\"\"Create RandLoRA scaling parameters.\"\"\"\n\n    def get_scaled_bases(self, adapter, device=None):\n        \"\"\"Compute scaled A and B matrices for forward.\"\"\"\n\nclass Linear(nn.Linear, RandLoraLayer):\n    \"\"\"RandLoRA implemented in Linear layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.randlora import RandLoraLayer, RandLoraConfig, RandLoraModel\nfrom peft import RandLoraConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained Linear layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| randlora_A || BufferDict || Yes || Shared frozen A matrices\n|-\n| randlora_B || BufferDict || Yes || Shared frozen B matrices\n|-\n| r || int || Yes || Rank for adaptation\n|-\n| randlora_alpha || int || No || Scaling factor\n|-\n| randlora_dropout || float || No || Dropout probability\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Base output + scaled random projection\n|-\n| get_delta_weight() || torch.Tensor || (B @ (lambda * A * gamma)) * scaling\n|}\n\n== Usage Examples ==\n\n=== Basic RandLoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# RandLoRA shares random matrices across layers\nconfig = RandLoraConfig(\n    r=32,\n    randlora_alpha=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    projection_prng_key=42,   # Reproducible random init\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n# Fewer parameters than LoRA with same rank\n</syntaxhighlight>\n\n=== RandLoRA with Sparse Projections ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\n\n# Use sparse random projections for efficiency\nconfig = RandLoraConfig(\n    r=64,\n    randlora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    sparse=True,              # Sparse random matrices\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Very Sparse RandLoRA ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\n\n# Maximum sparsity for lowest memory\nconfig = RandLoraConfig(\n    r=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    very_sparse=True,         # sqrt(min_dim) sparsity\n    save_projection=False,    # Don't save random matrices\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Random Projection"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "RandLoRA",
          "url": "https://arxiv.org/abs/2502.00987"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_RandLoraModel",
      "page_title": "huggingface peft RandLoraModel",
      "page_type": "Implementation",
      "overview": "Model class that creates RandLoRA adapters with shared random projection matrices across all target layers for parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|RandLoRA|https://arxiv.org/abs/2502.00987]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Random_Projection]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nModel class that creates RandLoRA adapters with shared random projection matrices across all target layers for parameter-efficient fine-tuning.\n\n=== Description ===\n\nRandLoraModel extends BaseTuner to implement Random projection LoRA. The model initializes shared randlora_A and randlora_B matrices once during _pre_injection_hook, which are then used across all adapted layers. The matrices can be dense (Kaiming initialized) or sparse (ternary projections). The model finds the largest layer dimensions to size shared matrices appropriately, ensuring they can be sliced for layers of different sizes.\n\n=== Usage ===\n\nUse RandLoraModel for parameter-efficient adaptation where you want to minimize trainable parameters. The shared random matrices mean only small lambda and gamma vectors are trained per layer. Use projection_prng_key for reproducible initialization and save_projection to control whether random matrices are saved with checkpoints.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/randlora/model.py src/peft/tuners/randlora/model.py]\n* '''Lines:''' 1-357\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef _kaiming_init(tensor_or_shape, generator: torch.Generator) -> torch.Tensor:\n    \"\"\"Kaiming initialization with PRNG generator.\"\"\"\n\nclass RandLoraModel(BaseTuner):\n    \"\"\"\n    Creates RandLoRA model from pretrained transformer.\n\n    Args:\n        model: The model to be adapted\n        config: RandLoraConfig with r, alpha, projection settings\n        adapter_name: Name for the adapter (default: \"default\")\n\n    Attributes:\n        prefix: \"randlora_\"\n        randlora_A: Shared BufferDict of A matrices\n        randlora_B: Shared BufferDict of B matrices\n    \"\"\"\n    prefix = \"randlora_\"\n    tuner_layer_cls = RandLoraLayer\n\n    def _find_dim(self, config) -> tuple[int, int]:\n        \"\"\"Find largest dimensions across target layers.\"\"\"\n\n    def _init_randlora_A_randlora_B(self, config, adapter_name):\n        \"\"\"Initialize dense random projection matrices.\"\"\"\n\n    def _init_randlora_A_randlora_B_sparse(self, config, adapter_name, sparsity):\n        \"\"\"Initialize sparse ternary projection matrices.\"\"\"\n\n    def _pre_injection_hook(self, model, config, adapter_name):\n        \"\"\"Initialize shared matrices before layer injection.\"\"\"\n\n    def _create_new_module(randlora_config, randlora_A, randlora_B, adapter_name, target, **kwargs):\n        \"\"\"Create RandLoRA module with shared bases.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraModel, RandLoraConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || The pretrained model to adapt\n|-\n| config || RandLoraConfig || Yes || Configuration with r, alpha, projection_prng_key\n|-\n| adapter_name || str || No || Name for adapter (default: \"default\")\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward() || ModelOutput || Model output with RandLoRA adaptation\n|-\n| randlora_A || BufferDict || Shared frozen A matrices\n|-\n| randlora_B || BufferDict || Shared frozen B matrices\n|}\n\n== Usage Examples ==\n\n=== Creating RandLoRA Model ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\nconfig = RandLoraConfig(\n    r=32,\n    randlora_alpha=64,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n    projection_prng_key=42,\n)\n\nmodel = get_peft_model(base_model, config)\n\n# Shared matrices are in model.randlora_A and model.randlora_B\n# Per-layer parameters are in each layer's lambda and gamma\n</syntaxhighlight>\n\n=== RandLoRA with Reproducible Projections ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\n\n# Same key = same random matrices\nconfig = RandLoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    projection_prng_key=12345,  # Deterministic\n    save_projection=False,      # Regenerate on load\n)\n\nmodel = get_peft_model(model, config)\n\n# When loading, use same key to recreate matrices\n</syntaxhighlight>\n\n=== Sparse RandLoRA for Efficiency ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RandLoraConfig, get_peft_model\n\n# Sparse ternary projections {-1, 0, 1}\nconfig = RandLoraConfig(\n    r=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    sparse=True,              # Sparsity = 3\n    # OR\n    very_sparse=True,         # Sparsity = sqrt(min_dim)\n)\n\nmodel = get_peft_model(model, config)\n# Faster matmuls with sparse matrices\n</syntaxhighlight>\n\n=== Multiple RandLoRA Adapters ===\n<syntaxhighlight lang=\"python\">\n# All adapters must use same projection_prng_key\nconfig1 = RandLoraConfig(\n    r=16,\n    projection_prng_key=42,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config1)\nmodel.add_adapter(\"task2\", config1)  # Same key required\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Random Projection"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "RandLoRA",
          "url": "https://arxiv.org/abs/2502.00987"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_RandLoraQuantized",
      "page_title": "huggingface peft RandLoraQuantized",
      "page_type": "Implementation",
      "overview": "Quantized RandLoRA layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling random projection adaptation on quantized models.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|bitsandbytes|https://github.com/bitsandbytes-foundation/bitsandbytes]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nQuantized RandLoRA layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling random projection adaptation on quantized models.\n\n=== Description ===\n\nLinear8bitLt and Linear4bit implement RandLoRA for bitsandbytes quantized layers. The layers use the same shared random projection matrices (randlora_A, randlora_B) with per-layer trainable lambda and gamma vectors. During forward pass, the scaled projection is applied to inputs before the quantized linear operation. Merging dequantizes weights, adds the delta, and requantizes.\n\n=== Usage ===\n\nUse RandLoRA quantized layers when fine-tuning quantized models (load_in_8bit or load_in_4bit). Layers are automatically dispatched when base layers are bitsandbytes Linear8bitLt or Linear4bit. The method combines the parameter efficiency of RandLoRA with memory savings of quantization.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/randlora/bnb.py src/peft/tuners/randlora/bnb.py]\n* '''Lines:''' 1-457\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass Linear8bitLt(torch.nn.Module, RandLoraLayer):\n    \"\"\"RandLoRA for 8-bit quantized layers.\"\"\"\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        randlora_A,\n        randlora_B,\n        r: int = 0,\n        randlora_alpha: int = 0,\n        randlora_dropout: float = 0.0,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize RandLoRA for 8-bit layer.\"\"\"\n\n    def get_scaled_bases(self, adapter, device=None):\n        \"\"\"Get scaled A and B with lambda/gamma applied.\"\"\"\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        \"\"\"Apply RandLoRA then 8-bit linear.\"\"\"\n\nclass Linear4bit(torch.nn.Module, RandLoraLayer):\n    \"\"\"RandLoRA for 4-bit quantized layers.\"\"\"\n\n    def merge(self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None):\n        \"\"\"Merge RandLoRA into dequantized weights.\"\"\"\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        \"\"\"Apply RandLoRA then 4-bit linear.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.randlora.bnb import Linear8bitLt, Linear4bit\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || bnb.nn.Linear8bitLt/Linear4bit || Yes || Quantized base layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| randlora_A || BufferDict || Yes || Shared frozen A matrices\n|-\n| randlora_B || BufferDict || Yes || Shared frozen B matrices\n|-\n| r || int || Yes || Rank for adaptation\n|-\n| randlora_alpha || int || No || Scaling factor\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Base quantized output + RandLoRA delta\n|-\n| get_delta_weight() || torch.Tensor || Scaled random projection delta\n|}\n\n== Usage Examples ==\n\n=== RandLoRA with 4-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import RandLoraConfig, get_peft_model\n\n# Load model in 4-bit\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n)\n\n# Apply RandLoRA\nconfig = RandLoraConfig(\n    r=32,\n    randlora_alpha=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    projection_prng_key=42,\n)\n\nmodel = get_peft_model(model, config)\n# Automatically uses Linear4bit class\n</syntaxhighlight>\n\n=== RandLoRA with 8-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import RandLoraConfig, get_peft_model\n\n# Load model in 8-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    load_in_8bit=True,\n)\n\nconfig = RandLoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n# Automatically uses Linear8bitLt class\n</syntaxhighlight>\n\n=== Forward Computation ===\n<syntaxhighlight lang=\"python\">\n# RandLoRA forward on quantized layers:\n# 1. Compute scaled bases: update_A, update_B from lambda * A * gamma\n# 2. Apply dropout\n# 3. result = base_layer(x) + linear(linear(x, update_B), update_A) * scaling\n\n# Merging (with rounding errors warning):\nmodel.merge_adapter()\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "bitsandbytes",
          "url": "https://github.com/bitsandbytes-foundation/bitsandbytes"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_RoadConfig",
      "page_title": "RoadConfig",
      "page_type": "Implementation",
      "overview": "=== Description === RoadConfig is the configuration class for storing the configuration of a RoadModel. RoAd (Rotation Adaptation) is a parameter-efficient fine-tuning method that adapts models by applying learned rotations to hidden representations. Unlike additive methods like LoRA, RoAd transforms representations through 2D rotations defined by scale and angle parameters. The method offers three variants (road_1, road_2, road_4) with increasing parameter counts, allowing users to trade off between efficiency and expressiveness. Elements are grouped into 2D vectors for rotation, with group size affecting inference speed on specialized hardware.",
      "content": "= RoadConfig =\n\n== Knowledge Sources ==\n* Source Repository: https://github.com/huggingface/peft\n* Paper: https://huggingface.co/papers/2409.00119\n\n== Domains ==\n* [[NLP]]\n* [[PEFT]] (Parameter-Efficient Fine-Tuning)\n* [[Rotation-Based Adaptation]]\n* [[Linear Transformations]]\n* [[Model Compression]]\n\n== Overview ==\n\n=== Description ===\nRoadConfig is the configuration class for storing the configuration of a RoadModel. RoAd (Rotation Adaptation) is a parameter-efficient fine-tuning method that adapts models by applying learned rotations to hidden representations. Unlike additive methods like LoRA, RoAd transforms representations through 2D rotations defined by scale and angle parameters.\n\nThe method offers three variants (road_1, road_2, road_4) with increasing parameter counts, allowing users to trade off between efficiency and expressiveness. Elements are grouped into 2D vectors for rotation, with group size affecting inference speed on specialized hardware.\n\n=== Usage ===\nRoadConfig is used to configure RoAd layers in a model for parameter-efficient fine-tuning. It specifies the rotation variant, group size for element pairing, and target modules to transform.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/road/config.py</code>\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass RoadConfig(PeftConfig):\n    def __init__(\n        self,\n        variant: Union[str, RoadVariant] = \"road_1\",\n        group_size: int = 64,\n        init_weights: bool = True,\n        target_modules: Optional[Union[list[str], str]] = None,\n        modules_to_save: Optional[list[str]] = None,\n    )\n</syntaxhighlight>\n\n=== RoadVariant Type ===\n<syntaxhighlight lang=\"python\">\nRoadVariant = Literal[\"road_1\", \"road_2\", \"road_4\"]\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.road.config import RoadConfig, RoadVariant\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| variant || Union[str, RoadVariant] || \"road_1\" || Variant of RoAd: \"road_1\" (same scale/angle for all pairs), \"road_2\" (per-element scale/angle), or \"road_4\" (two scale/angle pairs per element)\n|-\n| group_size || int || 64 || How elements are grouped into 2D vectors for rotation. Element i pairs with element i+group_size/2. Must be divisible by 2. Higher values (32-64) recommended for speed.\n|-\n| init_weights || bool || True || Whether to initialize RoAd layer weights\n|-\n| target_modules || Optional[Union[list[str], str]] || None || Names of modules to apply RoAd to. Can be list, regex, or \"all-linear\". If None, chosen by model architecture.\n|-\n| modules_to_save || Optional[list[str]] || None || Modules apart from RoAd layers to be trainable and saved\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n! Return Type !! Description\n|-\n| RoadConfig || A configured RoadConfig instance with peft_type set to PeftType.ROAD\n|}\n\n=== Validation ===\nThe <code>__post_init__</code> method validates:\n* <code>variant</code> must be one of: \"road_1\", \"road_2\", \"road_4\"\n* <code>group_size</code> must be positive and divisible by 2\n* Converts <code>target_modules</code> to set if provided as list\n\n== Variant Comparison ==\n\n{| class=\"wikitable\"\n! Variant !! Parameters per Layer !! Scale/Angle Sharing !! Relative Cost\n|-\n| road_1 || hidden_size || Same for all element pairs || 1\u00d7\n|-\n| road_2 || 2 \u00d7 hidden_size || Per-element || 2\u00d7\n|-\n| road_4 || 4 \u00d7 hidden_size || Two different per-element || 4\u00d7\n|}\n\n== Usage Examples ==\n\n=== Basic RoAd Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"base-model-name\")\n\n# Configure RoAd with minimal parameters (road_1)\nconfig = RoadConfig(\n    variant=\"road_1\",\n    target_modules=[\"q_proj\", \"v_proj\"],\n    group_size=64\n)\n\n# Apply RoAd\npeft_model = get_peft_model(model, config)\nprint(f\"Trainable params: {peft_model.num_parameters(only_trainable=True)}\")\n</syntaxhighlight>\n\n=== road_2 Variant ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig, get_peft_model\n\n# Use road_2 for 2x more parameters and expressiveness\nconfig = RoadConfig(\n    variant=\"road_2\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    group_size=64,\n    init_weights=True\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== road_4 Variant ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig\n\n# Use road_4 for maximum expressiveness\nconfig = RoadConfig(\n    variant=\"road_4\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    group_size=64\n)\n\n# road_4 has 4x the parameters of road_1\n</syntaxhighlight>\n\n=== All-Linear Target Modules ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig, get_peft_model\n\n# Apply RoAd to all linear layers\nconfig = RoadConfig(\n    variant=\"road_1\",\n    target_modules=\"all-linear\",  # Special keyword\n    group_size=64\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Regex Pattern for Target Modules ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig\n\n# Use regex to select specific modules\nconfig = RoadConfig(\n    variant=\"road_2\",\n    target_modules=\".*decoder.*(SelfAttention|EncDecAttention).*(q|v)$\",\n    group_size=64\n)\n</syntaxhighlight>\n\n=== Small Model with Smaller Group Size ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig, get_peft_model\n\n# For very small models, reduce group_size\n# Note: hidden_size must be divisible by group_size\nconfig = RoadConfig(\n    variant=\"road_1\",\n    target_modules=[\"q_proj\", \"v_proj\"],\n    group_size=32,  # Smaller for small models\n    init_weights=True\n)\n\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== With Modules to Save ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig\n\n# Save additional modules beyond RoAd layers\nconfig = RoadConfig(\n    variant=\"road_2\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    modules_to_save=[\"classifier\", \"score\"],  # Train these too\n    group_size=64\n)\n</syntaxhighlight>\n\n=== Inference-Optimized Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig\n\n# Optimize for VLLM and tensor parallelism\nconfig = RoadConfig(\n    variant=\"road_1\",  # Minimal parameters\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    group_size=64,  # Higher group size for better inference speed\n    init_weights=True\n)\n\n# Note: Ensure hidden_size is divisible by group_size\n# For tensor parallelism: (hidden_size // n_partitions) % group_size == 0\n</syntaxhighlight>\n\n=== Comparing Variants ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig, get_peft_model\nimport copy\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"base-model\")\n\n# Test all three variants\nvariants = [\"road_1\", \"road_2\", \"road_4\"]\nmodels = {}\n\nfor variant in variants:\n    model = copy.deepcopy(base_model)\n    config = RoadConfig(\n        variant=variant,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        group_size=64\n    )\n    models[variant] = get_peft_model(model, config)\n\n    trainable = models[variant].num_parameters(only_trainable=True)\n    print(f\"{variant}: {trainable:,} trainable parameters\")\n\n# Output example:\n# road_1: 524,288 trainable parameters\n# road_2: 1,048,576 trainable parameters  (2x road_1)\n# road_4: 2,097,152 trainable parameters  (4x road_1)\n</syntaxhighlight>\n\n=== Full Configuration Example ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\nconfig = RoadConfig(\n    variant=\"road_2\",  # Balance between efficiency and performance\n    group_size=64,  # Recommended for inference speed\n    init_weights=True,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\"\n    ],\n    modules_to_save=None\n)\n\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n</syntaxhighlight>\n\n== Key Concepts ==\n\n=== Element Grouping ===\nElements are paired for 2D rotation:\n* Within each group of size <code>group_size</code>\n* Element 0 pairs with element group_size/2\n* Element 1 pairs with element group_size/2 + 1\n* And so on...\n\nThis grouping does not affect model performance (elements are unordered) but affects inference speed on specialized hardware.\n\n=== Rotation Parameters ===\n* '''road_1''': Single (scale, angle) for entire layer\n* '''road_2''': Per-element (scale, angle)\n* '''road_4''': Two (scale, angle) pairs per element\n\n=== Hardware Considerations ===\n* Higher <code>group_size</code> (64+) improves inference speed\n* Important for VLLM and tensor parallelism\n* <code>hidden_size % group_size</code> must equal 0\n* With tensor parallelism: <code>(hidden_size // n_partitions) % group_size == 0</code>\n\n=== Rotation vs Additive Methods ===\nUnlike LoRA (additive), RoAd applies multiplicative rotations:\n* LoRA: <code>output = Wx + \u0394Wx</code>\n* RoAd: <code>output = Rotate(Wx)</code>\n\n== Related Pages ==\n* [[huggingface_peft_RoadModel|RoadModel]] - Model implementation\n* [[huggingface_peft_RoadLayer|RoadLayer]] - Layer implementation\n* [[huggingface_peft_RoadQuantized|RoadQuantized]] - Quantized RoAd layers\n* [[huggingface_peft_LoraConfig|LoraConfig]] - Related LoRA configuration\n* [[PEFT]] - Parameter-Efficient Fine-Tuning\n\n== Categories ==\n[[Category:PEFT]]\n[[Category:Configuration]]\n[[Category:Rotation-Based Methods]]\n[[Category:Model Adaptation]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_RoadLayer",
      "page_title": "huggingface peft RoadLayer",
      "page_type": "Implementation",
      "overview": "Rotation and Dimension adaptation layer that applies efficient 2D rotations with learned angles and scales to output vectors for parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|RoAD|https://arxiv.org/abs/2501.00029]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Rotation_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nRotation and Dimension adaptation layer that applies efficient 2D rotations with learned angles and scales to output vectors for parameter-efficient fine-tuning.\n\n=== Description ===\n\nRoadLayer implements RoAD (Rotation and Dimension) adaptation by splitting output vectors into groups and applying learned 2D rotations within each group. Each pair of elements is transformed as: y0 = x0 * alpha * cos(theta) - xn * alpha * sin(theta). The method supports three variants: road_1 (reuses parameters within groups), road_2 (unique per element), and road_4 (separate parameters for each rotation component). Only angles and scales are stored, enabling efficient elementwise inference.\n\n=== Usage ===\n\nUse RoAD when you want extremely parameter-efficient adaptation with interpretable rotational transformations. RoAD is particularly effective for models where rotational geometry matters. The method merges by applying R @ W where R is the block-diagonal rotation matrix.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/road/layer.py src/peft/tuners/road/layer.py]\n* '''Lines:''' 1-419\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass RoadLayer(BaseTunerLayer):\n    \"\"\"\n    Rotation and Dimension adaptation layer.\n\n    Attributes:\n        road_theta: ParameterDict of rotation angles\n        road_alpha: ParameterDict of scaling factors\n        variant: Dict mapping adapter to variant (\"road_1\", \"road_2\", \"road_4\")\n        group_size: Dict mapping adapter to group size\n    \"\"\"\n    adapter_layer_names = (\"road_theta\", \"road_alpha\")\n    other_param_names = (\"variant\", \"group_size\")\n\n    def update_layer(\n        self,\n        adapter_name,\n        variant,\n        group_size,\n        init_weights,\n        **kwargs,\n    ):\n        \"\"\"Create RoAD rotation parameters.\"\"\"\n\nclass Linear(nn.Module, RoadLayer):\n    \"\"\"RoAD implemented in Linear layer.\"\"\"\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        \"\"\"Apply rotational transformation to output.\"\"\"\n\n    def merge(self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None):\n        \"\"\"Merge R @ W into base weights.\"\"\"\n\ndef _apply_road(variant, group_size, road_theta, road_alpha, x):\n    \"\"\"Efficient elementwise rotation application.\"\"\"\n\ndef _get_delta_weight(variant, group_size, road_theta, road_alpha):\n    \"\"\"Construct full rotation matrix for merging.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.road import RoadLayer, RoadConfig, RoadModel\nfrom peft import RoadConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained Linear layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| variant || str || Yes || \"road_1\", \"road_2\", or \"road_4\"\n|-\n| group_size || int || Yes || Size of rotation groups (must divide out_features)\n|-\n| init_weights || bool || No || Initialize to identity (theta=0, alpha=1)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Base output with rotational transformation\n|-\n| _get_delta_weight() || torch.Tensor || Full rotation matrix R for merging\n|}\n\n== Usage Examples ==\n\n=== Basic RoAD Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# RoAD with minimal parameters (road_1)\nconfig = RoadConfig(\n    variant=\"road_1\",           # Reuse parameters within groups\n    group_size=64,              # Must divide out_features\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n# Very few parameters: out_features / 2 per layer\n</syntaxhighlight>\n\n=== RoAD Variant Comparison ===\n<syntaxhighlight lang=\"python\">\nfrom peft import RoadConfig, get_peft_model\n\n# road_1: out_features // 2 parameters (most efficient)\nconfig_1 = RoadConfig(variant=\"road_1\", group_size=64, target_modules=[\"q_proj\"])\n\n# road_2: out_features parameters (medium)\nconfig_2 = RoadConfig(variant=\"road_2\", group_size=64, target_modules=[\"q_proj\"])\n\n# road_4: out_features * 2 parameters (most expressive)\nconfig_4 = RoadConfig(variant=\"road_4\", group_size=64, target_modules=[\"q_proj\"])\n</syntaxhighlight>\n\n=== RoAD Forward Computation ===\n<syntaxhighlight lang=\"python\">\n# RoAD applies transformation to layer output:\n# 1. Split output into groups\n# 2. For each group, pair first half with second half elements\n# 3. Apply rotation: y = x * cos(theta) * alpha \u00b1 x_paired * sin(theta) * alpha\n# 4. This is equivalent to block-diagonal rotation matrix multiplication\n</syntaxhighlight>\n\n=== Mixed Adapter Batch Inference ===\n<syntaxhighlight lang=\"python\">\n# RoAD supports mixed adapter batches\noutputs = model(\n    input_ids,\n    adapter_names=[\"adapter1\", \"adapter1\", \"adapter2\"],\n)\n# Each sample uses its specified adapter\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Rotation Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "RoAD",
          "url": "https://arxiv.org/abs/2501.00029"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_RoadModel",
      "page_title": "RoadModel",
      "page_type": "Implementation",
      "overview": "=== Description === RoadModel is a BaseTuner implementation that applies the RoAd (Rotation Adaptation) parameter-efficient fine-tuning method to neural networks. It manages the creation, replacement, and lifecycle of RoAd layers within a model, enabling efficient adaptation through learned rotations applied to hidden representations. The model supports mixed adapter batches inference through adapter-specific routing, quantization integration (8-bit, 4-bit), and torchao tensor subclasses. It uses forward pre-hooks to inject adapter names during inference for dynamic adapter selection.",
      "content": "= RoadModel =\n\n== Knowledge Sources ==\n* Source Repository: https://github.com/huggingface/peft\n* Paper: https://huggingface.co/papers/2409.00119\n\n== Domains ==\n* [[NLP]]\n* [[PEFT]] (Parameter-Efficient Fine-Tuning)\n* [[Rotation-Based Adaptation]]\n* [[Model Adaptation]]\n* [[Quantization]]\n\n== Overview ==\n\n=== Description ===\nRoadModel is a BaseTuner implementation that applies the RoAd (Rotation Adaptation) parameter-efficient fine-tuning method to neural networks. It manages the creation, replacement, and lifecycle of RoAd layers within a model, enabling efficient adaptation through learned rotations applied to hidden representations.\n\nThe model supports mixed adapter batches inference through adapter-specific routing, quantization integration (8-bit, 4-bit), and torchao tensor subclasses. It uses forward pre-hooks to inject adapter names during inference for dynamic adapter selection.\n\n=== Usage ===\nRoadModel is typically instantiated through the <code>get_peft_model</code> function with a RoadConfig. It provides context managers for enabling adapter-specific inference and supports multiple adapters on the same base model.\n\n== Code Reference ==\n\n=== Source Location ===\n<code>src/peft/tuners/road/model.py</code>\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass RoadModel(BaseTuner):\n    prefix: str = \"road_\"\n    tuner_layer_cls = RoadLayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_ROAD_TARGET_MODULES_MAPPING\n\n    def _create_and_replace(self, road_config, adapter_name, target, target_name, parent, current_key)\n    @staticmethod\n    def _create_new_module(road_config, adapter_name, target, **kwargs)\n    @contextmanager\n    def _enable_peft_forward_hooks(self, *args, **kwargs)\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.road.model import RoadModel\n# Or use through get_peft_model:\nfrom peft import get_peft_model, RoadConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== _create_and_replace Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| road_config || RoadConfig || The RoAd configuration\n|-\n| adapter_name || str || Name of the adapter being added\n|-\n| target || nn.Module || The target module to replace or update\n|-\n| target_name || str || Name of the target module\n|-\n| parent || nn.Module || Parent module containing the target\n|-\n| current_key || str || Current key path to the module (required, cannot be None)\n|}\n\n=== _create_new_module Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| road_config || RoadConfig || The RoAd configuration\n|-\n| adapter_name || str || Name of the adapter\n|-\n| target || nn.Module || The target module to replace\n|-\n| **kwargs || dict || Additional arguments including device_map, variant, group_size, init_weights, loaded_in_8bit, loaded_in_4bit, get_apply_tensor_subclass\n|}\n\n'''Returns:''' RoadLayer - A new RoAd layer (Linear, 8-bit, 4-bit, or torchao variant)\n\n'''Raises:''' ValueError if target module type is not supported (only <code>torch.nn.Linear</code> is supported)\n\n=== _enable_peft_forward_hooks Method ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| adapter_names || Optional[List[str]] || List of adapter names for mixed batch inference (passed as kwarg)\n|-\n| *args || tuple || Additional positional arguments\n|-\n| **kwargs || dict || Additional keyword arguments\n|}\n\n'''Yields:''' Context where specified adapters are active for inference\n\n'''Raises:'''\n* ValueError if used during training (only inference mode supported)\n* ValueError if non-existing adapter names are provided\n\n== Usage Examples ==\n\n=== Basic Model Creation ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, RoadConfig\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Configure RoAd\nconfig = RoadConfig(\n    variant=\"road_2\",\n    target_modules=[\"q_proj\", \"v_proj\"],\n    group_size=64\n)\n\n# Create PEFT model with RoAd\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Adding Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, RoadConfig\n\n# Create model with first adapter\nconfig1 = RoadConfig(\n    variant=\"road_1\",\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\npeft_model = get_peft_model(model, config1, adapter_name=\"task_a\")\n\n# Add second adapter\nconfig2 = RoadConfig(\n    variant=\"road_2\",\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\npeft_model.add_adapter(\"task_b\", config2)\n\n# Switch between adapters\npeft_model.set_adapter(\"task_a\")\noutput_a = peft_model(**inputs)\n\npeft_model.set_adapter(\"task_b\")\noutput_b = peft_model(**inputs)\n</syntaxhighlight>\n\n=== Mixed Adapter Batches Inference ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, RoadConfig\nimport torch\n\n# Setup model with multiple adapters\npeft_model = get_peft_model(model, config, adapter_name=\"adapter1\")\npeft_model.add_adapter(\"adapter2\", config)\npeft_model.add_adapter(\"adapter3\", config)\n\n# Prepare batch with different adapters per sample\ninputs = tokenizer(\n    [\"Sample 1\", \"Sample 2\", \"Sample 3\"],\n    return_tensors=\"pt\",\n    padding=True\n)\n\n# Must be in eval mode\npeft_model.eval()\n\n# Specify which adapter for each sample\nadapter_names = [\"adapter1\", \"adapter2\", \"__base__\"]  # \"__base__\" = no adapter\n\nwith torch.no_grad():\n    outputs = peft_model(**inputs, adapter_names=adapter_names)\n\n# Each sample uses its specified adapter\n</syntaxhighlight>\n\n=== Quantized Model with RoAd ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, RoadConfig\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\n# Load model in 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config\n)\n\n# Apply RoAd to quantized model\nconfig = RoadConfig(\n    variant=\"road_1\",\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\npeft_model = get_peft_model(model, config)\n# RoadModel automatically creates quantized RoAd layers\n</syntaxhighlight>\n\n=== Context Manager for Forward Hooks ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, RoadConfig\n\npeft_model = get_peft_model(model, config)\npeft_model.eval()\n\n# Use context manager for adapter-specific inference\nwith peft_model._enable_peft_forward_hooks(adapter_names=[\"task_a\"]):\n    # All forward passes use \"task_a\" adapter\n    output = peft_model(**inputs)\n\n# Outside context, default adapter is used\noutput_default = peft_model(**inputs)\n</syntaxhighlight>\n\n=== Training and Inference Workflow ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, RoadConfig\nfrom transformers import Trainer, TrainingArguments\n\n# Setup\nconfig = RoadConfig(variant=\"road_2\", target_modules=[\"q_proj\", \"v_proj\"])\npeft_model = get_peft_model(model, config)\n\n# Training\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n\n# Save adapter\npeft_model.save_pretrained(\"./road_adapter\")\n\n# Inference\npeft_model.eval()\nwith torch.no_grad():\n    outputs = peft_model.generate(**inputs, max_length=50)\n</syntaxhighlight>\n\n=== Updating Existing RoadLayer ===\n<syntaxhighlight lang=\"python\">\n# When adding an adapter to a model that already has RoAd layers,\n# _create_and_replace updates existing layers instead of replacing\n\nconfig1 = RoadConfig(variant=\"road_1\", target_modules=[\"q_proj\"])\npeft_model = get_peft_model(model, config1, adapter_name=\"adapter1\")\n\n# Adding another adapter updates existing RoadLayers\nconfig2 = RoadConfig(variant=\"road_2\", target_modules=[\"q_proj\"])\npeft_model.add_adapter(\"adapter2\", config2)\n\n# The RoadLayer at \"q_proj\" now has both adapters\n# and can switch between them\n</syntaxhighlight>\n\n=== Device Map Support ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, RoadConfig\nfrom transformers import AutoModelForCausalLM\n\n# Load model with device map for multi-GPU\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-13b-hf\",\n    device_map=\"auto\"\n)\n\n# RoadModel respects device map\nconfig = RoadConfig(\n    variant=\"road_2\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n)\n\npeft_model = get_peft_model(model, config)\n# RoAd layers are created on appropriate devices\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Module Dispatching ===\n<code>_create_new_module</code> uses a dispatcher pattern to handle different layer types:\n# '''BitsAndBytes 8-bit''': <code>dispatch_bnb_8bit</code>\n# '''BitsAndBytes 4-bit''': <code>dispatch_bnb_4bit</code>\n# '''Default (torch.nn.Linear)''': <code>dispatch_default</code>\n\nFirst matching dispatcher wins.\n\n=== Adapter Name Injection ===\nDuring mixed adapter inference:\n# Pre-hooks are registered on all RoadLayers\n# <code>_adapter_names_pre_forward_hook</code> injects <code>adapter_names</code> into kwargs\n# RoadLayer uses adapter names to select appropriate parameters\n# Hooks are cleaned up after context exit\n\n=== Quantization Integration ===\nRoadModel detects quantization through model attributes:\n* <code>loaded_in_8bit</code>: Uses 8-bit RoAd layers\n* <code>loaded_in_4bit</code>: Uses 4-bit RoAd layers\n* Extracts <code>get_apply_tensor_subclass</code> for torchao support\n\n=== Active Adapter Management ===\n* New adapters not in <code>active_adapters</code> are created with <code>requires_grad=False</code>\n* Call <code>set_adapter()</code> or <code>enable_adapter()</code> to make them trainable\n\n=== Validation ===\nThe context manager validates:\n* Not in training mode (mixed adapters only for inference)\n* All specified adapter names exist in at least one layer\n* Uses \"__base__\" for no adapter on specific samples\n\n== Supported Target Modules ==\n\nCurrently only <code>torch.nn.Linear</code> modules are supported for RoAd transformation.\n\nQuantized variants supported:\n* <code>bnb.nn.Linear8bitLt</code> (if BitsAndBytes available)\n* <code>bnb.nn.Linear4bit</code> (if BitsAndBytes 4-bit available)\n\n== Related Pages ==\n* [[huggingface_peft_RoadConfig|RoadConfig]] - Configuration for RoadModel\n* [[huggingface_peft_RoadLayer|RoadLayer]] - Layer implementation\n* [[huggingface_peft_RoadQuantized|RoadQuantized]] - Quantized RoAd layers\n* [[huggingface_peft_BaseTuner|BaseTuner]] - Base class for tuners\n* [[PEFT]] - Parameter-Efficient Fine-Tuning\n* [[Quantization]]\n\n== Categories ==\n[[Category:PEFT]]\n[[Category:Model]]\n[[Category:Rotation-Based Methods]]\n[[Category:Quantization]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_RoadQuantized",
      "page_title": "huggingface peft RoadQuantized",
      "page_type": "Implementation",
      "overview": "Quantized RoAD layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling rotation adaptation on quantized models.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|bitsandbytes|https://github.com/bitsandbytes-foundation/bitsandbytes]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nQuantized RoAD layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling rotation adaptation on quantized models.\n\n=== Description ===\n\nLinear8bitLt and Linear4bit implement RoAD for bitsandbytes quantized layers. The rotational transformation (theta and alpha parameters) is applied to the layer output. During merge, the full rotation matrix R is constructed and applied to dequantized weights as R @ W, then requantized. For unmerge, the inverse rotation R^-1 is computed and applied.\n\n=== Usage ===\n\nUse RoAD quantized layers when fine-tuning quantized models with load_in_8bit or load_in_4bit. Layers are automatically dispatched when base layers are bitsandbytes Linear8bitLt or Linear4bit. Merging involves matrix inversion for unmerge which may introduce numerical errors.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/road/bnb.py src/peft/tuners/road/bnb.py]\n* '''Lines:''' 1-408\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass Linear8bitLt(torch.nn.Module, RoadLayer):\n    \"\"\"RoAD for 8-bit quantized layers.\"\"\"\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        variant: RoadVariant = \"road_1\",\n        group_size: int = 64,\n        init_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize RoAD for 8-bit layer.\"\"\"\n\n    def merge(self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None):\n        \"\"\"Merge rotation into dequantized weights.\"\"\"\n\n    def unmerge(self) -> None:\n        \"\"\"Apply inverse rotation to unmerge.\"\"\"\n\nclass Linear4bit(torch.nn.Module, RoadLayer):\n    \"\"\"RoAD for 4-bit quantized layers.\"\"\"\n\ndef dispatch_bnb_8bit(target, adapter_name, **kwargs):\n    \"\"\"Dispatch RoAD for 8-bit layers.\"\"\"\n\ndef dispatch_bnb_4bit(target, adapter_name, **kwargs):\n    \"\"\"Dispatch RoAD for 4-bit layers.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.road.bnb import Linear8bitLt, Linear4bit\nfrom peft.tuners.road.bnb import dispatch_bnb_8bit, dispatch_bnb_4bit\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || bnb.nn.Linear8bitLt/Linear4bit || Yes || Quantized base layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| variant || str || Yes || \"road_1\", \"road_2\", or \"road_4\"\n|-\n| group_size || int || Yes || Size of rotation groups\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Rotated output from quantized layer\n|-\n| _get_delta_weight() || torch.Tensor || Rotation matrix R for merging\n|}\n\n== Usage Examples ==\n\n=== RoAD with 4-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import RoadConfig, get_peft_model\n\n# Load model in 4-bit\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n)\n\n# Apply RoAD\nconfig = RoadConfig(\n    variant=\"road_1\",\n    group_size=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n# Automatically uses Linear4bit class\n</syntaxhighlight>\n\n=== RoAD with 8-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import RoadConfig, get_peft_model\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    load_in_8bit=True,\n)\n\nconfig = RoadConfig(\n    variant=\"road_2\",\n    group_size=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Merge/Unmerge with Quantization ===\n<syntaxhighlight lang=\"python\">\n# Merging applies R @ W to dequantized weights\nmodel.merge_adapter()\n\n# Unmerging applies R^-1 @ W\n# Warning: Matrix inversion may introduce numerical errors\nmodel.unmerge_adapter()\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "bitsandbytes",
          "url": "https://github.com/bitsandbytes-foundation/bitsandbytes"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_ShiraConfig",
      "page_title": "ShiraConfig",
      "page_type": "Implementation",
      "overview": "=== Description === ShiraConfig is a configuration class for the Sparse High Rank Adapter (SHiRA) method. SHiRA is a parameter-efficient fine-tuning technique that adapts pre-trained models using sparse high-rank adapters. Unlike LoRA which uses low-rank decomposition, SHiRA maintains the same parameter count as LoRA but uses a sparse masking approach to achieve high-rank adaptation. The configuration defines how SHiRA layers should be applied to target modules, including the rank parameter r, mask type, random seed for mask generation, and initialization settings.",
      "content": "= ShiraConfig =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* Source: src/peft/tuners/shira/config.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Low-Rank Adaptation]]\n* [[Model Compression]]\n* [[Transfer Learning]]\n\n== Overview ==\n\n=== Description ===\nShiraConfig is a configuration class for the Sparse High Rank Adapter (SHiRA) method. SHiRA is a parameter-efficient fine-tuning technique that adapts pre-trained models using sparse high-rank adapters. Unlike LoRA which uses low-rank decomposition, SHiRA maintains the same parameter count as LoRA but uses a sparse masking approach to achieve high-rank adaptation.\n\nThe configuration defines how SHiRA layers should be applied to target modules, including the rank parameter r, mask type, random seed for mask generation, and initialization settings.\n\n=== Usage ===\nShiraConfig is used to initialize and configure SHiRA adapters on pre-trained models. It extends PeftConfig and provides SHiRA-specific parameters for controlling the sparsity pattern, initialization strategy, and target modules to adapt. The configuration is typically passed to get_peft_model() to create a SHiRA-adapted model.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/shira/config.py\nLines: 28-130\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass ShiraConfig(PeftConfig):\n    \"\"\"\n    Configuration class for ShiraModel.\n\n    Args:\n        r: Number of SHiRA parameters (default: 32)\n        mask_type: Type of mask function (default: \"random\")\n        random_seed: Random seed for mask generation (default: None)\n        target_modules: Module names or regex to replace with SHiRA\n        fan_in_fan_out: Weight storage format flag (default: False)\n        init_weights: Initialize to zeros (default: True)\n        modules_to_save: Additional trainable modules\n    \"\"\"\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.shira.config import ShiraConfig\n# Or via the main PEFT interface\nfrom peft import ShiraConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Configuration Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| r || int || 32 || Number of SHiRA parameters computed as r(m+n) for m x n tensor\n|-\n| mask_type || Literal[\"random\"] || \"random\" || Type of mask function for sparsity pattern\n|-\n| random_seed || Optional[int] || None || Random seed for torch generator in random_mask\n|-\n| target_modules || Optional[Union[list[str], str]] || None || Module names or regex to replace with SHiRA\n|-\n| fan_in_fan_out || bool || False || True if layer stores weight as (fan_in, fan_out)\n|-\n| init_weights || bool || True || Initialize SHiRA weights to zeros; False uses randn\n|-\n| modules_to_save || Optional[list[str]] || None || Additional modules to set as trainable\n|}\n\n=== Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| peft_type || PeftType || Set to PeftType.SHIRA in __post_init__\n|-\n| mask_fn || Callable || Mask function set based on mask_type\n|}\n\n== Usage Examples ==\n\n=== Basic Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import ShiraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Create SHiRA configuration\nconfig = ShiraConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    mask_type=\"random\",\n    random_seed=42\n)\n\n# Apply SHiRA to the model\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Custom Target Modules with Regex ===\n<syntaxhighlight lang=\"python\">\nconfig = ShiraConfig(\n    r=64,\n    target_modules=r\".*decoder.*(SelfAttention|EncDecAttention).*(q|v)$\",\n    init_weights=True,\n    modules_to_save=[\"classifier\"]\n)\n</syntaxhighlight>\n\n=== Configuration with Custom Mask Function ===\n<syntaxhighlight lang=\"python\">\nimport torch\n\ndef custom_mask_fn(layer, r, **kwargs):\n    \"\"\"Custom sparse mask function\"\"\"\n    m, n = layer.weight.shape\n    num_params = r * (m + n)\n    mask = torch.zeros(m, n, device=layer.weight.device, dtype=layer.weight.dtype)\n    # Custom logic to set mask values\n    return mask\n\nconfig = ShiraConfig(r=32, target_modules=[\"q_proj\", \"v_proj\"])\nconfig.mask_fn = custom_mask_fn\n</syntaxhighlight>\n\n=== Fan-in Fan-out Configuration ===\n<syntaxhighlight lang=\"python\">\n# For models like GPT-2 that use Conv1D\nconfig = ShiraConfig(\n    r=32,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    fan_in_fan_out=True\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_ShiraLayer|ShiraLayer]] - Layer implementation for SHiRA adapters\n* [[huggingface_peft_ShiraModel|ShiraModel]] - Model class for SHiRA\n* [[huggingface_peft_LoraConfig|LoraConfig]] - Configuration for LoRA adapters\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Low-Rank Adaptation]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Model Configuration]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_ShiraLayer",
      "page_title": "ShiraLayer",
      "page_type": "Implementation",
      "overview": "=== Description === ShiraLayer is the layer implementation for Sparse High Rank Adapter (SHiRA), which applies sparse high-rank adaptation to neural network layers. The layer stores sparse weight updates using a sparse COO tensor representation, where trainable parameters are stored as a flat vector with corresponding indices. This approach enables parameter-efficient fine-tuning with the same parameter count as LoRA but maintains high-rank adaptation through sparsity. The implementation includes a base ShiraLayer class that manages adapter parameters and a Linear class that implements the SHiRA adapter for dense linear layers.",
      "content": "= ShiraLayer =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* Source: src/peft/tuners/shira/layer.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Neural Network Layers]]\n* [[Sparse Matrices]]\n* [[Low-Rank Adaptation]]\n\n== Overview ==\n\n=== Description ===\nShiraLayer is the layer implementation for Sparse High Rank Adapter (SHiRA), which applies sparse high-rank adaptation to neural network layers. The layer stores sparse weight updates using a sparse COO tensor representation, where trainable parameters are stored as a flat vector with corresponding indices. This approach enables parameter-efficient fine-tuning with the same parameter count as LoRA but maintains high-rank adaptation through sparsity.\n\nThe implementation includes a base ShiraLayer class that manages adapter parameters and a Linear class that implements the SHiRA adapter for dense linear layers.\n\n=== Usage ===\nShiraLayer is automatically instantiated when applying ShiraConfig to a model. It wraps base layers (currently only nn.Linear is supported) and adds sparse trainable parameters. The layer supports merging/unmerging adapters with base weights and can handle multiple adapters simultaneously.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/shira/layer.py\nLines: 26-218\n\n=== Class Signatures ===\n<syntaxhighlight lang=\"python\">\nclass ShiraLayer(BaseTunerLayer):\n    \"\"\"Base class for SHiRA layers\"\"\"\n    adapter_layer_names = (\"shira_weight\",)\n    other_param_names = (\"r\", \"scaling\", \"shira_indices\")\n\nclass Linear(nn.Module, ShiraLayer):\n    \"\"\"SHiRA implementation for dense linear layers\"\"\"\n    def __init__(\n        self,\n        base_layer,\n        mask,\n        adapter_name: str,\n        r: int = 0,\n        fan_in_fan_out: bool = False,\n        init_weights: bool = True,\n        **kwargs\n    ) -> None\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.shira.layer import ShiraLayer, Linear\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== ShiraLayer Initialization Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| base_layer || nn.Module || The base layer to wrap with SHiRA adapter\n|-\n| kwargs || dict || Additional keyword arguments\n|}\n\n=== Linear Layer Initialization Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || nn.Module || (required) || The base linear layer to adapt\n|-\n| mask || torch.Tensor || (required) || Binary mask defining sparse pattern\n|-\n| adapter_name || str || (required) || Name of the adapter\n|-\n| r || int || 0 || Rank parameter for SHiRA\n|-\n| fan_in_fan_out || bool || False || True if weights stored as (fan_in, fan_out)\n|-\n| init_weights || bool || True || Initialize weights to zeros\n|}\n\n=== Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| shira_weight || nn.ParameterDict || Dictionary of trainable sparse weight vectors per adapter\n|-\n| shira_indices || dict || Dictionary of indices for sparse COO tensor per adapter\n|-\n| r || dict || Dictionary of rank values per adapter\n|-\n| scaling || dict || Dictionary of scaling factors per adapter\n|-\n| merged_adapters || list || List of currently merged adapter names\n|}\n\n=== Key Methods ===\n{| class=\"wikitable\"\n! Method !! Parameters !! Returns !! Description\n|-\n| update_layer || adapter_name, mask, r, init_weights, inference_mode || None || Update layer with new adapter\n|-\n| merge || safe_merge, adapter_names || None || Merge adapter weights into base weights\n|-\n| unmerge || None || None || Unmerge adapter weights from base weights\n|-\n| get_delta_weight || adapter || torch.Tensor || Compute delta weight for given adapter\n|-\n| forward || x, *args, **kwargs || torch.Tensor || Forward pass with adapter\n|-\n| set_scale || adapter, scale || None || Set scaling factor for adapter\n|}\n\n== Usage Examples ==\n\n=== Forward Pass with SHiRA Layer ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import ShiraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Create model with SHiRA\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nconfig = ShiraConfig(r=32, target_modules=[\"q_proj\"])\nmodel = get_peft_model(base_model, config)\n\n# Forward pass\ninput_ids = torch.randint(0, 1000, (2, 10))\noutputs = model(input_ids)\n</syntaxhighlight>\n\n=== Merging and Unmerging Adapters ===\n<syntaxhighlight lang=\"python\">\n# Access a SHiRA layer\nshira_layer = model.base_model.model.decoder.layers[0].self_attn.q_proj\n\n# Merge adapter into base weights\nshira_layer.merge(safe_merge=True)\n\n# Check if merged\nprint(f\"Is merged: {shira_layer.merged}\")\n\n# Unmerge adapter\nshira_layer.unmerge()\n</syntaxhighlight>\n\n=== Setting Custom Scaling ===\n<syntaxhighlight lang=\"python\">\n# Set custom scaling for an adapter\nshira_layer.set_scale(\"default\", scale=2.0)\n\n# Forward pass with scaled adapter\noutput = shira_layer(input_tensor)\n</syntaxhighlight>\n\n=== Inspecting Sparse Pattern ===\n<syntaxhighlight lang=\"python\">\n# Get delta weight as sparse COO tensor\ndelta = shira_layer.get_delta_weight(\"default\")\n\n# Inspect sparsity\nnum_nonzero = delta._nnz()\ntotal_params = delta.shape[0] * delta.shape[1]\nsparsity = 1.0 - (num_nonzero / total_params)\nprint(f\"Sparsity: {sparsity:.2%}\")\n</syntaxhighlight>\n\n=== Disabling Adapters Temporarily ===\n<syntaxhighlight lang=\"python\">\n# Disable adapters for inference\nwith model.disable_adapter():\n    output = model(input_ids)  # Uses only base model weights\n</syntaxhighlight>\n\n=== Accessing Layer Parameters ===\n<syntaxhighlight lang=\"python\">\n# Access SHiRA parameters\nfor name, param in shira_layer.named_parameters():\n    if \"shira_weight\" in name:\n        print(f\"Adapter: {name}, Shape: {param.shape}, Trainable: {param.requires_grad}\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_ShiraConfig|ShiraConfig]] - Configuration class for SHiRA\n* [[huggingface_peft_ShiraModel|ShiraModel]] - Model class for SHiRA\n* [[huggingface_peft_LoraLayer|LoraLayer]] - Layer implementation for LoRA\n* [[Neural Network Layers]]\n* [[Sparse Matrices]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Neural Network Layers]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_ShiraModel",
      "page_title": "ShiraModel",
      "page_type": "Implementation",
      "overview": "=== Description === ShiraModel creates a Sparse High Rank Adapter (SHiRA) model from a pre-trained transformers model. It is the main model class that coordinates the injection of SHiRA adapters into target modules of the base model. The class extends BaseTuner and handles the creation, replacement, and management of SHiRA layers throughout the model architecture. SHiRA is a parameter-efficient fine-tuning method that maintains the same parameter count as LoRA but uses sparse high-rank adaptation instead of low-rank decomposition, potentially providing better adaptation capabilities.",
      "content": "= ShiraModel =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* Source: src/peft/tuners/shira/model.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Transfer Learning]]\n* [[Model Adaptation]]\n* [[Sparse High Rank Adaptation]]\n\n== Overview ==\n\n=== Description ===\nShiraModel creates a Sparse High Rank Adapter (SHiRA) model from a pre-trained transformers model. It is the main model class that coordinates the injection of SHiRA adapters into target modules of the base model. The class extends BaseTuner and handles the creation, replacement, and management of SHiRA layers throughout the model architecture.\n\nSHiRA is a parameter-efficient fine-tuning method that maintains the same parameter count as LoRA but uses sparse high-rank adaptation instead of low-rank decomposition, potentially providing better adaptation capabilities.\n\n=== Usage ===\nShiraModel is typically instantiated through the get_peft_model() function by passing a ShiraConfig. It automatically identifies and replaces target modules in the base model with SHiRA layers, manages adapter injection, and provides methods for adapter manipulation and merging.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/shira/model.py\nLines: 29-143\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\nclass ShiraModel(BaseTuner):\n    \"\"\"\n    Creates Sparse High Rank Adapter (SHiRA) Model from a pretrained model.\n\n    Args:\n        model: The model to be adapted\n        config: The ShiraConfig configuration\n        adapter_name: The name of the adapter (defaults to \"default\")\n\n    Returns:\n        torch.nn.Module: The SHiRA model\n    \"\"\"\n    prefix: str = \"shira_\"\n    tuner_layer_cls = ShiraLayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_SHIRA_TARGET_MODULES_MAPPING\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.shira.model import ShiraModel\n# Or via the main PEFT interface\nfrom peft import get_peft_model, ShiraConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Initialization Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| model || PreTrainedModel || The base model to be adapted\n|-\n| config || ShiraConfig || Configuration for SHiRA adaptation\n|-\n| adapter_name || str || Name of the adapter (default: \"default\")\n|}\n\n=== Class Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| prefix || str || Prefix for SHiRA parameters (\"shira_\")\n|-\n| tuner_layer_cls || Type || ShiraLayer class reference\n|-\n| target_module_mapping || dict || Mapping of model architectures to default target modules\n|}\n\n=== Key Methods ===\n{| class=\"wikitable\"\n! Method !! Parameters !! Returns !! Description\n|-\n| _create_and_replace || shira_config, adapter_name, target, target_name, parent, current_key, optional_kwargs || None || Create and replace target module with SHiRA layer\n|-\n| _create_new_module || shira_config, adapter_name, target, kwargs || Linear || Create new SHiRA module\n|}\n\n== Usage Examples ==\n\n=== Basic SHiRA Model Creation ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import ShiraConfig, get_peft_model\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Create SHiRA configuration\nconfig = ShiraConfig(r=32)\n\n# Create SHiRA model\nmodel = get_peft_model(base_model, config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Training with SHiRA ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import ShiraConfig, get_peft_model\n\n# Setup\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nconfig = ShiraConfig(\n    r=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    init_weights=True\n)\nmodel = get_peft_model(base_model, config)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./shira_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n)\n\n# Train\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\ntrainer.train()\n</syntaxhighlight>\n\n=== Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\n# Create model with first adapter\nconfig1 = ShiraConfig(r=32, target_modules=[\"q_proj\", \"v_proj\"])\nmodel = get_peft_model(base_model, config1, adapter_name=\"task1\")\n\n# Add second adapter\nconfig2 = ShiraConfig(r=64, target_modules=[\"q_proj\", \"v_proj\"])\nmodel.add_adapter(\"task2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"task1\")\noutput1 = model(input_ids)\n\nmodel.set_adapter(\"task2\")\noutput2 = model(input_ids)\n</syntaxhighlight>\n\n=== Save and Load SHiRA Model ===\n<syntaxhighlight lang=\"python\">\n# Save adapter\nmodel.save_pretrained(\"./shira_adapter\")\n\n# Load adapter\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nmodel = PeftModel.from_pretrained(base_model, \"./shira_adapter\")\n</syntaxhighlight>\n\n=== Merge and Save Full Model ===\n<syntaxhighlight lang=\"python\">\n# Merge adapter into base model\nmodel = model.merge_and_unload()\n\n# Save full model\nmodel.save_pretrained(\"./merged_model\")\n</syntaxhighlight>\n\n=== Custom Target Modules ===\n<syntaxhighlight lang=\"python\">\n# Target specific modules with regex\nconfig = ShiraConfig(\n    r=32,\n    target_modules=r\".*decoder.*attn.*(q|v)_proj$\",\n    modules_to_save=[\"lm_head\"]\n)\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Inference with SHiRA ===\n<syntaxhighlight lang=\"python\">\nimport torch\n\n# Set to evaluation mode\nmodel.eval()\n\n# Generate with SHiRA adapter\nwith torch.no_grad():\n    input_ids = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids\n    outputs = model.generate(input_ids, max_length=50)\n\nprint(tokenizer.decode(outputs[0]))\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_ShiraConfig|ShiraConfig]] - Configuration class for SHiRA\n* [[huggingface_peft_ShiraLayer|ShiraLayer]] - Layer implementation for SHiRA\n* [[huggingface_peft_LoraModel|LoraModel]] - Model class for LoRA adapters\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Transfer Learning]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Model Adaptation]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_TrainableTokensConfig",
      "page_title": "TrainableTokensConfig",
      "page_type": "Implementation",
      "overview": "=== Description === TrainableTokensConfig is a configuration class for the TrainableTokens method, which enables training new tokens or re-training existing tokens without training the full embedding matrix. This method marks select tokens (identified by their indices) as trainable while leaving the rest frozen, significantly reducing memory usage for both storage and working memory compared to training the entire embedding layer. This approach is particularly useful for domain adaptation, adding new vocabulary, or fine-tuning specific tokens while preserving the rest of the pre-trained embeddings.",
      "content": "= TrainableTokensConfig =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* Source: src/peft/tuners/trainable_tokens/config.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Token Embeddings]]\n* [[Transfer Learning]]\n* [[Memory Optimization]]\n\n== Overview ==\n\n=== Description ===\nTrainableTokensConfig is a configuration class for the TrainableTokens method, which enables training new tokens or re-training existing tokens without training the full embedding matrix. This method marks select tokens (identified by their indices) as trainable while leaving the rest frozen, significantly reducing memory usage for both storage and working memory compared to training the entire embedding layer.\n\nThis approach is particularly useful for domain adaptation, adding new vocabulary, or fine-tuning specific tokens while preserving the rest of the pre-trained embeddings.\n\n=== Usage ===\nTrainableTokensConfig is used to specify which token indices should be trainable during fine-tuning. It extends PeftConfig and provides configuration for targeting embedding layers and initializing token weights. The configuration is passed to get_peft_model() to create a model with trainable token adapters.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/trainable_tokens/config.py\nLines: 25-90\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass TrainableTokensConfig(PeftConfig):\n    \"\"\"\n    Configuration for the TrainableTokens method.\n\n    Args:\n        token_indices: List of token indices to make trainable\n        target_modules: Module names or regex for embedding layers\n        init_weights: Initialize to existing embeddings (default: True)\n    \"\"\"\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.trainable_tokens.config import TrainableTokensConfig\n# Or via the main PEFT interface\nfrom peft import TrainableTokensConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Configuration Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| token_indices || list[int] || [] || List of token indices to make trainable\n|-\n| target_modules || Optional[Union[list[str], str]] || None || Module names or regex for embedding layers to adapt\n|-\n| init_weights || bool || True || Initialize to existing embeddings; False for random initialization\n|}\n\n=== Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| peft_type || PeftType || Set to PeftType.TRAINABLE_TOKENS in __post_init__\n|}\n\n== Usage Examples ==\n\n=== Basic Configuration for New Tokens ===\n<syntaxhighlight lang=\"python\">\nfrom peft import TrainableTokensConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n\n# Add new tokens\nnew_tokens = [\"<special1>\", \"<special2>\", \"<domain_term>\"]\ntokenizer.add_tokens(new_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Get indices of new tokens\ntoken_indices = [tokenizer.convert_tokens_to_ids(token) for token in new_tokens]\n\n# Configure trainable tokens\nconfig = TrainableTokensConfig(\n    token_indices=token_indices,\n    init_weights=False  # Random init for new tokens\n)\n\n# Apply configuration\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Re-training Specific Existing Tokens ===\n<syntaxhighlight lang=\"python\">\n# Identify tokens to fine-tune (e.g., domain-specific terms)\ntokens_to_finetune = [\"medical\", \"diagnosis\", \"treatment\", \"patient\"]\ntoken_indices = [tokenizer.convert_tokens_to_ids(token) for token in tokens_to_finetune]\n\n# Configure with initialization from existing embeddings\nconfig = TrainableTokensConfig(\n    token_indices=token_indices,\n    init_weights=True  # Start from current embeddings\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Custom Target Modules ===\n<syntaxhighlight lang=\"python\">\n# Target specific embedding layers\nconfig = TrainableTokensConfig(\n    token_indices=[100, 101, 102, 103],\n    target_modules=[\"encoder.embed_tokens\", \"decoder.embed_tokens\"]\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Finding Token Indices ===\n<syntaxhighlight lang=\"python\">\n# Method 1: By token strings\ntokens = [\"hello\", \"world\", \"AI\"]\ntoken_indices = [tokenizer.convert_tokens_to_ids(t) for t in tokens]\n\n# Method 2: By tokenizing text\ntext = \"special domain terminology\"\nencoded = tokenizer(text, return_tensors=\"pt\")\ntoken_indices = encoded.input_ids[0].tolist()\n\n# Method 3: Range of new tokens after resizing\noriginal_vocab_size = len(tokenizer)\ntokenizer.add_tokens([\"<new1>\", \"<new2>\", \"<new3>\"])\nmodel.resize_token_embeddings(len(tokenizer))\ntoken_indices = list(range(original_vocab_size, len(tokenizer)))\n\nconfig = TrainableTokensConfig(token_indices=token_indices)\n</syntaxhighlight>\n\n=== Training with TrainableTokens ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments\n\n# Configure trainable tokens\nconfig = TrainableTokensConfig(\n    token_indices=[50000, 50001, 50002],  # Indices of new tokens\n    init_weights=False\n)\n\nmodel = get_peft_model(base_model, config)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./trainable_tokens_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n)\n\n# Train only the specified tokens\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\ntrainer.train()\n</syntaxhighlight>\n\n=== Checking Trainable Parameters ===\n<syntaxhighlight lang=\"python\">\n# Print trainable parameters\nmodel.print_trainable_parameters()\n\n# Verify only specified tokens are trainable\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"Trainable: {name}, Shape: {param.shape}\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_TrainableTokensLayer|TrainableTokensLayer]] - Layer implementation\n* [[huggingface_peft_TrainableTokensModel|TrainableTokensModel]] - Model class\n* [[Token Embeddings]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Domain Adaptation]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Model Configuration]]\n[[Category:NLP]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_TrainableTokensLayer",
      "page_title": "TrainableTokensLayer",
      "page_type": "Implementation",
      "overview": "=== Description === TrainableTokensLayer is a layer implementation that wraps embedding layers (nn.Embedding) or linear layers (nn.Linear) to enable selective training of specific token embeddings. Instead of training the entire embedding matrix, this layer maintains trainable delta weights only for specified token indices, significantly reducing memory requirements. The layer stores both the updated weights (trainable_tokens_delta) and original weights (trainable_tokens_original) for the specified tokens, allowing for efficient merging/unmerging operations. It supports weight tying scenarios where multiple layers share the same embeddings.",
      "content": "= TrainableTokensLayer =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* Source: src/peft/tuners/trainable_tokens/layer.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Token Embeddings]]\n* [[Neural Network Layers]]\n* [[Memory Optimization]]\n\n== Overview ==\n\n=== Description ===\nTrainableTokensLayer is a layer implementation that wraps embedding layers (nn.Embedding) or linear layers (nn.Linear) to enable selective training of specific token embeddings. Instead of training the entire embedding matrix, this layer maintains trainable delta weights only for specified token indices, significantly reducing memory requirements.\n\nThe layer stores both the updated weights (trainable_tokens_delta) and original weights (trainable_tokens_original) for the specified tokens, allowing for efficient merging/unmerging operations. It supports weight tying scenarios where multiple layers share the same embeddings.\n\n=== Usage ===\nTrainableTokensLayer is automatically instantiated when applying TrainableTokensConfig to a model. It intercepts forward passes through embedding layers and applies the trainable token updates using index_copy operations. The layer supports both nn.Embedding and nn.Linear base layers to handle tied weights scenarios.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/trainable_tokens/layer.py\nLines: 30-263\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\nclass TrainableTokensLayer(nn.Module, BaseTunerLayer):\n    \"\"\"\n    Layer for training specific tokens efficiently.\n\n    Args:\n        base_layer: The embedding or linear layer to wrap\n        adapter_name: Name of the adapter\n        token_indices: List of token indices to make trainable\n        tied_adapter: Optional tied adapter for weight sharing\n    \"\"\"\n    adapter_layer_names = (\"trainable_tokens_delta\",)\n    other_param_names = (\"token_indices\", \"trainable_tokens_original\")\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.trainable_tokens.layer import TrainableTokensLayer\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Initialization Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| base_layer || nn.Module || The base embedding or linear layer to wrap\n|-\n| adapter_name || str || Name of the adapter\n|-\n| token_indices || list[int] || List of token indices to make trainable\n|-\n| tied_adapter || Optional[TrainableTokensLayer] || Tied adapter for weight sharing (default: None)\n|-\n| kwargs || dict || Additional keyword arguments\n|}\n\n=== Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| trainable_tokens_delta || nn.ParameterDict || Trainable delta weights for specified tokens\n|-\n| trainable_tokens_original || BufferDict || Original embeddings for specified tokens\n|-\n| token_indices || dict || Mapping of adapter names to token indices\n|-\n| merged_adapters || list || List of currently merged adapter names\n|-\n| tied_adapter || Optional[TrainableTokensLayer] || Reference to tied adapter if applicable\n|}\n\n=== Key Methods ===\n{| class=\"wikitable\"\n! Method !! Parameters !! Returns !! Description\n|-\n| update_layer || adapter_name, **kwargs || None || Update layer with new adapter configuration\n|-\n| merge || safe_merge, adapter_names || None || Merge token updates into base weights\n|-\n| unmerge || None || None || Restore original token embeddings\n|-\n| get_merged_weights || active_adapters || torch.Tensor || Get weights with adapters applied\n|-\n| forward || x, *args, **kwargs || torch.Tensor || Forward pass with trainable tokens\n|-\n| forward_adapters || x, active_adapters, *args, **kwargs || torch.Tensor || Forward with specific adapters\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with Model ===\n<syntaxhighlight lang=\"python\">\nfrom peft import TrainableTokensConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Setup\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n\n# Add new tokens\nnew_tokens = [\"<task>\", \"<domain>\"]\ntokenizer.add_tokens(new_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Configure trainable tokens\ntoken_indices = [tokenizer.convert_tokens_to_ids(t) for t in new_tokens]\nconfig = TrainableTokensConfig(token_indices=token_indices)\n\n# Apply configuration (creates TrainableTokensLayer)\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Accessing the Layer ===\n<syntaxhighlight lang=\"python\">\n# Access the trainable tokens layer\nembedding_layer = model.get_input_embeddings()\n\n# Check if it's a TrainableTokensLayer\nfrom peft.tuners.trainable_tokens.layer import TrainableTokensLayer\nif isinstance(embedding_layer, TrainableTokensLayer):\n    print(\"TrainableTokensLayer detected\")\n    print(f\"Token indices: {embedding_layer.token_indices}\")\n</syntaxhighlight>\n\n=== Merging and Unmerging ===\n<syntaxhighlight lang=\"python\">\n# Get embedding layer\nembedding_layer = model.get_input_embeddings()\n\n# Merge trainable tokens into base embeddings\nembedding_layer.merge()\n\n# Check merged status\nprint(f\"Is merged: {embedding_layer.merged}\")\n\n# Unmerge to restore original behavior\nembedding_layer.unmerge()\n</syntaxhighlight>\n\n=== Inspecting Trainable Token Weights ===\n<syntaxhighlight lang=\"python\">\n# Access trainable token deltas\nfor adapter_name, delta in embedding_layer.trainable_tokens_delta.items():\n    print(f\"Adapter: {adapter_name}\")\n    print(f\"Shape: {delta.shape}\")  # (num_tokens, embedding_dim)\n    print(f\"Trainable: {delta.requires_grad}\")\n\n# Access original weights\nfor adapter_name, original in embedding_layer.trainable_tokens_original.items():\n    print(f\"Original weights for {adapter_name}: {original.shape}\")\n</syntaxhighlight>\n\n=== Checking for Overlapping Tokens ===\n<syntaxhighlight lang=\"python\">\n# The layer automatically checks for overlapping token indices\n# when multiple adapters are used\ntry:\n    embedding_layer.merge(adapter_names=[\"adapter1\", \"adapter2\"])\nexcept ValueError as e:\n    print(f\"Overlap detected: {e}\")\n</syntaxhighlight>\n\n=== Forward Pass with Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\n# Set active adapters\nmodel.set_adapter([\"adapter1\", \"adapter2\"])\n\n# Forward pass uses all active adapters\ninput_ids = torch.randint(0, len(tokenizer), (2, 10))\noutputs = model(input_ids)\n</syntaxhighlight>\n\n=== Safe Merging with NaN Detection ===\n<syntaxhighlight lang=\"python\">\n# Merge with safety check\ntry:\n    embedding_layer.merge(safe_merge=True)\n    print(\"Merge successful, no NaNs detected\")\nexcept ValueError as e:\n    print(f\"Merge failed: {e}\")\n</syntaxhighlight>\n\n=== Working with Tied Weights ===\n<syntaxhighlight lang=\"python\">\n# When model has tied embeddings (input + output)\n# The layer automatically handles weight tying\n\n# Check if tied\nif embedding_layer.tied_adapter is not None:\n    print(\"This layer is tied to another adapter\")\n    print(f\"Tied to: {embedding_layer.tied_adapter}\")\n</syntaxhighlight>\n\n=== DeepSpeed Zero3 Support ===\n<syntaxhighlight lang=\"python\">\n# The layer automatically handles DeepSpeed Zero3 context\n# when collecting token weights during initialization\n\n# Check if DeepSpeed is enabled\nfrom peft.utils.integrations import check_deepspeed_zero3_enabled\nif check_deepspeed_zero3_enabled():\n    print(\"DeepSpeed Zero3 detected - using distributed weight collection\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_TrainableTokensConfig|TrainableTokensConfig]] - Configuration class\n* [[huggingface_peft_TrainableTokensModel|TrainableTokensModel]] - Model class\n* [[Token Embeddings]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Memory Optimization]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Neural Network Layers]]\n[[Category:NLP]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_TrainableTokensModel",
      "page_title": "TrainableTokensModel",
      "page_type": "Implementation",
      "overview": "=== Description === TrainableTokensModel is the main model class that manages the injection and coordination of TrainableTokensLayer adapters into pre-trained models. It extends BaseTuner and handles the automatic detection of embedding layers, creation of trainable token adapters, and management of weight-tied scenarios where input and output embeddings share parameters. The model automatically infers the embedding layer name if not specified and supports weight tying between embedding and LM head layers, ensuring consistent updates across tied weights.",
      "content": "= TrainableTokensModel =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* Source: src/peft/tuners/trainable_tokens/model.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Transfer Learning]]\n* [[Token Embeddings]]\n* [[Domain Adaptation]]\n\n== Overview ==\n\n=== Description ===\nTrainableTokensModel is the main model class that manages the injection and coordination of TrainableTokensLayer adapters into pre-trained models. It extends BaseTuner and handles the automatic detection of embedding layers, creation of trainable token adapters, and management of weight-tied scenarios where input and output embeddings share parameters.\n\nThe model automatically infers the embedding layer name if not specified and supports weight tying between embedding and LM head layers, ensuring consistent updates across tied weights.\n\n=== Usage ===\nTrainableTokensModel is instantiated through get_peft_model() by passing a TrainableTokensConfig. It automatically identifies embedding layers (or uses specified target_modules), wraps them with TrainableTokensLayer, and handles weight tying for models where input embeddings and output layers share parameters.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/trainable_tokens/model.py\nLines: 26-140\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\nclass TrainableTokensModel(BaseTuner):\n    \"\"\"\n    Model class for TrainableTokens method.\n\n    Automatically handles:\n    - Embedding layer detection\n    - Weight tying between input/output embeddings\n    - Adapter injection and management\n    \"\"\"\n    prefix: str = \"trainable_tokens_\"\n    tuner_layer_cls = TrainableTokensLayer\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.trainable_tokens.model import TrainableTokensModel\n# Or via the main PEFT interface\nfrom peft import get_peft_model, TrainableTokensConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Initialization Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| model || nn.Module || The base model to adapt\n|-\n| config || TrainableTokensConfig || Configuration for trainable tokens\n|-\n| adapter_name || str || Name of the adapter (default: \"default\")\n|}\n\n=== Class Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| prefix || str || Prefix for trainable tokens parameters (\"trainable_tokens_\")\n|-\n| tuner_layer_cls || Type || TrainableTokensLayer class reference\n|}\n\n=== Key Methods ===\n{| class=\"wikitable\"\n! Method !! Parameters !! Returns !! Description\n|-\n| _prepare_adapter_config || peft_config, model_config || PeftConfig || Prepare config, infer embedding layer if needed\n|-\n| inject_adapter || model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, kwargs || None || Inject adapter and handle weight tying\n|-\n| _get_tied_target_modules || *args, **kwargs || list || Override to suppress tied weights warning\n|-\n| _create_and_replace || peft_config, adapter_name, target, target_name, parent, current_key || None || Create and replace module with adapter\n|-\n| _create_and_replace_dict || peft_config, adapter_name, target, target_name, parent, current_key || None || Create and replace with dict config\n|-\n| _create_new_module || peft_config, adapter_name, target, kwargs || TrainableTokensLayer || Create new TrainableTokensLayer\n|}\n\n== Usage Examples ==\n\n=== Basic Model Creation ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import TrainableTokensConfig, get_peft_model\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n\n# Add new tokens\nnew_tokens = [\"<special1>\", \"<special2>\"]\ntokenizer.add_tokens(new_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Get token indices\ntoken_indices = [tokenizer.convert_tokens_to_ids(t) for t in new_tokens]\n\n# Create trainable tokens model\nconfig = TrainableTokensConfig(token_indices=token_indices)\nmodel = get_peft_model(model, config)\n\nprint(model.print_trainable_parameters())\n</syntaxhighlight>\n\n=== Auto-detection of Embedding Layer ===\n<syntaxhighlight lang=\"python\">\n# Without specifying target_modules, it auto-detects embedding layer\nconfig = TrainableTokensConfig(\n    token_indices=[100, 101, 102]\n    # target_modules not specified - will use get_input_embeddings()\n)\n\nmodel = get_peft_model(base_model, config)\n\n# Check which layer was targeted\nprint(f\"Input embeddings: {model.get_input_embeddings()}\")\n</syntaxhighlight>\n\n=== Explicit Target Modules ===\n<syntaxhighlight lang=\"python\">\n# Specify embedding layer explicitly\nconfig = TrainableTokensConfig(\n    token_indices=[100, 101, 102],\n    target_modules=[\"model.embed_tokens\"]\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Weight Tying Support ===\n<syntaxhighlight lang=\"python\">\n# For models with tied input/output embeddings\n# (e.g., where lm_head shares weights with embeddings)\n\nconfig = TrainableTokensConfig(\n    token_indices=[50000, 50001, 50002]\n)\n\n# Automatically handles tied weights\nmodel = get_peft_model(base_model, config)\n\n# Both input embeddings and lm_head are adapted with tied weights\nprint(\"Input embeddings:\", type(model.get_input_embeddings()))\nprint(\"Output embeddings:\", type(model.get_output_embeddings()))\n</syntaxhighlight>\n\n=== Training New Domain Vocabulary ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments\n\n# Add domain-specific tokens\ndomain_tokens = [\"<medical>\", \"<diagnosis>\", \"<treatment>\"]\ntokenizer.add_tokens(domain_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Configure trainable tokens\ntoken_indices = [tokenizer.convert_tokens_to_ids(t) for t in domain_tokens]\nconfig = TrainableTokensConfig(\n    token_indices=token_indices,\n    init_weights=False  # Random init for new tokens\n)\n\nmodel = get_peft_model(model, config)\n\n# Training\ntraining_args = TrainingArguments(\n    output_dir=\"./domain_adapted_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n)\n\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\ntrainer.train()\n</syntaxhighlight>\n\n=== Multiple Adapters for Different Tasks ===\n<syntaxhighlight lang=\"python\">\n# First adapter for task 1\nconfig1 = TrainableTokensConfig(token_indices=[100, 101, 102])\nmodel = get_peft_model(base_model, config1, adapter_name=\"task1\")\n\n# Second adapter for task 2 (different tokens)\nconfig2 = TrainableTokensConfig(token_indices=[200, 201, 202])\nmodel.add_adapter(\"task2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"task1\")\noutput1 = model(input_ids)\n\nmodel.set_adapter(\"task2\")\noutput2 = model(input_ids)\n</syntaxhighlight>\n\n=== Save and Load ===\n<syntaxhighlight lang=\"python\">\n# Save adapter\nmodel.save_pretrained(\"./trainable_tokens_adapter\")\n\n# Load adapter\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nmodel = PeftModel.from_pretrained(base_model, \"./trainable_tokens_adapter\")\n</syntaxhighlight>\n\n=== Merge and Unload ===\n<syntaxhighlight lang=\"python\">\n# Merge trainable tokens into base model\nmodel = model.merge_and_unload()\n\n# Now it's a standard model with updated embeddings\nmodel.save_pretrained(\"./merged_model\")\n</syntaxhighlight>\n\n=== Checking Trainable Parameters ===\n<syntaxhighlight lang=\"python\">\n# Print parameter statistics\nmodel.print_trainable_parameters()\n\n# Manually inspect\ntrainable_params = 0\nall_params = 0\nfor name, param in model.named_parameters():\n    all_params += param.numel()\n    if param.requires_grad:\n        trainable_params += param.numel()\n        print(f\"Trainable: {name}, Shape: {param.shape}\")\n\nprint(f\"Trainable: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_TrainableTokensConfig|TrainableTokensConfig]] - Configuration class\n* [[huggingface_peft_TrainableTokensLayer|TrainableTokensLayer]] - Layer implementation\n* [[Token Embeddings]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Domain Adaptation]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Model Adaptation]]\n[[Category:NLP]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_Trainer_train",
      "page_title": "huggingface peft Trainer train",
      "page_type": "Implementation",
      "overview": "Concrete tool for executing the training loop using HuggingFace Trainer with PEFT models for LoRA fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|Transformers Trainer|https://huggingface.co/docs/transformers/main_classes/trainer]]\n|-\n! Domains\n| [[domain::Training]], [[domain::NLP]], [[domain::Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for executing the training loop using HuggingFace Trainer with PEFT models for LoRA fine-tuning.\n\n=== Description ===\n\n`Trainer.train()` executes the full training loop for the PEFT model. HuggingFace's Trainer handles all training infrastructure: batching, gradient accumulation, mixed precision, logging, checkpointing, and evaluation. PEFT models work seamlessly with Trainer since only adapter parameters have `requires_grad=True`.\n\n=== Usage ===\n\nUse this after setting up your PEFT model, tokenized dataset, and TrainingArguments. The Trainer handles the training loop automatically. For LoRA fine-tuning, typical settings include lower learning rates (1e-4 to 2e-4), gradient accumulation for larger effective batch sizes, and regular evaluation.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Library:''' [https://github.com/huggingface/transformers transformers]\n* '''Class:''' `transformers.Trainer`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef train(\n    self,\n    resume_from_checkpoint: Optional[Union[str, bool]] = None,\n    trial: Optional[\"optuna.Trial\"] = None,\n    ignore_keys_for_eval: Optional[List[str]] = None,\n    **kwargs,\n) -> TrainOutput:\n    \"\"\"\n    Main training entry point.\n\n    Args:\n        resume_from_checkpoint: Path to checkpoint or bool to auto-detect\n        trial: Optuna trial for hyperparameter search\n        ignore_keys_for_eval: Keys to ignore during evaluation\n\n    Returns:\n        TrainOutput: Contains global_step, training_loss, metrics\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PeftModel || Yes || PEFT-wrapped model (passed to Trainer constructor)\n|-\n| args || TrainingArguments || Yes || Training hyperparameters and configuration\n|-\n| train_dataset || Dataset || Yes || Tokenized training data\n|-\n| eval_dataset || Dataset || No || Tokenized evaluation data\n|-\n| data_collator || DataCollator || No || Batch collation function\n|-\n| resume_from_checkpoint || str or bool || No || Checkpoint to resume from\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| TrainOutput || dataclass || Contains global_step, training_loss, and metrics dict\n|-\n| checkpoints || Files || Saved to output_dir based on save_strategy\n|-\n| logs || Files || Training logs to logging_dir\n|}\n\n== Usage Examples ==\n\n=== Standard LoRA Training ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments\nfrom peft import get_peft_model, LoraConfig\n\n# 1. Create PEFT model\nmodel = get_peft_model(base_model, lora_config)\n\n# 2. Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./lora-output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,  # Effective batch size = 16\n    learning_rate=2e-4,             # Typical for LoRA\n    warmup_ratio=0.03,\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    fp16=True,                      # Mixed precision\n)\n\n# 3. Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n)\n\n# 4. Train\ntrainer.train()\n\n# 5. Save final adapter\nmodel.save_pretrained(\"./final-adapter\")\n</syntaxhighlight>\n\n=== With Custom Callbacks ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments, TrainerCallback\n\nclass SaveBestCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, metrics, **kwargs):\n        if metrics.get(\"eval_loss\", float(\"inf\")) < state.best_metric:\n            control.should_save = True\n        return control\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    callbacks=[SaveBestCallback()],\n)\n</syntaxhighlight>\n\n=== Resume from Checkpoint ===\n<syntaxhighlight lang=\"python\">\n# Resume training from checkpoint\ntrainer.train(resume_from_checkpoint=\"./lora-output/checkpoint-500\")\n\n# Or auto-detect latest checkpoint\ntrainer.train(resume_from_checkpoint=True)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Training_Execution]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Training",
        "NLP",
        "Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "Transformers Trainer",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Training_Execution"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_Trainer_train_qlora",
      "page_title": "huggingface peft Trainer train qlora",
      "page_type": "Implementation",
      "overview": "Concrete tool for training QLoRA models using the HuggingFace Trainer with quantization-appropriate settings.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|Transformers Trainer|https://huggingface.co/docs/transformers/main_classes/trainer]]\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Fine_Tuning]], [[domain::Quantization]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for training QLoRA models using the HuggingFace Trainer with quantization-appropriate settings.\n\n=== Description ===\n\nThis uses the same `Trainer.train()` method as standard LoRA but with `TrainingArguments` configured for quantized model training. Key settings include gradient accumulation, paged optimizers, and mixed precision training.\n\n=== Usage ===\n\nConfigure TrainingArguments for QLoRA, then call `trainer.train()` as usual.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Library:''' `transformers.Trainer` (external)\n* '''Method:''' `train()`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef train(\n    self,\n    resume_from_checkpoint: Optional[Union[str, bool]] = None,\n    trial: Optional[Any] = None,\n    ignore_keys_for_eval: Optional[list[str]] = None,\n    **kwargs,\n) -> TrainOutput:\n    \"\"\"Execute training loop.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== QLoRA Training Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./qlora-output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,  # Effective batch = 16\n    learning_rate=2e-4,\n    fp16=True,  # Or bf16=True for bfloat16\n    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    gradient_checkpointing=True,  # Additional memory savings\n)\n\ntrainer = Trainer(\n    model=qlora_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== Monitoring for Stability ===\n<syntaxhighlight lang=\"python\">\n# If training shows NaN losses, try:\ntraining_args = TrainingArguments(\n    # ... other args ...\n    max_grad_norm=0.3,  # Gradient clipping\n    warmup_ratio=0.03,  # Learning rate warmup\n    lr_scheduler_type=\"cosine\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_QLoRA_Training_Execution]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "Fine Tuning",
        "Quantization",
        "Training"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "Transformers Trainer",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        },
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_QLoRA_Training_Execution"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_VBLoRAConfig",
      "page_title": "VBLoRAConfig",
      "page_type": "Implementation",
      "overview": "=== Description === VBLoRAConfig is the configuration class for Vector Bank Low-Rank Adaptation (VB-LoRA), an advanced parameter-efficient fine-tuning method. VB-LoRA uses a shared vector bank to construct low-rank adaptation matrices, significantly reducing the number of parameters that need to be saved compared to standard LoRA while maintaining similar or better performance. The method uses learnable logits to select vectors from a shared bank, enabling efficient parameter sharing across different layers and adapters. The configuration controls the vector bank size, rank, top-K selection, and various initialization and training parameters.",
      "content": "= VBLoRAConfig =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://huggingface.co/papers/2405.15179 VB-LoRA Paper]\n* Source: src/peft/tuners/vblora/config.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Low-Rank Adaptation]]\n* [[Vector Quantization]]\n* [[Model Compression]]\n\n== Overview ==\n\n=== Description ===\nVBLoRAConfig is the configuration class for Vector Bank Low-Rank Adaptation (VB-LoRA), an advanced parameter-efficient fine-tuning method. VB-LoRA uses a shared vector bank to construct low-rank adaptation matrices, significantly reducing the number of parameters that need to be saved compared to standard LoRA while maintaining similar or better performance.\n\nThe method uses learnable logits to select vectors from a shared bank, enabling efficient parameter sharing across different layers and adapters. The configuration controls the vector bank size, rank, top-K selection, and various initialization and training parameters.\n\n=== Usage ===\nVBLoRAConfig is used to configure VB-LoRA adapters for pre-trained models. It extends PeftConfig and provides VB-LoRA-specific parameters for controlling the vector bank, selection mechanism, and adaptation behavior. The configuration is passed to get_peft_model() to create a VB-LoRA adapted model.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/vblora/config.py\nLines: 25-197\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass VBLoRAConfig(PeftConfig):\n    \"\"\"\n    Configuration class for VBLoRAModel.\n\n    Key Parameters:\n        r: Rank of incremental matrices (default: 4)\n        num_vectors: Number of vectors in vector bank (default: 256)\n        vector_length: Length of each vector (default: 256)\n        topk: K value for top-K selection (default: 2)\n        save_only_topk_weights: Save only topk weights for inference (default: False)\n    \"\"\"\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.vblora.config import VBLoRAConfig\n# Or via the main PEFT interface\nfrom peft import VBLoRAConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Configuration Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| r || int || 4 || Rank of incremental matrices\n|-\n| num_vectors || int || 256 || Number of vectors in the vector bank\n|-\n| vector_length || int || 256 || Length of vectors; must divide hidden dimension\n|-\n| topk || int || 2 || K value for top-K selection\n|-\n| target_modules || Optional[Union[list[str], str]] || None || Modules to replace with VB-LoRA\n|-\n| exclude_modules || Optional[Union[list[str], str]] || None || Modules to exclude from VB-LoRA\n|-\n| save_only_topk_weights || bool || False || Save only topk weights (inference only)\n|-\n| vblora_dropout || float || 0.0 || Dropout probability for VB-LoRA layers\n|-\n| fan_in_fan_out || bool || False || True if layer stores weight as (fan_in, fan_out)\n|-\n| bias || str || \"none\" || Bias type: 'none', 'all', or 'vblora_only'\n|-\n| modules_to_save || Optional[list[str]] || None || Additional trainable modules\n|-\n| init_vector_bank_bound || float || 0.02 || Uniform distribution bound for vector bank init\n|-\n| init_logits_std || float || 0.1 || Standard deviation for logits initialization\n|-\n| layers_to_transform || Optional[Union[list[int], int]] || None || Specific layer indices to transform\n|-\n| layers_pattern || Optional[Union[list[str], str]] || None || Layer pattern name for layer selection\n|}\n\n=== Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| peft_type || PeftType || Set to PeftType.VBLORA in __post_init__\n|}\n\n== Usage Examples ==\n\n=== Basic VB-LoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import VBLoRAConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Create VB-LoRA configuration\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    topk=2,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\n# Apply VB-LoRA\nmodel = get_peft_model(base_model, config)\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== High-Performance Configuration ===\n<syntaxhighlight lang=\"python\">\n# Configuration optimized for performance\nconfig = VBLoRAConfig(\n    r=8,\n    num_vectors=512,  # Larger bank for bigger models\n    vector_length=128,\n    topk=2,  # K=2 provides best balance\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    vblora_dropout=0.1\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Inference-Optimized Configuration ===\n<syntaxhighlight lang=\"python\">\n# Save only top-k weights for minimal storage\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    topk=2,\n    save_only_topk_weights=True,  # Significantly reduces storage\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\nmodel = get_peft_model(base_model, config)\n# Note: Models with save_only_topk_weights=True cannot resume training\n</syntaxhighlight>\n\n=== Target All Linear Layers ===\n<syntaxhighlight lang=\"python\">\n# Apply to all linear layers except output\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    target_modules=\"all-linear\",\n    exclude_modules=[\"lm_head\"]\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Specific Layer Selection ===\n<syntaxhighlight lang=\"python\">\n# Apply VB-LoRA to specific layers only\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=[0, 1, 2, 3, 4],  # First 5 layers only\n    layers_pattern=\"layers\"\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Custom Initialization ===\n<syntaxhighlight lang=\"python\">\n# Custom initialization parameters\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    init_vector_bank_bound=0.01,  # Smaller init for stability\n    init_logits_std=0.05,  # Smaller std for logits\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== With Bias Training ===\n<syntaxhighlight lang=\"python\">\n# Train biases along with VB-LoRA\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    bias=\"all\",  # Train all biases\n    modules_to_save=[\"classifier\"]  # Also save classifier\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Regex Target Selection ===\n<syntaxhighlight lang=\"python\">\n# Use regex to select target modules\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    target_modules=r\".*decoder.*(self_attn|encoder_attn).*(q|v)_proj$\"\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Full Training Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import TrainingArguments, Trainer\n\n# VB-LoRA config\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    topk=2,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    vblora_dropout=0.1,\n    bias=\"none\"\n)\n\nmodel = get_peft_model(base_model, config)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./vblora_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n)\n\n# Train\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\ntrainer.train()\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_VBLoRALayer|VBLoRALayer]] - Layer implementation for VB-LoRA\n* [[huggingface_peft_VBLoRAModel|VBLoRAModel]] - Model class for VB-LoRA\n* [[huggingface_peft_LoraConfig|LoraConfig]] - Configuration for standard LoRA\n* [[Low-Rank Adaptation]]\n* [[Vector Quantization]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Model Configuration]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_VBLoRALayer",
      "page_title": "VBLoRALayer",
      "page_type": "Implementation",
      "overview": "=== Description === VBLoRALayer is the layer implementation for Vector Bank Low-Rank Adaptation (VB-LoRA). It constructs low-rank adaptation matrices by selecting and combining vectors from a shared vector bank using learnable logits. The layer maintains logits for both A and B matrices (vblora_logits_A and vblora_logits_B) that determine which vectors to select via top-K selection, and the selected vectors are combined to form the low-rank adaptation. This approach dramatically reduces parameter storage requirements while maintaining adaptation quality, as multiple layers share the same vector bank. The implementation supports both nn.Linear and Conv1D base layers.",
      "content": "= VBLoRALayer =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://huggingface.co/papers/2405.15179 VB-LoRA Paper]\n* Source: src/peft/tuners/vblora/layer.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Low-Rank Adaptation]]\n* [[Neural Network Layers]]\n* [[Vector Quantization]]\n\n== Overview ==\n\n=== Description ===\nVBLoRALayer is the layer implementation for Vector Bank Low-Rank Adaptation (VB-LoRA). It constructs low-rank adaptation matrices by selecting and combining vectors from a shared vector bank using learnable logits. The layer maintains logits for both A and B matrices (vblora_logits_A and vblora_logits_B) that determine which vectors to select via top-K selection, and the selected vectors are combined to form the low-rank adaptation.\n\nThis approach dramatically reduces parameter storage requirements while maintaining adaptation quality, as multiple layers share the same vector bank. The implementation supports both nn.Linear and Conv1D base layers.\n\n=== Usage ===\nVBLoRALayer is automatically instantiated when applying VBLoRAConfig to a model. It wraps base layers and adds VB-LoRA adaptation through vector selection and combination. The layer supports merging/unmerging with base weights and handles both training and inference modes, including a special save_only_topk_weights mode for minimal storage.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/vblora/layer.py\nLines: 27-252\n\n=== Class Signatures ===\n<syntaxhighlight lang=\"python\">\nclass VBLoRALayer(BaseTunerLayer):\n    \"\"\"Base class for VB-LoRA layers\"\"\"\n    adapter_layer_names = (\"vblora_logits_A\", \"vblora_logits_B\", \"vblora_vector_bank\")\n\nclass Linear(nn.Linear, VBLoRALayer):\n    \"\"\"VB-LoRA implementation for dense linear layers\"\"\"\n    def __init__(\n        self,\n        base_layer,\n        vblora_vector_bank,\n        adapter_name: str,\n        r: int,\n        num_vectors: int,\n        vector_length: int,\n        topk: int = 2,\n        vblora_dropout: float = 0.0,\n        init_logits_std: float = 0.01,\n        fan_in_fan_out: bool = False,\n        is_target_conv_1d_layer: bool = False,\n        **kwargs\n    ) -> None\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.vblora.layer import VBLoRALayer, Linear\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== VBLoRALayer Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| vblora_logits_A || nn.ParameterDict || Logits for selecting vectors for A matrix\n|-\n| vblora_logits_B || nn.ParameterDict || Logits for selecting vectors for B matrix\n|-\n| vblora_vector_bank || nn.ParameterDict || Shared vector bank across adapters\n|-\n| r || dict || Dictionary of rank values per adapter\n|-\n| topk || dict || Dictionary of top-K values per adapter\n|-\n| vblora_dropout || nn.ModuleDict || Dropout modules per adapter\n|-\n| merged_adapters || list || List of currently merged adapters\n|}\n\n=== Linear Layer Initialization Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| base_layer || nn.Module || (required) || Base linear or Conv1D layer\n|-\n| vblora_vector_bank || nn.ParameterDict || (required) || Shared vector bank\n|-\n| adapter_name || str || (required) || Name of the adapter\n|-\n| r || int || (required) || Rank of the adaptation\n|-\n| num_vectors || int || (required) || Number of vectors in the bank\n|-\n| vector_length || int || (required) || Length of each vector\n|-\n| topk || int || 2 || Number of vectors to select (K)\n|-\n| vblora_dropout || float || 0.0 || Dropout probability\n|-\n| init_logits_std || float || 0.01 || Std for logits initialization\n|-\n| fan_in_fan_out || bool || False || Weight storage format\n|-\n| is_target_conv_1d_layer || bool || False || True if base is Conv1D\n|}\n\n=== Key Methods ===\n{| class=\"wikitable\"\n! Method !! Parameters !! Returns !! Description\n|-\n| update_layer || adapter_name, vblora_vector_bank, r, topk, num_vectors, vector_length, vblora_dropout, init_logits_std, inference_mode || None || Update layer with new adapter\n|-\n| merge || safe_merge, adapter_names || None || Merge adapter weights into base\n|-\n| unmerge || None || None || Unmerge adapter weights\n|-\n| get_delta_weight || adapter || torch.Tensor || Compute delta weight for adapter\n|-\n| _get_lora_matrices || adapter, cast_to_fp32 || tuple[Tensor, Tensor] || Get A and B matrices from vector bank\n|-\n| _get_low_rank_matrix || logits, vblora_vector_bank, topk || torch.Tensor || Select and combine vectors using top-K\n|-\n| forward || x, *args, **kwargs || torch.Tensor || Forward pass with VB-LoRA\n|}\n\n== Usage Examples ==\n\n=== Basic VB-LoRA Model Usage ===\n<syntaxhighlight lang=\"python\">\nfrom peft import VBLoRAConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Create model with VB-LoRA\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    topk=2,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\nmodel = get_peft_model(base_model, config)\n\n# Forward pass\ninput_ids = torch.randint(0, 1000, (2, 10))\noutputs = model(input_ids)\n</syntaxhighlight>\n\n=== Accessing VB-LoRA Layers ===\n<syntaxhighlight lang=\"python\">\n# Access a VB-LoRA layer\nvblora_layer = model.base_model.model.decoder.layers[0].self_attn.q_proj\n\n# Inspect logits\nprint(f\"Logits A shape: {vblora_layer.vblora_logits_A['default'].shape}\")\nprint(f\"Logits B shape: {vblora_layer.vblora_logits_B['default'].shape}\")\n\n# Check vector bank\nprint(f\"Vector bank shape: {vblora_layer.vblora_vector_bank['default'].shape}\")\n</syntaxhighlight>\n\n=== Understanding Vector Selection ===\n<syntaxhighlight lang=\"python\">\n# Get the low-rank matrices\nadapter_name = \"default\"\nA, B = vblora_layer._get_lora_matrices(adapter_name)\n\nprint(f\"A matrix shape: {A.shape}\")  # (rank, in_features)\nprint(f\"B matrix shape: {B.shape}\")  # (out_features, rank)\n\n# Compute delta weight\ndelta = vblora_layer.get_delta_weight(adapter_name)\nprint(f\"Delta weight shape: {delta.shape}\")\n</syntaxhighlight>\n\n=== Inspecting Top-K Selection ===\n<syntaxhighlight lang=\"python\">\nimport torch.nn.functional as F\n\n# Get logits for A matrix\nlogits_A = vblora_layer.vblora_logits_A[\"default\"]\n\n# See which vectors are selected (top-k)\ntopk = vblora_layer.topk[\"default\"]\ntop_k_logits, indices = logits_A[0, 0].topk(topk)\nprint(f\"Selected vector indices: {indices}\")\nprint(f\"Selection weights: {F.softmax(top_k_logits, dim=-1)}\")\n</syntaxhighlight>\n\n=== Merging and Unmerging ===\n<syntaxhighlight lang=\"python\">\n# Merge VB-LoRA into base weights\nvblora_layer.merge()\n\nprint(f\"Is merged: {vblora_layer.merged}\")\n\n# Unmerge\nvblora_layer.unmerge()\n</syntaxhighlight>\n\n=== Safe Merging with NaN Check ===\n<syntaxhighlight lang=\"python\">\n# Merge with safety check\ntry:\n    vblora_layer.merge(safe_merge=True)\n    print(\"Merge successful, no NaNs detected\")\nexcept ValueError as e:\n    print(f\"Merge failed: {e}\")\n</syntaxhighlight>\n\n=== Training with Dropout ===\n<syntaxhighlight lang=\"python\">\n# Create model with dropout\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    vblora_dropout=0.1,  # Add dropout\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\nmodel = get_peft_model(base_model, config)\n\n# Dropout is active in training mode\nmodel.train()\noutput_train = model(input_ids)\n\n# Dropout is disabled in eval mode\nmodel.eval()\noutput_eval = model(input_ids)\n</syntaxhighlight>\n\n=== Inspecting Parameter Efficiency ===\n<syntaxhighlight lang=\"python\">\n# Check parameter counts\nvblora_layer = model.base_model.model.decoder.layers[0].self_attn.q_proj\n\nlogits_A_params = vblora_layer.vblora_logits_A[\"default\"].numel()\nlogits_B_params = vblora_layer.vblora_logits_B[\"default\"].numel()\nvector_bank_params = vblora_layer.vblora_vector_bank[\"default\"].numel()\n\ntotal_vblora_params = logits_A_params + logits_B_params\nprint(f\"Logits A params: {logits_A_params:,}\")\nprint(f\"Logits B params: {logits_B_params:,}\")\nprint(f\"Total logits: {total_vblora_params:,}\")\nprint(f\"Vector bank params: {vector_bank_params:,}\")\nprint(f\"Shared across all layers!\")\n</syntaxhighlight>\n\n=== Handling save_only_topk_weights Mode ===\n<syntaxhighlight lang=\"python\">\n# After training with save_only_topk_weights=False\nmodel.save_pretrained(\"./full_vblora\")\n\n# For inference-only deployment\nconfig_inference = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    save_only_topk_weights=True  # Minimal storage\n)\n\n# Note: Cannot resume training with this mode\n# The layer checks for infinity values during training\n</syntaxhighlight>\n\n=== Custom Forward Pass ===\n<syntaxhighlight lang=\"python\">\n# Direct layer usage\nx = torch.randn(2, 10, 768)  # (batch, seq, hidden)\n\n# With adapters\nvblora_layer.train()\noutput = vblora_layer(x)\n\n# Disable adapters\nvblora_layer.disable_adapters = True\noutput_base = vblora_layer(x)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_VBLoRAConfig|VBLoRAConfig]] - Configuration class for VB-LoRA\n* [[huggingface_peft_VBLoRAModel|VBLoRAModel]] - Model class for VB-LoRA\n* [[huggingface_peft_LoraLayer|LoraLayer]] - Standard LoRA layer implementation\n* [[Low-Rank Adaptation]]\n* [[Vector Quantization]]\n* [[Neural Network Layers]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Neural Network Layers]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_VBLoRAModel",
      "page_title": "VBLoRAModel",
      "page_type": "Implementation",
      "overview": "=== Description === VBLoRAModel creates a Vector Bank Low-Rank Adaptation (VB-LoRA) model from a pre-trained transformers model. It is the main model class that coordinates the injection of VB-LoRA adapters into target modules, manages the shared vector bank across all layers, and provides specialized methods for parameter counting that account for the unique storage characteristics of VB-LoRA. The model extends BaseTuner and handles initialization of the shared vector bank, creation and replacement of target modules with VB-LoRA layers, and provides methods for computing savable parameters based on whether save_only_topk_weights mode is enabled.",
      "content": "= VBLoRAModel =\n\n== Knowledge Sources ==\n* [https://github.com/huggingface/peft HuggingFace PEFT Repository]\n* [https://huggingface.co/papers/2405.15179 VB-LoRA Paper]\n* Source: src/peft/tuners/vblora/model.py\n\n== Domains ==\n* [[Natural Language Processing (NLP)]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Transfer Learning]]\n* [[Low-Rank Adaptation]]\n* [[Model Compression]]\n\n== Overview ==\n\n=== Description ===\nVBLoRAModel creates a Vector Bank Low-Rank Adaptation (VB-LoRA) model from a pre-trained transformers model. It is the main model class that coordinates the injection of VB-LoRA adapters into target modules, manages the shared vector bank across all layers, and provides specialized methods for parameter counting that account for the unique storage characteristics of VB-LoRA.\n\nThe model extends BaseTuner and handles initialization of the shared vector bank, creation and replacement of target modules with VB-LoRA layers, and provides methods for computing savable parameters based on whether save_only_topk_weights mode is enabled.\n\n=== Usage ===\nVBLoRAModel is typically instantiated through get_peft_model() by passing a VBLoRAConfig. It automatically initializes the shared vector bank, identifies target modules, wraps them with VB-LoRA layers, and manages the adaptation process. The model provides special handling for parameter-efficient storage when save_only_topk_weights is enabled.\n\n== Code Reference ==\n\n=== Source Location ===\nFile: src/peft/tuners/vblora/model.py\nLines: 29-210\n\n=== Class Signature ===\n<syntaxhighlight lang=\"python\">\nclass VBLoRAModel(BaseTuner):\n    \"\"\"\n    Creates VB-LoRA model from a pretrained transformers model.\n\n    Args:\n        model: The model to be adapted\n        config: The VBLoRAConfig configuration\n        adapter_name: The name of the adapter (defaults to \"default\")\n        low_cpu_mem_usage: Create empty weights on meta device\n\n    Returns:\n        torch.nn.Module: The VB-LoRA model\n    \"\"\"\n    prefix: str = \"vblora_\"\n    tuner_layer_cls = VBLoRALayer\n    target_module_mapping = TRANSFORMERS_MODELS_TO_VBLORA_TARGET_MODULES_MAPPING\n</syntaxhighlight>\n\n=== Import Statement ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.vblora.model import VBLoRAModel\n# Or via the main PEFT interface\nfrom peft import get_peft_model, VBLoRAConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Initialization Parameters ===\n{| class=\"wikitable\"\n! Parameter !! Type !! Description\n|-\n| model || PreTrainedModel || The base model to be adapted\n|-\n| config || VBLoRAConfig || Configuration for VB-LoRA adaptation\n|-\n| adapter_name || str || Name of the adapter (default: \"default\")\n|-\n| low_cpu_mem_usage || bool || Create empty adapter weights on meta device (default: False)\n|}\n\n=== Class Attributes ===\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| prefix || str || Prefix for VB-LoRA parameters (\"vblora_\")\n|-\n| tuner_layer_cls || Type || VBLoRALayer class reference\n|-\n| target_module_mapping || dict || Mapping of architectures to default target modules\n|-\n| vblora_vector_bank || nn.ParameterDict || Shared vector bank across all layers\n|}\n\n=== Key Methods ===\n{| class=\"wikitable\"\n! Method !! Parameters !! Returns !! Description\n|-\n| _init_vblora_vector_bank || config, adapter_name || None || Initialize shared vector bank\n|-\n| _pre_injection_hook || model, config, adapter_name || None || Setup before adapter injection\n|-\n| _create_and_replace || vblora_config, adapter_name, target, target_name, parent, current_key || None || Create and replace module with VB-LoRA layer\n|-\n| _create_new_module || vblora_config, vblora_vector_bank, adapter_name, target, kwargs || Linear || Create new VB-LoRA module\n|-\n| get_nb_savable_parameters || adapter || tuple[int, int] || Get number of savable VB-LoRA and other parameters\n|-\n| print_savable_parameters || None || None || Print savable parameter statistics\n|}\n\n== Usage Examples ==\n\n=== Basic VB-LoRA Model Creation ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import VBLoRAConfig, get_peft_model\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Create VB-LoRA configuration\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    topk=2,\n    target_modules=[\"fc1\", \"fc2\", \"q_proj\", \"v_proj\"]\n)\n\n# Create VB-LoRA model\nmodel = get_peft_model(base_model, config)\n\n# Print trainable and savable parameters\nmodel.print_trainable_parameters()\nmodel.print_savable_parameters()\n</syntaxhighlight>\n\n=== Full Training Example ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import VBLoRAConfig, get_peft_model\n\n# Setup\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    topk=2,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    vblora_dropout=0.1\n)\n\nmodel = get_peft_model(base_model, config)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./vblora_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    learning_rate=1e-4,\n)\n\n# Train\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\ntrainer.train()\n</syntaxhighlight>\n\n=== Save for Inference Only ===\n<syntaxhighlight lang=\"python\">\n# Train with full weights\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    topk=2,\n    save_only_topk_weights=True,  # Minimal storage\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\nmodel = get_peft_model(base_model, config)\n\n# Train (save_only_topk_weights applies at save time)\ntrainer.train()\n\n# Save - only top-k weights are saved\nmodel.save_pretrained(\"./vblora_inference_only\")\n\n# Load for inference\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nmodel = PeftModel.from_pretrained(base_model, \"./vblora_inference_only\")\nmodel.eval()\n</syntaxhighlight>\n\n=== Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\n# First adapter\nconfig1 = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\nmodel = get_peft_model(base_model, config1, adapter_name=\"task1\")\n\n# Second adapter with different parameters\nconfig2 = VBLoRAConfig(\n    r=8,\n    num_vectors=512,\n    vector_length=128,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\nmodel.add_adapter(\"task2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"task1\")\noutput1 = model(input_ids)\n\nmodel.set_adapter(\"task2\")\noutput2 = model(input_ids)\n</syntaxhighlight>\n\n=== Checking Parameter Efficiency ===\n<syntaxhighlight lang=\"python\">\n# Get parameter counts\nvblora_params, other_params = model.get_nb_savable_parameters()\n\nprint(f\"VB-LoRA params (float32-equivalent): {vblora_params:,}\")\nprint(f\"Other trainable params: {other_params:,}\")\nprint(f\"Total params to-be-saved: {vblora_params + other_params:,}\")\n\n# Compare with base model\nbase_params = sum(p.numel() for p in base_model.parameters())\nprint(f\"Compression ratio: {base_params / (vblora_params + other_params):.2f}x\")\n</syntaxhighlight>\n\n=== Target Specific Layers ===\n<syntaxhighlight lang=\"python\">\n# Apply VB-LoRA to specific layers only\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    layers_to_transform=[0, 1, 2, 3],  # First 4 layers\n    layers_pattern=\"layers\"\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Merge and Save Full Model ===\n<syntaxhighlight lang=\"python\">\n# Merge VB-LoRA into base model\nmodel = model.merge_and_unload()\n\n# Save as standard model\nmodel.save_pretrained(\"./merged_vblora_model\")\ntokenizer.save_pretrained(\"./merged_vblora_model\")\n</syntaxhighlight>\n\n=== Low CPU Memory Usage ===\n<syntaxhighlight lang=\"python\">\n# Create model with low memory usage\nmodel = get_peft_model(\n    base_model,\n    config,\n    adapter_name=\"default\",\n    low_cpu_mem_usage=True  # Create empty weights on meta device\n)\n</syntaxhighlight>\n\n=== Inference Example ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import PeftModel\n\n# Load model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\nmodel = PeftModel.from_pretrained(base_model, \"./vblora_adapter\")\nmodel.eval()\n\n# Generate\nwith torch.no_grad():\n    input_ids = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids\n    outputs = model.generate(\n        input_ids,\n        max_length=50,\n        num_return_sequences=1\n    )\n\nprint(tokenizer.decode(outputs[0]))\n</syntaxhighlight>\n\n=== Custom Target Modules with Regex ===\n<syntaxhighlight lang=\"python\">\n# Use regex for complex targeting\nconfig = VBLoRAConfig(\n    r=4,\n    num_vectors=256,\n    vector_length=256,\n    target_modules=r\".*decoder.*(self_attn|encoder_attn).*(q|k|v)_proj$\",\n    exclude_modules=[\"lm_head\"]\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Comparing Storage Modes ===\n<syntaxhighlight lang=\"python\">\n# Mode 1: Full weights (can resume training)\nconfig_full = VBLoRAConfig(\n    r=4, num_vectors=256, vector_length=256,\n    save_only_topk_weights=False\n)\nmodel_full = get_peft_model(base_model, config_full)\nfull_params, _ = model_full.get_nb_savable_parameters()\n\n# Mode 2: Top-k only (inference only)\nconfig_topk = VBLoRAConfig(\n    r=4, num_vectors=256, vector_length=256,\n    save_only_topk_weights=True\n)\nmodel_topk = get_peft_model(base_model, config_topk)\ntopk_params, _ = model_topk.get_nb_savable_parameters()\n\nprint(f\"Full mode: {full_params:,} params\")\nprint(f\"Top-k mode: {topk_params:,} params\")\nprint(f\"Storage reduction: {full_params / topk_params:.2f}x\")\n</syntaxhighlight>\n\n== Related Pages ==\n* [[huggingface_peft_VBLoRAConfig|VBLoRAConfig]] - Configuration class for VB-LoRA\n* [[huggingface_peft_VBLoRALayer|VBLoRALayer]] - Layer implementation for VB-LoRA\n* [[huggingface_peft_LoraModel|LoraModel]] - Model class for standard LoRA\n* [[Low-Rank Adaptation]]\n* [[Parameter-Efficient Fine-Tuning (PEFT)]]\n* [[Model Compression]]\n\n[[Category:Machine Learning]]\n[[Category:PEFT]]\n[[Category:Model Adaptation]]\n[[Category:HuggingFace]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_VeraConfig",
      "page_title": "VeraConfig",
      "page_type": "Implementation",
      "overview": "=== Description === VeraConfig is the configuration class for storing parameters of a VeRA (Vector-based Random Matrix Adaptation) model. VeRA is a parameter-efficient fine-tuning technique that uses far fewer parameters than LoRA by employing shared random projection matrices (vera_A and vera_B) across all layers, with only small trainable scaling vectors (lambda_b and lambda_d) per layer. The key innovation of VeRA is using higher rank values (default 256) than LoRA while maintaining parameter efficiency through shared random projections initialized with a PRNG key. This allows VeRA to achieve competitive performance with significantly fewer trainable parameters.",
      "content": "= VeraConfig =\n\n== Knowledge Sources ==\n\n* '''Repository''': [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Paper''': [https://huggingface.co/papers/2310.11454 VeRA: Vector-based Random Matrix Adaptation]\n* '''Type''': Configuration Class\n* '''Module''': peft.tuners.vera.config\n\n== Domains ==\n\n[[Category:Natural_Language_Processing]]\n[[Category:Parameter_Efficient_Fine_Tuning]]\n[[Category:Vector_Adaptation]]\n[[Category:Low_Rank_Adaptation]]\n[[Category:Configuration]]\n\n== Overview ==\n\n=== Description ===\n\nVeraConfig is the configuration class for storing parameters of a VeRA (Vector-based Random Matrix Adaptation) model. VeRA is a parameter-efficient fine-tuning technique that uses far fewer parameters than LoRA by employing shared random projection matrices (vera_A and vera_B) across all layers, with only small trainable scaling vectors (lambda_b and lambda_d) per layer.\n\nThe key innovation of VeRA is using higher rank values (default 256) than LoRA while maintaining parameter efficiency through shared random projections initialized with a PRNG key. This allows VeRA to achieve competitive performance with significantly fewer trainable parameters.\n\n=== Usage ===\n\nVeraConfig is used to initialize a VeRA model through the PEFT library. It controls all aspects of the VeRA adaptation including rank, target modules, projection initialization, dropout, and layer-specific transformations.\n\n== Code Reference ==\n\n=== Source Location ===\n\n<code>/tmp/praxium_repo_zyf9ywdz/src/peft/tuners/vera/config.py</code>\n\n=== Signature ===\n\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass VeraConfig(PeftConfig):\n    r: int = field(default=256, metadata={\"help\": \"Vera attention dimension\"})\n    target_modules: Optional[Union[list[str], str]] = field(default=None)\n    projection_prng_key: int = field(default=0)\n    save_projection: bool = field(default=True)\n    vera_dropout: float = field(default=0.0)\n    d_initial: float = field(default=0.1)\n    fan_in_fan_out: bool = field(default=False)\n    bias: str = field(default=\"none\")\n    modules_to_save: Optional[list[str]] = field(default=None)\n    init_weights: bool = field(default=True)\n    layers_to_transform: Optional[Union[list[int], int]] = field(default=None)\n    layers_pattern: Optional[Union[list[str], str]] = field(default=None)\n</syntaxhighlight>\n\n=== Import ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Configuration Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| r || int || 256 || VeRA parameter dimension (rank). Higher values than LoRA ranks recommended\n|-\n| target_modules || Union[List[str], str] || None || Names or regex of modules to apply VeRA to (only linear layers)\n|-\n| projection_prng_key || int || 0 || PRNG init key for vera_A and vera_B initialization\n|-\n| save_projection || bool || True || Whether to save vera_A/vera_B projections in state dict\n|-\n| vera_dropout || float || 0.0 || Dropout probability for VeRA layers\n|-\n| d_initial || float || 0.1 || Initial value for vera_lambda_d vector (small values <=0.1 recommended)\n|-\n| fan_in_fan_out || bool || False || True if layer stores weights as (fan_in, fan_out) like Conv1D\n|-\n| bias || str || \"none\" || Bias type: 'none', 'all', or 'vera_only'\n|-\n| modules_to_save || List[str] || None || Modules besides VeRA layers to set as trainable\n|-\n| init_weights || bool || True || Whether to initialize weights with default initialization\n|-\n| layers_to_transform || Union[List[int], int] || None || Specific layer indexes to apply VeRA transformations\n|-\n| layers_pattern || Union[List[str], str] || None || Layer pattern name for nn.ModuleList (e.g., 'layers', 'h')\n|}\n\n=== Validation Rules ===\n\n* If <code>layers_pattern</code> is specified, <code>layers_to_transform</code> must also be specified\n* If <code>save_projection</code> is False, a warning is issued about potential restoration issues\n* <code>target_modules</code> is converted to a set if provided as a list\n* <code>peft_type</code> is automatically set to <code>PeftType.VERA</code>\n\n== Usage Examples ==\n\n=== Basic VeRA Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Create VeRA config with default settings\nconfig = VeraConfig(\n    r=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    vera_dropout=0.1,\n    d_initial=0.1\n)\n\n# Apply VeRA to model\npeft_model = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Advanced Configuration with Layer Selection ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig, get_peft_model\n\n# Apply VeRA only to specific layers\nconfig = VeraConfig(\n    r=512,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    vera_dropout=0.05,\n    d_initial=0.05,\n    layers_to_transform=[0, 1, 2, 3],  # First 4 layers only\n    layers_pattern=\"layers\",\n    save_projection=True,\n    projection_prng_key=42\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Configuration for GPT-2 Style Models ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig\n\n# GPT-2 uses Conv1D which stores weights as (fan_in, fan_out)\nconfig = VeraConfig(\n    r=256,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    fan_in_fan_out=True,  # Important for Conv1D layers\n    vera_dropout=0.1,\n    bias=\"vera_only\",\n    modules_to_save=[\"classifier\"]\n)\n</syntaxhighlight>\n\n=== Memory-Efficient Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig\n\n# Minimize checkpoint size by not saving projections\nconfig = VeraConfig(\n    r=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    save_projection=False,  # Reduces checkpoint size\n    projection_prng_key=12345,  # Must be consistent for loading\n    vera_dropout=0.0,\n    d_initial=0.1\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n* [[huggingface_peft_VeraModel|VeraModel]] - Model class that uses this configuration\n* [[huggingface_peft_VeraLayer|VeraLayer]] - Layer implementation for VeRA\n* [[huggingface_peft_LoraConfig|LoraConfig]] - Similar configuration for LoRA\n* [[huggingface_peft_PeftConfig|PeftConfig]] - Base configuration class\n* [[Parameter_Efficient_Fine_Tuning|PEFT Overview]]\n* [[Low_Rank_Adaptation|LoRA and Variants]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_VeraLayer",
      "page_title": "huggingface peft VeraLayer",
      "page_type": "Implementation",
      "overview": "Vector-based Random matrix Adaptation layer that uses shared frozen random matrices with per-layer trainable scaling vectors for extreme parameter efficiency.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|VeRA|https://arxiv.org/abs/2310.11454]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Vector_Adaptation]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nVector-based Random matrix Adaptation layer that uses shared frozen random matrices with per-layer trainable scaling vectors for extreme parameter efficiency.\n\n=== Description ===\n\nVeraLayer implements VeRA, which shares a single pair of random matrices (A and B) across all adapted layers. Only two small vectors per layer are trained: lambda_d (scales the rank dimension) and lambda_b (scales the output dimension). The forward pass computes: lambda_b * (B @ (lambda_d * (A @ x))). This achieves 10x fewer parameters than LoRA while maintaining similar performance, as the random matrices capture sufficient expressivity.\n\n=== Usage ===\n\nUse VeRA when you need extreme parameter efficiency, especially for deploying many adapters. Since the A/B matrices are shared and frozen, VeRA adapters are much smaller than LoRA. VeRA works best when the shared matrices are initialized with sufficient rank to cover all target layers' dimensions.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/vera/layer.py src/peft/tuners/vera/layer.py]\n* '''Lines:''' 1-292\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass VeraLayer(BaseTunerLayer):\n    \"\"\"\n    Vector-based Random matrix Adaptation layer.\n\n    Attributes:\n        vera_lambda_b: ParameterDict of output scaling vectors\n        vera_lambda_d: ParameterDict of rank scaling vectors\n        vera_A: BufferDict reference to shared frozen A matrices\n        vera_B: BufferDict reference to shared frozen B matrices\n        vera_dropout: ModuleDict of dropout layers\n    \"\"\"\n    adapter_layer_names = (\"vera_lambda_b\", \"vera_lambda_d\")\n    other_param_names = (\"vera_A\", \"vera_B\")\n\n    def update_layer(\n        self,\n        adapter_name,\n        vera_A: BufferDict,\n        vera_B: BufferDict,\n        r,\n        vera_dropout,\n        init_weights,\n        d_initial: float = 0.1,\n        inference_mode: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Create VeRA scaling vectors.\"\"\"\n\n    def get_delta_weight(self, adapter) -> torch.Tensor:\n        \"\"\"Compute delta weight: (lambda_b * B) @ (lambda_d * A).\"\"\"\n\nclass Linear(nn.Linear, VeraLayer):\n    \"\"\"VeRA implemented in Linear layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.vera import VeraLayer, VeraConfig, VeraModel\nfrom peft import VeraConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || nn.Module || Yes || The pretrained Linear layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| vera_A || BufferDict || Yes || Shared frozen A matrices\n|-\n| vera_B || BufferDict || Yes || Shared frozen B matrices\n|-\n| r || int || Yes || Rank of the shared matrices\n|-\n| vera_dropout || float || No || Dropout probability\n|-\n| d_initial || float || No || Initial value for lambda_d (default: 0.1)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Base output + scaled VeRA adaptation\n|-\n| get_delta_weight() || torch.Tensor || (lambda_b * B) @ (lambda_d * A)\n|}\n\n== Usage Examples ==\n\n=== Basic VeRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# VeRA shares random matrices across layers\nconfig = VeraConfig(\n    r=256,                    # Shared rank (can be larger since matrices are shared)\n    target_modules=[\"q_proj\", \"v_proj\"],\n    vera_dropout=0.0,\n    d_initial=0.1,            # Initial scaling factor\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n# Much fewer parameters than LoRA with same rank\n</syntaxhighlight>\n\n=== VeRA with Dropout ===\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig, get_peft_model\n\nconfig = VeraConfig(\n    r=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    vera_dropout=0.1,\n    d_initial=0.1,\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== Comparing VeRA vs LoRA Parameters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig, LoraConfig, get_peft_model\n\n# LoRA parameters: 2 * r * d per layer\nlora_config = LoraConfig(r=16, target_modules=[\"q_proj\", \"v_proj\"])\n\n# VeRA parameters: (in_features + rank) per layer + shared matrices\nvera_config = VeraConfig(r=256, target_modules=[\"q_proj\", \"v_proj\"])\n\n# VeRA achieves similar quality with ~10x fewer trainable parameters\n# because A and B matrices are shared across all layers\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Vector Adaptation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "VeRA",
          "url": "https://arxiv.org/abs/2310.11454"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_VeraModel",
      "page_title": "VeraModel",
      "page_type": "Implementation",
      "overview": "=== Description === VeraModel creates Vector-based Random Matrix Adaptation (VeRA) models from pretrained transformers. VeRA is a parameter-efficient fine-tuning technique that achieves significant parameter reduction compared to LoRA by using shared random projection matrices (vera_A and vera_B) across all adapted layers. The key innovation is that instead of having separate low-rank matrices per layer like LoRA, VeRA uses: * Shared frozen random matrices vera_A and vera_B (initialized with Kaiming uniform using a PRNG seed) * Small trainable scaling vectors lambda_b and lambda_d per layer This allows VeRA to use higher rank values (e.g., 256) while maintaining fewer trainable parameters than LoRA with lower ranks.",
      "content": "= VeraModel =\n\n== Knowledge Sources ==\n\n* '''Repository''': [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Paper''': [https://huggingface.co/papers/2310.11454 VeRA: Vector-based Random Matrix Adaptation]\n* '''Type''': Model Class\n* '''Module''': peft.tuners.vera.model\n\n== Domains ==\n\n[[Category:Natural_Language_Processing]]\n[[Category:Parameter_Efficient_Fine_Tuning]]\n[[Category:Vector_Adaptation]]\n[[Category:Low_Rank_Adaptation]]\n[[Category:Model_Architecture]]\n\n== Overview ==\n\n=== Description ===\n\nVeraModel creates Vector-based Random Matrix Adaptation (VeRA) models from pretrained transformers. VeRA is a parameter-efficient fine-tuning technique that achieves significant parameter reduction compared to LoRA by using shared random projection matrices (vera_A and vera_B) across all adapted layers.\n\nThe key innovation is that instead of having separate low-rank matrices per layer like LoRA, VeRA uses:\n* Shared frozen random matrices vera_A and vera_B (initialized with Kaiming uniform using a PRNG seed)\n* Small trainable scaling vectors lambda_b and lambda_d per layer\n\nThis allows VeRA to use higher rank values (e.g., 256) while maintaining fewer trainable parameters than LoRA with lower ranks.\n\n=== Usage ===\n\nVeraModel extends BaseTuner and handles the injection of VeRA layers into pretrained models. It manages the initialization and sharing of projection matrices, creates appropriate layer types (including quantized variants), and ensures consistent configuration across adapters.\n\n== Code Reference ==\n\n=== Source Location ===\n\n<code>/tmp/praxium_repo_zyf9ywdz/src/peft/tuners/vera/model.py</code>\n\n=== Signature ===\n\n<syntaxhighlight lang=\"python\">\nclass VeraModel(BaseTuner):\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        config: VeraConfig,\n        adapter_name: str = \"default\",\n        low_cpu_mem_usage: bool = False\n    )\n</syntaxhighlight>\n\n=== Import ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraModel, VeraConfig\n# Or use the high-level API\nfrom peft import get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Initialization Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| model || PreTrainedModel || Required || The pretrained transformers model to adapt\n|-\n| config || VeraConfig || Required || VeRA configuration object\n|-\n| adapter_name || str || \"default\" || Name for the adapter\n|-\n| low_cpu_mem_usage || bool || False || Create empty adapter weights on meta device for faster loading\n|}\n\n=== Key Attributes ===\n\n{| class=\"wikitable\"\n! Attribute !! Type !! Description\n|-\n| prefix || str || \"vera_lambda_\" - prefix for VeRA parameters\n|-\n| tuner_layer_cls || class || VeraLayer - the layer class used for VeRA\n|-\n| vera_A || BufferDict || Shared projection matrix A (r \u00d7 in_features)\n|-\n| vera_B || BufferDict || Shared projection matrix B (out_features \u00d7 r)\n|-\n| target_module_mapping || dict || Mapping of model types to default target modules\n|}\n\n=== Return Values ===\n\n{| class=\"wikitable\"\n! Method !! Return Type !! Description\n|-\n| __init__ || VeraModel || Initialized VeRA model ready for training\n|-\n| _find_dim || tuple[int, int] || Largest (out_features, in_features) across target layers\n|-\n| _init_vera_A_vera_B || None || Initializes shared projection matrices\n|}\n\n== Usage Examples ==\n\n=== Basic VeRA Model Creation ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import VeraConfig, get_peft_model\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n\n# Create VeRA configuration\nconfig = VeraConfig(\n    r=128,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    vera_dropout=0.1\n)\n\n# Create VeRA model\nmodel = get_peft_model(base_model, config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: ~200K || all params: 125M || trainable%: 0.16%\n</syntaxhighlight>\n\n=== Using with Quantized Models ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import VeraConfig, get_peft_model\nimport torch\n\n# Load 4-bit quantized model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Apply VeRA to quantized model\nconfig = VeraConfig(\n    r=256,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    vera_dropout=0.05,\n    d_initial=0.1\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Multiple Adapter Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig, get_peft_model\n\n# Create model with first adapter\nconfig1 = VeraConfig(\n    r=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    projection_prng_key=42  # Must be same for all adapters\n)\nmodel = get_peft_model(base_model, config1, adapter_name=\"task1\")\n\n# Add second adapter (shares vera_A and vera_B)\nconfig2 = VeraConfig(\n    r=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    projection_prng_key=42,  # Same key required\n    vera_dropout=0.2\n)\nmodel.add_adapter(\"task2\", config2)\n\n# Switch between adapters\nmodel.set_adapter(\"task1\")\n# ... train or infer\nmodel.set_adapter(\"task2\")\n# ... train or infer\n</syntaxhighlight>\n\n=== Layer-Specific Adaptation ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig, get_peft_model\n\n# Apply VeRA only to specific transformer layers\nconfig = VeraConfig(\n    r=512,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    layers_to_transform=[0, 1, 2],  # Only first 3 layers\n    layers_pattern=\"layers\",  # Depends on model architecture\n    vera_dropout=0.1,\n    d_initial=0.05,\n    save_projection=True\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Training and Saving ===\n\n<syntaxhighlight lang=\"python\">\nfrom transformers import Trainer, TrainingArguments\nfrom peft import VeraConfig, get_peft_model\n\n# Create and train VeRA model\nconfig = VeraConfig(r=256, target_modules=[\"q_proj\", \"v_proj\"])\nmodel = get_peft_model(base_model, config)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./vera_model\",\n    per_device_train_batch_size=4,\n    num_train_epochs=3,\n    save_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset\n)\n\ntrainer.train()\n\n# Save VeRA adapter weights\nmodel.save_pretrained(\"./vera_adapter\")\n\n# Load later\nfrom peft import PeftModel\nloaded_model = PeftModel.from_pretrained(base_model, \"./vera_adapter\")\n</syntaxhighlight>\n\n=== Custom Initialization ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import VeraConfig, get_peft_model\n\n# Use specific PRNG key for reproducible initialization\nconfig = VeraConfig(\n    r=256,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    projection_prng_key=12345,  # Deterministic initialization\n    d_initial=0.1,  # Small initial scaling\n    save_projection=True  # Save vera_A and vera_B in checkpoint\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Kaiming Initialization ===\n\nVeRA uses a custom Kaiming uniform initialization for the shared projection matrices:\n\n<syntaxhighlight lang=\"python\">\ndef _kaiming_init(tensor_or_shape, generator):\n    \"\"\"\n    Kaiming Uniform with PRNG generator for deterministic initialization.\n    fan = in_features, gain = sqrt(2), bound = sqrt(3) * gain / sqrt(fan)\n    \"\"\"\n    # Returns tensor uniformly distributed in [-bound, bound]\n</syntaxhighlight>\n\n=== Dimension Finding ===\n\nThe model automatically finds the largest dimensions across all target layers to size the shared projection matrices:\n\n<syntaxhighlight lang=\"python\">\nlargest_out_dim, largest_in_dim = model._find_dim(config)\nvera_A = [r \u00d7 largest_in_dim]\nvera_B = [largest_out_dim \u00d7 r]\n</syntaxhighlight>\n\n=== Supported Layer Types ===\n\n* <code>torch.nn.Linear</code> - Standard linear layers\n* <code>Conv1D</code> - GPT-2 style convolution layers\n* <code>bnb.nn.Linear8bitLt</code> - 8-bit quantized layers\n* <code>bnb.nn.Linear4bit</code> - 4-bit quantized layers\n\n== Related Pages ==\n\n* [[huggingface_peft_VeraConfig|VeraConfig]] - Configuration class for VeRA\n* [[huggingface_peft_VeraLayer|VeraLayer]] - Layer implementation\n* [[huggingface_peft_VeraQuantized|VeraQuantized]] - Quantized VeRA variants\n* [[huggingface_peft_LoraModel|LoraModel]] - Similar LoRA model class\n* [[huggingface_peft_BaseTuner|BaseTuner]] - Base class for PEFT tuners\n* [[Parameter_Efficient_Fine_Tuning|PEFT Overview]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_VeraQuantized",
      "page_title": "huggingface peft VeraQuantized",
      "page_type": "Implementation",
      "overview": "Quantized VeRA layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling vector-based random matrix adaptation on quantized models.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|bitsandbytes|https://github.com/bitsandbytes-foundation/bitsandbytes]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nQuantized VeRA layer implementations for 8-bit and 4-bit bitsandbytes linear layers, enabling vector-based random matrix adaptation on quantized models.\n\n=== Description ===\n\nLinear8bitLt and Linear4bit implement VeRA for bitsandbytes quantized layers. The layers use shared frozen random matrices (vera_A, vera_B) with per-layer trainable scaling vectors (lambda_d, lambda_b). The forward pass computes: result + lambda_b * linear(lambda_d * linear(x, sliced_A), sliced_B). During merge, the delta weight is added to dequantized weights then requantized.\n\n=== Usage ===\n\nUse VeRA quantized layers for extreme parameter efficiency on quantized models. VeRA adapters are much smaller than LoRA since only scaling vectors are trained. Automatic dispatch creates these layers when base layers are bitsandbytes Linear8bitLt or Linear4bit.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/vera/bnb.py src/peft/tuners/vera/bnb.py]\n* '''Lines:''' 1-412\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass Linear8bitLt(torch.nn.Module, VeraLayer):\n    \"\"\"VeRA for 8-bit quantized layers.\"\"\"\n    def __init__(\n        self,\n        base_layer: torch.nn.Module,\n        adapter_name: str,\n        vera_A,\n        vera_B,\n        r: int = 0,\n        vera_dropout: float = 0.0,\n        d_initial: float = 0.1,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initialize VeRA for 8-bit layer.\"\"\"\n\n    def get_delta_weight(self, adapter) -> torch.Tensor:\n        \"\"\"Compute (lambda_b * B) @ (lambda_d * A).\"\"\"\n\n    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        \"\"\"Apply VeRA adaptation to quantized layer.\"\"\"\n\nclass Linear4bit(torch.nn.Module, VeraLayer):\n    \"\"\"VeRA for 4-bit quantized layers.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.vera.bnb import Linear8bitLt, Linear4bit\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| base_layer || bnb.nn.Linear8bitLt/Linear4bit || Yes || Quantized base layer\n|-\n| adapter_name || str || Yes || Name for the adapter\n|-\n| vera_A || BufferDict || Yes || Shared frozen A matrices\n|-\n| vera_B || BufferDict || Yes || Shared frozen B matrices\n|-\n| r || int || Yes || Rank of shared matrices\n|-\n| d_initial || float || No || Initial value for lambda_d\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Base output + VeRA adaptation\n|-\n| get_delta_weight() || torch.Tensor || (lambda_b * B) @ (lambda_d * A)\n|}\n\n== Usage Examples ==\n\n=== VeRA with 4-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import VeraConfig, get_peft_model\n\n# Load model in 4-bit\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n)\n\n# Apply VeRA - extremely parameter efficient\nconfig = VeraConfig(\n    r=256,                      # Shared rank\n    target_modules=[\"q_proj\", \"v_proj\"],\n    d_initial=0.1,\n)\n\nmodel = get_peft_model(model, config)\n# Automatically uses Linear4bit class\n</syntaxhighlight>\n\n=== VeRA with 8-bit Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import VeraConfig, get_peft_model\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    load_in_8bit=True,\n)\n\nconfig = VeraConfig(\n    r=128,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    vera_dropout=0.1,\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== VeRA Forward Computation ===\n<syntaxhighlight lang=\"python\">\n# VeRA forward on quantized layers:\n# 1. Slice shared matrices to layer dimensions\n# 2. result = base_layer(x)\n# 3. adapter_output = lambda_b * linear(lambda_d * linear(x, sliced_A), sliced_B)\n# 4. return result + adapter_output\n\n# Only lambda_b and lambda_d are trained per layer\n# A and B matrices are shared and frozen\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "bitsandbytes",
          "url": "https://github.com/bitsandbytes-foundation/bitsandbytes"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_XLoraClassifier",
      "page_title": "XLoraClassifier",
      "page_type": "Implementation",
      "overview": "=== Description === XLoraClassifier is a neural network classifier that dynamically selects and weights multiple LoRA adapters for the X-LoRA (Mixture of LoRA Experts) technique. It takes hidden states from the base model and predicts scaling values (weights) for each LoRA adapter at each layer and position in the sequence. The classifier acts as a gating mechanism in a mixture-of-experts setup, where each LoRA adapter is an expert. It can operate in two modes: * '''Dense mode''': Uses softmax to produce normalized weights across all adapters * '''Sparse mode''': Selects top-k adapters and optionally applies softmax over them The classifier supports configurable depth (number of hidden layers), size, dropout, and temperature-scaled softmax for controlling prediction sharpness.",
      "content": "= XLoraClassifier =\n\n== Knowledge Sources ==\n\n* '''Repository''': [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Paper''': X-LoRA: Mixture of Low-Rank Adapters\n* '''Type''': Classifier Module\n* '''Module''': peft.tuners.xlora.classifier\n\n== Domains ==\n\n[[Category:Natural_Language_Processing]]\n[[Category:Parameter_Efficient_Fine_Tuning]]\n[[Category:Mixture_of_Experts]]\n[[Category:Low_Rank_Adaptation]]\n[[Category:Neural_Networks]]\n\n== Overview ==\n\n=== Description ===\n\nXLoraClassifier is a neural network classifier that dynamically selects and weights multiple LoRA adapters for the X-LoRA (Mixture of LoRA Experts) technique. It takes hidden states from the base model and predicts scaling values (weights) for each LoRA adapter at each layer and position in the sequence.\n\nThe classifier acts as a gating mechanism in a mixture-of-experts setup, where each LoRA adapter is an expert. It can operate in two modes:\n* '''Dense mode''': Uses softmax to produce normalized weights across all adapters\n* '''Sparse mode''': Selects top-k adapters and optionally applies softmax over them\n\nThe classifier supports configurable depth (number of hidden layers), size, dropout, and temperature-scaled softmax for controlling prediction sharpness.\n\n=== Usage ===\n\nXLoraClassifier is used internally by XLoraModel to determine adapter mixing weights dynamically based on input. It performs two forward passes: a \"scaling pass\" with dummy scalings to get logits, then a \"real pass\" with predicted scalings.\n\n== Code Reference ==\n\n=== Source Location ===\n\n<code>/tmp/praxium_repo_zyf9ywdz/src/peft/tuners/xlora/classifier.py</code>\n\n=== Signature ===\n\n<syntaxhighlight lang=\"python\">\nclass XLoraClassifier(nn.Module):\n    def __init__(\n        self,\n        model: nn.Module,\n        config: XLoraConfig,\n        n_classes: int,\n        n_layers: int,\n        device: torch.device\n    )\n\n    def forward(\n        self,\n        result,\n        input_ids: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        *args,\n        **kwargs\n    ) -> torch.Tensor\n</syntaxhighlight>\n\n=== Import ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.xlora.classifier import XLoraClassifier\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Initialization Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| model || nn.Module || Required || PeftModel containing LoRA adapters\n|-\n| config || XLoraConfig || Required || Configuration for X-LoRA\n|-\n| n_classes || int || Required || Number of LoRA adapters (experts)\n|-\n| n_layers || int || Required || Number of LoRA adapter layers\n|-\n| device || torch.device || Required || Device to place classifier on\n|}\n\n=== Forward Method Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| result || ModelOutput || Required || Model output containing hidden_states\n|-\n| input_ids || torch.LongTensor || None || Input token IDs (batch_size, seq_len)\n|-\n| inputs_embeds || torch.FloatTensor || None || Input embeddings (batch_size, seq_len, hidden_size)\n|}\n\n=== Return Values ===\n\n{| class=\"wikitable\"\n! Method !! Return Type !! Shape !! Description\n|-\n| forward || torch.Tensor || (batch_size, seq_len, n_layers, n_classes) || Scaling weights for each adapter\n|-\n| make_dummy_scalings || torch.Tensor || (batch_size, seq_len, n_layers, n_classes) || Dummy scalings for scaling pass\n|}\n\n=== Output Tensor Details ===\n\nThe output tensor has shape <code>(batch_size, seq_len, n_layers, n_classes)</code>:\n* '''batch_size''': Number of sequences in batch\n* '''seq_len''': Sequence length\n* '''n_layers''': Number of LoRA adapter layers in model\n* '''n_classes''': Number of LoRA adapters (experts)\n\nIf <code>enable_softmax=True</code>, values are softmax-normalized across the n_classes dimension, summing to 1.\n\n== Usage Examples ==\n\n=== Basic Classifier Setup ===\n\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom peft import XLoraConfig\nfrom peft.tuners.xlora.classifier import XLoraClassifier\n\n# Configuration\nconfig = XLoraConfig(\n    hidden_size=4096,\n    xlora_depth=2,\n    xlora_size=2048,\n    xlora_dropout_p=0.1,\n    softmax_temperature=1.0,\n    enable_softmax=True\n)\n\n# Create classifier\nn_adapters = 4  # Number of LoRA experts\nn_layers = 32   # Number of adapter layers\ndevice = torch.device(\"cuda\")\n\nclassifier = XLoraClassifier(\n    model=peft_model,\n    config=config,\n    n_classes=n_adapters,\n    n_layers=n_layers,\n    device=device\n)\n</syntaxhighlight>\n\n=== Forward Pass Example ===\n\n<syntaxhighlight lang=\"python\">\nimport torch\n\n# Prepare inputs\ninput_ids = torch.randint(0, 50000, (2, 128))  # batch=2, seq_len=128\n\n# Get model output with hidden states\nresult = model(\n    input_ids,\n    output_hidden_states=True,\n    return_dict=True\n)\n\n# Get scalings from classifier\nscalings = classifier(result, input_ids=input_ids)\n# scalings.shape = (2, 128, 32, 4) for batch=2, seq=128, layers=32, adapters=4\n\nprint(f\"Scalings shape: {scalings.shape}\")\nprint(f\"Scalings sum per position: {scalings[0, 0, 0].sum()}\")  # Should be ~1.0 with softmax\n</syntaxhighlight>\n\n=== Shallow vs Deep Classifier ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\nfrom peft.tuners.xlora.classifier import XLoraClassifier\n\n# Shallow classifier (single layer, fast)\nshallow_config = XLoraConfig(\n    hidden_size=4096,\n    xlora_depth=1,  # Single linear layer\n    enable_softmax=True\n)\n\nshallow_classifier = XLoraClassifier(\n    model=model,\n    config=shallow_config,\n    n_classes=4,\n    n_layers=32,\n    device=device\n)\n\n# Deep classifier (multiple layers, more capacity)\ndeep_config = XLoraConfig(\n    hidden_size=4096,\n    xlora_depth=4,  # Multiple hidden layers\n    xlora_size=2048,\n    xlora_dropout_p=0.2,\n    enable_softmax=True\n)\n\ndeep_classifier = XLoraClassifier(\n    model=model,\n    config=deep_config,\n    n_classes=4,\n    n_layers=32,\n    device=device\n)\n</syntaxhighlight>\n\n=== Temperature-Scaled Softmax ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Sharp predictions (low temperature)\nsharp_config = XLoraConfig(\n    hidden_size=4096,\n    xlora_depth=2,\n    softmax_temperature=0.5,  # Lower = sharper, more decisive\n    enable_softmax=True\n)\n\n# Smooth predictions (high temperature)\nsmooth_config = XLoraConfig(\n    hidden_size=4096,\n    xlora_depth=2,\n    softmax_temperature=2.0,  # Higher = smoother, more uniform\n    enable_softmax=True\n)\n\nclassifier_sharp = XLoraClassifier(model, sharp_config, 4, 32, device)\nclassifier_smooth = XLoraClassifier(model, smooth_config, 4, 32, device)\n\n# With temperature=0.5, output more concentrated on best adapter\n# With temperature=2.0, output more evenly distributed across adapters\n</syntaxhighlight>\n\n=== Layerwise vs Shared Scalings ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Layerwise scalings (different weights per layer)\nlayerwise_config = XLoraConfig(\n    hidden_size=4096,\n    xlora_depth=2,\n    layerwise_scalings=True,  # Predict per-layer weights\n    enable_softmax=True\n)\n\n# Shared scalings (same weights broadcast to all layers)\nshared_config = XLoraConfig(\n    hidden_size=4096,\n    xlora_depth=2,\n    layerwise_scalings=False,  # Single set of weights for all layers\n    enable_softmax=True\n)\n\n# Layerwise has more parameters but allows layer-specific adapter selection\nlayerwise_classifier = XLoraClassifier(model, layerwise_config, 4, 32, device)\n\n# Shared has fewer parameters and assumes consistent adapter importance\nshared_classifier = XLoraClassifier(model, shared_config, 4, 32, device)\n</syntaxhighlight>\n\n=== Dummy Scalings for Initial Pass ===\n\n<syntaxhighlight lang=\"python\">\nimport torch\n\n# Create dummy scalings for the first forward pass\n# (needed to get hidden states for actual classification)\ninput_ids = torch.randint(0, 50000, (2, 64))\n\ndummy_scalings = classifier.make_dummy_scalings(input_ids=input_ids)\n# Shape: (2, 64, n_layers, n_classes)\n# Filled with scaling_pass_value (typically 0 or 1/n_classes)\n\nprint(f\"Dummy scalings shape: {dummy_scalings.shape}\")\nprint(f\"Dummy value: {dummy_scalings[0, 0, 0, 0].item()}\")\n</syntaxhighlight>\n\n=== Logging Scalings for Analysis ===\n\n<syntaxhighlight lang=\"python\">\n# Enable scalings logging\nclassifier.scalings_logging = True\n\n# Run multiple forward passes\nfor batch in dataloader:\n    result = model(batch[\"input_ids\"], output_hidden_states=True)\n    scalings = classifier(result, input_ids=batch[\"input_ids\"])\n\n# Access logged scalings\nall_scalings = classifier.log_scalings\nprint(f\"Logged {len(all_scalings)} scaling tensors\")\n\n# Analyze adapter usage\nfor i, scalings in enumerate(all_scalings):\n    mean_weights = scalings.mean(dim=(0, 1, 2))  # Average across batch, seq, layers\n    print(f\"Batch {i} adapter weights: {mean_weights}\")\n\n# Get bucketed scalings by sequence length\nbucketed = classifier._get_bucketed_scalings()\nfor seq_len, (positions, tensors) in bucketed.items():\n    print(f\"Sequence length {seq_len}: {len(tensors)} samples\")\n</syntaxhighlight>\n\n=== Override Scaling Pass Value ===\n\n<syntaxhighlight lang=\"python\">\n# Set custom scaling pass value\nclassifier._set_override_scaling_pass_value(0.25)\n\n# Or reset to default (1/n_classes)\nclassifier._set_override_scaling_pass_value(None)\n\n# This affects dummy_scalings used in the initial pass\ndummy = classifier.make_dummy_scalings(input_ids=input_ids)\nprint(f\"Override value: {dummy[0, 0, 0, 0].item()}\")\n</syntaxhighlight>\n\n== Implementation Details ==\n\n=== Architecture ===\n\nThe classifier network structure depends on <code>xlora_depth</code>:\n\n'''Depth = 1''' (Shallow):\n<syntaxhighlight lang=\"python\">\nLinear(hidden_size -> n_classes * n_layers)  # if layerwise_scalings\nLinear(hidden_size -> n_classes)             # if not layerwise_scalings\n</syntaxhighlight>\n\n'''Depth > 1''' (Deep):\n<syntaxhighlight lang=\"python\">\nLinear(hidden_size -> xlora_size)\nReLU()\nDropout(p=xlora_dropout_p)  # if dropout > 0\nLinear(xlora_size -> xlora_size)  # repeated (depth-2) times\nReLU()\nDropout(p=xlora_dropout_p)\nLinear(xlora_size -> n_classes * n_layers)  # or n_classes\n</syntaxhighlight>\n\n=== Temperature Scaling ===\n\n<syntaxhighlight lang=\"python\">\nclass TemperatureScaledSoftmax(nn.Module):\n    def forward(self, logits):\n        scaled_logits = logits / self.temperature\n        return softmax(scaled_logits)\n</syntaxhighlight>\n\nLower temperature (<1.0) makes distribution sharper, higher temperature (>1.0) makes it smoother.\n\n== Related Pages ==\n\n* [[huggingface_peft_XLoraConfig|XLoraConfig]] - Configuration for X-LoRA\n* [[huggingface_peft_XLoraModel|XLoraModel]] - Main X-LoRA model class\n* [[huggingface_peft_XLoraLayer|XLoraLayer]] - Layer implementation\n* [[huggingface_peft_LoraModel|LoraModel]] - Base LoRA implementation\n* [[Mixture_of_Experts|MoE Architectures]]\n* [[Parameter_Efficient_Fine_Tuning|PEFT Overview]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_XLoraConfig",
      "page_title": "XLoraConfig",
      "page_type": "Implementation",
      "overview": "=== Description === XLoraConfig is the configuration class for X-LoRA (Mixture of LoRA Experts), a technique that dynamically combines multiple LoRA adapters using a learned classifier. Unlike standard LoRA which applies a single adapter, X-LoRA trains a gating network to predict mixing weights for multiple LoRA adapters based on the input, enabling dynamic expert selection. The configuration controls: * The classifier architecture (depth, size, dropout) * The softmax behavior (temperature, top-k sparsity) * Whether scalings are layer-specific or shared * Which LoRA adapters to use as experts * Training and inference parameters",
      "content": "= XLoraConfig =\n\n== Knowledge Sources ==\n\n* '''Repository''': [https://github.com/huggingface/peft HuggingFace PEFT]\n* '''Paper''': X-LoRA: Mixture of Low-Rank Adapters\n* '''Type''': Configuration Class\n* '''Module''': peft.tuners.xlora.config\n\n== Domains ==\n\n[[Category:Natural_Language_Processing]]\n[[Category:Parameter_Efficient_Fine_Tuning]]\n[[Category:Mixture_of_Experts]]\n[[Category:Low_Rank_Adaptation]]\n[[Category:Configuration]]\n\n== Overview ==\n\n=== Description ===\n\nXLoraConfig is the configuration class for X-LoRA (Mixture of LoRA Experts), a technique that dynamically combines multiple LoRA adapters using a learned classifier. Unlike standard LoRA which applies a single adapter, X-LoRA trains a gating network to predict mixing weights for multiple LoRA adapters based on the input, enabling dynamic expert selection.\n\nThe configuration controls:\n* The classifier architecture (depth, size, dropout)\n* The softmax behavior (temperature, top-k sparsity)\n* Whether scalings are layer-specific or shared\n* Which LoRA adapters to use as experts\n* Training and inference parameters\n\n=== Usage ===\n\nXLoraConfig is used to initialize an X-LoRA model that manages multiple LoRA adapters. The adapters dictionary specifies which LoRA experts to load, and the classifier configuration determines how they are dynamically weighted.\n\n== Code Reference ==\n\n=== Source Location ===\n\n<code>/tmp/praxium_repo_zyf9ywdz/src/peft/tuners/xlora/config.py</code>\n\n=== Signature ===\n\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass XLoraConfig(PeftConfig):\n    hidden_size: int = None\n    adapters: dict[str, str] = None\n    enable_softmax: bool = True\n    enable_softmax_topk: bool = False\n    layerwise_scalings: bool = False\n    xlora_depth: int = 1\n    xlora_size: int = 2048\n    xlora_dropout_p: float = 0.2\n    use_trainable_adapters: bool = False\n    softmax_temperature: float = 1.0\n    top_k_lora: Optional[int] = None\n    scaling_pass_value: float = 0.0\n    global_scaling_weight: float = 1.0\n</syntaxhighlight>\n\n=== Import ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Configuration Parameters ===\n\n{| class=\"wikitable\"\n! Parameter !! Type !! Default !! Description\n|-\n| hidden_size || int || None (4096) || Hidden size of the base model\n|-\n| adapters || dict[str, str] || None ({}) || Mapping of adapter names to LoRA adapter IDs for loading\n|-\n| enable_softmax || bool || True || Enable softmax normalization for classifier output\n|-\n| enable_softmax_topk || bool || False || Enable softmax only for top-k adapters (requires top_k_lora)\n|-\n| softmax_temperature || float || 1.0 || Temperature for softmax (lower = sharper predictions)\n|-\n| layerwise_scalings || bool || False || Generate per-layer scalings vs broadcasting same scalings\n|-\n| top_k_lora || int || None || Sparse selection of top-k LoRA experts (None = dense)\n|-\n| xlora_depth || int || 1 || Number of layers in X-LoRA classifier network\n|-\n| xlora_size || int || 2048 || Hidden size of classifier (irrelevant if xlora_depth=1)\n|-\n| xlora_dropout_p || float || 0.2 || Dropout probability in classifier (irrelevant if xlora_depth=1)\n|-\n| use_trainable_adapters || bool || False || Whether to make the LoRA adapters trainable\n|-\n| scaling_pass_value || float || 0.0 || Value for dummy scalings in initial forward pass\n|-\n| global_scaling_weight || float || 1.0 || Global multiplier for all LoRA adapter outputs\n|}\n\n=== Validation Rules ===\n\n* If <code>hidden_size</code> is None, defaults to 4096 with a warning\n* If <code>adapters</code> is None, defaults to empty dict with a warning\n* If <code>enable_softmax_topk=True</code> and <code>top_k_lora</code> is None, issues warning\n* If both <code>enable_softmax_topk</code> and <code>enable_softmax</code> are True, warns about worse performance\n* If <code>top_k_lora < 1</code>, issues warning about invalid value\n* <code>peft_type</code> is automatically set to <code>PeftType.XLORA</code>\n\n== Usage Examples ==\n\n=== Basic X-LoRA Configuration ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Define X-LoRA configuration with multiple adapters\nconfig = XLoraConfig(\n    hidden_size=4096,\n    adapters={\n        \"math\": \"path/to/math_lora\",\n        \"code\": \"path/to/code_lora\",\n        \"general\": \"path/to/general_lora\"\n    },\n    enable_softmax=True,\n    xlora_depth=2,\n    xlora_size=2048,\n    xlora_dropout_p=0.1\n)\n\n# Create X-LoRA model that dynamically mixes the three adapters\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== Shallow Classifier (Fast) ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Single-layer classifier for fast inference\nconfig = XLoraConfig(\n    hidden_size=4096,\n    adapters={\n        \"adapter1\": \"path/to/adapter1\",\n        \"adapter2\": \"path/to/adapter2\"\n    },\n    xlora_depth=1,  # Single linear layer\n    enable_softmax=True,\n    softmax_temperature=1.0\n)\n</syntaxhighlight>\n\n=== Deep Classifier (More Capacity) ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Multi-layer classifier with dropout for complex mixing\nconfig = XLoraConfig(\n    hidden_size=4096,\n    adapters={\n        \"expert1\": \"path/to/expert1\",\n        \"expert2\": \"path/to/expert2\",\n        \"expert3\": \"path/to/expert3\",\n        \"expert4\": \"path/to/expert4\"\n    },\n    xlora_depth=4,  # Four layers with ReLU and dropout\n    xlora_size=2048,\n    xlora_dropout_p=0.2,\n    enable_softmax=True\n)\n</syntaxhighlight>\n\n=== Sparse Top-K Selection ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Use sparse top-k instead of dense mixing\nconfig = XLoraConfig(\n    hidden_size=4096,\n    adapters={\n        \"expert1\": \"path/to/expert1\",\n        \"expert2\": \"path/to/expert2\",\n        \"expert3\": \"path/to/expert3\",\n        \"expert4\": \"path/to/expert4\",\n        \"expert5\": \"path/to/expert5\"\n    },\n    top_k_lora=2,  # Only use top 2 adapters per token\n    enable_softmax_topk=True,  # Softmax only over selected adapters\n    enable_softmax=False,  # Disable full softmax\n    xlora_depth=2\n)\n</syntaxhighlight>\n\n=== Temperature-Controlled Sharpness ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Sharp predictions (favor single adapter)\nsharp_config = XLoraConfig(\n    hidden_size=4096,\n    adapters={\"a1\": \"p1\", \"a2\": \"p2\", \"a3\": \"p3\"},\n    softmax_temperature=0.5,  # Lower temperature = sharper\n    enable_softmax=True\n)\n\n# Smooth predictions (more uniform mixing)\nsmooth_config = XLoraConfig(\n    hidden_size=4096,\n    adapters={\"a1\": \"p1\", \"a2\": \"p2\", \"a3\": \"p3\"},\n    softmax_temperature=2.0,  # Higher temperature = smoother\n    enable_softmax=True\n)\n</syntaxhighlight>\n\n=== Layerwise vs Shared Scalings ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Layerwise scalings (different adapter weights per layer)\nlayerwise_config = XLoraConfig(\n    hidden_size=4096,\n    adapters={\"task1\": \"path1\", \"task2\": \"path2\"},\n    layerwise_scalings=True,  # Predict weights for each layer\n    xlora_depth=2,\n    enable_softmax=True\n)\n\n# Shared scalings (same weights broadcast to all layers)\nshared_config = XLoraConfig(\n    hidden_size=4096,\n    adapters={\"task1\": \"path1\", \"task2\": \"path2\"},\n    layerwise_scalings=False,  # Single set of weights for all layers\n    xlora_depth=1,  # Can use simpler classifier\n    enable_softmax=True\n)\n</syntaxhighlight>\n\n=== Trainable Adapters Mode ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Make adapters trainable (fine-tune both classifier and adapters)\nconfig = XLoraConfig(\n    hidden_size=4096,\n    adapters={\n        \"base_math\": \"path/to/math_adapter\",\n        \"base_code\": \"path/to/code_adapter\"\n    },\n    use_trainable_adapters=True,  # Allow adapter parameters to update\n    xlora_depth=2,\n    xlora_dropout_p=0.1,\n    enable_softmax=True\n)\n</syntaxhighlight>\n\n=== Global Scaling Weight ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Apply global scaling to all adapter outputs\nconfig = XLoraConfig(\n    hidden_size=4096,\n    adapters={\"a1\": \"p1\", \"a2\": \"p2\"},\n    global_scaling_weight=0.5,  # Scale all adapter outputs by 0.5\n    enable_softmax=True,\n    xlora_depth=1\n)\n</syntaxhighlight>\n\n=== Custom Scaling Pass Value ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig\n\n# Set custom value for dummy scalings in initial pass\nconfig = XLoraConfig(\n    hidden_size=4096,\n    adapters={\"a1\": \"p1\", \"a2\": \"p2\", \"a3\": \"p3\"},\n    scaling_pass_value=0.333,  # Use 1/n_adapters instead of 0\n    enable_softmax=True\n)\n</syntaxhighlight>\n\n=== Loading from Pretrained ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel, XLoraConfig\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n\n# Load X-LoRA model with new adapters\n# The saved adapter paths are replaced by new ones\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"path/to/xlora_checkpoint\",\n    adapters={\n        \"new_adapter1\": \"path/to/new1\",\n        \"new_adapter2\": \"path/to/new2\"\n    }\n)\n</syntaxhighlight>\n\n=== Multiple Domain Experts ===\n\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig, get_peft_model\n\n# Configure X-LoRA with domain-specific experts\nconfig = XLoraConfig(\n    hidden_size=4096,\n    adapters={\n        \"mathematics\": \"username/math_lora\",\n        \"programming\": \"username/code_lora\",\n        \"science\": \"username/science_lora\",\n        \"literature\": \"username/literature_lora\",\n        \"general\": \"username/general_lora\"\n    },\n    xlora_depth=3,\n    xlora_size=2048,\n    xlora_dropout_p=0.15,\n    softmax_temperature=0.8,\n    layerwise_scalings=True,\n    enable_softmax=True,\n    global_scaling_weight=1.0\n)\n\nmodel = get_peft_model(base_model, config)\n\n# The classifier will learn to route different inputs to appropriate experts\n# Math questions -> mathematics adapter\n# Code questions -> programming adapter\n# etc.\n</syntaxhighlight>\n\n== Configuration Combinations ==\n\n=== Dense vs Sparse ===\n\n{| class=\"wikitable\"\n! Mode !! enable_softmax !! enable_softmax_topk !! top_k_lora !! Behavior\n|-\n| Dense || True || False || None || All adapters weighted with softmax\n|-\n| Sparse || False || True || k || Top-k adapters with softmax\n|-\n| Hard || False || False || k || Top-k adapters without normalization\n|}\n\n=== Classifier Complexity ===\n\n{| class=\"wikitable\"\n! xlora_depth !! Parameters !! Use Case\n|-\n| 1 || hidden_size * n_adapters || Fast inference, simple mixing\n|-\n| 2-3 || Moderate || Balanced capacity and speed\n|-\n| 4+ || High || Complex routing, many adapters\n|}\n\n== Related Pages ==\n\n* [[huggingface_peft_XLoraClassifier|XLoraClassifier]] - Classifier implementation\n* [[huggingface_peft_XLoraModel|XLoraModel]] - Main X-LoRA model class\n* [[huggingface_peft_XLoraLayer|XLoraLayer]] - Layer implementation\n* [[huggingface_peft_LoraConfig|LoraConfig]] - Base LoRA configuration\n* [[huggingface_peft_PeftConfig|PeftConfig]] - Base configuration class\n* [[Mixture_of_Experts|MoE Architectures]]\n* [[Parameter_Efficient_Fine_Tuning|PEFT Overview]]\n",
      "domains": [],
      "sources": [],
      "last_updated": null,
      "outgoing_links": []
    },
    {
      "id": "Implementation/huggingface_peft_XLoraLayer",
      "page_title": "huggingface peft XLoraLayer",
      "page_type": "Implementation",
      "overview": "X-LoRA layer wrapper that dynamically routes and scales multiple LoRA adapters using learned per-token scalings for mixture-of-experts style adaptation.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|X-LoRA|https://arxiv.org/abs/2402.07148]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Mixture_of_Experts]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nX-LoRA layer wrapper that dynamically routes and scales multiple LoRA adapters using learned per-token scalings for mixture-of-experts style adaptation.\n\n=== Description ===\n\nXLoraLayer wraps existing LoRA layers to enable dynamic mixing of multiple LoRA adapters. A classifier network generates per-token, per-layer scaling factors that weight each adapter's contribution. The layer supports top-k selection to activate only the most relevant adapters per token, and optional softmax normalization over selected adapters. This enables a single model to leverage multiple specialized LoRA experts dynamically.\n\n=== Usage ===\n\nUse X-LoRA when you have multiple task-specific LoRA adapters and want to combine them dynamically based on input. X-LoRA is particularly effective for handling diverse inputs that may benefit from different combinations of expertise. The classifier learns to route tokens to appropriate experts during training.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/xlora/layer.py src/peft/tuners/xlora/layer.py]\n* '''Lines:''' 1-238\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass XLoraLayer:\n    \"\"\"\n    X-LoRA layer wrapper for dynamic adapter mixing.\n\n    Args:\n        model: The XLoraModel parent\n        target: The LoraLayer being wrapped\n        target_forward: Original forward method\n        layer_number: Layer index for scaling lookup\n        config: XLoraConfig with routing parameters\n    \"\"\"\n\n    @staticmethod\n    def apply_scalings_to_x(\n        x: torch.Tensor,\n        scalings_layer: torch.Tensor,\n        adapter: int,\n    ) -> torch.Tensor:\n        \"\"\"Apply per-token scalings for an adapter.\"\"\"\n\n    def get_maybe_topk_scalings(self, scalings) -> torch.Tensor:\n        \"\"\"Get scalings with optional top-k and softmax.\"\"\"\n\nclass XLoraLinearLayer(XLoraLayer):\n    \"\"\"X-LoRA for Linear layers.\"\"\"\n    def forward(\n        self,\n        x: Tensor,\n        *args,\n        scalings: Optional[Tensor] = None,\n        **kwargs,\n    ) -> Tensor:\n        \"\"\"Forward with dynamic LoRA scaling.\"\"\"\n\nclass XLoraEmbeddingLayer(XLoraLayer):\n    \"\"\"X-LoRA for Embedding layers.\"\"\"\n\nclass XLoraConv2dLayer(XLoraLayer):\n    \"\"\"X-LoRA for Conv2d layers.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft.tuners.xlora import XLoraLayer, XLoraConfig, XLoraModel\nfrom peft import XLoraConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || nn.Module || Yes || Parent XLoraModel\n|-\n| target || LoraLayer || Yes || The LoRA layer to wrap\n|-\n| layer_number || int || Yes || Layer index for scaling lookup\n|-\n| config || XLoraConfig || Yes || Configuration with top_k, softmax settings\n|-\n| scalings || torch.Tensor || No || Per-token scalings [batch, seq, layers, adapters]\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward output || torch.Tensor || Dynamically weighted LoRA adaptation\n|}\n\n== Usage Examples ==\n\n=== Basic X-LoRA Setup ===\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# First, load a model with multiple LoRA adapters\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\n# Configure X-LoRA to mix multiple adapters\nconfig = XLoraConfig(\n    peft_type=\"XLORA\",\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    adapters=[\"adapter1\", \"adapter2\", \"adapter3\"],\n    hidden_size=4096,\n    xlora_depth=2,             # Classifier depth\n    global_scaling_weight=1.0,\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== X-LoRA with Top-K Selection ===\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraConfig, get_peft_model\n\n# Only activate top-k adapters per token\nconfig = XLoraConfig(\n    adapters=[\"math\", \"code\", \"writing\", \"reasoning\"],\n    target_modules=[\"q_proj\", \"v_proj\"],\n    top_k_lora=2,              # Only use top 2 adapters per token\n    enable_softmax_topk=True,  # Softmax over selected adapters\n)\n\nmodel = get_peft_model(model, config)\n</syntaxhighlight>\n\n=== X-LoRA Inference ===\n<syntaxhighlight lang=\"python\">\n# During inference, scalings are computed by the classifier\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=100,\n)\n# The model automatically routes tokens through appropriate adapters\n</syntaxhighlight>\n\n=== Training X-LoRA Classifier ===\n<syntaxhighlight lang=\"python\">\n# During training, the classifier learns optimal routing\n# The LoRA weights can be frozen while training classifier\nmodel.set_xlora_trainability(trainable=True)\n\n# Or train everything together\nfor param in model.parameters():\n    param.requires_grad = True\n\noutputs = model(**batch)\nloss = outputs.loss\nloss.backward()\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Mixture of Experts"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "X-LoRA",
          "url": "https://arxiv.org/abs/2402.07148"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_XLoraModel",
      "page_title": "huggingface peft XLoraModel",
      "page_type": "Implementation",
      "overview": "X-LoRA model class that creates a mixture-of-experts system over multiple LoRA adapters with a learned classifier for dynamic per-token routing.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|X-LoRA|https://arxiv.org/abs/2402.07148]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::PEFT]], [[domain::Mixture_of_Experts]]\n|-\n! Last Updated\n| [[last_updated::2025-12-18 14:00 GMT]]\n|}\n\n== Overview ==\n\nX-LoRA model class that creates a mixture-of-experts system over multiple LoRA adapters with a learned classifier for dynamic per-token routing.\n\n=== Description ===\n\nXLoraModel implements X-LoRA (Mixture of LoRA experts) by wrapping a LoraModel and adding a classifier network that predicts per-token, per-layer scaling factors for each adapter. The model performs a two-pass forward: first with dummy scalings to compute hidden states, then with the classifier to predict real scalings for the actual forward. This enables dynamic routing where different tokens can leverage different combinations of specialized LoRA experts.\n\n=== Usage ===\n\nUse XLoraModel when you have multiple task-specific LoRA adapters and want to automatically route between them. The classifier learns optimal combinations during training. X-LoRA supports top-k selection for sparse expert activation and scalings logging for analysis.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft PEFT]\n* '''File:''' [https://github.com/huggingface/peft/blob/main/src/peft/tuners/xlora/model.py src/peft/tuners/xlora/model.py]\n* '''Lines:''' 1-525\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef convert_layers_to_xlora(base, xloramodel, config) -> tuple[int, torch.device | None]:\n    \"\"\"Convert LoRA layers to X-LoRA layers with routing.\"\"\"\n\nclass XLoraModel(BaseTuner):\n    \"\"\"\n    Creates X-LoRA (Mixture of LoRA experts) model.\n\n    Args:\n        model: Base model to apply X-LoRA to\n        config: XLoraConfig with adapters dict and classifier settings\n        adapter_name: Name for the X-LoRA adapter\n\n    Attributes:\n        lora_model: Underlying LoraModel with loaded adapters\n        internal_xlora_classifier: XLoraClassifier for routing\n        internal_xlora_scalings: Latest computed scalings\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        config: XLoraConfig,\n        adapter_name: str,\n        **kwargs,\n    ) -> None:\n        \"\"\"Load adapters and initialize classifier.\"\"\"\n\n    def set_topk_lora(self, value: Optional[int]):\n        \"\"\"Set top-k expert selection.\"\"\"\n\n    def set_global_scaling_weight(self, weight: float):\n        \"\"\"Set global LoRA weight multiplier.\"\"\"\n\n    def get_latest_scalings(self) -> Optional[torch.Tensor]:\n        \"\"\"Get most recent scalings [batch, seq, layers, adapters].\"\"\"\n\n    def enable_scalings_logging(self):\n        \"\"\"Enable logging of all scalings.\"\"\"\n\n    def get_scalings_log(self) -> list[torch.Tensor]:\n        \"\"\"Get logged scalings history.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import XLoraModel, XLoraConfig, get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || nn.Module || Yes || Base transformer model\n|-\n| config || XLoraConfig || Yes || Configuration with adapters dict\n|-\n| adapters || dict[str, str] || Yes || Map of adapter names to paths/IDs\n|-\n| hidden_size || int || Yes || Model hidden dimension for classifier\n|-\n| xlora_depth || int || No || Classifier MLP depth (default: 2)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| forward() || ModelOutput || Model output with dynamic expert routing\n|-\n| get_latest_scalings() || torch.Tensor || [batch, seq, layers, adapters] scalings\n|}\n\n== Usage Examples ==\n\n=== Creating X-LoRA Model ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, AutoConfig\nfrom peft import XLoraConfig, get_peft_model\n\nmodel_config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nconfig = XLoraConfig(\n    task_type=\"CAUSAL_LM\",\n    hidden_size=model_config.hidden_size,\n    xlora_depth=4,              # Classifier depth\n    adapters={\n        \"math\": \"./path/to/math_adapter/\",\n        \"code\": \"./path/to/code_adapter/\",\n        \"writing\": \"./path/to/writing_adapter/\",\n    },\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.1\",\n    use_cache=False,            # Required for X-LoRA\n)\n\nmodel = get_peft_model(base_model, config)\n</syntaxhighlight>\n\n=== X-LoRA with Top-K Selection ===\n<syntaxhighlight lang=\"python\">\n# Enable sparse top-k expert selection\nmodel.set_topk_lora(2)  # Only use top 2 adapters per token\n\n# Adjust global scaling weight\nmodel.set_global_scaling_weight(0.8)\n\n# Generate with dynamic routing\noutputs = model.generate(input_ids, max_new_tokens=100)\n</syntaxhighlight>\n\n=== Analyzing Scalings ===\n<syntaxhighlight lang=\"python\">\n# Enable scalings logging\nmodel.enable_scalings_logging()\n\n# Run inference\noutputs = model(input_ids)\n\n# Get latest scalings\nscalings = model.get_latest_scalings()\n# Shape: [batch_size, seq_len, n_layers, n_adapters]\n\n# Get full log\nscalings_log = model.get_scalings_log()\n\n# Get bucketed by sequence length\nbucketed = model.get_bucketed_scalings_log()\n\n# Disable logging\nmodel.disable_scalings_logging()\n</syntaxhighlight>\n\n=== X-LoRA Training ===\n<syntaxhighlight lang=\"python\">\n# By default, LoRA adapters are frozen\n# Only the classifier is trained\n\n# To train adapters too:\nconfig = XLoraConfig(\n    use_trainable_adapters=True,  # Unfreeze LoRA weights\n    # ...\n)\n\n# Or manually:\nfor name, param in model.named_parameters():\n    if \"lora_\" in name:\n        param.requires_grad = True\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "PEFT",
        "Mixture of Experts"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "X-LoRA",
          "url": "https://arxiv.org/abs/2402.07148"
        }
      ],
      "last_updated": "2025-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_add_weighted_adapter",
      "page_title": "huggingface peft add weighted adapter",
      "page_type": "Implementation",
      "overview": "Concrete tool for combining multiple LoRA adapters into a single merged adapter using various merging algorithms.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|TIES|https://arxiv.org/abs/2306.01708]]\n* [[source::Paper|DARE|https://arxiv.org/abs/2311.03099]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Model_Merging]], [[domain::Multi_Task]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for combining multiple LoRA adapters into a single merged adapter using various merging algorithms.\n\n=== Description ===\n\n`add_weighted_adapter` creates a new adapter by merging multiple existing adapters with specified weights. It supports various merging algorithms including linear interpolation, SVD-based methods, TIES (TrIm, Elect Sign & Merge), and DARE (Drop And REscale). This enables model composition and multi-task learning.\n\n=== Usage ===\n\nUse this after loading multiple adapters to combine their capabilities. Choose the combination type based on your needs: \"linear\" for simple interpolation, \"ties\" for conflict resolution, \"dare_ties\" for sparsification with TIES. Weights can be positive or negative.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/tuners/lora/model.py\n* '''Lines:''' L573-708\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef add_weighted_adapter(\n    self,\n    adapters: list[str],\n    weights: list[float],\n    adapter_name: str,\n    combination_type: str = \"svd\",\n    svd_rank: int | None = None,\n    svd_clamp: int | None = None,\n    svd_full_matrices: bool = True,\n    svd_driver: str | None = None,\n    density: float | None = None,\n    majority_sign_method: Literal[\"total\", \"frequency\"] = \"total\",\n) -> None:\n    \"\"\"\n    Merge adapters into a new adapter.\n\n    Args:\n        adapters: List of adapter names to merge\n        weights: Weights for each adapter (can be negative)\n        adapter_name: Name for the merged adapter\n        combination_type: Merge algorithm (svd, linear, ties, dare_ties, etc.)\n        density: Pruning density for TIES/DARE (0.0-1.0)\n        majority_sign_method: Sign election for TIES (\"total\" or \"frequency\")\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Method on LoraModel (accessed via PeftModel.base_model)\n# model.add_weighted_adapter(...)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| adapters || list[str] || Yes || Names of adapters to combine\n|-\n| weights || list[float] || Yes || Weight per adapter (can be negative for subtraction)\n|-\n| adapter_name || str || Yes || Name for the new merged adapter\n|-\n| combination_type || str || No || Algorithm: svd, linear, ties, dare_ties, etc.\n|-\n| density || float || No || Pruning density for TIES/DARE (0.0-1.0)\n|-\n| majority_sign_method || str || No || \"total\" or \"frequency\" for TIES\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| None || - || Adapter added in-place to model\n|}\n\n== Usage Examples ==\n\n=== Linear Combination ===\n<syntaxhighlight lang=\"python\">\n# Load adapters\nmodel.load_adapter(\"path/to/math\", adapter_name=\"math\")\nmodel.load_adapter(\"path/to/code\", adapter_name=\"code\")\n\n# Linear merge: 70% math + 30% code\nmodel.add_weighted_adapter(\n    adapters=[\"math\", \"code\"],\n    weights=[0.7, 0.3],\n    adapter_name=\"math_code\",\n    combination_type=\"linear\",\n)\n\nmodel.set_adapter(\"math_code\")\n</syntaxhighlight>\n\n=== TIES Merging ===\n<syntaxhighlight lang=\"python\">\n# TIES: Handles sign conflicts between adapters\nmodel.add_weighted_adapter(\n    adapters=[\"adapter1\", \"adapter2\", \"adapter3\"],\n    weights=[1.0, 1.0, 1.0],\n    adapter_name=\"ties_merged\",\n    combination_type=\"ties\",\n    density=0.5,  # Keep top 50% of parameters\n    majority_sign_method=\"total\",\n)\n</syntaxhighlight>\n\n=== DARE-TIES ===\n<syntaxhighlight lang=\"python\">\n# DARE-TIES: Random pruning + TIES merging\nmodel.add_weighted_adapter(\n    adapters=[\"math\", \"code\", \"writing\"],\n    weights=[0.5, 0.3, 0.2],\n    adapter_name=\"multi_task\",\n    combination_type=\"dare_ties\",\n    density=0.7,  # Random drop 30% of parameters\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Adapter_Merge_Execution]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Adapter",
        "Model Merging",
        "Multi Task"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "TIES",
          "url": "https://arxiv.org/abs/2306.01708"
        },
        {
          "type": "Paper",
          "title": "DARE",
          "url": "https://arxiv.org/abs/2311.03099"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Merge_Execution"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_delete_adapter",
      "page_title": "huggingface peft delete adapter",
      "page_type": "Implementation",
      "overview": "Concrete tool for removing an adapter from a PeftModel to free memory.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Memory_Management]], [[domain::Multi_Task]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for removing an adapter from a PeftModel to free memory.\n\n=== Description ===\n\n`delete_adapter` permanently removes an adapter from the model, freeing the associated GPU memory. This is useful when you no longer need a specific adapter and want to reclaim VRAM for other adapters or operations.\n\n=== Usage ===\n\nUse this to remove adapters you no longer need. You cannot delete the currently active adapter\u2014switch to a different adapter first. After deletion, the adapter name can be reused.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/peft_model.py\n* '''Lines:''' L1083-1101\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef delete_adapter(self, adapter_name: str) -> None:\n    \"\"\"\n    Delete an adapter from the model.\n\n    Args:\n        adapter_name: Name of adapter to remove.\n\n    Raises:\n        ValueError: If adapter doesn't exist or is currently active.\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Method on PeftModel\n# model.delete_adapter(\"adapter_name\")\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| adapter_name || str || Yes || Name of adapter to delete\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| None || - || Adapter removed, memory freed\n|}\n\n== Usage Examples ==\n\n=== Delete Unused Adapter ===\n<syntaxhighlight lang=\"python\">\n# Load multiple adapters\nmodel.load_adapter(\"adapter1\", adapter_name=\"task1\")\nmodel.load_adapter(\"adapter2\", adapter_name=\"task2\")\n\n# Switch away from adapter to delete\nmodel.set_adapter(\"task2\")\n\n# Delete unused adapter\nmodel.delete_adapter(\"task1\")\n\n# Memory freed, task1 no longer available\nprint(list(model.peft_config.keys()))  # [\"task2\"]\n</syntaxhighlight>\n\n=== Cleanup After Merging ===\n<syntaxhighlight lang=\"python\">\n# After merging adapters, delete originals\nmodel.add_weighted_adapter(\n    [\"math\", \"code\"],\n    [0.5, 0.5],\n    \"merged\",\n    combination_type=\"linear\"\n)\n\nmodel.set_adapter(\"merged\")\n\n# Clean up original adapters\nmodel.delete_adapter(\"math\")\nmodel.delete_adapter(\"code\")\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Adapter_Deletion]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Adapter",
        "Memory Management",
        "Multi Task"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Deletion"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_disable_adapter_context",
      "page_title": "huggingface peft disable adapter context",
      "page_type": "Implementation",
      "overview": "Concrete tool for temporarily disabling all adapters to run inference on the base model.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Inference]], [[domain::Context_Manager]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for temporarily disabling all adapters to run inference on the base model.\n\n=== Description ===\n\n`disable_adapter` is a context manager that temporarily disables all adapter layers, allowing inference on the original base model without adapter effects. This is useful for comparing adapter vs. base model outputs or when base model behavior is needed temporarily.\n\n=== Usage ===\n\nUse this as a context manager with `with model.disable_adapter():`. Within the context, the model behaves as the original base model. Adapters are automatically re-enabled when exiting the context.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/peft_model.py\n* '''Lines:''' L940-992\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@contextmanager\ndef disable_adapter(self):\n    \"\"\"\n    Context manager to temporarily disable adapters.\n\n    Usage:\n        with model.disable_adapter():\n            base_output = model(inputs)  # No adapter effect\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Context manager on PeftModel\n# with model.disable_adapter(): ...\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| (none) || - || - || Context manager takes no arguments\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| (context) || - || Within context, adapters are disabled. Re-enabled on exit.\n|}\n\n== Usage Examples ==\n\n=== Compare Base vs Adapted Output ===\n<syntaxhighlight lang=\"python\">\n# Get adapted output\nmodel.set_adapter(\"math\")\nadapted_output = model.generate(**inputs)\n\n# Get base model output\nwith model.disable_adapter():\n    base_output = model.generate(**inputs)\n\n# Compare outputs\nprint(\"Adapted:\", tokenizer.decode(adapted_output[0]))\nprint(\"Base:\", tokenizer.decode(base_output[0]))\n</syntaxhighlight>\n\n=== Selective Inference ===\n<syntaxhighlight lang=\"python\">\n# Process some inputs with adapter, some without\nfor batch in dataloader:\n    if batch[\"use_adapter\"]:\n        output = model(**batch[\"inputs\"])\n    else:\n        with model.disable_adapter():\n            output = model(**batch[\"inputs\"])\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Adapter_Enable_Disable]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Adapter",
        "Inference",
        "Context Manager"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Enable_Disable"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_get_peft_model",
      "page_title": "huggingface peft get peft model",
      "page_type": "Implementation",
      "overview": "Concrete tool for wrapping a pre-trained model with PEFT adapter layers for parameter-efficient fine-tuning.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft/quicktour]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Fine_Tuning]], [[domain::Adapter]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for wrapping a pre-trained model with PEFT adapter layers for parameter-efficient fine-tuning.\n\n=== Description ===\n\n`get_peft_model` is the primary factory function that transforms a base transformer model into a PEFT-enabled model. It injects adapter layers (LoRA, etc.) based on the provided configuration, freezes the base model weights, and sets up the model for efficient fine-tuning where only the adapter parameters are trained.\n\n=== Usage ===\n\nCall this function after loading your base model and creating a `LoraConfig`. The function modifies the model in-place and returns a `PeftModel` wrapper. After calling this, use `model.print_trainable_parameters()` to verify that only adapter weights are trainable.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/mapping_func.py\n* '''Lines:''' L30-128\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef get_peft_model(\n    model: PreTrainedModel,\n    peft_config: PeftConfig,\n    adapter_name: str = \"default\",\n    mixed: bool = False,\n    autocast_adapter_dtype: bool = True,\n    revision: Optional[str] = None,\n    low_cpu_mem_usage: bool = False,\n) -> PeftModel | PeftMixedModel:\n    \"\"\"\n    Returns a Peft model object from a model and a config.\n\n    The model is modified in-place with adapter layers injected.\n\n    Args:\n        model: Base transformer model to wrap\n        peft_config: Configuration object (LoraConfig, etc.)\n        adapter_name: Name for the adapter (default: \"default\")\n        mixed: Allow mixing different adapter types\n        autocast_adapter_dtype: Auto-cast adapter weights to float32\n        revision: Base model revision for saving\n        low_cpu_mem_usage: Create empty weights on meta device\n\n    Returns:\n        PeftModel: Model with adapter layers ready for training\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || Base transformer model to adapt\n|-\n| peft_config || PeftConfig || Yes || LoraConfig or other PEFT configuration\n|-\n| adapter_name || str || No || Identifier for the adapter. Default: \"default\"\n|-\n| mixed || bool || No || Allow mixing adapter types (LoRA + IA3). Default: False\n|-\n| autocast_adapter_dtype || bool || No || Cast float16/bfloat16 adapters to float32. Default: True\n|-\n| low_cpu_mem_usage || bool || No || Initialize empty weights on meta device. Default: False\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| peft_model || PeftModel || Wrapped model with adapter layers injected and base weights frozen\n|}\n\n== Usage Examples ==\n\n=== Standard LoRA Model Creation ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# 1. Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# 2. Configure LoRA\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# 3. Create PEFT model\nmodel = get_peft_model(model, config)\n\n# 4. Verify trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n</syntaxhighlight>\n\n=== With Named Adapter ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LoraConfig\n\n# Create model with named adapter\nmodel = get_peft_model(\n    model,\n    config,\n    adapter_name=\"math_adapter\",  # Custom name for later reference\n)\n\n# Access adapter by name\nprint(model.active_adapter)  # \"math_adapter\"\n</syntaxhighlight>\n\n=== Low Memory Mode ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LoraConfig\n\n# For large models, use low_cpu_mem_usage\nmodel = get_peft_model(\n    model,\n    config,\n    low_cpu_mem_usage=True,  # Defer weight materialization\n)\n# Weights will be loaded when adapter is loaded via load_adapter()\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_PEFT_Model_Creation]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "NLP",
        "Fine Tuning",
        "Adapter"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft/quicktour"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_PEFT_Model_Creation"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_load_adapter",
      "page_title": "huggingface peft load adapter",
      "page_type": "Implementation",
      "overview": "Concrete tool for loading additional adapters into an existing PeftModel for multi-adapter scenarios.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft/conceptual_guides/lora#manage-multiple-adapters]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Multi_Task]], [[domain::Model_Loading]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for loading additional adapters into an existing PeftModel for multi-adapter scenarios.\n\n=== Description ===\n\n`load_adapter` loads an additional adapter into a model that already has PEFT enabled. Unlike `from_pretrained`, this adds to an existing PeftModel rather than creating a new one. This enables multi-adapter inference where you can switch between different task-specific adapters.\n\n=== Usage ===\n\nUse this after creating a PeftModel to add additional adapters. Each adapter needs a unique name. The newly loaded adapter is not automatically activated\u2014use `set_adapter()` to switch to it.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/peft_model.py\n* '''Lines:''' L1309-1475\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef load_adapter(\n    self,\n    model_id: Union[str, os.PathLike],\n    adapter_name: str,\n    is_trainable: bool = False,\n    torch_device: Optional[str] = None,\n    autocast_adapter_dtype: bool = True,\n    ephemeral_gpu_offload: bool = False,\n    low_cpu_mem_usage: bool = False,\n    **kwargs: Any,\n):\n    \"\"\"\n    Load an additional adapter into the model.\n\n    Args:\n        model_id: Adapter path (local or HuggingFace Hub)\n        adapter_name: Unique name for this adapter\n        is_trainable: Enable gradients for training. Default: False\n        torch_device: Target device. Default: auto-infer\n        autocast_adapter_dtype: Cast weights for stability\n\n    Returns:\n        Load result with missing/unexpected keys\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Method on PeftModel\n# model.load_adapter(...)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model_id || str || Yes || Adapter path (local or Hub)\n|-\n| adapter_name || str || Yes || Unique name for this adapter\n|-\n| is_trainable || bool || No || Load with gradients enabled. Default: False\n|-\n| torch_device || str || No || Target device. Default: auto\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| load_result || LoadResult || Object with missing_keys, unexpected_keys lists\n|}\n\n== Usage Examples ==\n\n=== Load Multiple Task Adapters ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n\n# Start with one adapter\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"username/general-adapter\",\n    adapter_name=\"general\",\n)\n\n# Add task-specific adapters\nmodel.load_adapter(\n    \"username/math-adapter\",\n    adapter_name=\"math\",\n)\nmodel.load_adapter(\n    \"username/code-adapter\",\n    adapter_name=\"code\",\n)\n\n# Switch between adapters\nmodel.set_adapter(\"math\")\nmath_output = model.generate(**inputs)\n\nmodel.set_adapter(\"code\")\ncode_output = model.generate(**inputs)\n</syntaxhighlight>\n\n=== Load Local Adapter ===\n<syntaxhighlight lang=\"python\">\n# Load from local path\nmodel.load_adapter(\n    \"./my-local-adapter\",\n    adapter_name=\"custom\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Multi_Adapter_Loading]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Adapter",
        "Multi Task",
        "Model Loading"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft/conceptual_guides/lora#manage-multiple-adapters"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Multi_Adapter_Loading"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_merge_and_unload",
      "page_title": "huggingface peft merge and unload",
      "page_type": "Implementation",
      "overview": "Concrete tool for merging LoRA adapter weights into the base model and removing the PEFT wrapper for deployment.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft/conceptual_guides/lora#merge-lora-weights-into-the-base-model]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Inference]], [[domain::Deployment]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for merging LoRA adapter weights into the base model and removing the PEFT wrapper for deployment.\n\n=== Description ===\n\n`merge_and_unload` permanently merges the adapter weights into the base model weights and removes the PEFT infrastructure. The result is a standard transformers model with the adapter effects baked in. This enables deployment without PEFT dependencies and may provide faster inference.\n\n=== Usage ===\n\nUse this when deploying a trained model where you don't need adapter switching. The operation is irreversible on the model instance, so assign the result to a new variable. Use `safe_merge=True` to detect potential numerical issues.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/tuners/tuners_utils.py\n* '''Lines:''' L611-647\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef merge_and_unload(\n    self,\n    progressbar: bool = False,\n    safe_merge: bool = False,\n    adapter_names: Optional[list[str]] = None\n) -> torch.nn.Module:\n    \"\"\"\n    Merge adapter weights into base model and remove PEFT wrapper.\n\n    Args:\n        progressbar: Show progress bar during merge\n        safe_merge: Check for NaNs during merge (slower but safer)\n        adapter_names: Specific adapters to merge. None = all active\n\n    Returns:\n        Base model with merged weights (no PEFT wrapper)\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Method on PeftModel, no explicit import\n# merged_model = model.merge_and_unload()\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| progressbar || bool || No || Show tqdm progress. Default: False\n|-\n| safe_merge || bool || No || Check for NaNs (slower). Default: False\n|-\n| adapter_names || list[str] || No || Adapters to merge. Default: all active\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || torch.nn.Module || Standard transformers model with merged weights\n|}\n\n== Usage Examples ==\n\n=== Basic Merge for Deployment ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n\n# Load adapter\nmodel = PeftModel.from_pretrained(base_model, \"path/to/adapter\")\n\n# Merge and get standard model\nmerged_model = model.merge_and_unload()\n\n# Save as standard transformers model\nmerged_model.save_pretrained(\"./merged-model\")\n</syntaxhighlight>\n\n=== Safe Merge with Progress ===\n<syntaxhighlight lang=\"python\">\n# Merge with safety checks\nmerged_model = model.merge_and_unload(\n    progressbar=True,\n    safe_merge=True,  # Slower but detects NaN issues\n)\n</syntaxhighlight>\n\n=== Merge Specific Adapters ===\n<syntaxhighlight lang=\"python\">\n# Model with multiple adapters\nmodel.load_adapter(\"adapter1\", adapter_name=\"math\")\nmodel.load_adapter(\"adapter2\", adapter_name=\"code\")\n\n# Merge only the math adapter\nmodel.set_adapter(\"math\")\nmerged_model = model.merge_and_unload(adapter_names=[\"math\"])\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Adapter_Merging_Into_Base]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Adapter",
        "Inference",
        "Deployment"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft/conceptual_guides/lora#merge-lora-weights-into-the-base-model"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Merging_Into_Base"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_merge_strategy_selection",
      "page_title": "huggingface peft merge strategy selection",
      "page_type": "Implementation",
      "overview": "Concrete tool for configuring merge algorithm parameters before calling `add_weighted_adapter()`.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::File|merge_utils.py|src/peft/utils/merge_utils.py]]\n|-\n! Domains\n| [[domain::Model_Merging]], [[domain::Multi_Task]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for configuring merge algorithm parameters before calling `add_weighted_adapter()`.\n\n=== Description ===\n\nThe merge strategy is configured via parameters to `add_weighted_adapter()`. The underlying algorithms are implemented in `src/peft/utils/merge_utils.py` and include:\n* `ties()` - TIES merging algorithm\n* `dare_ties()` - DARE + TIES combination\n* `dare_linear()` - DARE + linear average\n* `magnitude_prune()` - Pruning by magnitude\n\n=== Usage ===\n\nConfigure merge parameters as arguments to `add_weighted_adapter()`. This is a configuration step, not a separate API call.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''File:''' `src/peft/utils/merge_utils.py`\n* '''Lines:''' L144-269\n\n=== Available Algorithms ===\n<syntaxhighlight lang=\"python\">\n# combination_type options for add_weighted_adapter():\nCOMBINATION_TYPES = [\n    \"svd\",              # SVD-based merge\n    \"linear\",           # Simple weighted average\n    \"cat\",              # Concatenation (increases rank)\n    \"ties\",             # TIES merging\n    \"ties_svd\",         # TIES + SVD\n    \"dare_ties\",        # DARE + TIES\n    \"dare_linear\",      # DARE + linear\n    \"dare_ties_svd\",    # DARE + TIES + SVD\n    \"dare_linear_svd\",  # DARE + linear + SVD\n    \"magnitude_prune\",  # Prune by magnitude\n    \"magnitude_prune_svd\",\n]\n</syntaxhighlight>\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef ties(task_tensors, weights, density, majority_sign_method=\"total\"):\n    \"\"\"\n    TIES: TrIm, Elect Sign, Merge.\n\n    Args:\n        task_tensors: List of adapter weight tensors\n        weights: Per-adapter weights\n        density: Fraction of weights to keep (0.0-1.0)\n        majority_sign_method: \"total\" or \"frequency\"\n\n    Returns:\n        Merged tensor\n    \"\"\"\n\ndef dare_ties(task_tensors, weights, density, majority_sign_method=\"total\"):\n    \"\"\"DARE pruning followed by TIES merge.\"\"\"\n\ndef dare_linear(task_tensors, weights, density):\n    \"\"\"DARE pruning followed by linear average.\"\"\"\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== TIES Merge ===\n<syntaxhighlight lang=\"python\">\nmodel.add_weighted_adapter(\n    adapters=[\"task1\", \"task2\", \"task3\"],\n    weights=[0.4, 0.3, 0.3],\n    adapter_name=\"merged_ties\",\n    combination_type=\"ties\",\n    density=0.7,  # Keep 70% of weights\n    majority_sign_method=\"total\",\n)\n</syntaxhighlight>\n\n=== DARE Linear Merge ===\n<syntaxhighlight lang=\"python\">\nmodel.add_weighted_adapter(\n    adapters=[\"task1\", \"task2\"],\n    weights=[0.5, 0.5],\n    adapter_name=\"merged_dare\",\n    combination_type=\"dare_linear\",\n    density=0.5,  # Random 50% pruning with rescale\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Merge_Strategy_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Model Merging",
        "Multi Task",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "File",
          "title": "merge_utils.py",
          "url": "src/peft/utils/merge_utils.py"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Merge_Strategy_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_merged_adapter_evaluation",
      "page_title": "huggingface peft merged adapter evaluation",
      "page_type": "Implementation",
      "overview": "Pattern for evaluating merged adapter quality by testing on tasks from each source adapter.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|HF Evaluate|https://huggingface.co/docs/evaluate]]\n|-\n! Domains\n| [[domain::Model_Merging]], [[domain::Evaluation]], [[domain::Multi_Task]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nPattern for evaluating merged adapter quality by testing on tasks from each source adapter.\n\n=== Description ===\n\nThis is a Pattern Doc - there is no single PEFT API for evaluation. Users implement evaluation loops using standard evaluation libraries (evaluate, datasets) combined with PEFT's adapter switching.\n\n=== Usage ===\n\nImplement a custom evaluation loop that:\n1. Activates the merged adapter\n2. Runs inference on test sets from each source task\n3. Computes task-specific metrics\n4. Compares to individual adapter baselines\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Type:''' Pattern Doc (user-implemented)\n* '''No specific PEFT API''' - uses standard evaluation patterns\n\n=== Pattern ===\n<syntaxhighlight lang=\"python\">\nfrom evaluate import load\nfrom peft import PeftModel\n\ndef evaluate_merged_adapter(model, merged_name, task_datasets):\n    \"\"\"\n    Evaluate merged adapter on multiple task datasets.\n\n    Args:\n        model: PeftModel with merged adapter loaded\n        merged_name: Name of the merged adapter\n        task_datasets: Dict mapping task names to (dataset, metric_name)\n\n    Returns:\n        Dict of task -> metric value\n    \"\"\"\n    model.set_adapter(merged_name)\n    model.eval()\n\n    results = {}\n    for task_name, (dataset, metric_name) in task_datasets.items():\n        metric = load(metric_name)\n\n        for batch in dataset:\n            with torch.no_grad():\n                outputs = model.generate(batch[\"input_ids\"], max_new_tokens=50)\n            predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\n        results[task_name] = metric.compute()\n\n    return results\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Comparing Merged vs Individual ===\n<syntaxhighlight lang=\"python\">\n# Load model with individual adapters\nmodel.load_adapter(\"task1_adapter\", adapter_name=\"task1\")\nmodel.load_adapter(\"task2_adapter\", adapter_name=\"task2\")\n\n# Create merged adapter\nmodel.add_weighted_adapter(\n    adapters=[\"task1\", \"task2\"],\n    weights=[0.5, 0.5],\n    adapter_name=\"merged\"\n)\n\n# Evaluate individual adapters\nmodel.set_adapter(\"task1\")\ntask1_individual = evaluate_on_task1(model)\n\nmodel.set_adapter(\"task2\")\ntask2_individual = evaluate_on_task2(model)\n\n# Evaluate merged adapter\nmodel.set_adapter(\"merged\")\ntask1_merged = evaluate_on_task1(model)\ntask2_merged = evaluate_on_task2(model)\n\n# Compare results\nprint(f\"Task1: individual={task1_individual:.3f}, merged={task1_merged:.3f}\")\nprint(f\"Task2: individual={task2_individual:.3f}, merged={task2_merged:.3f}\")\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Merge_Evaluation]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Model Merging",
        "Evaluation",
        "Multi Task"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "HF Evaluate",
          "url": "https://huggingface.co/docs/evaluate"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Merge_Evaluation"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_model_eval",
      "page_title": "huggingface peft model eval",
      "page_type": "Implementation",
      "overview": "Concrete tool for setting a PEFT model to evaluation mode for inference.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|PyTorch|https://pytorch.org/docs/stable/generated/torch.nn.Module.html]]\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Inference]], [[domain::Model_Serving]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for setting a PEFT model to evaluation mode for inference.\n\n=== Description ===\n\n`model.eval()` is a PyTorch method that sets the model to evaluation mode. For PEFT models, this propagates through both the base model and adapter layers, disabling dropout and setting BatchNorm to use running statistics.\n\n=== Usage ===\n\nCall before running inference on a PEFT model to ensure deterministic outputs and optimal performance.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Library:''' `torch.nn.Module` (PyTorch base)\n* '''Method:''' `eval()`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef eval(self) -> T:\n    \"\"\"\n    Set the module to evaluation mode.\n\n    This is equivalent to self.train(False).\n\n    Returns:\n        self: The module in evaluation mode\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# No import needed - method of all nn.Module subclasses\n# PeftModel inherits from nn.Module\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Basic Inference Setup ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\nimport torch\n\n# Load model with adapter\nmodel = PeftModel.from_pretrained(base_model, \"path/to/adapter\")\n\n# Configure for inference\nmodel.eval()\n\n# Run inference without gradient computation\nwith torch.no_grad():\n    outputs = model.generate(input_ids, max_new_tokens=100)\n</syntaxhighlight>\n\n=== With Inference Optimizations ===\n<syntaxhighlight lang=\"python\">\n# Set eval mode and compile for repeated inference\nmodel.eval()\ncompiled_model = torch.compile(model)\n\nwith torch.no_grad():\n    # First call triggers compilation\n    outputs = compiled_model.generate(input_ids)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Inference_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Inference",
        "Model Serving",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "PyTorch",
          "url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
        },
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Inference_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_model_generate",
      "page_title": "huggingface peft model generate",
      "page_type": "Implementation",
      "overview": "Concrete tool for running text generation through a PEFT model using the transformers generation API.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|Transformers Generation|https://huggingface.co/docs/transformers/main_classes/text_generation]]\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n|-\n! Domains\n| [[domain::Inference]], [[domain::Text_Generation]], [[domain::Model_Serving]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for running text generation through a PEFT model using the transformers generation API.\n\n=== Description ===\n\n`model.generate()` is inherited from transformers' `GenerationMixin` and works transparently with PEFT models. The adapter weights are applied during each forward pass of the generation loop, modifying outputs according to the task-specific fine-tuning.\n\n=== Usage ===\n\nUse for autoregressive text generation with PEFT models. Supports all standard transformers generation parameters including sampling, beam search, and constraints.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Library:''' `transformers.GenerationMixin` (external)\n* '''Method:''' `generate()`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor = None,\n    max_new_tokens: int = None,\n    max_length: int = None,\n    do_sample: bool = False,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = 50,\n    num_beams: int = 1,\n    repetition_penalty: float = 1.0,\n    **kwargs\n) -> torch.Tensor:\n    \"\"\"\n    Generate sequences from input_ids.\n\n    Args:\n        input_ids: Tokenized input tensor\n        max_new_tokens: Maximum tokens to generate\n        do_sample: Use sampling vs greedy decoding\n        temperature: Sampling temperature (higher = more random)\n        top_p: Nucleus sampling probability threshold\n        top_k: Top-k sampling\n        num_beams: Beam search width\n\n    Returns:\n        Generated token IDs tensor\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# No import needed - method inherited by PeftModel\nfrom peft import PeftModel\n# model.generate() is available on PeftModel instance\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Greedy Generation ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\nimport torch\n\nmodel = PeftModel.from_pretrained(base_model, \"path/to/adapter\")\nmodel.eval()\n\nwith torch.no_grad():\n    outputs = model.generate(\n        input_ids,\n        max_new_tokens=100,\n        do_sample=False,  # Greedy decoding\n    )\n</syntaxhighlight>\n\n=== Sampling with Temperature ===\n<syntaxhighlight lang=\"python\">\nwith torch.no_grad():\n    outputs = model.generate(\n        input_ids,\n        max_new_tokens=200,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n    )\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Inference_Execution]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Inference",
        "Text Generation",
        "Model Serving"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "Transformers Generation",
          "url": "https://huggingface.co/docs/transformers/main_classes/text_generation"
        },
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Inference_Execution"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_model_train_mode",
      "page_title": "huggingface peft model train mode",
      "page_type": "Implementation",
      "overview": "Concrete tool for setting the PEFT model to training mode, enabling dropout and gradient computation for adapter training.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PyTorch Docs|https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train]]\n|-\n! Domains\n| [[domain::Training]], [[domain::Model_State]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for setting the PEFT model to training mode, enabling dropout and gradient computation for adapter training.\n\n=== Description ===\n\n`model.train()` is a PyTorch method that sets the model to training mode. For PEFT models, this enables dropout in LoRA layers and ensures gradients are tracked for the trainable adapter parameters. This is a standard PyTorch API inherited by PeftModel.\n\n=== Usage ===\n\nCall this before starting the training loop. In practice, HuggingFace's `Trainer` handles this automatically, but for custom training loops, explicitly calling `model.train()` is required. Pair with `model.eval()` for inference.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Library:''' [https://pytorch.org/ PyTorch]\n* '''Class:''' `torch.nn.Module`\n* '''Method:''' `train(mode: bool = True)`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef train(self, mode: bool = True) -> T:\n    \"\"\"\n    Sets the module in training mode.\n\n    This has effect only on certain modules. E.g., Dropout, BatchNorm, etc.\n    are affected by this, and are typically different in training and\n    evaluation modes.\n\n    Args:\n        mode: Whether to set training mode (True) or evaluation mode (False)\n\n    Returns:\n        self: The module itself\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# No explicit import needed - method is on the model\n# model.train() is called directly\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| mode || bool || No || Training mode flag. True for training, False for eval. Default: True\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| self || PeftModel || The model in training mode (returns self for chaining)\n|}\n\n== Usage Examples ==\n\n=== Basic Training Mode Setup ===\n<syntaxhighlight lang=\"python\">\nfrom peft import get_peft_model, LoraConfig\n\n# After creating PEFT model\nmodel = get_peft_model(base_model, lora_config)\n\n# Set to training mode\nmodel.train()\n\n# Now dropout is enabled and gradients tracked\n# Ready for training loop\n</syntaxhighlight>\n\n=== Custom Training Loop ===\n<syntaxhighlight lang=\"python\">\nimport torch\nfrom torch.optim import AdamW\n\n# Setup\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Training loop\nfor batch in dataloader:\n    optimizer.zero_grad()\n\n    outputs = model(**batch)\n    loss = outputs.loss\n\n    loss.backward()\n    optimizer.step()\n\n# Switch to eval for validation\nmodel.eval()\nwith torch.no_grad():\n    # ... validation code\n</syntaxhighlight>\n\n=== With Gradient Checkpointing ===\n<syntaxhighlight lang=\"python\">\n# Enable gradient checkpointing before training\nmodel.gradient_checkpointing_enable()\n\n# Set training mode\nmodel.train()\n\n# Now training uses gradient checkpointing to save memory\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Training_Preparation]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Training",
        "Model State"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PyTorch Docs",
          "url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Training_Preparation"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_prepare_model_for_kbit_training",
      "page_title": "huggingface peft prepare model for kbit training",
      "page_type": "Implementation",
      "overview": "Concrete tool for preparing quantized models for k-bit training by enabling gradient checkpointing and proper dtype handling.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::Training]], [[domain::Memory_Efficiency]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for preparing quantized models for k-bit training by enabling gradient checkpointing and proper dtype handling.\n\n=== Description ===\n\n`prepare_model_for_kbit_training` prepares a quantized model for fine-tuning by:\n1. Freezing all base model parameters\n2. Casting layer norms to float32 for stability\n3. Enabling gradient checkpointing for memory efficiency\n4. Setting up input embeddings to require gradients\n\nThis function is essential for stable QLoRA training.\n\n=== Usage ===\n\nCall this immediately after loading a quantized model, before applying PEFT. Set `use_gradient_checkpointing=True` (default) to reduce VRAM usage at the cost of ~20% slower training.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/utils/other.py\n* '''Lines:''' L130-215\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef prepare_model_for_kbit_training(\n    model: PreTrainedModel,\n    use_gradient_checkpointing: bool = True,\n    gradient_checkpointing_kwargs: Optional[dict] = None\n) -> PreTrainedModel:\n    \"\"\"\n    Prepare a quantized model for training.\n\n    Args:\n        model: Quantized model from transformers\n        use_gradient_checkpointing: Enable gradient checkpointing. Default: True\n        gradient_checkpointing_kwargs: Additional args for checkpointing\n\n    Returns:\n        Model prepared for k-bit training\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import prepare_model_for_kbit_training\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || Quantized model (4-bit or 8-bit)\n|-\n| use_gradient_checkpointing || bool || No || Enable gradient checkpointing. Default: True\n|-\n| gradient_checkpointing_kwargs || dict || No || Args like {\"use_reentrant\": False}\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PreTrainedModel || Model prepared for k-bit training (modified in-place, also returned)\n|}\n\n== Usage Examples ==\n\n=== Standard QLoRA Preparation ===\n<syntaxhighlight lang=\"python\">\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training\nimport torch\n\n# 1. Load quantized model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\n# 2. Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# 3. Now apply PEFT\n# model = get_peft_model(model, lora_config)\n</syntaxhighlight>\n\n=== With Custom Checkpointing ===\n<syntaxhighlight lang=\"python\">\nfrom peft import prepare_model_for_kbit_training\n\n# Use non-reentrant checkpointing (recommended for newer PyTorch)\nmodel = prepare_model_for_kbit_training(\n    model,\n    use_gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Kbit_Training_Preparation]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Quantization_Environment]]\n",
      "domains": [
        "Quantization",
        "Training",
        "Memory Efficiency"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Kbit_Training_Preparation"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Quantization_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_query_adapter_state",
      "page_title": "huggingface peft query adapter state",
      "page_type": "Implementation",
      "overview": "Concrete tool for querying the state of loaded adapters on a PEFT model, providing introspection for active adapters and configurations.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::Multi_Task]], [[domain::Adapter]], [[domain::Model_Management]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for querying the state of loaded adapters on a PEFT model, providing introspection for active adapters and configurations.\n\n=== Description ===\n\nThis implementation covers the properties and methods for inspecting PEFT model adapter state:\n* `model.active_adapter` - Property returning the currently active adapter name(s)\n* `model.peft_config` - Dictionary of all loaded adapter configurations\n* `model.get_model_status()` - Detailed status report of all adapters\n\n=== Usage ===\n\nUse these properties to inspect adapter state during multi-adapter serving, debugging, or logging scenarios.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''File:''' `src/peft/peft_model.py`\n* '''Lines:''' L180-250\n\n=== Properties ===\n<syntaxhighlight lang=\"python\">\n@property\ndef active_adapter(self) -> Union[str, list[str]]:\n    \"\"\"Return the name(s) of the currently active adapter(s).\"\"\"\n    return self.base_model.active_adapter\n\n@property\ndef peft_config(self) -> dict[str, PeftConfig]:\n    \"\"\"Return dictionary mapping adapter names to their configurations.\"\"\"\n    return self._peft_config\n\ndef get_model_status(self) -> PeftModelStatus:\n    \"\"\"Get detailed status of all adapters including type, trainability, etc.\"\"\"\n    ...\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n# Properties accessed on PeftModel instance\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Inspecting Active Adapter ===\n<syntaxhighlight lang=\"python\">\nfrom peft import PeftModel\n\n# Load model with multiple adapters\nmodel = PeftModel.from_pretrained(base_model, \"adapter1\")\nmodel.load_adapter(\"adapter2\", adapter_name=\"task2\")\n\n# Check active adapter\nprint(model.active_adapter)  # \"default\"\n\n# Switch and check\nmodel.set_adapter(\"task2\")\nprint(model.active_adapter)  # \"task2\"\n</syntaxhighlight>\n\n=== Listing All Adapters ===\n<syntaxhighlight lang=\"python\">\n# Get all loaded adapter configurations\nfor name, config in model.peft_config.items():\n    print(f\"Adapter '{name}': r={config.r}, target_modules={config.target_modules}\")\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Adapter_State_Query]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Multi Task",
        "Adapter",
        "Model Management"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_State_Query"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Implementation/huggingface_peft_set_adapter",
      "page_title": "huggingface peft set adapter",
      "page_type": "Implementation",
      "overview": "Concrete tool for switching the active adapter in a multi-adapter PeftModel.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft/conceptual_guides/lora#manage-multiple-adapters]]\n|-\n! Domains\n| [[domain::Adapter]], [[domain::Multi_Task]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2025-01-15 12:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for switching the active adapter in a multi-adapter PeftModel.\n\n=== Description ===\n\n`set_adapter` activates a specific adapter (or combination of adapters) for inference or training. Only the active adapter affects model outputs. This enables rapid switching between task-specific adapters without reloading models.\n\n=== Usage ===\n\nUse this after loading multiple adapters to select which one should be active. Pass a single adapter name for exclusive activation, or a list to combine multiple adapters (outputs summed).\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/huggingface/peft peft]\n* '''File:''' src/peft/peft_model.py\n* '''Lines:''' L1477-1504\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef set_adapter(self, adapter_name: Union[str, list[str]]) -> None:\n    \"\"\"\n    Set the active adapter(s).\n\n    Args:\n        adapter_name: Single adapter name or list of adapters to activate.\n                     If list, their outputs are summed.\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Method on PeftModel\n# model.set_adapter(\"adapter_name\")\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| adapter_name || str or list[str] || Yes || Adapter name(s) to activate\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| None || - || Model state updated in-place\n|}\n\n== Usage Examples ==\n\n=== Switch Between Adapters ===\n<syntaxhighlight lang=\"python\">\n# Load multiple adapters\nmodel.load_adapter(\"math-adapter\", adapter_name=\"math\")\nmodel.load_adapter(\"code-adapter\", adapter_name=\"code\")\n\n# Use math adapter\nmodel.set_adapter(\"math\")\nmath_output = model.generate(**math_prompt)\n\n# Switch to code adapter\nmodel.set_adapter(\"code\")\ncode_output = model.generate(**code_prompt)\n</syntaxhighlight>\n\n=== Combine Multiple Adapters ===\n<syntaxhighlight lang=\"python\">\n# Activate multiple adapters (outputs summed)\nmodel.set_adapter([\"math\", \"code\"])\ncombined_output = model.generate(**inputs)\n</syntaxhighlight>\n\n=== Check Active Adapter ===\n<syntaxhighlight lang=\"python\">\n# Check current adapter\nprint(model.active_adapter)  # \"math\"\n\n# Check all available\nprint(list(model.peft_config.keys()))  # [\"math\", \"code\"]\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:huggingface_peft_Adapter_Switching]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:huggingface_peft_Core_Environment]]\n",
      "domains": [
        "Adapter",
        "Multi Task",
        "Inference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft/conceptual_guides/lora#manage-multiple-adapters"
        }
      ],
      "last_updated": "2025-01-15 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "huggingface_peft_Adapter_Switching"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "huggingface_peft_Core_Environment"
        }
      ]
    },
    {
      "id": "Environment/huggingface_peft_Core_Environment",
      "page_title": "huggingface peft Core Environment",
      "page_type": "Environment",
      "overview": "Python 3.10+ environment with PyTorch 1.13+, HuggingFace Transformers, and Accelerate for PEFT adapter training.",
      "content": "# Environment: huggingface_peft_Core_Environment\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Installation|https://huggingface.co/docs/peft/install]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::Deep_Learning]], [[domain::Parameter_Efficient_Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nPython 3.10+ environment with PyTorch 1.13+, HuggingFace Transformers, and Accelerate for PEFT adapter training.\n\n=== Description ===\nThis environment provides the core dependencies for Parameter-Efficient Fine-Tuning (PEFT). It includes the base requirements for loading pre-trained models, configuring LoRA/other adapters, and saving trained adapters. This is the minimum viable environment for using PEFT without any quantization or specialized initialization methods.\n\n=== Usage ===\nUse this environment for any basic PEFT workflow including LoRA fine-tuning, adapter loading, inference, and multi-adapter management. This environment is sufficient when:\n- Training adapters on full-precision models (fp16/bf16/fp32)\n- Loading and merging adapters for inference\n- Managing multiple adapters on a single model\n\nFor quantized models (QLoRA), see `huggingface_peft_Quantization_Environment`.\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n| OS || Linux, macOS, Windows || Linux recommended for GPU training\n|-\n| Python || >= 3.10.0 || Required by PEFT\n|-\n| Hardware || CPU or CUDA GPU || GPU recommended for training\n|-\n| Disk || 10GB+ || For model weights and checkpoints\n|}\n\n== Dependencies ==\n=== System Packages ===\n* No special system packages required for basic usage\n* CUDA toolkit if using NVIDIA GPU\n\n=== Python Packages ===\n* `torch` >= 1.13.0\n* `transformers` (any recent version)\n* `accelerate` >= 0.21.0\n* `safetensors` (any recent version)\n* `huggingface_hub` >= 0.25.0\n* `numpy` >= 1.17\n* `packaging` >= 20.0\n* `pyyaml`\n* `tqdm`\n* `psutil`\n\n== Credentials ==\nThe following environment variables are optional but useful:\n* `HF_TOKEN`: HuggingFace API token for accessing gated models (e.g., Llama)\n* `HF_HUB_OFFLINE`: Set to \"1\" to enable offline mode\n\n== Quick Install ==\n<syntaxhighlight lang=\"bash\">\n# Install PEFT with all core dependencies\npip install peft\n\n# Or install from source\npip install git+https://github.com/huggingface/peft.git\n\n# Verify installation\npython -c \"import peft; print(peft.__version__)\"\n</syntaxhighlight>\n\n== Code Evidence ==\n\nCore dependency requirements from `setup.py:60-71`:\n<syntaxhighlight lang=\"python\">\ninstall_requires=[\n    \"numpy>=1.17\",\n    \"packaging>=20.0\",\n    \"psutil\",\n    \"pyyaml\",\n    \"torch>=1.13.0\",\n    \"transformers\",\n    \"tqdm\",\n    \"accelerate>=0.21.0\",\n    \"safetensors\",\n    \"huggingface_hub>=0.25.0\",\n],\n</syntaxhighlight>\n\nPython version requirement from `setup.py:59`:\n<syntaxhighlight lang=\"python\">\npython_requires=\">=3.10.0\",\n</syntaxhighlight>\n\nDevice inference logic from `other.py:116-127`:\n<syntaxhighlight lang=\"python\">\ndef infer_device() -> str:\n    if torch.cuda.is_available():\n        return \"cuda\"\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        return \"mps\"\n    elif mlu_available:\n        return \"mlu\"\n    elif is_xpu_available():\n        return \"xpu\"\n    elif is_npu_available():\n        return \"npu\"\n    return \"cpu\"\n</syntaxhighlight>\n\n== Common Errors ==\n\n{| class=\"wikitable\"\n|-\n! Error Message !! Cause !! Solution\n|-\n|| `ModuleNotFoundError: No module named 'peft'` || PEFT not installed || `pip install peft`\n|-\n|| `ImportError: accelerate` || Accelerate not installed or too old || `pip install accelerate>=0.21.0`\n|-\n|| `RuntimeError: CUDA out of memory` || Insufficient GPU VRAM || Use gradient checkpointing or smaller batch size\n|-\n|| `ValueError: You have to provide either input_ids or inputs_embeds` || Missing input to model || Ensure proper tokenization\n|}\n\n== Compatibility Notes ==\n\n* **macOS MPS**: Apple Silicon supported via MPS backend\n* **Intel XPU**: Supported via PyTorch XPU integration\n* **Huawei NPU**: Supported via Ascend integration with accelerate >= 0.29.0\n* **Cambricon MLU**: Supported via MLU integration with accelerate >= 0.29.0\n* **CPU-only**: Training possible but significantly slower\n\n== Related Pages ==\n* [[requires_env::Implementation:huggingface_peft_LoraConfig_init]]\n* [[requires_env::Implementation:huggingface_peft_get_peft_model]]\n* [[requires_env::Implementation:huggingface_peft_PeftModel_from_pretrained]]\n* [[requires_env::Implementation:huggingface_peft_PeftModel_save_pretrained]]\n* [[requires_env::Implementation:huggingface_peft_merge_and_unload]]\n",
      "domains": [
        "Infrastructure",
        "Deep Learning",
        "Parameter Efficient Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Installation",
          "url": "https://huggingface.co/docs/peft/install"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_LoraConfig_init"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_get_peft_model"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_PeftModel_from_pretrained"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_PeftModel_save_pretrained"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_merge_and_unload"
        }
      ]
    },
    {
      "id": "Environment/huggingface_peft_GPTQ_Environment",
      "page_title": "huggingface peft GPTQ Environment",
      "page_type": "Environment",
      "overview": "PEFT environment with GPTQ quantization support via AutoGPTQ or GPTQModel backends.",
      "content": "# Environment: huggingface_peft_GPTQ_Environment\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|AutoGPTQ|https://github.com/AutoGPTQ/AutoGPTQ]]\n* [[source::Repo|GPTQModel|https://github.com/ModelCloud/GPTQModel]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::Quantization]], [[domain::GPTQ]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nPEFT environment with GPTQ quantization support via AutoGPTQ or GPTQModel backends.\n\n=== Description ===\nThis environment provides support for training LoRA adapters on GPTQ-quantized models. GPTQ is a one-shot weight quantization method that achieves high compression ratios while maintaining model quality. This environment supports both the AutoGPTQ and GPTQModel backends.\n\n=== Usage ===\nUse this environment when you need to:\n- Train LoRA adapters on GPTQ-quantized models\n- Use pre-quantized models from HuggingFace Hub (e.g., TheBloke's GPTQ models)\n- Apply QALoRA (Quantization-Aware LoRA) for GPTQ models\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n| OS || Linux || Windows has limited support\n|-\n| Python || >= 3.10.0 || Required by PEFT\n|-\n| Hardware || NVIDIA GPU with CUDA || Required for GPTQ inference/training\n|-\n| CUDA || >= 11.0 || Check AutoGPTQ/GPTQModel requirements\n|}\n\n== Dependencies ==\n=== System Packages ===\n* CUDA toolkit\n* cuDNN\n\n=== Python Packages ===\n* All packages from `huggingface_peft_Core_Environment`\n* Option A: `auto_gptq` >= 0.5.0\n* Option B: `gptqmodel` >= 2.0.0 AND `optimum` >= 1.24.0\n\n== Credentials ==\n* Same as `huggingface_peft_Core_Environment`\n\n== Quick Install ==\n<syntaxhighlight lang=\"bash\">\n# Option A: Install with AutoGPTQ\npip install peft auto-gptq\n\n# Option B: Install with GPTQModel (newer)\npip install peft gptqmodel optimum>=1.24.0\n\n# Verify installation\npython -c \"from peft.import_utils import is_auto_gptq_available; print(is_auto_gptq_available())\"\n</syntaxhighlight>\n\n== Code Evidence ==\n\nAutoGPTQ version check from `import_utils.py:39-49`:\n<syntaxhighlight lang=\"python\">\n@lru_cache\ndef is_auto_gptq_available():\n    if importlib.util.find_spec(\"auto_gptq\") is not None:\n        AUTOGPTQ_MINIMUM_VERSION = packaging.version.parse(\"0.5.0\")\n        version_autogptq = packaging.version.parse(importlib_metadata.version(\"auto_gptq\"))\n        if AUTOGPTQ_MINIMUM_VERSION <= version_autogptq:\n            return True\n        else:\n            raise ImportError(\n                f\"Found an incompatible version of auto-gptq. Found version {version_autogptq}, \"\n                f\"but only versions above {AUTOGPTQ_MINIMUM_VERSION} are supported\"\n            )\n</syntaxhighlight>\n\nGPTQModel version check from `import_utils.py:53-76`:\n<syntaxhighlight lang=\"python\">\n@lru_cache\ndef is_gptqmodel_available():\n    if importlib.util.find_spec(\"gptqmodel\") is not None:\n        GPTQMODEL_MINIMUM_VERSION = packaging.version.parse(\"2.0.0\")\n        OPTIMUM_MINIMUM_VERSION = packaging.version.parse(\"1.24.0\")\n        version_gptqmodel = packaging.version.parse(importlib_metadata.version(\"gptqmodel\"))\n        if GPTQMODEL_MINIMUM_VERSION <= version_gptqmodel:\n            if is_optimum_available():\n                version_optimum = packaging.version.parse(importlib_metadata.version(\"optimum\"))\n                if OPTIMUM_MINIMUM_VERSION <= version_optimum:\n                    return True\n                else:\n                    raise ImportError(\n                        f\"gptqmodel requires optimum version `{OPTIMUM_MINIMUM_VERSION}` or higher.\"\n                    )\n</syntaxhighlight>\n\nQALoRA support from `config.py:663-673`:\n<syntaxhighlight lang=\"python\">\nuse_qalora: bool = field(\n    default=False,\n    metadata={\n        \"help\": (\n            \"It is only implemented in GPTQ for now. Enable Quantization-Aware Low-Rank Adaptation (QALoRA).\"\n            \"This technique combines quantization-aware training \"\n            \"with LoRA to improve performance for quantized models.\"\n        )\n    },\n)\n</syntaxhighlight>\n\n== Common Errors ==\n\n{| class=\"wikitable\"\n|-\n! Error Message !! Cause !! Solution\n|-\n|| `ImportError: Found an incompatible version of auto-gptq` || AutoGPTQ too old || `pip install -U auto-gptq>=0.5.0`\n|-\n|| `ImportError: gptqmodel requires optimum version 1.24.0 or higher` || Optimum missing or too old || `pip install optimum>=1.24.0`\n|-\n|| `RuntimeError: ExLlama kernel not available` || ExLlama not compiled || Reinstall auto-gptq with CUDA\n|}\n\n== Compatibility Notes ==\n\n* **AutoGPTQ vs GPTQModel**: GPTQModel is newer and may have better performance\n* **ExLlama kernels**: AutoGPTQ supports ExLlama v1/v2 for faster inference\n* **Mixed precision**: GPTQ models typically use fp16 for non-quantized components\n* **QALoRA**: Only supported with GPTQ quantization\n\n== Related Pages ==\n* [[requires_env::Implementation:huggingface_peft_get_peft_model]]\n",
      "domains": [
        "Infrastructure",
        "Quantization",
        "GPTQ"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "AutoGPTQ",
          "url": "https://github.com/AutoGPTQ/AutoGPTQ"
        },
        {
          "type": "Repo",
          "title": "GPTQModel",
          "url": "https://github.com/ModelCloud/GPTQModel"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_get_peft_model"
        }
      ]
    },
    {
      "id": "Environment/huggingface_peft_LoftQ_Environment",
      "page_title": "huggingface peft LoftQ Environment",
      "page_type": "Environment",
      "overview": "PEFT environment with scipy for LoftQ (LoRA-Fine-Tuning-Aware Quantization) initialization.",
      "content": "# Environment: huggingface_peft_LoftQ_Environment\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|LoftQ|https://huggingface.co/papers/2310.08659]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::Quantization]], [[domain::Initialization]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nPEFT environment with scipy for LoftQ (LoRA-Fine-Tuning-Aware Quantization) initialization.\n\n=== Description ===\nThis environment extends the Quantization Environment with scipy support for LoftQ initialization. LoftQ is an initialization method that quantizes backbone weights while simultaneously initializing LoRA matrices to minimize the quantization error. This results in better starting points for QLoRA training.\n\n=== Usage ===\nUse this environment when you need to:\n- Initialize LoRA weights using LoftQ method (`init_lora_weights='loftq'`)\n- Reduce quantization error compared to standard QLoRA initialization\n- Train with `LoftQConfig` for custom quantization parameters\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n| OS || Linux, macOS, Windows || Linux recommended\n|-\n| Python || >= 3.10.0 || Required by PEFT\n|-\n| Hardware || NVIDIA GPU with CUDA || Required for quantization\n|}\n\n== Dependencies ==\n=== Python Packages ===\n* All packages from `huggingface_peft_Quantization_Environment`\n* `scipy` (required for SVD operations in LoftQ)\n\n== Quick Install ==\n<syntaxhighlight lang=\"bash\">\n# Install PEFT with LoftQ dependencies\npip install peft bitsandbytes scipy\n\n# Verify scipy\npython -c \"import scipy; print(scipy.__version__)\"\n</syntaxhighlight>\n\n== Code Evidence ==\n\nScipy requirement check from `config.py:799-803`:\n<syntaxhighlight lang=\"python\">\n# handle init_lora_weights and loftq_config\nif self.init_lora_weights == \"loftq\":\n    import importlib\n\n    if not importlib.util.find_spec(\"scipy\"):\n        raise ImportError(\"The required package 'scipy' is not installed. Please install it to continue.\")\n</syntaxhighlight>\n\nScipy import in LoftQ utils from `loftq_utils.py:69`:\n<syntaxhighlight lang=\"python\">\nif not importlib.util.find_spec(\"scipy\"):\n    raise ImportError(\"The required package 'scipy' is not installed. Please install it to continue.\")\n</syntaxhighlight>\n\nLoftQ config validation from `config.py:804-810`:\n<syntaxhighlight lang=\"python\">\nif not self.loftq_config:\n    raise ValueError(\"`loftq_config` must be specified when `init_lora_weights` is 'loftq'.\")\nif not isinstance(self.loftq_config, dict):\n    # convert loftq_config to dict\n    self.loftq_config = vars(self.loftq_config)\n</syntaxhighlight>\n\n== Common Errors ==\n\n{| class=\"wikitable\"\n|-\n! Error Message !! Cause !! Solution\n|-\n|| `ImportError: The required package 'scipy' is not installed` || scipy missing || `pip install scipy`\n|-\n|| `ValueError: loftq_config must be specified when init_lora_weights is 'loftq'` || Missing config || Pass `LoftQConfig()` when using LoftQ\n|-\n|| `ValueError: Only support 2, 4, 8 bits quantization` || Invalid bit setting || Use 2, 4, or 8 bits in LoftQConfig\n|-\n|| `ValueError: bitsandbytes is not available` || bitsandbytes not installed || `pip install bitsandbytes`\n|}\n\n== Compatibility Notes ==\n\n* **Quantization bits**: LoftQ supports 2, 4, and 8-bit quantization\n* **SVD operations**: Uses scipy for efficient SVD computation\n* **First-time initialization**: Set `fake=True` in LoftQConfig for first run to save weights, then load with `fake=False`\n\n== Related Pages ==\n* [[requires_env::Implementation:huggingface_peft_LoraConfig_init]]\n",
      "domains": [
        "Infrastructure",
        "Quantization",
        "Initialization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "LoftQ",
          "url": "https://huggingface.co/papers/2310.08659"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_LoraConfig_init"
        }
      ]
    },
    {
      "id": "Environment/huggingface_peft_Quantization_Environment",
      "page_title": "huggingface peft Quantization Environment",
      "page_type": "Environment",
      "overview": "Extended PEFT environment with bitsandbytes for 4-bit/8-bit quantized model training (QLoRA).",
      "content": "# Environment: huggingface_peft_Quantization_Environment\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Repo|bitsandbytes|https://github.com/bitsandbytes-foundation/bitsandbytes]]\n* [[source::Doc|QLoRA Paper|https://huggingface.co/papers/2305.14314]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::Quantization]], [[domain::QLoRA]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nExtended PEFT environment with bitsandbytes for 4-bit/8-bit quantized model training (QLoRA).\n\n=== Description ===\nThis environment extends the Core PEFT Environment with bitsandbytes support for quantized model loading and QLoRA training. It enables training large language models on consumer GPUs by quantizing base model weights to 4-bit or 8-bit precision while keeping LoRA adapters in full precision.\n\n=== Usage ===\nUse this environment when you need to:\n- Train LoRA adapters on quantized models (QLoRA workflow)\n- Load models in 4-bit (`load_in_4bit=True`) or 8-bit (`load_in_8bit=True`)\n- Use `BitsAndBytesConfig` for quantization configuration\n- Call `prepare_model_for_kbit_training()` to prepare quantized models\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n| OS || Linux || Windows/macOS have limited bitsandbytes support\n|-\n| Python || >= 3.10.0 || Required by PEFT\n|-\n| Hardware || NVIDIA GPU with CUDA || Required for bitsandbytes quantization\n|-\n| CUDA || >= 11.0 || Check bitsandbytes compatibility matrix\n|-\n| VRAM || 8GB+ recommended || 4-bit allows 7B models on 8GB VRAM\n|}\n\n== Dependencies ==\n=== System Packages ===\n* CUDA toolkit (11.0+)\n* cuDNN\n\n=== Python Packages ===\n* All packages from `huggingface_peft_Core_Environment`\n* `bitsandbytes` (any recent version for 8-bit, must have `Linear4bit` for 4-bit)\n\n== Credentials ==\n* Same as `huggingface_peft_Core_Environment`\n* `HF_TOKEN` often required for accessing quantization-friendly models (e.g., Llama)\n\n== Quick Install ==\n<syntaxhighlight lang=\"bash\">\n# Install PEFT with bitsandbytes\npip install peft bitsandbytes\n\n# Verify 4-bit support\npython -c \"import bitsandbytes as bnb; print(hasattr(bnb.nn, 'Linear4bit'))\"\n</syntaxhighlight>\n\n== Code Evidence ==\n\nbitsandbytes availability check from `import_utils.py:24-35`:\n<syntaxhighlight lang=\"python\">\n@lru_cache\ndef is_bnb_available() -> bool:\n    return importlib.util.find_spec(\"bitsandbytes\") is not None\n\n@lru_cache\ndef is_bnb_4bit_available() -> bool:\n    if not is_bnb_available():\n        return False\n\n    import bitsandbytes as bnb\n\n    return hasattr(bnb.nn, \"Linear4bit\")\n</syntaxhighlight>\n\nQLoRA model preparation from `other.py:130-215`:\n<syntaxhighlight lang=\"python\">\ndef prepare_model_for_kbit_training(model, use_gradient_checkpointing=True, gradient_checkpointing_kwargs=None):\n    \"\"\"\n    This method wraps the entire protocol for preparing a model before running a training.\n    This includes:\n        1- Cast the layernorm in fp32\n        2- making output embedding layer require grads\n        3- Add the upcasting of the lm head to fp32\n        4- Freezing the base model layers to ensure they are not updated during training\n    \"\"\"\n    loaded_in_kbit = getattr(model, \"is_loaded_in_8bit\", False) or getattr(model, \"is_loaded_in_4bit\", False)\n    # ...\n</syntaxhighlight>\n\nConditional 4-bit layer definition from `bnb.py:322`:\n<syntaxhighlight lang=\"python\">\nif is_bnb_4bit_available():\n\n    class Linear4bit(torch.nn.Module, LoraLayer):\n        # Lora implemented in a dense layer\n</syntaxhighlight>\n\nClone required for 4-bit backprop from `bnb.py:548-553`:\n<syntaxhighlight lang=\"python\">\n# As per Tim Dettmers, for 4bit, we need to defensively clone here.\n# The reason is that in some cases, an error can occur that backprop\n# does not work on a manipulated view. This issue may be solved with\n# newer PyTorch versions but this would need extensive testing to be\n# sure.\nresult = result.clone()\n</syntaxhighlight>\n\n== Common Errors ==\n\n{| class=\"wikitable\"\n|-\n! Error Message !! Cause !! Solution\n|-\n|| `ImportError: bitsandbytes` || bitsandbytes not installed || `pip install bitsandbytes`\n|-\n|| `AttributeError: 'NoneType' object has no attribute 'Linear4bit'` || Old bitsandbytes version || `pip install -U bitsandbytes`\n|-\n|| `RuntimeError: CUDA error: no kernel image is available` || CUDA/bitsandbytes mismatch || Reinstall bitsandbytes matching CUDA version\n|-\n|| Merge warning: \"may get different generations due to rounding errors\" || Merging quantized weights || Expected behavior; verify outputs post-merge\n|}\n\n== Compatibility Notes ==\n\n* **Linux only**: bitsandbytes has full support only on Linux with CUDA\n* **Windows**: Limited support via WSL2\n* **macOS**: Not officially supported for quantization\n* **Multi-GPU**: Works with `device_map=\"auto\"` and accelerate\n* **Merge limitations**: Merging 4-bit/8-bit models may introduce rounding errors\n\n== Related Pages ==\n* [[requires_env::Implementation:huggingface_peft_BitsAndBytesConfig_4bit]]\n* [[requires_env::Implementation:huggingface_peft_prepare_model_for_kbit_training]]\n",
      "domains": [
        "Infrastructure",
        "Quantization",
        "QLoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Repo",
          "title": "bitsandbytes",
          "url": "https://github.com/bitsandbytes-foundation/bitsandbytes"
        },
        {
          "type": "Doc",
          "title": "QLoRA Paper",
          "url": "https://huggingface.co/papers/2305.14314"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_BitsAndBytesConfig_4bit"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_prepare_model_for_kbit_training"
        }
      ]
    },
    {
      "id": "Heuristic/huggingface_peft_DoRA_Overhead",
      "page_title": "huggingface peft DoRA Overhead",
      "page_type": "Heuristic",
      "overview": "DoRA improves low-rank adaptation quality but adds computational overhead; merge weights for efficient inference.",
      "content": "# Heuristic: huggingface_peft_DoRA_Overhead\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|DoRA Paper|https://huggingface.co/papers/2402.09353]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Optimization]], [[domain::DoRA]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nDoRA improves low-rank adaptation quality but adds computational overhead; merge weights for efficient inference.\n\n=== Description ===\nDoRA (Weight-Decomposed Low-Rank Adaptation) decomposes weight updates into magnitude and direction components. This improves LoRA quality, especially at low ranks, but introduces additional overhead during training and inference. Understanding this trade-off is crucial for deciding when to use DoRA.\n\n=== Usage ===\nUse this heuristic when:\n- Deciding whether to enable `use_dora=True` in LoraConfig\n- Training at low ranks (r <= 8) where DoRA shows most benefit\n- Planning inference deployment of DoRA-trained adapters\n\n== The Insight (Rule of Thumb) ==\n\n* **When to Use DoRA:**\n  * Low-rank scenarios (r <= 8) where quality matters\n  * Tasks where standard LoRA underperforms\n  * When you can afford the training overhead\n\n* **When to Avoid DoRA:**\n  * High-rank scenarios (r >= 32) where LoRA is sufficient\n  * Megatron parallelism (not supported)\n  * When `lora_bias=True` (not compatible)\n\n* **Overhead:**\n  * Training: ~10-20% slower per step\n  * Inference (unmerged): Additional magnitude computation\n  * Inference (merged): No overhead after merging\n\n* **Recommendation:**\n  * For production: Merge DoRA weights after training (`merge_and_unload()`)\n  * For experimentation: Keep unmerged for easy adapter switching\n\n* **Layer Support:**\n  * Currently only supports Linear and Conv2D layers\n  * Does not support Megatron parallel layers\n\n* **Ephemeral GPU Offload:**\n  * DoRA initialization can be slow on CPU-offloaded models\n  * Use `ephemeral_gpu_offload=True` to speed up DoRA initialization\n\n== Reasoning ==\n\n### How DoRA Works\nDoRA decomposes weight updates as:\n- **Direction:** Handled by standard LoRA (low-rank matrices)\n- **Magnitude:** Handled by a separate learnable parameter\n\nThis decomposition better mimics full fine-tuning behavior and typically achieves better results at the same rank.\n\n### Overhead Source\nThe additional magnitude parameter requires:\n1. Extra forward pass computation for magnitude normalization\n2. Extra backward pass computation for magnitude gradients\n3. Additional memory for magnitude parameters\n\n### Merging Benefit\nOnce merged, DoRA weights become regular dense weights with no inference overhead. This makes DoRA attractive for production:\n- Train with DoRA for better quality\n- Merge for deployment with no overhead\n\n== Code Evidence ==\n\nDoRA configuration from `config.py:634-645`:\n<syntaxhighlight lang=\"python\">\nuse_dora: bool = field(\n    default=False,\n    metadata={\n        \"help\": (\n            \"Enable 'Weight-Decomposed Low-Rank Adaptation' (DoRA). This technique decomposes the updates of the \"\n            \"weights into two parts, magnitude and direction. Direction is handled by normal LoRA, whereas the \"\n            \"magnitude is handled by a separate learnable parameter. This can improve the performance of LoRA, \"\n            \"especially at low ranks. Right now, DoRA only supports linear and Conv2D layers. DoRA introduces a bigger\"\n            \"overhead than pure LoRA, so it is recommended to merge weights for inference.\"\n        )\n    },\n)\n</syntaxhighlight>\n\nMegatron incompatibility from `config.py:795-796`:\n<syntaxhighlight lang=\"python\">\nif self.use_dora and self.megatron_config:\n    raise ValueError(\"DoRA does not support megatron_core, please set `use_dora=False`.\")\n</syntaxhighlight>\n\nlora_bias incompatibility from `config.py:833-834`:\n<syntaxhighlight lang=\"python\">\nif self.use_dora:\n    raise ValueError(\"The argument lora_bias=True is not supported for DoRA, please pass use_dora=False\")\n</syntaxhighlight>\n\nEphemeral GPU offload for DoRA from `config.py:37-50`:\n<syntaxhighlight lang=\"python\">\nephemeral_gpu_offload: bool = field(\n    default=False,\n    metadata={\n        \"help\": (\n            \"Whether to use ephemeral GPU offloading for models partially kept in CPU memory. \"\n            \"...Currently only affects DoRA initialization.\"\n        )\n    },\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[uses_heuristic::Implementation:huggingface_peft_LoraConfig_init]]\n* [[uses_heuristic::Principle:huggingface_peft_LoRA_Configuration]]\n* [[uses_heuristic::Workflow:huggingface_peft_LoRA_Fine_Tuning]]\n",
      "domains": [
        "LLMs",
        "Optimization",
        "DoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "DoRA Paper",
          "url": "https://huggingface.co/papers/2402.09353"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_LoraConfig_init"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "huggingface_peft_LoRA_Configuration"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "huggingface_peft_LoRA_Fine_Tuning"
        }
      ]
    },
    {
      "id": "Heuristic/huggingface_peft_Gradient_Checkpointing",
      "page_title": "huggingface peft Gradient Checkpointing",
      "page_type": "Heuristic",
      "overview": "Memory optimization technique that trades compute for VRAM by recomputing activations during backprop.",
      "content": "# Heuristic: huggingface_peft_Gradient_Checkpointing\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|HuggingFace Transformers|https://huggingface.co/docs/transformers]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nMemory optimization technique that trades compute for VRAM by recomputing activations during backprop.\n\n=== Description ===\nGradient checkpointing (activation checkpointing) reduces peak memory usage during training by not storing all intermediate activations. Instead, activations are recomputed during the backward pass. This is especially important for QLoRA training where models barely fit in VRAM.\n\n=== Usage ===\nUse this heuristic when:\n- Training large models (7B+ parameters) on limited VRAM\n- Running out of GPU memory during training\n- Using `prepare_model_for_kbit_training()` (enabled by default)\n- Need to increase batch size but memory is constrained\n\n== The Insight (Rule of Thumb) ==\n\n* **Action:** Enable gradient checkpointing via:\n  1. `prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)` (default)\n  2. `model.gradient_checkpointing_enable()` directly\n  3. `TrainingArguments(gradient_checkpointing=True)` in Trainer\n\n* **Value:** Boolean flag - enable when VRAM is constrained\n\n* **Memory Savings:** Typically 50-60% reduction in activation memory\n\n* **Trade-off:**\n  * ~20-30% slower training due to recomputation\n  * Requires slightly more compute per step\n  * Allows significantly larger batch sizes or models\n\n* **Note:** Must disable KV cache (`use_cache=False`) when gradient checkpointing is enabled\n\n* **Gradient Checkpointing Kwargs:**\n  * Can pass custom kwargs via `gradient_checkpointing_kwargs`\n  * Example: `{\"use_reentrant\": False}` for newer PyTorch behavior\n\n== Reasoning ==\n\n### Memory-Compute Trade-off\nDeep transformers store activation tensors at each layer for backpropagation. For a model with L layers:\n- Without checkpointing: Store ~L activation tensors\n- With checkpointing: Store ~sqrt(L) checkpoints, recompute others\n\n### Why It Matters for QLoRA\nQLoRA already pushes memory limits by using 4-bit quantization. Gradient checkpointing provides additional headroom to:\n- Use larger batch sizes for better gradient estimates\n- Train longer sequences\n- Fit models that would otherwise OOM\n\n### KV Cache Conflict\nGradient checkpointing recomputes forward passes, but KV cache assumes stored key-value pairs. These are incompatible, so `use_cache` must be False during training.\n\n== Code Evidence ==\n\nDefault enabled in `prepare_model_for_kbit_training` from `other.py:130-140`:\n<syntaxhighlight lang=\"python\">\ndef prepare_model_for_kbit_training(model, use_gradient_checkpointing=True, gradient_checkpointing_kwargs=None):\n    \"\"\"\n    This method wraps the entire protocol for preparing a model before running a training.\n    This includes:\n        1- Cast the layernorm in fp32\n        2- making output embedding layer require grads\n        3- Add the upcasting of the lm head to fp32\n        4- Freezing the base model layers to ensure they are not updated during training\n    \"\"\"\n</syntaxhighlight>\n\nGradient checkpointing enablement from `other.py:195-207`:\n<syntaxhighlight lang=\"python\">\nif use_gradient_checkpointing:\n    # For backward compatibility\n    if hasattr(model, \"enable_input_require_grads\"):\n        model.enable_input_require_grads()\n    else:\n\n        def make_inputs_require_grad(module, input, output):\n            output.requires_grad_(True)\n\n        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n    # To support older transformers versions, check if the model supports gradient_checkpointing_kwargs\n    _supports_gc_kwargs = \"gradient_checkpointing_kwargs\" in list(\n        inspect.signature(model.gradient_checkpointing_enable).parameters\n    )\n</syntaxhighlight>\n\nuse_cache conflict handling from `peft_model.py:2099-2100`:\n<syntaxhighlight lang=\"python\">\n# TODO: starting with transformers 4.38, all architectures should support caching.\n# We can remove the hasattr check once we drop support for older transformers versions.\n</syntaxhighlight>\n\n== Related Pages ==\n* [[uses_heuristic::Implementation:huggingface_peft_prepare_model_for_kbit_training]]\n* [[uses_heuristic::Workflow:huggingface_peft_QLoRA_Training]]\n* [[uses_heuristic::Workflow:huggingface_peft_LoRA_Fine_Tuning]]\n",
      "domains": [
        "Optimization",
        "Memory",
        "Training"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Transformers",
          "url": "https://huggingface.co/docs/transformers"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_prepare_model_for_kbit_training"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "huggingface_peft_QLoRA_Training"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "huggingface_peft_LoRA_Fine_Tuning"
        }
      ]
    },
    {
      "id": "Heuristic/huggingface_peft_LoRA_Rank_Selection",
      "page_title": "huggingface peft LoRA Rank Selection",
      "page_type": "Heuristic",
      "overview": "Guidelines for selecting LoRA rank (`r`) based on task complexity and memory constraints.",
      "content": "# Heuristic: huggingface_peft_LoRA_Rank_Selection\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|LoRA Paper|https://huggingface.co/papers/2106.09685]]\n* [[source::Paper|RSLoRA Paper|https://huggingface.co/papers/2312.03732]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Optimization]], [[domain::LoRA]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nGuidelines for selecting LoRA rank (`r`) based on task complexity and memory constraints.\n\n=== Description ===\nThe LoRA rank (`r`) determines the dimensionality of the low-rank decomposition matrices. A higher rank allows for more expressive adaptations but increases trainable parameters and memory usage. Selecting the right rank is crucial for balancing model quality and computational efficiency.\n\n=== Usage ===\nUse this heuristic when configuring `LoraConfig` to determine the optimal `r` value. Consider this heuristic when:\n- Starting a new LoRA fine-tuning project\n- Memory is constrained and you need to minimize trainable parameters\n- Task complexity varies (simple classification vs complex reasoning)\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set LoRA rank `r` in `LoraConfig(r=...)`\n* **Value Recommendations:**\n  * **Simple tasks (classification, NER):** `r=4-8`\n  * **Medium tasks (QA, summarization):** `r=8-16`\n  * **Complex tasks (instruction-following, reasoning):** `r=16-64`\n  * **Maximum expressiveness:** `r=64-128` (diminishing returns)\n* **Default:** `r=8` is a reasonable starting point for most tasks\n* **Trade-off:** Higher `r` = more parameters = more memory = potentially better quality\n\n* **Scaling with RSLoRA:**\n  * When `use_rslora=True`, scaling factor becomes `lora_alpha/sqrt(r)` instead of `lora_alpha/r`\n  * This stabilizes training at higher ranks\n  * Recommended when using `r >= 32`\n\n* **Pattern-based ranks:**\n  * Use `rank_pattern` to set different ranks for different layers\n  * Attention layers often benefit from higher ranks than MLP layers\n\n== Reasoning ==\n\n### Theoretical Foundation\nLoRA approximates weight updates as `\u0394W = BA` where `B \u2208 R^{d\u00d7r}` and `A \u2208 R^{r\u00d7k}`. The rank `r` controls:\n\n1. **Expressiveness**: Higher `r` can capture more complex adaptations\n2. **Parameter count**: Trainable params = `r \u00d7 (d + k)` per adapted layer\n3. **Memory**: Both forward/backward pass memory scale with `r`\n\n### Empirical Evidence\nThe original LoRA paper showed that `r=4-8` was sufficient for many NLU tasks. However, for more complex tasks like instruction-following, practitioners have found that `r=16-64` yields better results.\n\n### RSLoRA Benefit\nRank-Stabilized LoRA (`use_rslora=True`) adjusts the scaling factor to `lora_alpha/sqrt(r)`, which:\n- Prevents gradient explosion at high ranks\n- Allows effective use of larger rank values\n- Maintains stable training dynamics across rank choices\n\n== Code Evidence ==\n\nDefault rank value from `config.py:459`:\n<syntaxhighlight lang=\"python\">\nr: int = field(default=8, metadata={\"help\": \"Lora attention dimension\"})\n</syntaxhighlight>\n\nRSLoRA documentation from `config.py:488-498`:\n<syntaxhighlight lang=\"python\">\nuse_rslora: bool = field(\n    default=False,\n    metadata={\n        \"help\": (\n            \"When set to True, uses [Rank-Stabilized LoRA](https://huggingface.co/papers/2312.03732)\"\n            \" which sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it\"\n            \" was proven to work better. Otherwise, it will use the original default\"\n            \" value of `lora_alpha/r`.\"\n        )\n    },\n)\n</syntaxhighlight>\n\nRank pattern support from `config.py:548-556`:\n<syntaxhighlight lang=\"python\">\nrank_pattern: Optional[dict] = field(\n    default_factory=dict,\n    metadata={\n        \"help\": (\n            \"The mapping from layer names or regexp expression to ranks which are different from the default rank specified by `r`. \"\n            \"For example, `{'^model.decoder.layers.0.encoder_attn.k_proj': 16}`.\"\n        )\n    },\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[uses_heuristic::Implementation:huggingface_peft_LoraConfig_init]]\n* [[uses_heuristic::Principle:huggingface_peft_LoRA_Configuration]]\n* [[uses_heuristic::Workflow:huggingface_peft_LoRA_Fine_Tuning]]\n* [[uses_heuristic::Workflow:huggingface_peft_QLoRA_Training]]\n",
      "domains": [
        "LLMs",
        "Optimization",
        "LoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "LoRA Paper",
          "url": "https://huggingface.co/papers/2106.09685"
        },
        {
          "type": "Paper",
          "title": "RSLoRA Paper",
          "url": "https://huggingface.co/papers/2312.03732"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_LoraConfig_init"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "huggingface_peft_LoRA_Configuration"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "huggingface_peft_LoRA_Fine_Tuning"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "huggingface_peft_QLoRA_Training"
        }
      ]
    },
    {
      "id": "Heuristic/huggingface_peft_Quantized_Merge_Warning",
      "page_title": "huggingface peft Quantized Merge Warning",
      "page_type": "Heuristic",
      "overview": "Warning: Merging LoRA adapters into 4-bit or 8-bit quantized models may produce different outputs due to rounding errors.",
      "content": "# Heuristic: huggingface_peft_Quantized_Merge_Warning\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Discussion|bitsandbytes issues|https://github.com/TimDettmers/bitsandbytes/issues]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::Debugging]], [[domain::QLoRA]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nWarning: Merging LoRA adapters into 4-bit or 8-bit quantized models may produce different outputs due to rounding errors.\n\n=== Description ===\nWhen merging LoRA adapters into quantized base models (4-bit or 8-bit via bitsandbytes), the dequantization \u2192 merge \u2192 requantization process introduces rounding errors. This is a known limitation that can cause minor discrepancies between merged model outputs and adapter-based inference outputs.\n\n=== Usage ===\nBe aware of this heuristic when:\n- Calling `model.merge_and_unload()` on a quantized model\n- Calling `merge()` on 4-bit or 8-bit LoRA layers\n- Exporting merged weights from a QLoRA-trained model\n\n== The Insight (Rule of Thumb) ==\n\n* **Warning:** Merging into quantized models will produce warnings like:\n  * \"Merge lora module to 4-bit linear may get different generations due to rounding errors.\"\n  * \"Merge lora module to 8-bit linear may get different generations due to rounding errors.\"\n\n* **Recommendation:**\n  1. **For inference:** Keep adapters separate and use `model.set_adapter()` - no rounding errors\n  2. **For export:** If you must merge, verify outputs on validation set before deploying\n  3. **For production:** Consider training with full-precision base model if merge quality is critical\n\n* **Safe Merge Option:**\n  * Use `safe_merge=True` to check for NaNs before completing merge\n  * This catches catastrophic failures but not subtle rounding errors\n\n* **Trade-off:** Merged models are simpler to deploy but may have slight quality degradation\n\n== Reasoning ==\n\n### Why Rounding Errors Occur\n1. **Dequantization:** 4-bit/8-bit weights are converted to float16/float32\n2. **LoRA Addition:** Delta weights are added: `W_new = W_dequantized + \u0394W`\n3. **Requantization:** Result is quantized back to 4-bit/8-bit\n\nEach quantization step introduces error because not all float values can be represented exactly in reduced precision.\n\n### Empirical Observations\nIn practice, the errors are usually small enough to be acceptable for most use cases. However:\n- 4-bit merges have more error than 8-bit merges\n- Longer sequences may accumulate more error\n- Some model architectures are more sensitive than others\n\n### Tim Dettmers' Note\nThe bitsandbytes author notes that 4-bit operations require defensive cloning to avoid backprop issues, suggesting the precision limitations are fundamental to the approach.\n\n== Code Evidence ==\n\n4-bit merge warning from `bnb.py:397-399`:\n<syntaxhighlight lang=\"python\">\nwarnings.warn(\n    \"Merge lora module to 4-bit linear may get different generations due to rounding errors.\"\n)\n</syntaxhighlight>\n\n8-bit merge warning from `bnb.py:110-112`:\n<syntaxhighlight lang=\"python\">\nwarnings.warn(\n    \"Merge lora module to 8-bit linear may get different generations due to rounding errors.\"\n)\n</syntaxhighlight>\n\nSafe merge NaN check from `bnb.py:411-414`:\n<syntaxhighlight lang=\"python\">\nif safe_merge and not torch.isfinite(w_data).all():\n    raise ValueError(\n        f\"NaNs detected in the merged weights. The adapter {active_adapter} seems to be broken\"\n    )\n</syntaxhighlight>\n\nDefensive clone for 4-bit from `bnb.py:548-553`:\n<syntaxhighlight lang=\"python\">\n# As per Tim Dettmers, for 4bit, we need to defensively clone here.\n# The reason is that in some cases, an error can occur that backprop\n# does not work on a manipulated view. This issue may be solved with\n# newer PyTorch versions but this would need extensive testing to be\n# sure.\nresult = result.clone()\n</syntaxhighlight>\n\n== Related Pages ==\n* [[uses_heuristic::Implementation:huggingface_peft_merge_and_unload]]\n* [[uses_heuristic::Implementation:huggingface_peft_BitsAndBytesConfig_4bit]]\n* [[uses_heuristic::Workflow:huggingface_peft_QLoRA_Training]]\n",
      "domains": [
        "Quantization",
        "Debugging",
        "QLoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Discussion",
          "title": "bitsandbytes issues",
          "url": "https://github.com/TimDettmers/bitsandbytes/issues"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_merge_and_unload"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_BitsAndBytesConfig_4bit"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "huggingface_peft_QLoRA_Training"
        }
      ]
    },
    {
      "id": "Heuristic/huggingface_peft_Target_Module_Selection",
      "page_title": "huggingface peft Target Module Selection",
      "page_type": "Heuristic",
      "overview": "Guidelines for selecting which modules to apply LoRA adapters using `target_modules` configuration.",
      "content": "# Heuristic: huggingface_peft_Target_Module_Selection\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|HuggingFace PEFT|https://github.com/huggingface/peft]]\n* [[source::Paper|LoRA Paper|https://huggingface.co/papers/2106.09685]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Optimization]], [[domain::LoRA]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 00:00 GMT]]\n|}\n\n== Overview ==\nGuidelines for selecting which modules to apply LoRA adapters using `target_modules` configuration.\n\n=== Description ===\nThe `target_modules` parameter in `LoraConfig` determines which layers receive LoRA adapters. Choosing the right target modules is crucial for balancing adaptation quality with parameter efficiency. PEFT supports automatic selection, wildcard patterns, and explicit module lists.\n\n=== Usage ===\nUse this heuristic when configuring `LoraConfig.target_modules` to decide which layers should be adapted. Consider this when:\n- Starting LoRA fine-tuning on a new model architecture\n- Optimizing for memory by targeting fewer modules\n- Maximizing quality by targeting more modules\n\n== The Insight (Rule of Thumb) ==\n\n### Strategy Options\n\n* **Quick Start (`\"all-linear\"`):**\n  * Targets all linear layers (excluding output layer for PreTrainedModel)\n  * Most comprehensive but highest memory usage\n  * Good for maximum quality when memory allows\n  * Action: `target_modules=\"all-linear\"`\n\n* **Attention-Only (Classic LoRA):**\n  * Target only Q and V projections (original LoRA paper recommendation)\n  * Good balance of quality and efficiency\n  * Action: `target_modules=[\"q_proj\", \"v_proj\"]`\n\n* **Full Attention:**\n  * Target all attention projections (Q, K, V, O)\n  * Better than attention-only for most tasks\n  * Action: `target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`\n\n* **Attention + MLP:**\n  * Target attention and MLP layers\n  * Best quality, moderate memory\n  * Action: `target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]`\n\n### Model-Specific Conventions\n\n* **Llama/Mistral:** `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`\n* **GPT-2/GPT-J:** `c_attn`, `c_proj`, `c_fc`\n* **BERT:** `query`, `key`, `value`, `dense`\n* **T5:** `q`, `k`, `v`, `o`, `wi`, `wo`\n\n* **Trade-off:** More modules = more parameters = better quality but higher memory\n\n* **Exclusion:** Use `exclude_modules` to skip specific patterns from `all-linear`\n\n== Reasoning ==\n\n### Original LoRA Findings\nThe original LoRA paper experimented with adapting different subsets of weight matrices and found that adapting attention matrices (especially Q and V) provided most of the benefit. However, recent practice shows that adapting more layers often improves results.\n\n### Empirical Best Practices\nFor instruction-following and complex tasks, targeting all linear layers or at least full attention + MLP layers typically yields better results than Q/V only. The `all-linear` shortcut makes this easy.\n\n### Automatic Detection\nWhen `target_modules=None`, PEFT attempts to automatically detect appropriate modules based on model architecture. This works for known architectures but may fail for custom models.\n\n== Code Evidence ==\n\n`all-linear` support from `config.py:460-472`:\n<syntaxhighlight lang=\"python\">\ntarget_modules: Optional[Union[list[str], str]] = field(\n    default=None,\n    metadata={\n        \"help\": (\n            \"List of module names or regex expression of the module names to replace with LoRA. \"\n            \"For example, ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$'. \"\n            \"This can also be a wildcard 'all-linear' which matches all linear/Conv1D \"\n            \"(if the model is a PreTrainedModel, the output layer excluded). \"\n            \"If not specified, modules will be chosen according to the model architecture, If the architecture is \"\n            \"not known, an error will be raised -- in this case, you should specify the target modules manually.\"\n        ),\n    },\n)\n</syntaxhighlight>\n\nModule exclusion from `config.py:475-478`:\n<syntaxhighlight lang=\"python\">\nexclude_modules: Optional[Union[list[str], str]] = field(\n    default=None,\n    metadata={\"help\": \"List of module names or regex expression of the module names to exclude from Lora.\"},\n)\n</syntaxhighlight>\n\nArchitecture-specific defaults from model configs in transformers integration:\n<syntaxhighlight lang=\"python\">\n# Example for Llama (detected automatically)\ntarget_modules = [\"q_proj\", \"v_proj\"]  # Minimal default\n# OR with all-linear:\ntarget_modules = \"all-linear\"  # All linear layers\n</syntaxhighlight>\n\n== Related Pages ==\n* [[uses_heuristic::Implementation:huggingface_peft_LoraConfig_init]]\n* [[uses_heuristic::Principle:huggingface_peft_LoRA_Configuration]]\n* [[uses_heuristic::Workflow:huggingface_peft_LoRA_Fine_Tuning]]\n",
      "domains": [
        "LLMs",
        "Optimization",
        "LoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "HuggingFace PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Paper",
          "title": "LoRA Paper",
          "url": "https://huggingface.co/papers/2106.09685"
        }
      ],
      "last_updated": "2024-12-18 00:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_LoraConfig_init"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Principle",
          "target_id": "huggingface_peft_LoRA_Configuration"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Workflow",
          "target_id": "huggingface_peft_LoRA_Fine_Tuning"
        }
      ]
    },
    {
      "id": "Heuristic/huggingface_peft_Warning_Deprecated_Bone",
      "page_title": "huggingface peft Warning Deprecated Bone",
      "page_type": "Heuristic",
      "overview": "Deprecation warning for Bone (Block Affine) adapter - will be removed in PEFT v0.19.0, migrate to MiSS.",
      "content": "{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|PEFT|https://github.com/huggingface/peft]]\n* [[source::Doc|PEFT Docs|https://huggingface.co/docs/peft]]\n|-\n! Domains\n| [[domain::PEFT]], [[domain::Deprecation]]\n|-\n! Last Updated\n| [[last_updated::2024-12-18 14:00 GMT]]\n|}\n\n== Overview ==\nDeprecation warning for Bone (Block Affine) adapter - will be removed in PEFT v0.19.0, migrate to MiSS.\n\n=== Description ===\nThe Bone (Block Affine) adapter implementation is deprecated and scheduled for removal in PEFT version 0.19.0. The Bone architecture has been superseded by MiSS (Mixed-rank Sparse Subspace), which provides equivalent functionality with improved performance characteristics.\n\nBone uses block-affine transformations based on Householder reflections. MiSS is the evolution of this approach with better rank handling and sparsity patterns.\n\n=== Usage ===\nApply this heuristic when you encounter:\n* Existing codebases using `BoneConfig` or `BoneModel`\n* New projects considering Bone as an adapter choice\n* Migration planning for PEFT version upgrades\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Replace `BoneConfig` with `MissConfig` in new code\n* **Migration:** Use the provided script `/scripts/convert-bone-to-miss.py` to convert existing Bone checkpoints to MiSS format\n* **Timeline:** Bone will be removed in PEFT v0.19.0\n* **Alternative:** `MissConfig` provides equivalent functionality with the same hyperparameters\n\n== Reasoning ==\nMiSS (Mixed-rank Sparse Subspace) is a generalization of Bone that:\n1. Supports mixed rank configurations across different layers\n2. Provides better memory efficiency\n3. Has improved compatibility with quantization backends\n4. Is actively maintained while Bone is in deprecation\n\nThe conversion script handles:\n* Weight tensor format conversion\n* Config parameter mapping\n* Checkpoint metadata updates\n\n== Related Pages ==\n* [[uses_heuristic::Implementation:huggingface_peft_BoneConfig]]\n* [[uses_heuristic::Implementation:huggingface_peft_BoneModel]]\n* [[uses_heuristic::Implementation:huggingface_peft_BoneLayer]]\n* [[uses_heuristic::Implementation:huggingface_peft_MissConfig]]\n",
      "domains": [
        "PEFT",
        "Deprecation"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "PEFT",
          "url": "https://github.com/huggingface/peft"
        },
        {
          "type": "Doc",
          "title": "PEFT Docs",
          "url": "https://huggingface.co/docs/peft"
        }
      ],
      "last_updated": "2024-12-18 14:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_BoneConfig"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_BoneModel"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_BoneLayer"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Implementation",
          "target_id": "huggingface_peft_MissConfig"
        }
      ]
    }
  ],
  "metadata": {
    "collection": "KGWikiPages",
    "embedding_model": "text-embedding-3-large"
  }
}