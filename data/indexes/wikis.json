{
  "pages": [
    {
      "id": "Workflow/Unslothai_Unsloth_CLI_Finetuning",
      "page_title": "Unslothai Unsloth CLI Finetuning",
      "page_type": "Workflow",
      "overview": "Command-line interface workflow for fine-tuning language models using Unsloth's optimized pipeline with argparse-based configuration.",
      "content": "# Workflow: CLI_Finetuning\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Documentation|https://docs.unsloth.ai]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Fine_Tuning]], [[domain::CLI]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 17:00 GMT]]\n|}\n\n== Overview ==\nCommand-line interface workflow for fine-tuning language models using Unsloth's optimized pipeline with argparse-based configuration.\n\n=== Description ===\nThis workflow provides a complete CLI-driven approach to fine-tuning large language models. It wraps the QLoRA fine-tuning pipeline into a standalone Python script (`unsloth-cli.py`) that accepts command-line arguments for all configuration options. The CLI integrates RawTextDataLoader for processing local text files and supports both HuggingFace datasets and ModelScope datasets.\n\nKey features:\n* Argparse-based configuration for all training parameters\n* Smart dataset loading with automatic format detection\n* Support for raw text files (.txt, .md, .json, .jsonl)\n* Optional GGUF export and Hub push\n* Distributed training support\n\n=== Usage ===\nExecute this workflow when you want to:\n* Fine-tune models from the command line without writing Python code\n* Process local text files directly without manual preprocessing\n* Run training jobs in automated pipelines or CI/CD systems\n* Quickly experiment with different configurations via CLI flags\n\n== Execution Steps ==\n\n=== Step 1: Model Loading ===\n[[step::Principle:Unslothai_Unsloth_Model_Loading]]\n\nInitialize the language model from HuggingFace Hub or a local path with 4-bit quantization. The CLI accepts `--model_name`, `--max_seq_length`, `--dtype`, and `--load_in_4bit` flags to configure loading.\n\n'''Key CLI arguments:'''\n* `--model_name \"unsloth/llama-3-8b\"` - Model identifier\n* `--max_seq_length 2048` - Maximum sequence length\n* `--load_in_4bit` - Enable 4-bit quantization\n\n=== Step 2: LoRA Configuration ===\n[[step::Principle:Unslothai_Unsloth_LoRA_Configuration]]\n\nConfigure LoRA adapters with customizable rank and target modules. All PEFT parameters are exposed as CLI arguments.\n\n'''Key CLI arguments:'''\n* `--r 16` - LoRA rank\n* `--lora_alpha 16` - LoRA alpha scaling\n* `--lora_dropout 0.0` - Dropout rate\n* `--use_rslora` - Enable rank-stabilized LoRA\n* `--use_gradient_checkpointing \"unsloth\"` - Memory optimization\n\n=== Step 3: Dataset Loading ===\n[[step::Principle:Unslothai_Unsloth_CLI_Data_Loading]]\n\nSmart dataset loading that automatically detects data format:\n* Local text files (.txt, .md, .json, .jsonl) use RawTextDataLoader\n* HuggingFace datasets use standard `load_dataset`\n* ModelScope datasets supported via environment variable\n\n'''Key CLI arguments:'''\n* `--dataset \"yahma/alpaca-cleaned\"` - Dataset path or HF identifier\n* `--raw_text_file path/to/file.txt` - Direct path to raw text\n* `--chunk_size` - Chunk size for raw text processing\n* `--stride` - Stride for overlapping chunks\n\n=== Step 4: Training Configuration ===\n[[step::Principle:Unslothai_Unsloth_Training_Configuration]]\n\nConfigure SFTConfig with all standard training hyperparameters exposed via CLI.\n\n'''Key CLI arguments:'''\n* `--per_device_train_batch_size 4` - Batch size per GPU\n* `--gradient_accumulation_steps 8` - Gradient accumulation\n* `--learning_rate 2e-6` - Learning rate\n* `--max_steps 400` - Maximum training steps\n* `--warmup_steps 5` - Warmup steps\n* `--optim \"adamw_8bit\"` - Optimizer\n* `--packing` - Enable sample packing\n\n=== Step 5: Training Execution ===\n[[step::Principle:Unslothai_Unsloth_Supervised_Finetuning]]\n\nExecute SFTTrainer with the configured parameters. Supports distributed training via device map auto-detection.\n\n=== Step 6: Model Export ===\n[[step::Principle:Unslothai_Unsloth_Model_Saving]]\n\nSave the trained model in various formats with optional Hub push.\n\n'''Key CLI arguments:'''\n* `--save_model` - Enable saving\n* `--save_path \"model\"` - Save directory\n* `--save_gguf` - Export to GGUF format\n* `--quantization \"q4_k_m\"` - GGUF quantization method\n* `--push_model` - Push to HuggingFace Hub\n* `--hub_path \"username/model\"` - Hub repository\n* `--hub_token` - HuggingFace token\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Parse CLI Arguments] --> B[Load Model with 4-bit Quantization]\n    B --> C[Configure LoRA Adapters]\n    C --> D[Smart Dataset Loading]\n    D --> E[Configure SFTTrainer]\n    E --> F[Execute Training]\n    F --> G{Save Options}\n    G -->|GGUF| H[Export GGUF]\n    G -->|Merged| I[Save Merged Weights]\n    H --> J{Push to Hub?}\n    I --> J\n    J -->|Yes| K[Upload to HuggingFace]\n    J -->|No| L[Done]\n    K --> L\n}}\n\n== Related Pages ==\n* [[step::Principle:Unslothai_Unsloth_Model_Loading]]\n* [[step::Principle:Unslothai_Unsloth_LoRA_Configuration]]\n* [[step::Principle:Unslothai_Unsloth_Training_Configuration]]\n* [[step::Principle:Unslothai_Unsloth_Supervised_Finetuning]]\n* [[step::Principle:Unslothai_Unsloth_Model_Saving]]\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Gradient_Checkpointing_Tip]]\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Sample_Packing_Tip]]\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_LoRA_Rank_Selection_Tip]]\n",
      "domains": [
        "LLMs",
        "Fine Tuning",
        "CLI"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Documentation",
          "url": "https://docs.unsloth.ai"
        }
      ],
      "last_updated": "2026-01-09 17:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_CLI_Data_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Training_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Supervised_Finetuning"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Saving"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Training_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Supervised_Finetuning"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Saving"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Gradient_Checkpointing_Tip"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Sample_Packing_Tip"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_LoRA_Rank_Selection_Tip"
        }
      ]
    },
    {
      "id": "Workflow/Unslothai_Unsloth_GGUF_Export",
      "page_title": "Unslothai Unsloth GGUF Export",
      "page_type": "Workflow",
      "overview": "End-to-end process for exporting fine-tuned models to GGUF format for deployment with llama.cpp, Ollama, and other inference engines.",
      "content": "# Workflow: GGUF_Export\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Saving to GGUF|https://unsloth.ai/docs/basics/inference-and-deployment/saving-to-gguf]]\n* [[source::Doc|Deployment Guide|https://unsloth.ai/docs/basics/inference-and-deployment]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Deployment]], [[domain::GGUF]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for exporting fine-tuned models to GGUF format for deployment with llama.cpp, Ollama, and other inference engines.\n\n=== Description ===\nThis workflow converts fine-tuned Unsloth models to GGUF (GPT-Generated Unified Format) for efficient CPU and edge deployment. GGUF is the standard format used by llama.cpp, Ollama, LM Studio, and similar inference engines.\n\nThe workflow supports:\n1. **LoRA Merging**: Merging trained LoRA adapters with base model weights\n2. **GGUF Conversion**: Converting merged weights to GGUF format\n3. **Quantization**: Applying various quantization methods (Q4_K_M, Q5_K_M, Q8_0, etc.)\n4. **Ollama Integration**: Automatic Modelfile generation and local deployment\n\nUnsloth handles the entire pipeline automatically, including:\n* Building llama.cpp if not present\n* Fixing tokenizer compatibility issues (especially for SentencePiece models)\n* Generating appropriate chat templates for Ollama\n\n=== Usage ===\nExecute this workflow when:\n* You need to deploy a fine-tuned model for CPU inference\n* You want to use the model with Ollama, LM Studio, or llama.cpp\n* You need a compact, quantized model for edge deployment\n* You want to share your model in a widely-compatible format\n\n'''Input requirements:'''\n* Fine-tuned model (LoRA adapter or merged weights)\n* Base model for merging (if using LoRA)\n* Sufficient disk space for conversion (2x model size)\n\n'''Expected outputs:'''\n* GGUF file(s) in specified quantization format(s)\n* Optionally: Ollama Modelfile for local deployment\n* Model ready for llama.cpp/Ollama inference\n\n== Execution Steps ==\n\n=== Step 1: Model Preparation ===\n[[step::Principle:Unslothai_Unsloth_Model_Preparation]]\n\nEnsure the model is ready for GGUF export. If you have a LoRA adapter, it will be merged with the base model during the export process.\n\n'''Key considerations:'''\n* GGUF export works directly from trained Unsloth models\n* LoRA adapters are automatically merged during conversion\n* Ensure you have the same base model that was used for training\n* Full fine-tuned models can also be exported\n\n=== Step 2: Quantization Method Selection ===\n[[step::Principle:Unslothai_Unsloth_Quantization_Selection]]\n\nChoose the appropriate quantization method based on your deployment requirements. Unsloth supports all standard llama.cpp quantization formats.\n\n'''Available quantization methods:'''\n| Method | Description | Use Case |\n|--------|-------------|----------|\n| `f16` | 16-bit float, no quantization | Highest accuracy, large files |\n| `q8_0` | 8-bit quantization | Good accuracy, moderate size |\n| `q4_k_m` | 4-bit with K-quants (recommended) | Best balance of size/quality |\n| `q5_k_m` | 5-bit with K-quants | Better quality than Q4 |\n| `q2_k` | 2-bit quantization | Smallest size, lower quality |\n\n'''Key considerations:'''\n* `q4_k_m` is recommended for most use cases\n* Use `q8_0` when accuracy is critical\n* `f16` or `bf16` for maximum quality (no quantization)\n* Can export multiple quantizations in one call\n\n=== Step 3: GGUF Export Execution ===\n[[step::Principle:Unslothai_Unsloth_GGUF_Export]]\n\nExecute the GGUF export using `model.save_pretrained_gguf()`. This handles LoRA merging, conversion, and quantization automatically.\n\n'''Key considerations:'''\n* First export may take longer as llama.cpp is built\n* Tokenizer fixes are applied automatically\n* Export creates files in the specified directory\n* Multiple quantization formats can be specified as a list\n\n=== Step 4: Ollama Modelfile Generation ===\n[[step::Principle:Unslothai_Unsloth_Ollama_Modelfile]]\n\nOptionally generate an Ollama Modelfile for easy local deployment. Unsloth automatically detects the appropriate chat template.\n\n'''Key considerations:'''\n* Ollama templates are auto-mapped from HuggingFace templates\n* System prompts can be customized in the Modelfile\n* Use `ollama create` to register the model locally\n* Test with `ollama run` before deployment\n\n=== Step 5: Hub Upload ===\n[[step::Principle:Unslothai_Unsloth_GGUF_Hub_Upload]]\n\nOptionally upload GGUF files to Hugging Face Hub using `model.push_to_hub_gguf()` for sharing and deployment.\n\n'''Key considerations:'''\n* Requires Hugging Face authentication token\n* Creates a new repository or updates existing\n* Includes quantization method in model card\n* Can push multiple quantizations to same repo\n\n=== Step 6: Deployment Verification ===\n[[step::Principle:Unslothai_Unsloth_GGUF_Verification]]\n\nVerify the exported GGUF model works correctly with your target inference engine.\n\n'''Verification steps:'''\n1. Test with llama-cli for basic inference\n2. Test with Ollama for chat capabilities\n3. Compare outputs with original model\n4. Benchmark inference speed and memory usage\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Model Preparation] --> B[Quantization Method Selection]\n    B --> C[GGUF Export Execution]\n    C --> D{Ollama Deployment?}\n    D -->|Yes| E[Modelfile Generation]\n    D -->|No| F{Hub Upload?}\n    E --> F\n    F -->|Yes| G[Push to Hub]\n    F -->|No| H[Deployment Verification]\n    G --> H\n    H --> I[llama.cpp/Ollama Ready]\n}}\n\n== Related Pages ==\n* [[step::Principle:Unslothai_Unsloth_Model_Preparation]]\n* [[step::Principle:Unslothai_Unsloth_Quantization_Selection]]\n* [[step::Principle:Unslothai_Unsloth_GGUF_Export]]\n* [[step::Principle:Unslothai_Unsloth_Ollama_Modelfile]]\n* [[step::Principle:Unslothai_Unsloth_GGUF_Hub_Upload]]\n* [[step::Principle:Unslothai_Unsloth_GGUF_Verification]]\n",
      "domains": [
        "LLMs",
        "Deployment",
        "GGUF",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Saving to GGUF",
          "url": "https://unsloth.ai/docs/basics/inference-and-deployment/saving-to-gguf"
        },
        {
          "type": "Doc",
          "title": "Deployment Guide",
          "url": "https://unsloth.ai/docs/basics/inference-and-deployment"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Quantization_Selection"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Export"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Ollama_Modelfile"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Hub_Upload"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Verification"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Quantization_Selection"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Export"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Ollama_Modelfile"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Hub_Upload"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Verification"
        }
      ]
    },
    {
      "id": "Workflow/Unslothai_Unsloth_GRPO_Training",
      "page_title": "Unslothai Unsloth GRPO Training",
      "page_type": "Workflow",
      "overview": "End-to-end process for training reasoning models using Group Relative Policy Optimization (GRPO) with Unsloth's memory-efficient reinforcement learning pipeline.",
      "content": "# Workflow: GRPO_Training\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|RL Guide|https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide]]\n* [[source::Doc|GRPO Documentation|https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide#training-with-grpo]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Reinforcement_Learning]], [[domain::GRPO]], [[domain::Reasoning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for training reasoning models using Group Relative Policy Optimization (GRPO) with Unsloth's memory-efficient reinforcement learning pipeline.\n\n=== Description ===\nThis workflow implements reinforcement learning fine-tuning using GRPO (Group Relative Policy Optimization), a technique that enables training reasoning models to solve complex problems like mathematics, coding, and logical reasoning. GRPO is more memory-efficient than PPO and produces models that can show their reasoning process.\n\nThe workflow typically follows a two-stage approach:\n1. **Optional SFT Stage**: Pre-train with supervised fine-tuning on reasoning examples\n2. **GRPO Stage**: Reinforce correct reasoning patterns using reward functions\n\nUnsloth's GRPO implementation achieves 80% less VRAM usage compared to standard implementations by:\n* Using vLLM for fast batch generation during rollouts\n* Memory-efficient reward computation\n* Optimized gradient accumulation for RL training\n\nThis enables training reasoning models on consumer GPUs with as little as 5GB VRAM.\n\n=== Usage ===\nExecute this workflow when:\n* You want to train a model to solve reasoning tasks (math, coding, logic)\n* You need the model to show step-by-step reasoning in its outputs\n* You have verifiable reward signals (correct/incorrect answers)\n* You want to improve model capabilities beyond what SFT alone achieves\n\n'''Input requirements:'''\n* Base model (optionally pre-trained with SFT)\n* Dataset with problems and verifiable answers (e.g., GSM8K, MATH)\n* Reward functions that evaluate answer correctness and format compliance\n* CUDA-capable GPU with 8GB+ VRAM\n\n'''Expected outputs:'''\n* Model that produces structured reasoning traces\n* Improved accuracy on reasoning benchmarks\n* LoRA adapter or merged model for deployment\n\n== Execution Steps ==\n\n=== Step 1: Model Loading with vLLM ===\n[[step::Principle:Unslothai_Unsloth_RL_Model_Loading]]\n\nInitialize the language model with vLLM fast inference enabled using `FastLanguageModel.from_pretrained()` with `fast_inference=True`. This enables efficient batch generation required for GRPO rollouts.\n\n'''Key considerations:'''\n* Set `fast_inference=True` to enable vLLM backend for generation\n* Configure `gpu_memory_utilization` to balance training and inference memory\n* Set `max_lora_rank` to accommodate the LoRA configuration\n* Use `load_in_4bit=False` for 16-bit LoRA (recommended for RL)\n\n=== Step 2: RL LoRA Configuration ===\n[[step::Principle:Unslothai_Unsloth_RL_LoRA_Configuration]]\n\nConfigure Low-Rank Adapters for RL training. GRPO requires trainable parameters for policy updates. Use higher ranks for complex reasoning tasks.\n\n'''Key considerations:'''\n* Use rank 64-128 for reasoning tasks (higher than typical SFT)\n* Enable `use_gradient_checkpointing=\"unsloth\"` for memory efficiency\n* Target all linear layers for comprehensive adaptation\n* Consider the memory trade-off between rank and batch size\n\n=== Step 3: Chat Template Configuration ===\n[[step::Principle:Unslothai_Unsloth_Chat_Template_Configuration]]\n\nConfigure chat templates to establish consistent formatting for reasoning traces. Define special tokens for reasoning boundaries (e.g., `<reasoning>`, `<answer>`).\n\n'''Key considerations:'''\n* Use `get_chat_template()` to apply model-specific formatting\n* Define clear boundary tokens for reasoning and answer sections\n* Ensure the model can generate properly formatted outputs\n* Consider using system prompts to guide reasoning behavior\n\n=== Step 4: Dataset Preparation ===\n[[step::Principle:Unslothai_Unsloth_RL_Dataset_Preparation]]\n\nPrepare the training dataset with prompts that include problems and expected answer formats. Each example needs a verifiable \"answer\" field for reward computation.\n\n'''Key considerations:'''\n* Format prompts with clear problem statements\n* Include expected answer format in system prompt or instructions\n* Ensure answers are extractable for reward computation\n* Split data appropriately for multi-stage training\n\n=== Step 5: Reward Function Definition ===\n[[step::Principle:Unslothai_Unsloth_Reward_Functions]]\n\nDefine reward functions that evaluate model completions. GRPO supports multiple reward functions that are combined during training.\n\n'''Typical reward functions:'''\n* **Format compliance**: Check if output follows expected structure (reasoning tags, answer tags)\n* **Answer correctness**: Verify extracted answer matches ground truth\n* **Numerical accuracy**: Score based on how close numerical answers are\n\n'''Key considerations:'''\n* Reward functions receive completions in message format\n* Return list of scores corresponding to each completion\n* Balance positive and negative rewards for stable training\n* Consider partial credit for approximately correct answers\n\n=== Step 6: Optional SFT Pre-training ===\n[[step::Principle:Unslothai_Unsloth_SFT_Pretraining]]\n\nOptionally pre-train the model with supervised fine-tuning on reasoning examples before GRPO. This warm-starts the policy to produce well-formatted outputs.\n\n'''Key considerations:'''\n* Use high-quality reasoning examples (e.g., LIMO dataset)\n* Train until format compliance is reasonable\n* Use `train_on_responses_only()` to focus on reasoning outputs\n* Save checkpoint before GRPO stage\n\n=== Step 7: GRPO Training Configuration ===\n[[step::Principle:Unslothai_Unsloth_GRPO_Configuration]]\n\nConfigure the GRPO trainer with appropriate hyperparameters for policy optimization. Set generation parameters and training schedule.\n\n'''Key considerations:'''\n* Set `num_generations` for rollout batch size (8-16 typical)\n* Configure `max_prompt_length` and `max_completion_length`\n* Use lower learning rates than SFT (5e-6 typical)\n* Set `max_grad_norm` for gradient clipping (0.1 recommended)\n\n=== Step 8: GRPO Training Execution ===\n[[step::Principle:Unslothai_Unsloth_GRPO_Execution]]\n\nExecute the GRPO training loop using TRL's GRPOTrainer. The trainer handles rollout generation, reward computation, and policy updates.\n\n'''Training loop:'''\n1. Generate multiple completions per prompt using vLLM\n2. Compute rewards for each completion using defined functions\n3. Calculate relative advantages within each group\n4. Update policy using gradient descent on clipped objective\n\n'''Key considerations:'''\n* Monitor reward distribution for training stability\n* Check format compliance rates during training\n* Save checkpoints periodically\n* Evaluate on held-out reasoning benchmarks\n\n=== Step 9: Model Saving and Evaluation ===\n[[step::Principle:Unslothai_Unsloth_Model_Saving]]\n\nSave the trained model and evaluate on reasoning benchmarks. Compare performance against base model and SFT-only baseline.\n\n'''Key considerations:'''\n* Save intermediate checkpoints during training\n* Merge to 16-bit for final deployment\n* Evaluate on AIME, GSM8K, or task-specific benchmarks\n* Test reasoning quality on novel problems\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Model Loading with vLLM] --> B[LoRA Configuration]\n    B --> C[Chat Template Configuration]\n    C --> D[Dataset Preparation]\n    D --> E[Reward Function Definition]\n    E --> F{SFT Pre-training?}\n    F -->|Yes| G[SFT Pre-training]\n    F -->|No| H[GRPO Configuration]\n    G --> H\n    H --> I[GRPO Training Execution]\n    I --> J[Model Saving]\n    J --> K[Evaluation]\n}}\n\n== Related Pages ==\n* [[step::Principle:Unslothai_Unsloth_RL_Model_Loading]]\n* [[step::Principle:Unslothai_Unsloth_RL_LoRA_Configuration]]\n* [[step::Principle:Unslothai_Unsloth_Chat_Template_Configuration]]\n* [[step::Principle:Unslothai_Unsloth_RL_Dataset_Preparation]]\n* [[step::Principle:Unslothai_Unsloth_Reward_Functions]]\n* [[step::Principle:Unslothai_Unsloth_SFT_Pretraining]]\n* [[step::Principle:Unslothai_Unsloth_GRPO_Configuration]]\n* [[step::Principle:Unslothai_Unsloth_GRPO_Execution]]\n* [[step::Principle:Unslothai_Unsloth_Model_Saving]]\n",
      "domains": [
        "LLMs",
        "Reinforcement Learning",
        "GRPO",
        "Reasoning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "RL Guide",
          "url": "https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide"
        },
        {
          "type": "Doc",
          "title": "GRPO Documentation",
          "url": "https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide#training-with-grpo"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Chat_Template_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_Dataset_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Reward_Functions"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_SFT_Pretraining"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GRPO_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GRPO_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Saving"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Chat_Template_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_Dataset_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Reward_Functions"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_SFT_Pretraining"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GRPO_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GRPO_Execution"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Saving"
        }
      ]
    },
    {
      "id": "Workflow/Unslothai_Unsloth_QLoRA_Finetuning",
      "page_title": "Unslothai Unsloth QLoRA Finetuning",
      "page_type": "Workflow",
      "overview": "End-to-end process for parameter-efficient fine-tuning of Large Language Models using QLoRA (Quantized Low-Rank Adaptation) with Unsloth's optimized training pipeline.",
      "content": "# Workflow: QLoRA_Finetuning\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Docs|https://unsloth.ai/docs]]\n* [[source::Doc|Fine-tuning Guide|https://unsloth.ai/docs/get-started/fine-tuning-llms-guide]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Fine_Tuning]], [[domain::QLoRA]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for parameter-efficient fine-tuning of Large Language Models using QLoRA (Quantized Low-Rank Adaptation) with Unsloth's optimized training pipeline.\n\n=== Description ===\nThis workflow outlines the standard procedure for fine-tuning Large Language Models (LLMs) on consumer hardware using Unsloth's optimized QLoRA implementation. The process leverages 4-bit NormalFloat quantization to reduce memory requirements by up to 70%, allowing training of 7B+ parameter models on single GPUs with less than 24GB VRAM.\n\nThe workflow covers:\n1. **Model Loading**: Loading pre-quantized base models with Unsloth's memory-optimized loader\n2. **LoRA Configuration**: Injecting low-rank adapter matrices into attention and MLP layers\n3. **Data Preparation**: Formatting datasets with appropriate chat templates for instruction tuning\n4. **Training**: Supervised fine-tuning using TRL's SFTTrainer with Unsloth optimizations\n5. **Model Export**: Saving merged weights or LoRA adapters for deployment\n\nUnsloth achieves 2x faster training through custom Triton kernels for cross-entropy loss, RMS normalization, and fused LoRA operations.\n\n=== Usage ===\nExecute this workflow when:\n* You have a domain-specific dataset (instruction-tuning, conversational, or task-specific format)\n* You need to adapt a base LLM to follow specific instructions or exhibit specialized behavior\n* You have limited GPU resources (8GB-24GB VRAM) and cannot afford full fine-tuning\n* You want to preserve the base model's general capabilities while adding specialized knowledge\n\n'''Input requirements:'''\n* Pre-trained base model (LLaMA, Mistral, Qwen, Gemma, etc.)\n* Training dataset in conversational or instruction format\n* CUDA-capable GPU with 8GB+ VRAM\n\n'''Expected outputs:'''\n* Trained LoRA adapter weights (can be merged with base model)\n* Optionally: Merged 16-bit model for deployment\n* Optionally: GGUF file for llama.cpp/Ollama deployment\n\n== Execution Steps ==\n\n=== Step 1: Model Loading ===\n[[step::Principle:Unslothai_Unsloth_Model_Loading]]\n\nInitialize the language model with memory-optimized settings using `FastLanguageModel.from_pretrained()`. The loader automatically applies 4-bit NormalFloat quantization with double quantization for additional memory savings. The attention layers are patched for efficient training on consumer GPUs.\n\n'''Key considerations:'''\n* Set `load_in_4bit=True` for QLoRA training (default)\n* Choose `max_seq_length` based on your training data and available VRAM\n* Use pre-quantized models from Unsloth Hub for faster loading\n\n=== Step 2: LoRA Configuration ===\n[[step::Principle:Unslothai_Unsloth_LoRA_Configuration]]\n\nConfigure and inject Low-Rank Adapter matrices into the model using `FastLanguageModel.get_peft_model()`. This sets up trainable adapter weights while keeping the base model frozen, dramatically reducing memory requirements and training time.\n\n'''Key considerations:'''\n* Choose rank (r) based on task complexity: 8-16 for simple tasks, 32-128 for complex reasoning\n* Target all linear layers for best results: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n* Enable `use_gradient_checkpointing=\"unsloth\"` for 30% VRAM reduction\n* Set `lora_alpha` equal to rank for stable training\n\n=== Step 3: Data Formatting ===\n[[step::Principle:Unslothai_Unsloth_Data_Formatting]]\n\nTransform raw training data into the structured prompt format expected by the model. Apply the appropriate chat template to ensure proper tokenization boundaries and special token insertion.\n\n'''Key considerations:'''\n* Use `get_chat_template()` to apply model-specific formatting (llama-3, chatml, etc.)\n* Ensure all examples follow a consistent schema (system/user/assistant roles)\n* For instruction tuning, use `train_on_responses_only()` to mask non-response tokens\n* Validate that special tokens (BOS, EOS, header tokens) are properly inserted\n\n=== Step 4: Training Configuration ===\n[[step::Principle:Unslothai_Unsloth_Training_Configuration]]\n\nConfigure the training loop using TRL's SFTTrainer with Unsloth-optimized settings. Set hyperparameters for batch size, learning rate, and gradient accumulation based on available hardware.\n\n'''Key considerations:'''\n* Use `adamw_8bit` optimizer for memory efficiency\n* Set gradient accumulation to simulate larger batch sizes on limited VRAM\n* Enable `bf16` training on Ampere+ GPUs for best performance\n* Configure warmup steps and learning rate schedule\n\n=== Step 5: Supervised Fine-Tuning ===\n[[step::Principle:Unslothai_Unsloth_Supervised_Finetuning]]\n\nExecute the training loop using SFTTrainer. The trainer handles batching, gradient computation, and optimization while Unsloth's Triton kernels accelerate the forward and backward passes.\n\n'''Key considerations:'''\n* Monitor loss curves for convergence\n* Use `logging_steps` to track training progress\n* Enable checkpointing for long training runs\n* Consider sample packing for datasets with short sequences\n\n=== Step 6: Model Saving ===\n[[step::Principle:Unslothai_Unsloth_Model_Saving]]\n\nSave the trained model using one of several export methods: LoRA adapter only, merged 16-bit weights, or GGUF format for deployment with llama.cpp/Ollama.\n\n'''Key considerations:'''\n* Use `save_pretrained()` for LoRA adapter only (smallest, requires base model for inference)\n* Use `save_pretrained_merged(save_method=\"merged_16bit\")` for standalone deployment\n* Use `save_pretrained_gguf()` for llama.cpp/Ollama deployment\n* Push to Hugging Face Hub with `push_to_hub_merged()` for sharing\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Model Loading] --> B[LoRA Configuration]\n    B --> C[Data Formatting]\n    C --> D[Training Configuration]\n    D --> E[Supervised Fine-Tuning]\n    E --> F[Model Saving]\n    F --> G{Export Format}\n    G -->|LoRA Only| H[LoRA Adapter]\n    G -->|Merged| I[16-bit Model]\n    G -->|GGUF| J[llama.cpp Format]\n}}\n\n== Related Pages ==\n* [[step::Principle:Unslothai_Unsloth_Model_Loading]]\n* [[step::Principle:Unslothai_Unsloth_LoRA_Configuration]]\n* [[step::Principle:Unslothai_Unsloth_Data_Formatting]]\n* [[step::Principle:Unslothai_Unsloth_Training_Configuration]]\n* [[step::Principle:Unslothai_Unsloth_Supervised_Finetuning]]\n* [[step::Principle:Unslothai_Unsloth_Model_Saving]]\n",
      "domains": [
        "LLMs",
        "Fine Tuning",
        "QLoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Docs",
          "url": "https://unsloth.ai/docs"
        },
        {
          "type": "Doc",
          "title": "Fine-tuning Guide",
          "url": "https://unsloth.ai/docs/get-started/fine-tuning-llms-guide"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Data_Formatting"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Training_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Supervised_Finetuning"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Saving"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Data_Formatting"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Training_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Supervised_Finetuning"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Saving"
        }
      ]
    },
    {
      "id": "Workflow/Unslothai_Unsloth_Vision_Finetuning",
      "page_title": "Unslothai Unsloth Vision Finetuning",
      "page_type": "Workflow",
      "overview": "End-to-end process for fine-tuning Vision-Language Models (VLMs) using Unsloth's optimized pipeline for multimodal understanding tasks like OCR, image captioning, and visual question answering.",
      "content": "# Workflow: Vision_Finetuning\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Vision Fine-tuning|https://unsloth.ai/docs/basics/vision-fine-tuning]]\n* [[source::Doc|VLM Guide|https://unsloth.ai/blog/vision]]\n|-\n! Domains\n| [[domain::LLMs]], [[domain::Vision]], [[domain::Multimodal]], [[domain::Fine_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\nEnd-to-end process for fine-tuning Vision-Language Models (VLMs) using Unsloth's optimized pipeline for multimodal understanding tasks like OCR, image captioning, and visual question answering.\n\n=== Description ===\nThis workflow enables fine-tuning of Vision-Language Models (VLMs) such as Qwen2-VL, Llama 3.2 Vision, and Pixtral on custom image-text datasets. Unsloth supports training both the vision encoder and language model components with memory-efficient techniques.\n\nThe workflow covers:\n1. **Vision Model Loading**: Loading VLMs with quantization and vision preprocessing\n2. **Vision LoRA Configuration**: Configuring adapters for both vision and language layers\n3. **Multimodal Data Preparation**: Formatting image-text pairs with proper message structure\n4. **Vision Training**: Fine-tuning using UnslothVisionDataCollator for proper batching\n5. **Model Export**: Saving merged multimodal models for deployment\n\nSupported Vision Models:\n* Qwen2-VL (2B, 7B, 72B)\n* Qwen2.5-VL (all sizes)\n* Llama 3.2 Vision (11B, 90B)\n* Pixtral (12B)\n* DeepSeek-OCR\n\n=== Usage ===\nExecute this workflow when:\n* You need to fine-tune a VLM on custom image-text data\n* Your task involves OCR, document understanding, or visual QA\n* You want to improve image captioning or visual reasoning capabilities\n* You need a specialized multimodal model for domain-specific images\n\n'''Input requirements:'''\n* Vision-Language Model (Qwen2-VL, Llama 3.2 Vision, etc.)\n* Training dataset with images and text conversations\n* CUDA-capable GPU with 16GB+ VRAM (for 7B models)\n\n'''Expected outputs:'''\n* Fine-tuned VLM with improved task-specific performance\n* LoRA adapter or merged model for deployment\n* OCR accuracy improvements on domain-specific documents\n\n== Execution Steps ==\n\n=== Step 1: Vision Model Loading ===\n[[step::Principle:Unslothai_Unsloth_Vision_Model_Loading]]\n\nInitialize the Vision-Language Model using `FastVisionModel.from_pretrained()`. This loads both the vision encoder and language model with appropriate quantization settings.\n\n'''Key considerations:'''\n* Use `FastVisionModel` instead of `FastLanguageModel` for VLMs\n* Set `load_in_4bit=True` for memory-efficient training\n* Choose appropriate `max_seq_length` for your image-text data\n* Vision models require more VRAM than text-only models\n\n=== Step 2: Vision LoRA Configuration ===\n[[step::Principle:Unslothai_Unsloth_Vision_LoRA_Configuration]]\n\nConfigure LoRA adapters for vision model fine-tuning using `FastVisionModel.get_peft_model()`. This allows selective training of vision encoder and/or language model components.\n\n'''Key considerations:'''\n* Set `finetune_vision_layers=True` to train vision encoder\n* Set `finetune_language_layers=True` to train LLM components\n* Use `finetune_attention_modules=True` for attention layer adaptation\n* Use `finetune_mlp_modules=True` for MLP layer adaptation\n* Enable `use_gradient_checkpointing=\"unsloth\"` for memory efficiency\n\n=== Step 3: Multimodal Dataset Preparation ===\n[[step::Principle:Unslothai_Unsloth_Multimodal_Data_Preparation]]\n\nFormat the training dataset with proper multimodal message structure. Each example should include images and text in the OpenAI-compatible message format.\n\n'''Message structure:'''\n```\n{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"...\"}]},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": \"...\"},\n            {\"type\": \"image\", \"image\": <PIL.Image>}\n        ]},\n        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"...\"}]}\n    ]\n}\n```\n\n'''Key considerations:'''\n* Use PIL.Image objects for images (not file paths)\n* Structure content as list of type/text or type/image dicts\n* Include system prompts to guide model behavior\n* Process images using model-specific utilities (e.g., qwen_vl_utils)\n\n=== Step 4: Vision Training Mode ===\n[[step::Principle:Unslothai_Unsloth_Vision_Training_Mode]]\n\nSet the model to training mode and configure the trainer with vision-specific settings using `FastVisionModel.for_training()`.\n\n'''Key considerations:'''\n* Call `FastVisionModel.for_training(model)` before training\n* Disable `use_cache` for training compatibility\n* Use `UnslothVisionDataCollator` for proper batching\n* Set `remove_unused_columns=False` in training config\n\n=== Step 5: Vision Model Training ===\n[[step::Principle:Unslothai_Unsloth_Vision_Training]]\n\nExecute the training loop using SFTTrainer with vision-specific configuration. The UnslothVisionDataCollator handles image preprocessing and batching.\n\n'''Key considerations:'''\n* Use smaller batch sizes than text-only training (2-4)\n* Enable gradient checkpointing with `use_reentrant=False`\n* Set `dataset_kwargs={\"skip_prepare_dataset\": True}`\n* Monitor OCR/visual QA metrics during training\n* Use `dataset_text_field=\"\"` (empty string for vision)\n\n=== Step 6: Vision Model Inference Mode ===\n[[step::Principle:Unslothai_Unsloth_Vision_Inference_Mode]]\n\nSwitch to inference mode for evaluation using `FastVisionModel.for_inference()`. This optimizes the model for generation tasks.\n\n'''Key considerations:'''\n* Call `FastVisionModel.for_inference(model)` before evaluation\n* Enable `use_cache` for efficient generation\n* Use `process_vision_info()` for proper image preprocessing during inference\n\n=== Step 7: Model Saving and Evaluation ===\n[[step::Principle:Unslothai_Unsloth_Vision_Model_Saving]]\n\nSave the trained vision model and evaluate on relevant benchmarks. Vision models support the same export options as text models.\n\n'''Key considerations:'''\n* Save LoRA adapter with `save_pretrained()`\n* Merge with `save_pretrained_merged()` for deployment\n* Evaluate on OCR benchmarks (WER, CER metrics)\n* Test on domain-specific visual QA tasks\n\n== Execution Diagram ==\n{{#mermaid:graph TD\n    A[Vision Model Loading] --> B[Vision LoRA Configuration]\n    B --> C[Multimodal Dataset Preparation]\n    C --> D[Vision Training Mode]\n    D --> E[Vision Model Training]\n    E --> F[Vision Inference Mode]\n    F --> G[Evaluation]\n    G --> H[Model Saving]\n    H --> I{Export Format}\n    I -->|LoRA| J[Vision Adapter]\n    I -->|Merged| K[Full VLM]\n}}\n\n== Related Pages ==\n* [[step::Principle:Unslothai_Unsloth_Vision_Model_Loading]]\n* [[step::Principle:Unslothai_Unsloth_Vision_LoRA_Configuration]]\n* [[step::Principle:Unslothai_Unsloth_Multimodal_Data_Preparation]]\n* [[step::Principle:Unslothai_Unsloth_Vision_Training_Mode]]\n* [[step::Principle:Unslothai_Unsloth_Vision_Training]]\n* [[step::Principle:Unslothai_Unsloth_Vision_Inference_Mode]]\n* [[step::Principle:Unslothai_Unsloth_Vision_Model_Saving]]\n",
      "domains": [
        "LLMs",
        "Vision",
        "Multimodal",
        "Fine Tuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Vision Fine-tuning",
          "url": "https://unsloth.ai/docs/basics/vision-fine-tuning"
        },
        {
          "type": "Doc",
          "title": "VLM Guide",
          "url": "https://unsloth.ai/blog/vision"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Multimodal_Data_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Training_Mode"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Training"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Inference_Mode"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Model_Saving"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Model_Loading"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_LoRA_Configuration"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Multimodal_Data_Preparation"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Training_Mode"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Training"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Inference_Mode"
        },
        {
          "edge_type": "step",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Model_Saving"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_CLI_Data_Loading",
      "page_title": "Unslothai Unsloth CLI Data Loading",
      "page_type": "Principle",
      "overview": "Smart dataset loading mechanism that automatically detects and processes various data formats including raw text files, HuggingFace datasets, and ModelScope datasets.",
      "content": "# Principle: CLI_Data_Loading\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Data_Preparation]], [[domain::CLI]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 17:00 GMT]]\n|}\n\n== Overview ==\nSmart dataset loading mechanism that automatically detects and processes various data formats including raw text files, HuggingFace datasets, and ModelScope datasets.\n\n=== Description ===\nCLI Data Loading is a flexible data ingestion principle that abstracts away format-specific loading logic. It enables users to pass any dataset path to the CLI without needing to know the underlying format. The system:\n\n1. **Format Detection**: Analyzes file extension (.txt, .md, .json, .jsonl) or path type (HuggingFace identifier)\n2. **Automatic Loader Selection**: Routes to RawTextDataLoader for local files or datasets.load_dataset for remote datasets\n3. **ModelScope Support**: Detects UNSLOTH_USE_MODELSCOPE environment variable for Chinese model ecosystem integration\n4. **Alpaca Formatting**: Applies standard instruction/input/output formatting for structured datasets\n\n=== Usage ===\nUse this principle when implementing CLI tools that need to accept heterogeneous data inputs without requiring users to specify the format explicitly. The pattern is particularly useful for:\n* Command-line training scripts\n* Batch processing pipelines\n* User-facing tools where simplicity is paramount\n\n== Theoretical Basis ==\nThe smart loading pattern follows a chain-of-responsibility design:\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\ndef load_dataset_smart(args):\n    if args.raw_text_file:\n        # Explicit raw text file\n        return RawTextDataLoader(tokenizer).load_from_file(args.raw_text_file)\n    elif args.dataset.endswith((\".txt\", \".md\", \".json\", \".jsonl\")):\n        # Auto-detect local file\n        return RawTextDataLoader(tokenizer).load_from_file(args.dataset)\n    elif use_modelscope:\n        # ModelScope dataset\n        return MsDataset.load(args.dataset, split=\"train\")\n    else:\n        # Default: HuggingFace dataset\n        return load_dataset(args.dataset, split=\"train\").map(formatting_func)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[implemented_by::Implementation:Unslothai_Unsloth_CLI]]\n* [[implemented_by::Implementation:Unslothai_Unsloth_RawTextDataLoader]]\n",
      "domains": [
        "Data Preparation",
        "CLI"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 17:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_CLI"
        },
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_RawTextDataLoader"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Chat_Template_Configuration",
      "page_title": "Unslothai Unsloth Chat Template Configuration",
      "page_type": "Principle",
      "overview": "Configuration of chat templates for reinforcement learning workflows, ensuring consistent formatting between generation prompts and reward computation.",
      "content": "# Principle: Chat_Template_Configuration\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Blog|HuggingFace Chat Templates|https://huggingface.co/docs/transformers/chat_templating]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Reinforcement_Learning]], [[domain::Data_Preprocessing]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration of chat templates for reinforcement learning workflows, ensuring consistent formatting between generation prompts and reward computation.\n\n=== Description ===\n\nChat Template Configuration for RL training ensures that:\n1. Prompts for generation are correctly formatted\n2. Generated completions can be properly extracted\n3. Reward functions receive consistently formatted text\n\nFor GRPO specifically, the template must support reasoning formats (chain-of-thought) with clear delimiters for the reward function to parse.\n\nThis principle shares the same implementation as Data_Formatting (get_chat_template) but is applied in the RL context.\n\n=== Usage ===\n\nConfigure chat templates after model loading, before dataset preparation. For RL, consider:\n* Templates supporting reasoning tags (e.g., `<think>...</think>`)\n* Clear delimiters for reward function parsing\n* Consistency with base model's pre-training format\n\n== Theoretical Basis ==\n\n=== Template Requirements for RL ===\n\nRL generation requires templates that:\n\n1. **Clearly separate roles**: Generation starts at assistant turn\n2. **Support reasoning markers**: For step-by-step thinking\n3. **Define stop conditions**: EOS token for generation termination\n\n<math>\n\\text{Prompt} = \\text{Template}(\\text{system}, \\text{user\\_turn})\n</math>\n\n<math>\n\\text{Completion} = \\text{Model}.\\text{generate}(\\text{Prompt})\n</math>\n\n=== Reasoning Format Example ===\n\nFor math reasoning tasks:\n```\n<think>\nLet me solve this step by step...\n1. First observation...\n2. Key insight...\n</think>\nThe answer is \\boxed{42}\n```\n\nThe reward function parses this structure to verify:\n* Reasoning is present (`<think>` tags)\n* Final answer is correctly formatted (`\\boxed{}`)\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_get_chat_template]]\n\n(Note: Same implementation as Data_Formatting, different context)\n",
      "domains": [
        "NLP",
        "Reinforcement Learning",
        "Data Preprocessing"
      ],
      "sources": [
        {
          "type": "Blog",
          "title": "HuggingFace Chat Templates",
          "url": "https://huggingface.co/docs/transformers/chat_templating"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_chat_template"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Data_Formatting",
      "page_title": "Unslothai Unsloth Data Formatting",
      "page_type": "Principle",
      "overview": "Technique for formatting conversational data into structured templates that instruction-tuned language models expect during training and inference.",
      "content": "# Principle: Data_Formatting\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Blog|HuggingFace Chat Templates|https://huggingface.co/docs/transformers/chat_templating]]\n* [[source::Blog|ChatML Specification|https://github.com/openai/openai-python/blob/main/chatml.md]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Data_Engineering]], [[domain::Instruction_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechnique for formatting conversational data into structured templates that instruction-tuned language models expect during training and inference.\n\n=== Description ===\n\nData Formatting for LLM fine-tuning involves converting raw conversation data into a consistent template format that distinguishes between system instructions, user inputs, and model responses. This structured formatting is critical because instruction-tuned models learn to recognize these boundaries during pre-training.\n\nCommon formats include:\n* **ChatML**: `<|im_start|>role\\ncontent<|im_end|>`\n* **Llama-3**: `<|start_header_id|>role<|end_header_id|>\\ncontent<|eot_id|>`\n* **Alpaca**: `### Instruction:\\n{instruction}\\n### Response:\\n{response}`\n\nThe principle ensures that fine-tuning data matches the model's expected input structure, enabling effective transfer of instruction-following capabilities to new tasks.\n\n=== Usage ===\n\nApply data formatting when:\n* Fine-tuning on conversational/instruction datasets\n* Training chat or assistant models\n* Ensuring consistency between training and inference formats\n* Converting between dataset formats (ShareGPT \u2192 ChatML)\n\nFormat selection guidelines:\n* Match the base model's pre-training format when possible\n* Use ChatML for deployment to Ollama (default format)\n* Ensure EOS token placement for proper generation stopping\n\n== Theoretical Basis ==\n\n=== Template Structure ===\n\nA chat template defines:\n1. **Role markers**: Tokens identifying speaker (user, assistant, system)\n2. **Content boundaries**: Where the actual text begins and ends\n3. **Turn separators**: How to distinguish between turns\n4. **Stop tokens**: Signals for generation termination\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# Chat template formatting (abstract)\ndef format_conversation(messages, template):\n    output = \"\"\n    for msg in messages:\n        role = msg[\"role\"]  # \"user\", \"assistant\", \"system\"\n        content = msg[\"content\"]\n\n        # Apply template structure\n        output += template.start_token(role)\n        output += content\n        output += template.end_token(role)\n\n    # Add generation prompt for inference\n    if not messages[-1][\"role\"] == \"assistant\":\n        output += template.start_token(\"assistant\")\n\n    return output\n</syntaxhighlight>\n\n=== Loss Masking ===\n\nDuring training, loss is typically computed only on assistant responses:\n\n<math>\n\\mathcal{L} = -\\sum_{t \\in \\text{assistant\\_tokens}} \\log P(x_t | x_{<t})\n</math>\n\nUser and system tokens are masked from the loss to prevent the model from learning to generate user messages.\n\n=== Token Efficiency ===\n\nDifferent templates have different token overheads:\n* ChatML: ~6 tokens per turn\n* Llama-3: ~8 tokens per turn\n* Alpaca: ~12 tokens per turn\n\nFor long conversations, this overhead compounds and affects effective context utilization.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_get_chat_template]]\n",
      "domains": [
        "NLP",
        "Data Engineering",
        "Instruction Tuning"
      ],
      "sources": [
        {
          "type": "Blog",
          "title": "HuggingFace Chat Templates",
          "url": "https://huggingface.co/docs/transformers/chat_templating"
        },
        {
          "type": "Blog",
          "title": "ChatML Specification",
          "url": "https://github.com/openai/openai-python/blob/main/chatml.md"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_chat_template"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_GGUF_Export",
      "page_title": "Unslothai Unsloth GGUF Export",
      "page_type": "Principle",
      "overview": "Converting fine-tuned models to GGUF format for deployment with llama.cpp, Ollama, and other GGML-based inference engines.",
      "content": "# Principle: GGUF_Export\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|GGUF Specification|https://github.com/ggerganov/ggml/blob/master/docs/gguf.md]]\n* [[source::Doc|llama.cpp|https://github.com/ggerganov/llama.cpp]]\n|-\n! Domains\n| [[domain::Model_Export]], [[domain::GGUF]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConverting fine-tuned models to GGUF format for deployment with llama.cpp, Ollama, and other GGML-based inference engines.\n\n=== Description ===\n\nGGUF Export converts HuggingFace models to GGUF (GPT-Generated Unified Format):\n\n1. **Model Serialization**: Convert weights to GGUF tensor format\n2. **Tokenizer Embedding**: Embed tokenizer vocabulary and special tokens\n3. **Quantization**: Apply selected quantization method\n4. **Metadata**: Include model architecture and hyperparameters\n\n=== Usage ===\n\nUse GGUF Export when deploying fine-tuned models for:\n* Local inference with llama.cpp\n* Ollama deployment\n* Edge device deployment\n* CPU-only inference\n\n== Theoretical Basis ==\n\n=== GGUF Structure ===\n\nGGUF files contain:\n* Magic number and version\n* Tensor count and metadata count\n* Key-value metadata (architecture, hyperparameters)\n* Tensor info (names, shapes, dtypes, offsets)\n* Tensor data (aligned to 32 bytes)\n\n=== Conversion Pipeline ===\n\n1. **HF \u2192 Float16**: Dequantize 4-bit, merge LoRA\n2. **Float16 \u2192 GGUF**: Convert using llama.cpp's convert.py\n3. **GGUF \u2192 Quantized GGUF**: Apply llama-quantize\n\n=== Architecture Support ===\n\nSupported architectures:\n* LLaMA, Mistral, Mixtral\n* Qwen, Qwen2\n* Gemma, Gemma2\n* Phi, Phi-3\n* LLaVA (vision models - partial)\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_save_to_gguf]]\n\n",
      "domains": [
        "Model Export",
        "GGUF",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "GGUF Specification",
          "url": "https://github.com/ggerganov/ggml/blob/master/docs/gguf.md"
        },
        {
          "type": "Doc",
          "title": "llama.cpp",
          "url": "https://github.com/ggerganov/llama.cpp"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_save_to_gguf"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_GGUF_Hub_Upload",
      "page_title": "Unslothai Unsloth GGUF Hub Upload",
      "page_type": "Principle",
      "overview": "Uploading GGUF-quantized models to HuggingFace Hub for sharing and distribution.",
      "content": "# Principle: GGUF_Hub_Upload\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|HuggingFace Hub|https://huggingface.co/docs/huggingface_hub/guides/upload]]\n|-\n! Domains\n| [[domain::Model_Sharing]], [[domain::HuggingFace_Hub]], [[domain::GGUF]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nUploading GGUF-quantized models to HuggingFace Hub for sharing and distribution.\n\n=== Description ===\n\nGGUF Hub Upload handles the full upload pipeline:\n\n1. **Repository Creation**: Create or reuse HuggingFace repository\n2. **GGUF Upload**: Upload quantized model files\n3. **Metadata Upload**: Include config.json and Modelfile\n4. **README Generation**: Create model card with usage instructions\n\n=== Usage ===\n\nUse after GGUF export to share quantized models publicly or privately.\n\n== Theoretical Basis ==\n\n=== Upload Strategy ===\n\nFor large GGUF files:\n* Single file upload for files < 50GB\n* Automatic chunking handled by huggingface_hub\n* Progress tracking during upload\n\n=== Repository Structure ===\n\nTypical GGUF repository layout:\n```\nmy-model-GGUF/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 Modelfile\n\u251c\u2500\u2500 model-Q4_K_M.gguf\n\u251c\u2500\u2500 model-Q5_K_M.gguf\n\u2514\u2500\u2500 model-Q8_0.gguf\n```\n\n=== Naming Convention ===\n\nGGUF files follow naming pattern:\n```\n{model_name}.{quantization}.gguf\n```\n\nExample: `llama-3-8b.Q4_K_M.gguf`\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_push_to_hub_gguf]]\n\n",
      "domains": [
        "Model Sharing",
        "HuggingFace Hub",
        "GGUF"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "HuggingFace Hub",
          "url": "https://huggingface.co/docs/huggingface_hub/guides/upload"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_push_to_hub_gguf"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_GGUF_Verification",
      "page_title": "Unslothai Unsloth GGUF Verification",
      "page_type": "Principle",
      "overview": "Verification of GGUF model exports by running inference with llama.cpp CLI tools.",
      "content": "# Principle: GGUF_Verification\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|llama.cpp CLI|https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md]]\n|-\n! Domains\n| [[domain::Testing]], [[domain::GGUF]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nVerification of GGUF model exports by running inference with llama.cpp CLI tools.\n\n=== Description ===\n\nGGUF Verification ensures exported models work correctly:\n\n1. **Load Test**: Verify GGUF file loads without errors\n2. **Generation Test**: Run sample prompts and check output quality\n3. **Tokenizer Test**: Verify tokenization matches expected behavior\n4. **Performance Test**: Measure inference speed (optional)\n\n=== Usage ===\n\nRun after GGUF export before deployment to catch conversion errors early.\n\n== Theoretical Basis ==\n\n=== Verification Checklist ===\n\n| Test | Command | Expected Result |\n|------|---------|-----------------|\n| Load | `llama-cli -m model.gguf -p \"\" -n 1` | No errors |\n| Generate | `llama-cli -m model.gguf -p \"Hello\" -n 50` | Coherent output |\n| Template | `llama-cli -m model.gguf --chat` | Correct formatting |\n\n=== Common Issues ===\n\n* **Tokenizer mismatch**: BOS/EOS token handling differs\n* **Architecture unsupported**: Some architectures not in llama.cpp\n* **Quantization errors**: Rare numerical issues in low-bit quants\n\n=== VLM Verification ===\n\nFor vision-language models:\n```bash\nllama-mtmd-cli -m model.gguf --mmproj mmproj.gguf\n/image test.jpg\nDescribe this image.\n```\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_llama_cli_validation]]\n\n",
      "domains": [
        "Testing",
        "GGUF",
        "Inference"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "llama.cpp CLI",
          "url": "https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_llama_cli_validation"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_GRPO_Configuration",
      "page_title": "Unslothai Unsloth GRPO Configuration",
      "page_type": "Principle",
      "overview": "Selection and tuning of hyperparameters for Group Relative Policy Optimization training, balancing exploration, stability, and computational efficiency.",
      "content": "# Principle: GRPO_Configuration\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Paper|DeepSeekMath|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::Reinforcement_Learning]], [[domain::Optimization]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nSelection and tuning of hyperparameters for Group Relative Policy Optimization training, balancing exploration, stability, and computational efficiency.\n\n=== Description ===\n\nGRPO (Group Relative Policy Optimization) is a simplified variant of PPO designed for language model training. Configuration involves:\n\n1. **Generation parameters**: How many completions to sample, at what length\n2. **Optimization parameters**: Learning rate, batch size, KL penalty\n3. **Efficiency parameters**: Memory management, gradient accumulation\n\nGRPO eliminates the need for a separate value network by using group-relative advantages computed within each batch.\n\n=== Usage ===\n\nConfigure GRPO when:\n* Training reasoning models (math, code)\n* Aligning models with verifiable reward signals\n* Preference optimization with process-based rewards\n\nKey trade-offs:\n* More generations \u2192 better advantage estimation but slower\n* Higher beta \u2192 more stable but less exploration\n* Larger batch \u2192 smoother gradients but more memory\n\n== Theoretical Basis ==\n\n=== GRPO Objective ===\n\n<math>\n\\mathcal{L}_{GRPO} = -\\mathbb{E}_{x}\\mathbb{E}_{y_1,...,y_G \\sim \\pi_\\theta}\\left[\\sum_{i=1}^G (r_i - \\bar{r}) \\log \\pi_\\theta(y_i|x)\\right] + \\beta D_{KL}(\\pi_\\theta || \\pi_{ref})\n</math>\n\nWhere:\n* G = num_generations completions per prompt\n* r_i = reward for completion i\n* r\u0304 = mean reward (baseline)\n* \u03b2 = KL penalty coefficient\n\n=== Group-Relative Advantage ===\n\nUnlike PPO, GRPO computes advantages within each prompt group:\n\n<math>\nA_i = r_i - \\frac{1}{G}\\sum_{j=1}^G r_j\n</math>\n\nBenefits:\n* No value network needed\n* Automatic reward normalization\n* Reduced variance compared to global baselines\n\n=== Key Hyperparameters ===\n\n| Parameter | Typical Range | Effect |\n|-----------|---------------|--------|\n| num_generations | 6-16 | More = better estimation, slower |\n| beta | 0.01-0.2 | Higher = more conservative |\n| learning_rate | 1e-6 - 5e-5 | Higher = faster but unstable |\n| max_completion_length | 100-500 | Task-dependent |\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# GRPO hyperparameter selection (abstract)\ndef configure_grpo(task_type, gpu_memory):\n    if task_type == \"math_reasoning\":\n        config = {\n            \"num_generations\": 16,      # More samples for hard tasks\n            \"max_completion_length\": 500,\n            \"beta\": 0.05,               # Allow exploration\n            \"learning_rate\": 1e-5,\n        }\n    elif task_type == \"instruction_following\":\n        config = {\n            \"num_generations\": 8,\n            \"max_completion_length\": 200,\n            \"beta\": 0.1,                # Moderate constraint\n            \"learning_rate\": 5e-6,\n        }\n\n    # Adjust for memory\n    if gpu_memory < 24:\n        config[\"num_generations\"] //= 2\n        config[\"gradient_accumulation_steps\"] *= 2\n\n    return config\n</syntaxhighlight>\n\n=== Computational Cost ===\n\nPer training step:\n<math>\n\\text{Cost} = G \\times \\text{Cost}_{generate} + G \\times \\text{Cost}_{reward} + \\text{Cost}_{update}\n</math>\n\nGeneration dominates cost, making vLLM integration critical.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_UnslothGRPOConfig]]\n\n",
      "domains": [
        "Reinforcement Learning",
        "Optimization",
        "Training"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Paper",
          "title": "DeepSeekMath",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_UnslothGRPOConfig"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_GRPO_Execution",
      "page_title": "Unslothai Unsloth GRPO Execution",
      "page_type": "Principle",
      "overview": "Execution of Group Relative Policy Optimization training loop, iteratively generating completions, computing rewards, and updating the policy network.",
      "content": "# Principle: GRPO_Execution\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Paper|PPO|https://arxiv.org/abs/1707.06347]]\n* [[source::Paper|DeepSeekMath|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::Reinforcement_Learning]], [[domain::Policy_Optimization]], [[domain::NLP]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nExecution of Group Relative Policy Optimization training loop, iteratively generating completions, computing rewards, and updating the policy network.\n\n=== Description ===\n\nGRPO Execution implements the training loop for Group Relative Policy Optimization:\n\n1. **Sample prompts** from the training dataset\n2. **Generate completions** using vLLM (G per prompt)\n3. **Compute rewards** via user-defined reward functions\n4. **Calculate group-relative advantages** (r_i - mean(r))\n5. **Update policy** with weighted log-probability gradients\n6. **Repeat** until convergence\n\nGRPO simplifies PPO by eliminating the value network and using within-group baselines.\n\n=== Usage ===\n\nExecute GRPO training after configuring model, reward functions, and dataset. Monitor:\n* Mean reward progression\n* KL divergence from reference\n* Generation quality (sample outputs)\n\n== Theoretical Basis ==\n\n=== GRPO Training Loop ===\n\n<math>\n\\text{For each batch:}\n</math>\n\n1. Sample prompts: x_1, ..., x_B\n2. Generate: y_{i,1}, ..., y_{i,G} ~ \u03c0_\u03b8(\u00b7|x_i) for each prompt\n3. Reward: r_{i,j} = R(y_{i,j}, x_i)\n4. Advantage: A_{i,j} = r_{i,j} - (1/G)\u03a3_k r_{i,k}\n5. Loss: L = -\u03a3_{i,j} A_{i,j} \u00b7 log \u03c0_\u03b8(y_{i,j}|x_i) + \u03b2\u00b7KL\n6. Update: \u03b8 \u2190 \u03b8 - \u03b7\u00b7\u2207L\n\n=== Group-Relative Baseline ===\n\nThe key innovation of GRPO is computing the baseline within each group:\n\n<math>\n\\bar{r}_i = \\frac{1}{G}\\sum_{j=1}^G r_{i,j}\n</math>\n\nThis provides:\n* Automatic normalization (mean-centered rewards)\n* No value network training\n* Lower variance than global baselines\n\n=== KL Divergence Regularization ===\n\nTo prevent catastrophic forgetting:\n\n<math>\nD_{KL}(\\pi_\\theta || \\pi_{ref}) = \\mathbb{E}_{\\pi_\\theta}\\left[\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}\\right]\n</math>\n\nThe reference model (\u03c0_ref) is typically the SFT checkpoint.\n\n=== Training Dynamics ===\n\nGRPO exhibits:\n* **Initial exploration**: Rewards vary widely\n* **Policy sharpening**: Model converges on rewarded behaviors\n* **KL constraint binding**: Policy stabilizes near reference\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# GRPO training loop (abstract)\ndef grpo_train(model, dataset, reward_funcs, num_generations, beta):\n    for batch in dataset:\n        prompts = batch[\"prompt\"]\n\n        # Generate G completions per prompt\n        with model.inference_mode():\n            completions = model.generate(\n                prompts,\n                num_return_sequences=num_generations\n            )\n\n        # Compute rewards\n        rewards = sum(rf(completions, prompts) for rf in reward_funcs)\n        rewards = reshape(rewards, (batch_size, num_generations))\n\n        # Group-relative advantages\n        baseline = rewards.mean(dim=1, keepdim=True)\n        advantages = rewards - baseline\n\n        # Compute loss\n        log_probs = model.log_prob(completions, prompts)\n        policy_loss = -(advantages * log_probs).mean()\n        kl_loss = beta * compute_kl(model, ref_model, completions)\n        loss = policy_loss + kl_loss\n\n        # Update\n        loss.backward()\n        optimizer.step()\n</syntaxhighlight>\n\n=== Convergence Indicators ===\n\nTraining is progressing well when:\n* Mean reward increases steadily\n* Reward variance decreases\n* KL divergence stabilizes (not exploding)\n* Sample generations improve qualitatively\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_UnslothGRPOTrainer]]\n\n",
      "domains": [
        "Reinforcement Learning",
        "Policy Optimization",
        "NLP"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Paper",
          "title": "PPO",
          "url": "https://arxiv.org/abs/1707.06347"
        },
        {
          "type": "Paper",
          "title": "DeepSeekMath",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_UnslothGRPOTrainer"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_LoRA_Configuration",
      "page_title": "Unslothai Unsloth LoRA Configuration",
      "page_type": "Principle",
      "overview": "Technique for adding trainable low-rank decomposition matrices to frozen pre-trained weights, enabling parameter-efficient fine-tuning with minimal memory overhead.",
      "content": "# Principle: LoRA_Configuration\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n* [[source::Paper|RSLoRA|https://arxiv.org/abs/2312.03732]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::Parameter_Efficient_Finetuning]], [[domain::NLP]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechnique for adding trainable low-rank decomposition matrices to frozen pre-trained weights, enabling parameter-efficient fine-tuning with minimal memory overhead.\n\n=== Description ===\n\nLow-Rank Adaptation (LoRA) injects trainable rank decomposition matrices into transformer layers while keeping pre-trained weights frozen. Instead of updating a weight matrix W \u2208 R^(d\u00d7k), LoRA adds a parallel path through two smaller matrices: A \u2208 R^(r\u00d7k) and B \u2208 R^(d\u00d7r), where r << min(d, k).\n\nThis approach reduces trainable parameters by orders of magnitude while achieving comparable performance to full fine-tuning. For a 7B parameter model, typical LoRA configurations train only 0.1-1% of total parameters.\n\nThe principle extends to QLoRA by applying LoRA to quantized base models, combining the memory savings of quantization with the efficiency of low-rank adaptation.\n\n=== Usage ===\n\nApply LoRA configuration when:\n* Fine-tuning large models with limited GPU memory\n* Training task-specific adapters for deployment\n* Rapid experimentation with different fine-tuning objectives\n* Multi-task learning (separate adapters per task)\n\nKey configuration decisions:\n* **Rank (r)**: Higher rank = more capacity but more memory/compute\n* **Alpha**: Scaling factor; alpha/r determines effective learning rate multiplier\n* **Target modules**: Which layers receive LoRA adapters\n\n== Theoretical Basis ==\n\n=== Low-Rank Decomposition ===\n\nFor a pre-trained weight matrix W\u2080, LoRA parameterizes the update as:\n\n<math>\nW = W_0 + \\Delta W = W_0 + BA\n</math>\n\nWhere:\n* W\u2080 \u2208 R^(d\u00d7k) is frozen\n* B \u2208 R^(d\u00d7r) initialized to zeros\n* A \u2208 R^(r\u00d7k) initialized from N(0, \u03c3\u00b2)\n* r << min(d, k) is the rank\n\n=== Forward Pass ===\n\n<math>\nh = W_0 x + \\frac{\\alpha}{r} BA x\n</math>\n\nThe scaling factor \u03b1/r controls the magnitude of the LoRA contribution relative to the base weights.\n\n=== Parameter Efficiency ===\n\nFor a single linear layer:\n* Full fine-tuning: d \u00d7 k parameters\n* LoRA: r \u00d7 (d + k) parameters\n\nWith r=16, d=4096, k=4096:\n* Full: 16.7M parameters\n* LoRA: 131K parameters (0.8% of full)\n\n=== Rank-Stabilized LoRA (RSLoRA) ===\n\nStandard LoRA scales by \u03b1/r. RSLoRA instead uses:\n\n<math>\nh = W_0 x + \\frac{\\alpha}{\\sqrt{r}} BA x\n</math>\n\nThis stabilizes training across different rank values.\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# LoRA forward pass (abstract)\nclass LoRALinear:\n    def __init__(self, base_layer, r, alpha):\n        self.base = base_layer  # Frozen W0\n        self.lora_A = init_kaiming(r, in_features)  # Trainable\n        self.lora_B = init_zeros(out_features, r)   # Trainable\n        self.scale = alpha / r  # or alpha / sqrt(r) for RSLoRA\n\n    def forward(self, x):\n        base_out = self.base(x)  # W0 @ x (dequantized if 4-bit)\n        lora_out = (self.lora_B @ self.lora_A @ x) * self.scale\n        return base_out + lora_out\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_get_peft_model]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_LoRA_Rank_Selection_Tip]]\n",
      "domains": [
        "Deep Learning",
        "Parameter Efficient Finetuning",
        "NLP"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Paper",
          "title": "RSLoRA",
          "url": "https://arxiv.org/abs/2312.03732"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_peft_model"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_LoRA_Rank_Selection_Tip"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Model_Loading",
      "page_title": "Unslothai Unsloth Model Loading",
      "page_type": "Principle",
      "overview": "Technique for loading pre-trained Large Language Models with memory-efficient quantization while preserving model quality for fine-tuning.",
      "content": "# Principle: Model_Loading\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n* [[source::Paper|LLM.int8()|https://arxiv.org/abs/2208.07339]]\n* [[source::Blog|Hugging Face Quantization|https://huggingface.co/docs/transformers/quantization]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::NLP]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechnique for loading pre-trained Large Language Models with memory-efficient quantization while preserving model quality for fine-tuning.\n\n=== Description ===\n\nModel Loading for QLoRA fine-tuning involves loading a pre-trained transformer model with 4-bit NF4 (Normal Float 4-bit) quantization. This approach reduces the memory footprint of large models by approximately 75% while maintaining sufficient precision for effective fine-tuning through Low-Rank Adaptation (LoRA).\n\nThe key insight from QLoRA is that base model weights can be aggressively quantized to 4-bit precision during loading, while LoRA adapter weights are trained in higher precision (float16/bfloat16). The quantized base weights serve as a frozen foundation, with only the small LoRA matrices being updated during training.\n\nThis principle addresses the memory bottleneck that prevents fine-tuning large models on consumer GPUs. A 7B parameter model that would normally require ~28GB of VRAM in float32 can be loaded in ~4GB with 4-bit quantization.\n\n=== Usage ===\n\nApply this principle when:\n* Fine-tuning models larger than your GPU memory in float16\n* Using consumer GPUs (RTX 3090, RTX 4090, A10G) for LLM training\n* Training with LoRA/QLoRA adapters\n* Prioritizing memory efficiency over maximum throughput\n\nDo NOT apply when:\n* Full parameter fine-tuning is required (no LoRA)\n* Maximum inference speed is critical (quantization adds dequantization overhead)\n* Model accuracy requirements exceed QLoRA's capabilities\n\n== Theoretical Basis ==\n\n=== NF4 Quantization ===\n\nNormal Float 4-bit (NF4) is an information-theoretically optimal quantization scheme for normally distributed data:\n\n<math>\nQ_{NF4}(x) = \\text{round}\\left(\\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\times 15\\right)\n</math>\n\nNF4 uses a non-uniform quantization grid that matches the empirical distribution of neural network weights, which tend to follow a normal distribution.\n\n=== Double Quantization ===\n\nQLoRA further reduces memory by quantizing the quantization constants:\n\n<math>\n\\text{Memory} = \\frac{n \\times 4}{8} + \\frac{n}{64} \\times 32 + \\frac{n}{64 \\times 256} \\times 32\n</math>\n\nWhere:\n* First term: 4-bit weights\n* Second term: FP32 quantization scales (one per 64 weights)\n* Third term: 8-bit quantization of the scales themselves\n\n=== Precision Hierarchy ===\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# QLoRA precision hierarchy (abstract)\nbase_weights = quantize_nf4(pretrained_weights)  # 4-bit frozen\nlora_A = init_random(r, d_in, dtype=float16)     # 16-bit trainable\nlora_B = init_zeros(d_out, r, dtype=float16)     # 16-bit trainable\n\n# Forward pass\nh = dequantize(base_weights) @ x + (lora_B @ lora_A) @ x * scale\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_FastLanguageModel_from_pretrained]]\n\n",
      "domains": [
        "Deep Learning",
        "NLP",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        },
        {
          "type": "Paper",
          "title": "LLM.int8()",
          "url": "https://arxiv.org/abs/2208.07339"
        },
        {
          "type": "Blog",
          "title": "Hugging Face Quantization",
          "url": "https://huggingface.co/docs/transformers/quantization"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastLanguageModel_from_pretrained"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Model_Preparation",
      "page_title": "Unslothai Unsloth Model Preparation",
      "page_type": "Principle",
      "overview": "Preparing fine-tuned models with LoRA adapters for GGUF conversion by merging adapters into base weights.",
      "content": "# Principle: Model_Preparation\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|HuggingFace Model Saving|https://huggingface.co/docs/transformers/main_classes/model]]\n* [[source::Doc|PEFT Merging|https://huggingface.co/docs/peft/conceptual_guides/lora]]\n|-\n! Domains\n| [[domain::Model_Serialization]], [[domain::GGUF]], [[domain::LoRA]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nPreparing fine-tuned models with LoRA adapters for GGUF conversion by merging adapters into base weights.\n\n=== Description ===\n\nModel Preparation for GGUF export requires merging LoRA adapters into the base model weights:\n\n1. **LoRA Merging**: Combine adapter weights with base model\n2. **Precision Selection**: Choose float16 or bfloat16 for merged weights\n3. **Memory Management**: Handle large model dequantization for 4-bit models\n4. **Tokenizer Alignment**: Ensure tokenizer is saved alongside model\n\n=== Usage ===\n\nUse Model Preparation before GGUF export. The merged model serves as input to the quantization pipeline.\n\n== Theoretical Basis ==\n\n=== LoRA Merging Formula ===\n\nFor each LoRA layer, merged weight:\n\n<math>\nW_{merged} = W_{base} + \\frac{\\alpha}{r} \\cdot B \\cdot A\n</math>\n\nWhere:\n* W_base: Original base model weight\n* B, A: LoRA decomposition matrices\n* \u03b1: LoRA alpha (scaling factor)\n* r: LoRA rank\n\n=== 4-bit Dequantization ===\n\nFor QLoRA models:\n\n1. Dequantize 4-bit base weights to float16\n2. Merge LoRA weights\n3. Save as standard float16 safetensors\n\nMemory requirement peaks during dequantization:\n\n<math>\n\\text{Peak Memory} \\approx 2 \\times \\text{Model Size (4-bit)} + \\text{16-bit layer}\n</math>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_unsloth_save_model_merged]]\n\n",
      "domains": [
        "Model Serialization",
        "GGUF",
        "LoRA"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "HuggingFace Model Saving",
          "url": "https://huggingface.co/docs/transformers/main_classes/model"
        },
        {
          "type": "Doc",
          "title": "PEFT Merging",
          "url": "https://huggingface.co/docs/peft/conceptual_guides/lora"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_unsloth_save_model_merged"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Model_Saving",
      "page_title": "Unslothai Unsloth Model Saving",
      "page_type": "Principle",
      "overview": "Techniques for serializing fine-tuned model weights to disk in formats suitable for different deployment scenarios and downstream processing.",
      "content": "# Principle: Model_Saving\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|Safetensors|https://huggingface.co/docs/safetensors]]\n* [[source::Doc|PEFT Saving|https://huggingface.co/docs/peft/quicktour#save-and-load-a-model]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::Model_Serialization]], [[domain::Deployment]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechniques for serializing fine-tuned model weights to disk in formats suitable for different deployment scenarios and downstream processing.\n\n=== Description ===\n\nModel Saving after LoRA fine-tuning involves choosing between three fundamental strategies:\n\n1. **Adapter-only saving**: Store just the LoRA matrices (A, B weights). Requires base model for inference but is fastest and smallest.\n\n2. **Merged saving**: Combine LoRA weights with base model (W' = W\u2080 + BA). Creates standalone model but requires dequantization if base was quantized.\n\n3. **Format conversion**: Transform merged weights into deployment formats (GGUF for llama.cpp, ONNX for production).\n\nThe choice impacts inference requirements, model size, and downstream compatibility.\n\n=== Usage ===\n\nChoose saving strategy based on deployment target:\n\n| Strategy | Size | Speed | Use Case |\n|----------|------|-------|----------|\n| LoRA only | ~100MB | Fast | HuggingFace inference, continued training |\n| Merged 16-bit | ~14GB (7B) | Slow | GGUF export, framework conversion |\n| Merged 4-bit | ~4GB (7B) | Medium | Direct quantized inference |\n\n== Theoretical Basis ==\n\n=== LoRA Merging ===\n\nFor a layer with LoRA adapters:\n\n<math>\nW' = W_0 + \\frac{\\alpha}{r} BA\n</math>\n\nMerging computes W' explicitly, eliminating the separate forward pass through LoRA matrices.\n\nIf W\u2080 is quantized, merging requires:\n1. Dequantize W\u2080 to float16/float32\n2. Compute W' = W\u2080 + (\u03b1/r)BA\n3. Save W' in target precision\n\n=== Sharding Strategy ===\n\nLarge models are split into shards for efficient loading:\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# Model sharding (abstract)\ndef save_sharded(state_dict, max_shard_size):\n    shards = []\n    current_shard = {}\n    current_size = 0\n\n    for name, tensor in state_dict.items():\n        tensor_size = tensor.numel() * tensor.element_size()\n        if current_size + tensor_size > max_shard_size:\n            shards.append(current_shard)\n            current_shard = {}\n            current_size = 0\n        current_shard[name] = tensor\n        current_size += tensor_size\n\n    return shards  # model-00001-of-00003.safetensors, etc.\n</syntaxhighlight>\n\n=== Safetensors Format ===\n\nModern format replacing pickle-based `.bin` files:\n* Memory-mapped loading (lazy load)\n* No arbitrary code execution (security)\n* Parallel loading across shards\n* Header contains tensor metadata\n\n=== Hub Integration ===\n\nSaving to HuggingFace Hub enables:\n* Version control (git-based)\n* Model cards (documentation)\n* Inference API endpoints\n* Community sharing\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_save_pretrained]]\n",
      "domains": [
        "Deep Learning",
        "Model Serialization",
        "Deployment"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "Safetensors",
          "url": "https://huggingface.co/docs/safetensors"
        },
        {
          "type": "Doc",
          "title": "PEFT Saving",
          "url": "https://huggingface.co/docs/peft/quicktour#save-and-load-a-model"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_save_pretrained"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Multimodal_Data_Preparation",
      "page_title": "Unslothai Unsloth Multimodal Data Preparation",
      "page_type": "Principle",
      "overview": "Technique for formatting image-text datasets into the structure expected by Vision-Language Model training pipelines.",
      "content": "# Principle: Multimodal_Data_Preparation\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LLaVA|https://arxiv.org/abs/2304.08485]]\n* [[source::Doc|HuggingFace Datasets|https://huggingface.co/docs/datasets]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Data_Engineering]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechnique for formatting image-text datasets into the structure expected by Vision-Language Model training pipelines.\n\n=== Description ===\n\nMultimodal Data Preparation creates datasets with interleaved image and text content. Unlike text-only datasets, VLM datasets must:\n\n1. **Include images** as PIL.Image objects or paths\n2. **Mark image positions** in the message content\n3. **Handle multiple images** per conversation\n4. **Support multi-turn** conversations with images at any turn\n\n=== Usage ===\n\nPrepare multimodal data when:\n* Fine-tuning VLMs on custom image-text pairs\n* Training for VQA, captioning, or document tasks\n* Creating instruction-following datasets with visual context\n\n== Theoretical Basis ==\n\n=== Message Structure ===\n\nVLM messages use content arrays with typed elements:\n\n<syntaxhighlight lang=\"python\">\n{\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"image\"},           # Image placeholder\n        {\"type\": \"text\", \"text\": \"...\"} # Text content\n    ]\n}\n</syntaxhighlight>\n\nThe processor converts this to:\n* Image \u2192 Visual tokens (e.g., 576 tokens for ViT-L/14)\n* Text \u2192 Text tokens\n\n=== Image Processing ===\n\nAutoProcessor handles:\n* Resizing to model's expected resolution\n* Normalization (mean, std)\n* Conversion to tensor format\n* Padding/batching for variable-size images\n\n=== Data Flow ===\n\n<math>\n\\text{Raw Data} \\xrightarrow{\\text{Formatting}} \\text{Messages + Images} \\xrightarrow{\\text{Processor}} \\text{Tokens + Pixel Values}\n</math>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_multimodal_dataset_pattern]]\n",
      "domains": [
        "Computer Vision",
        "Data Engineering",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LLaVA",
          "url": "https://arxiv.org/abs/2304.08485"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Datasets",
          "url": "https://huggingface.co/docs/datasets"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_multimodal_dataset_pattern"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Ollama_Modelfile",
      "page_title": "Unslothai Unsloth Ollama Modelfile",
      "page_type": "Principle",
      "overview": "Generation of Ollama Modelfiles that map chat templates to Ollama's template syntax for local deployment.",
      "content": "# Principle: Ollama_Modelfile\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|Ollama Modelfile|https://github.com/ollama/ollama/blob/main/docs/modelfile.md]]\n|-\n! Domains\n| [[domain::Deployment]], [[domain::Ollama]], [[domain::Chat_Templates]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nGeneration of Ollama Modelfiles that map chat templates to Ollama's template syntax for local deployment.\n\n=== Description ===\n\nOllama Modelfile generation translates HuggingFace chat templates to Ollama format:\n\n* **FROM directive**: Points to GGUF file location\n* **TEMPLATE directive**: Defines prompt formatting with Go template syntax\n* **PARAMETER directives**: Sets inference parameters (temperature, stop tokens)\n* **SYSTEM directive**: Optional default system message\n\n=== Usage ===\n\nGenerated automatically during GGUF export. Use with `ollama create` to register the model.\n\n== Theoretical Basis ==\n\n=== Template Translation ===\n\nHuggingFace templates (Jinja2) must be converted to Ollama's Go template syntax:\n\n| HuggingFace | Ollama |\n|-------------|--------|\n| `{{ message['content'] }}` | `{{ .Content }}` |\n| `{% if system %}` | `{{ if .System }}` |\n| `{% for message in messages %}` | `{{ range .Messages }}` |\n\n=== Stop Tokens ===\n\nStop tokens prevent generation beyond expected bounds:\n* EOS token from tokenizer\n* Role delimiters (e.g., `<|im_end|>` for ChatML)\n* Special tokens from chat template\n\n=== Supported Templates ===\n\nPre-defined templates include:\n* ChatML (chatml)\n* Llama-3 (llama-3)\n* Alpaca (alpaca)\n* Mistral (mistral)\n* Gemma (gemma)\n* Phi-3 (phi-3)\n* Zephyr (zephyr)\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_OLLAMA_TEMPLATES]]\n\n",
      "domains": [
        "Deployment",
        "Ollama",
        "Chat Templates"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "Ollama Modelfile",
          "url": "https://github.com/ollama/ollama/blob/main/docs/modelfile.md"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_OLLAMA_TEMPLATES"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Quantization_Selection",
      "page_title": "Unslothai Unsloth Quantization Selection",
      "page_type": "Principle",
      "overview": "Selection of appropriate quantization methods for GGUF export based on accuracy/size/speed trade-offs.",
      "content": "# Principle: Quantization_Selection\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|llama.cpp Quantization|https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md]]\n* [[source::Paper|GGML Format|https://github.com/ggerganov/ggml]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::GGUF]], [[domain::Model_Compression]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nSelection of appropriate quantization methods for GGUF export based on accuracy/size/speed trade-offs.\n\n=== Description ===\n\nQuantization Selection determines the bit-width and quantization scheme for GGUF files:\n\n* **Higher precision** (f16, bf16, q8_0): Larger files, faster conversion, higher accuracy\n* **Lower precision** (q4_k_m, q5_k_m): Smaller files, slower conversion, acceptable accuracy\n* **Ultra-low precision** (q2_k, q3_k_m): Smallest files, significant accuracy loss\n\n=== Usage ===\n\nChoose quantization based on deployment constraints:\n* Local deployment with limited VRAM: q4_k_m, q5_k_m\n* Cloud deployment with more resources: q8_0, f16\n* Accuracy-critical applications: bf16, f16\n\n== Theoretical Basis ==\n\n=== Quantization Methods ===\n\n| Method | Bits per Weight | Use Case |\n|--------|-----------------|----------|\n| f32 | 32 | Debugging only (very slow) |\n| f16/bf16 | 16 | Full precision, slow inference |\n| q8_0 | 8 | High quality, balanced |\n| q6_k | 6 | Good quality, smaller |\n| q5_k_m | 5 | Recommended balance |\n| q4_k_m | 4 | Recommended small |\n| q3_k_m | 3 | Aggressive compression |\n| q2_k | 2 | Maximum compression |\n\n=== K-Quant Variants ===\n\n* **_s (small)**: Uses same quant for all tensors\n* **_m (medium)**: Higher precision for attention/FFN key tensors\n* **_l (large)**: Even higher precision for critical tensors\n\n=== Quality vs Size Trade-off ===\n\n<math>\n\\text{Perplexity Increase} \\approx \\frac{k}{bits^2}\n</math>\n\nWhere k depends on model architecture and training data.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_ALLOWED_QUANTS]]\n\n",
      "domains": [
        "Quantization",
        "GGUF",
        "Model Compression"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "llama.cpp Quantization",
          "url": "https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md"
        },
        {
          "type": "Paper",
          "title": "GGML Format",
          "url": "https://github.com/ggerganov/ggml"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_ALLOWED_QUANTS"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_RL_Dataset_Preparation",
      "page_title": "Unslothai Unsloth RL Dataset Preparation",
      "page_type": "Principle",
      "overview": "Technique for preparing datasets for reinforcement learning training, transforming raw data into prompt-only format for model generation and reward evaluation.",
      "content": "# Principle: RL_Dataset_Preparation\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Doc|HuggingFace Datasets|https://huggingface.co/docs/datasets]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Reinforcement_Learning]], [[domain::Data_Engineering]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechnique for preparing datasets for reinforcement learning training, transforming raw data into prompt-only format for model generation and reward evaluation.\n\n=== Description ===\n\nRL Dataset Preparation differs fundamentally from SFT preparation:\n\n| Aspect | SFT | RL (GRPO) |\n|--------|-----|-----------|\n| Dataset contains | Prompt + Response | Prompt only |\n| Model role | Learn to reproduce response | Generate new responses |\n| Responses | Fixed in dataset | Generated during training |\n| Reward | Implicit (cross-entropy) | Explicit (reward function) |\n\nFor GRPO, the dataset provides prompts that the model completes during training. Multiple completions are generated per prompt and scored by reward functions.\n\n=== Usage ===\n\nApply RL Dataset Preparation when:\n* Training with GRPO, PPO, or similar on-policy RL\n* You have problems/questions but want the model to learn solutions\n* Reward functions will evaluate generated responses\n\nKey requirements:\n* \"prompt\" column with formatted prompts\n* No ground truth responses (model generates these)\n* Prompts should trigger the desired behavior (reasoning, coding, etc.)\n\n== Theoretical Basis ==\n\n=== On-Policy Data Flow ===\n\n<math>\n\\text{Prompt} \\xrightarrow{\\text{Model}} \\text{Completions} \\xrightarrow{\\text{Reward}} \\text{Scores} \\xrightarrow{\\text{GRPO}} \\nabla\\theta\n</math>\n\n1. Sample prompt from dataset\n2. Generate N completions with current policy\n3. Score completions with reward function\n4. Compute group-relative advantages\n5. Update policy parameters\n\n=== Prompt Design ===\n\nEffective prompts should:\n* Clearly state the task\n* Include formatting instructions (if reward checks format)\n* Set up for verifiable outputs\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# RL dataset preparation (abstract)\ndef prepare_rl_dataset(raw_dataset, tokenizer, system_prompt):\n    def format_for_rl(example):\n        # Build prompt with chat template\n        messages = []\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        messages.append({\"role\": \"user\", \"content\": example[\"question\"]})\n\n        # Format with template, add generation prompt\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True  # Start assistant turn\n        )\n\n        return {\"prompt\": prompt}\n\n    return raw_dataset.map(format_for_rl)\n</syntaxhighlight>\n\n=== Dataset Size Considerations ===\n\nRL training typically uses smaller datasets than SFT:\n* Each prompt is used multiple times (different completions)\n* Reward signal provides more information per example\n* Focus on diverse, high-quality prompts over quantity\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_dataset_mapping_pattern]]\n",
      "domains": [
        "NLP",
        "Reinforcement Learning",
        "Data Engineering"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Datasets",
          "url": "https://huggingface.co/docs/datasets"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_dataset_mapping_pattern"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_RL_LoRA_Configuration",
      "page_title": "Unslothai Unsloth RL LoRA Configuration",
      "page_type": "Principle",
      "overview": "Configuration of LoRA adapters for reinforcement learning training, with specific constraints for vLLM integration and capacity requirements for policy optimization.",
      "content": "# Principle: RL_LoRA_Configuration\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Paper|PPO|https://arxiv.org/abs/1707.06347]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::Reinforcement_Learning]], [[domain::Parameter_Efficient_Finetuning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration of LoRA adapters for reinforcement learning training, with specific constraints for vLLM integration and capacity requirements for policy optimization.\n\n=== Description ===\n\nLoRA Configuration for RL differs from standard SFT in several key aspects:\n\n1. **Higher ranks typical** - RL often requires r=32-64 vs r=8-16 for SFT, as policy optimization needs more model capacity to learn from reward signals\n2. **vLLM rank constraint** - LoRA rank must not exceed `max_lora_rank` specified during model loading, as vLLM pre-allocates memory\n3. **Same target modules** - Typically all attention + MLP projections\n\nThe policy network in RL must be flexible enough to explore the reward landscape while being constrained by the LoRA rank bottleneck.\n\n=== Usage ===\n\nApply RL LoRA Configuration when:\n* Setting up GRPO, PPO, or DPO training\n* Model was loaded with `fast_inference=True`\n* Need higher capacity than standard SFT\n\nKey constraint: `r <= max_lora_rank` from model loading\n\n== Theoretical Basis ==\n\n=== Capacity Requirements ===\n\nRL training optimizes:\n\n<math>\nJ(\\theta) = \\mathbb{E}_{\\pi_\\theta}[R(s, a)]\n</math>\n\nWhere \u03c0_\u03b8 is the policy parameterized by LoRA weights \u03b8. Higher rank provides more capacity for:\n* Exploring diverse policy behaviors\n* Fitting to complex reward landscapes\n* Capturing nuanced generation patterns\n\n=== Rank and vLLM Memory ===\n\nvLLM pre-allocates for maximum rank:\n\n<math>\n\\text{LoRA Memory} = 2 \\times r_{max} \\times d \\times n_{layers} \\times \\text{sizeof(dtype)}\n</math>\n\nUsing actual rank r < r_max doesn't save memory in vLLM, but:\n* Reduces compute per forward pass\n* May improve generalization (regularization)\n\n=== Policy Gradient with LoRA ===\n\nGradient flows only through LoRA parameters:\n\n<math>\n\\nabla_\\theta J = \\mathbb{E}\\left[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot A_t\\right]\n</math>\n\nWhere A_t is the advantage. LoRA's low-rank structure provides implicit regularization against overfitting to reward hacking.\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# RL LoRA configuration (abstract)\ndef configure_lora_for_rl(model, max_lora_rank):\n    # Determine rank based on task complexity\n    # Typical: r=32-64 for GRPO, r=16-32 for DPO\n\n    if task_complexity == \"high\":\n        r = max_lora_rank  # Use full capacity\n    else:\n        r = max_lora_rank // 2  # Balance capacity vs regularization\n\n    # Apply LoRA to all key modules\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"]\n\n    return apply_lora(model, r=r, targets=target_modules)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_get_peft_model_rl]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_LoRA_Rank_Selection_Tip]]\n",
      "domains": [
        "Deep Learning",
        "Reinforcement Learning",
        "Parameter Efficient Finetuning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Paper",
          "title": "PPO",
          "url": "https://arxiv.org/abs/1707.06347"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_peft_model_rl"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_LoRA_Rank_Selection_Tip"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_RL_Model_Loading",
      "page_title": "Unslothai Unsloth RL Model Loading",
      "page_type": "Principle",
      "overview": "Technique for loading language models with high-throughput inference backends that enable efficient generation sampling required for reinforcement learning training.",
      "content": "# Principle: RL_Model_Loading\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Paper|vLLM|https://arxiv.org/abs/2309.06180]]\n* [[source::Doc|vLLM Documentation|https://docs.vllm.ai]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::Reinforcement_Learning]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechnique for loading language models with high-throughput inference backends that enable efficient generation sampling required for reinforcement learning training.\n\n=== Description ===\n\nRL Model Loading prepares a language model for reinforcement learning workflows like GRPO (Group Relative Policy Optimization) or PPO (Proximal Policy Optimization). Unlike standard SFT, RL training requires generating multiple completions per prompt during training:\n\n* **GRPO**: 6-16 completions per prompt for group-relative reward comparison\n* **PPO**: Multiple trajectories for advantage estimation\n\nThis makes inference throughput critical to training speed. vLLM integration enables:\n* Continuous batching for variable-length generation\n* PagedAttention for efficient KV cache management\n* LoRA serving without full model duplication\n\n=== Usage ===\n\nApply RL Model Loading when:\n* Training with GRPO, PPO, or other on-policy RL algorithms\n* Generation throughput is a training bottleneck\n* Multiple completions per prompt are needed\n\nPrerequisites:\n* vLLM installed and functional\n* Sufficient GPU memory for model + vLLM KV cache\n* CUDA-capable GPU\n\n== Theoretical Basis ==\n\n=== Generation Bottleneck in RL ===\n\nFor on-policy RL, each training step requires:\n\n<math>\n\\text{Time}_{step} = \\text{Time}_{generate} + \\text{Time}_{reward} + \\text{Time}_{update}\n</math>\n\nWith standard HuggingFace generation, `Time_generate` dominates due to:\n* Sequential token generation\n* No batching across different prompts\n* Memory-inefficient KV cache\n\n=== vLLM Optimizations ===\n\nvLLM reduces generation time through:\n\n1. **Continuous Batching**: Process tokens from different requests together\n2. **PagedAttention**: Non-contiguous KV cache with virtual memory\n3. **Speculative Decoding**: Draft model for faster token acceptance\n\nMemory equation for vLLM:\n\n<math>\n\\text{GPU}_{total} = \\text{GPU}_{model} + \\text{GPU}_{utilization} \\times \\text{GPU}_{available}\n</math>\n\nWhere `gpu_memory_utilization` controls KV cache allocation.\n\n=== LoRA + vLLM Integration ===\n\nvLLM supports serving LoRA adapters with:\n* Pre-allocation based on `max_lora_rank`\n* Efficient adapter weight loading\n* No model recompilation for rank changes\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# RL model loading (abstract)\ndef load_for_rl(model_name, max_lora_rank):\n    # Load base model with quantization\n    model = load_quantized(model_name)\n\n    # Initialize vLLM engine\n    vllm_engine = vLLM(\n        model = model,\n        max_lora_rank = max_lora_rank,  # Pre-allocate for LoRA\n        gpu_memory_utilization = 0.5,   # Reserve for KV cache\n        enable_lora = True,\n    )\n\n    # Attach engine to model\n    model.vllm_engine = vllm_engine\n    model.generate = vllm_engine.generate  # Override generate\n\n    return model\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_FastLanguageModel_from_pretrained_vllm]]\n\n",
      "domains": [
        "Deep Learning",
        "Reinforcement Learning",
        "Inference"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Paper",
          "title": "vLLM",
          "url": "https://arxiv.org/abs/2309.06180"
        },
        {
          "type": "Doc",
          "title": "vLLM Documentation",
          "url": "https://docs.vllm.ai"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastLanguageModel_from_pretrained_vllm"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Reward_Functions",
      "page_title": "Unslothai Unsloth Reward Functions",
      "page_type": "Principle",
      "overview": "Design and implementation of reward signals that guide reinforcement learning policy optimization toward desired model behaviors.",
      "content": "# Principle: Reward_Functions\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Paper|RLHF|https://arxiv.org/abs/2203.02155]]\n* [[source::Paper|Constitutional AI|https://arxiv.org/abs/2212.08073]]\n|-\n! Domains\n| [[domain::Reinforcement_Learning]], [[domain::Reward_Modeling]], [[domain::NLP]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nDesign and implementation of reward signals that guide reinforcement learning policy optimization toward desired model behaviors.\n\n=== Description ===\n\nReward Functions are the mechanism by which we communicate objectives to RL algorithms. In GRPO (Group Relative Policy Optimization), rewards score multiple completions per prompt, enabling the algorithm to identify which behaviors to reinforce.\n\nTypes of reward functions:\n1. **Rule-based**: Heuristic checks (format, length, keywords)\n2. **Verification-based**: Programmatic correctness (math, code execution)\n3. **Model-based**: Trained reward models or LLM-as-judge\n4. **Hybrid**: Combination of multiple signals\n\nEffective reward functions should be:\n* Aligned with the true objective\n* Informative (provide gradient signal)\n* Robust to gaming/hacking\n\n=== Usage ===\n\nDesign reward functions when:\n* Training with GRPO, PPO, or other RL methods\n* You can define success criteria for completions\n* Ground truth or verification is available\n\nKey design decisions:\n* What behaviors to reward/penalize\n* How to weight multiple objectives\n* Whether to use sparse or dense rewards\n\n== Theoretical Basis ==\n\n=== GRPO Objective ===\n\nGRPO optimizes:\n\n<math>\n\\mathcal{L}_{GRPO} = -\\mathbb{E}_{x \\sim D} \\mathbb{E}_{y_1, \\ldots, y_G \\sim \\pi_\\theta(\\cdot|x)} \\left[ \\sum_{i=1}^{G} (r_i - \\bar{r}) \\log \\pi_\\theta(y_i|x) \\right]\n</math>\n\nWhere:\n* G completions are generated per prompt x\n* r_i is the reward for completion y_i\n* r\u0304 is the mean reward (group reference)\n* The advantage (r_i - r\u0304) determines if behavior is reinforced or suppressed\n\n=== Reward Shaping ===\n\nDense rewards provide more learning signal than sparse rewards:\n\n'''Sparse:''' Score = 1 if correct, 0 otherwise\n'''Dense:''' Score = 0.5 for format + 0.5 \u00d7 similarity(answer, target)\n\nDense rewards guide learning more effectively but risk reward hacking.\n\n=== Reward Hacking ===\n\nModels may find unintended ways to maximize reward:\n\n<math>\n\\pi^* = \\arg\\max_\\pi \\mathbb{E}_{\\pi}[r] \\neq \\pi_{desired}\n</math>\n\nMitigations:\n* Multiple orthogonal reward functions\n* KL divergence penalty from reference model\n* Human evaluation checkpoints\n\n=== Multi-Objective Rewards ===\n\nCombining rewards:\n\n<math>\nr_{total} = \\sum_{i} w_i \\cdot r_i\n</math>\n\nOr using reward stacking (GRPOTrainer sums rewards automatically):\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# Reward function design (abstract)\ndef design_reward(task_type):\n    if task_type == \"math\":\n        rewards = [\n            format_reward,        # Check for reasoning structure\n            correctness_reward,   # Verify answer\n        ]\n    elif task_type == \"code\":\n        rewards = [\n            syntax_reward,        # Valid code\n            execution_reward,     # Runs correctly\n            style_reward,         # Code quality\n        ]\n    elif task_type == \"general\":\n        rewards = [\n            model_reward,         # LLM-as-judge\n            length_reward,        # Appropriate length\n        ]\n\n    return rewards\n</syntaxhighlight>\n\n=== Reward Scaling ===\n\nRewards should be normalized for stable training:\n* Mean-centered: subtract running average\n* Scaled: divide by standard deviation\n* Clipped: limit extreme values\n\nGRPO's group-relative formulation provides automatic normalization within each batch.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_reward_function_pattern]]\n\n",
      "domains": [
        "Reinforcement Learning",
        "Reward Modeling",
        "NLP"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Paper",
          "title": "RLHF",
          "url": "https://arxiv.org/abs/2203.02155"
        },
        {
          "type": "Paper",
          "title": "Constitutional AI",
          "url": "https://arxiv.org/abs/2212.08073"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_reward_function_pattern"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_SFT_Pretraining",
      "page_title": "Unslothai Unsloth SFT Pretraining",
      "page_type": "Principle",
      "overview": "Technique for initializing RL policy networks with supervised fine-tuning to establish baseline generation quality before reinforcement learning optimization.",
      "content": "# Principle: SFT_Pretraining\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|InstructGPT|https://arxiv.org/abs/2203.02155]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::Reinforcement_Learning]], [[domain::NLP]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechnique for initializing RL policy networks with supervised fine-tuning to establish baseline generation quality before reinforcement learning optimization.\n\n=== Description ===\n\nSFT Pretraining (or warm-up) bridges pre-trained models and RL training. Starting RL from a raw pre-trained model often fails because:\n\n1. **Random policy**: Model generates incoherent text, no reward signal\n2. **Exploration inefficiency**: Most generations are garbage\n3. **Gradient noise**: High variance updates from poor samples\n\nA brief SFT phase teaches the model basic task structure, providing a competent starting point for RL refinement.\n\n=== Usage ===\n\nApply SFT Pretraining when:\n* Starting GRPO/PPO training from a general model\n* The task has complex output format (reasoning, code)\n* Initial generations are too poor for reward evaluation\n\nTypically 100-1000 SFT steps are sufficient for warm-up.\n\n== Theoretical Basis ==\n\n=== Policy Initialization ===\n\nRLHF pipeline:\n\n<math>\n\\text{Pretrained} \\xrightarrow{\\text{SFT}} \\text{SFT Model} \\xrightarrow{\\text{RL}} \\text{Final Model}\n</math>\n\nSFT provides a good initialization \u03c0_SFT that:\n* Generates grammatical, coherent text\n* Follows basic task instructions\n* Produces parseable outputs for reward functions\n\n=== Response-Only Loss ===\n\nSFT loss is computed only on response tokens:\n\n<math>\n\\mathcal{L}_{SFT} = -\\sum_{t \\in \\text{response}} \\log P_\\theta(y_t | x, y_{<t})\n</math>\n\nThis prevents the model from learning to generate user turns, focusing capacity on response quality.\n\n=== KL Divergence Reference ===\n\nIn RL, the SFT model often serves as the KL reference:\n\n<math>\n\\mathcal{L}_{RL} = -r(y) + \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{SFT})\n</math>\n\nThis regularizes the RL policy to not drift too far from coherent generation.\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# SFT pretraining for RL (abstract)\ndef pretrain_for_rl(model, sft_dataset, target_steps=100):\n    # 1. Create SFT trainer with response-only loss\n    trainer = SFTTrainer(model, sft_dataset)\n    trainer = train_on_responses_only(trainer, user_marker, assistant_marker)\n\n    # 2. Brief training to establish baseline\n    trainer.train(max_steps=target_steps)\n\n    # 3. Model now generates coherent task-relevant output\n    # Ready for RL training\n\n    return model\n</syntaxhighlight>\n\n=== When to Skip SFT Pretraining ===\n\nMay skip if:\n* Model is already instruction-tuned for the task\n* Task is simple (single-word answers)\n* Pre-trained model already generates reasonable outputs\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_train_on_responses_only]]\n\n",
      "domains": [
        "Deep Learning",
        "Reinforcement Learning",
        "NLP"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "InstructGPT",
          "url": "https://arxiv.org/abs/2203.02155"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_train_on_responses_only"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Supervised_Finetuning",
      "page_title": "Unslothai Unsloth Supervised Finetuning",
      "page_type": "Principle",
      "overview": "Training methodology where a pre-trained language model learns to follow instructions by minimizing cross-entropy loss on human-labeled input-output pairs.",
      "content": "# Principle: Supervised_Finetuning\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|Instruction Tuning|https://arxiv.org/abs/2109.01652]]\n* [[source::Paper|FLAN|https://arxiv.org/abs/2210.11416]]\n* [[source::Blog|TRL Documentation|https://huggingface.co/docs/trl/sft_trainer]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::NLP]], [[domain::Instruction_Tuning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTraining methodology where a pre-trained language model learns to follow instructions by minimizing cross-entropy loss on human-labeled input-output pairs.\n\n=== Description ===\n\nSupervised Fine-Tuning (SFT) adapts a pre-trained language model to follow specific instructions or complete particular tasks. The model learns from demonstrations\u2014pairs of inputs (instructions/prompts) and desired outputs (completions/responses).\n\nSFT differs from pre-training in several key aspects:\n* **Data**: Curated instruction-response pairs vs raw text\n* **Objective**: Task completion vs next-token prediction on arbitrary text\n* **Scale**: Thousands to millions of examples vs trillions of tokens\n* **Format**: Structured templates vs unstructured documents\n\nIn the QLoRA context, SFT updates only the low-rank adapter weights while keeping quantized base weights frozen.\n\n=== Usage ===\n\nApply supervised fine-tuning when:\n* Creating instruction-following assistants\n* Adapting models to specific domains (medical, legal, code)\n* Teaching new output formats (JSON, markdown, specific structures)\n* Improving task performance with labeled examples\n\nSFT is typically the first alignment step, followed by preference optimization (RLHF/DPO) for further refinement.\n\n== Theoretical Basis ==\n\n=== Cross-Entropy Loss ===\n\nSFT minimizes the negative log-likelihood of target tokens:\n\n<math>\n\\mathcal{L}_{SFT} = -\\sum_{t=1}^{T} \\log P_\\theta(y_t | x, y_{<t})\n</math>\n\nWhere:\n* x: Input prompt/instruction\n* y: Target response\n* \u03b8: Model parameters (LoRA adapters in QLoRA)\n\n=== Loss Masking ===\n\nOnly response tokens contribute to the loss:\n\n<math>\n\\mathcal{L} = -\\sum_{t \\in \\text{response}} \\log P_\\theta(y_t | \\text{prompt}, y_{<t})\n</math>\n\nPrompt tokens are attended to but not predicted, preventing the model from learning to generate instructions.\n\n=== Sample Packing ===\n\nFor efficiency, multiple short sequences can be packed into single training examples:\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# Sample packing (abstract)\ndef pack_sequences(sequences, max_length):\n    packed = []\n    current = []\n    current_length = 0\n\n    for seq in sequences:\n        if current_length + len(seq) <= max_length:\n            current.append(seq)\n            current_length += len(seq)\n        else:\n            packed.append(concat_with_separator(current))\n            current = [seq]\n            current_length = len(seq)\n\n    return packed\n</syntaxhighlight>\n\nBenefits:\n* Better GPU utilization (fewer padding tokens)\n* Faster training (more examples per batch)\n* Consistent sequence lengths\n\n=== Gradient Checkpointing ===\n\nTrade compute for memory by recomputing activations during backward pass:\n\n<math>\n\\text{Memory}_{checkpoint} = O(\\sqrt{L}) \\text{ vs } O(L) \\text{ standard}\n</math>\n\nWhere L is the number of layers. Unsloth's implementation further optimizes this with selective checkpointing.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_SFTTrainer_train]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Sample_Packing_Tip]]\n",
      "domains": [
        "Deep Learning",
        "NLP",
        "Instruction Tuning"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "Instruction Tuning",
          "url": "https://arxiv.org/abs/2109.01652"
        },
        {
          "type": "Paper",
          "title": "FLAN",
          "url": "https://arxiv.org/abs/2210.11416"
        },
        {
          "type": "Blog",
          "title": "TRL Documentation",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_SFTTrainer_train"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Sample_Packing_Tip"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Training_Configuration",
      "page_title": "Unslothai Unsloth Training Configuration",
      "page_type": "Principle",
      "overview": "Selection and configuration of training hyperparameters including learning rate, batch size, optimizer, and precision settings for effective LoRA fine-tuning.",
      "content": "# Principle: Training_Configuration\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|AdamW Optimizer|https://arxiv.org/abs/1711.05101]]\n* [[source::Blog|Learning Rate Schedules|https://huggingface.co/docs/transformers/main_classes/optimizer_schedules]]\n* [[source::Paper|8-bit Optimizers|https://arxiv.org/abs/2110.02861]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::Optimization]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nSelection and configuration of training hyperparameters including learning rate, batch size, optimizer, and precision settings for effective LoRA fine-tuning.\n\n=== Description ===\n\nTraining Configuration for QLoRA fine-tuning involves selecting hyperparameters that balance training stability, convergence speed, and memory efficiency. Key decisions include:\n\n* **Learning rate**: Typically 1e-4 to 3e-4 for LoRA (higher than full fine-tuning)\n* **Batch size \u00d7 Gradient accumulation**: Effective batch size for stable gradients\n* **Optimizer**: 8-bit AdamW for memory efficiency with minimal accuracy loss\n* **Precision**: bf16 preferred (if supported), fp16 as fallback\n* **Warmup**: Brief warmup (1-5% of steps) for stable initial training\n\nThese choices are interdependent\u2014higher learning rates may require smaller batches or more warmup.\n\n=== Usage ===\n\nConfigure training parameters after model and LoRA setup, before trainer initialization. Consider:\n* GPU memory constraints (batch size, precision)\n* Dataset size (epochs vs max_steps)\n* Task complexity (learning rate, total steps)\n* Checkpoint strategy (save frequency vs disk space)\n\n== Theoretical Basis ==\n\n=== Learning Rate Selection ===\n\nLoRA typically uses higher learning rates than full fine-tuning because:\n1. Fewer parameters \u2192 larger per-parameter gradients needed for same model change\n2. Low-rank constraint limits expressivity \u2192 need stronger signal\n3. Base weights frozen \u2192 only adapters must capture task knowledge\n\nRecommended range: 1e-4 to 5e-4 (vs 1e-5 to 5e-5 for full fine-tuning)\n\n=== Effective Batch Size ===\n\n<math>\n\\text{Effective Batch} = \\text{per\\_device\\_batch} \\times \\text{num\\_gpus} \\times \\text{gradient\\_accumulation}\n</math>\n\nLarger effective batches:\n* More stable gradients\n* Can support higher learning rates\n* Better hardware utilization\n\nTypical effective batch sizes: 8-64 for fine-tuning\n\n=== 8-bit Optimizer States ===\n\nAdamW maintains two momentum buffers per parameter:\n* First moment (m): Running mean of gradients\n* Second moment (v): Running mean of squared gradients\n\n8-bit quantization reduces optimizer state memory by 75%:\n\n<math>\n\\text{Memory}_{8bit} = \\frac{1}{4} \\times \\text{Memory}_{fp32}\n</math>\n\nWith dynamic scaling and block-wise quantization, accuracy loss is negligible.\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# Training configuration selection (abstract)\ndef configure_training(model, dataset_size, gpu_memory):\n    # Start with defaults\n    config = {\n        \"learning_rate\": 2e-4,\n        \"batch_size\": 2,\n        \"grad_accum\": 4,\n        \"warmup_ratio\": 0.03,\n        \"optimizer\": \"adamw_8bit\",\n    }\n\n    # Adjust for GPU memory\n    if gpu_memory < 16:\n        config[\"batch_size\"] = 1\n        config[\"grad_accum\"] = 8\n\n    # Adjust for dataset size\n    steps_per_epoch = dataset_size // (config[\"batch_size\"] * config[\"grad_accum\"])\n    config[\"warmup_steps\"] = int(steps_per_epoch * config[\"warmup_ratio\"])\n\n    return config\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_UnslothTrainingArguments]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Embedding_Learning_Rate_Tip]]\n",
      "domains": [
        "Deep Learning",
        "Optimization",
        "Training"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "AdamW Optimizer",
          "url": "https://arxiv.org/abs/1711.05101"
        },
        {
          "type": "Blog",
          "title": "Learning Rate Schedules",
          "url": "https://huggingface.co/docs/transformers/main_classes/optimizer_schedules"
        },
        {
          "type": "Paper",
          "title": "8-bit Optimizers",
          "url": "https://arxiv.org/abs/2110.02861"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_UnslothTrainingArguments"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Embedding_Learning_Rate_Tip"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Vision_Inference_Mode",
      "page_title": "Unslothai Unsloth Vision Inference Mode",
      "page_type": "Principle",
      "overview": "Configuration of Vision-Language Models for efficient inference with gradients disabled and checkpointing deactivated.",
      "content": "# Principle: Vision_Inference_Mode\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|PyTorch Inference|https://pytorch.org/docs/stable/notes/autograd.html]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::Inference]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration of Vision-Language Models for efficient inference with gradients disabled and checkpointing deactivated.\n\n=== Description ===\n\nVision Inference Mode optimizes VLMs for generation:\n* **No gradients**: Frees memory used for gradient computation\n* **No checkpointing**: Faster forward pass\n* **Eval mode**: Disables dropout for deterministic outputs\n\n=== Usage ===\n\nSwitch to inference mode after training, before running generation or evaluation.\n\n== Theoretical Basis ==\n\n=== Memory Savings ===\n\nInference mode reduces memory by:\n* No gradient tensors stored\n* No activation checkpoints\n* Reduced intermediate states\n\n<math>\n\\text{Memory}_{inference} < \\text{Memory}_{training}\n</math>\n\nTypical reduction: 30-50% depending on model architecture.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_FastBaseModel_for_inference]]\n",
      "domains": [
        "Deep Learning",
        "Inference",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "PyTorch Inference",
          "url": "https://pytorch.org/docs/stable/notes/autograd.html"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastBaseModel_for_inference"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Vision_LoRA_Configuration",
      "page_title": "Unslothai Unsloth Vision LoRA Configuration",
      "page_type": "Principle",
      "overview": "Configuration of LoRA adapters for Vision-Language Models with selective targeting of vision encoder, language model, and intermediate layers.",
      "content": "# Principle: Vision_LoRA_Configuration\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Paper|LLaVA|https://arxiv.org/abs/2304.08485]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Parameter_Efficient_Finetuning]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration of LoRA adapters for Vision-Language Models with selective targeting of vision encoder, language model, and intermediate layers.\n\n=== Description ===\n\nVision LoRA Configuration extends standard LoRA to multimodal architectures. VLMs have three trainable regions:\n\n1. **Vision Encoder**: Extracts visual features (ViT layers)\n2. **Projector**: Maps visual to language space\n3. **Language Model**: Generates text output\n\nLoRA can be applied selectively:\n* Vision-only: Visual understanding improvements\n* Language-only: Text generation quality\n* Both: Full multimodal adaptation\n\n=== Usage ===\n\nConfigure Vision LoRA based on task:\n* **Image captioning**: Both vision + language\n* **Document OCR**: Language-heavy\n* **Visual reasoning**: Balanced approach\n* **Domain adaptation**: Vision-heavy\n\n== Theoretical Basis ==\n\n=== Component-Wise LoRA ===\n\nFor a VLM, LoRA can be applied to different components:\n\n<math>\n\\Delta W = \\Delta W_{vision} + \\Delta W_{projector} + \\Delta W_{language}\n</math>\n\nEach component's LoRA:\n\n<math>\n\\Delta W_{component} = B_{component} A_{component}\n</math>\n\n=== Parameter Efficiency ===\n\nVLM LoRA parameters by configuration:\n\n| Configuration | Trainable % | Parameters (11B VLM) |\n|---------------|-------------|---------------------|\n| Vision only | ~0.3% | ~30M |\n| Language only | ~0.8% | ~80M |\n| Both | ~1.1% | ~110M |\n| Attention only | ~0.5% | ~50M |\n\n=== Vision Encoder Considerations ===\n\nVision encoders (ViT) may behave differently with LoRA:\n* **Pre-trained vision features** may be sufficient\n* **Domain shift** (medical, satellite) benefits from vision LoRA\n* **Smaller ranks** often sufficient for vision (r=8 vs r=16)\n\n'''Pseudo-code Logic:'''\n<syntaxhighlight lang=\"python\">\n# Vision LoRA configuration (abstract)\ndef configure_vision_lora(task_type, domain_shift):\n    if task_type == \"document_ocr\":\n        # Text-heavy task\n        config = {\n            \"finetune_vision_layers\": False,\n            \"finetune_language_layers\": True,\n            \"r\": 16,\n        }\n    elif domain_shift == \"high\":\n        # New visual domain (medical, satellite)\n        config = {\n            \"finetune_vision_layers\": True,\n            \"finetune_language_layers\": True,\n            \"r\": 32,  # Higher capacity\n        }\n    else:\n        # General VQA\n        config = {\n            \"finetune_vision_layers\": True,\n            \"finetune_language_layers\": True,\n            \"r\": 16,\n        }\n\n    return config\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_FastBaseModel_get_peft_model]]\n",
      "domains": [
        "Computer Vision",
        "Parameter Efficient Finetuning",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Paper",
          "title": "LLaVA",
          "url": "https://arxiv.org/abs/2304.08485"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastBaseModel_get_peft_model"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Vision_Model_Loading",
      "page_title": "Unslothai Unsloth Vision Model Loading",
      "page_type": "Principle",
      "overview": "Technique for loading Vision-Language Models with memory-efficient quantization while preserving multimodal understanding capabilities.",
      "content": "# Principle: Vision_Model_Loading\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LLaVA|https://arxiv.org/abs/2304.08485]]\n* [[source::Paper|Qwen-VL|https://arxiv.org/abs/2308.12966]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::NLP]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechnique for loading Vision-Language Models with memory-efficient quantization while preserving multimodal understanding capabilities.\n\n=== Description ===\n\nVision Model Loading prepares VLMs for multimodal fine-tuning by:\n\n1. Loading vision encoder (often ViT-based)\n2. Loading language model (transformer decoder)\n3. Loading projector/connector between modalities\n4. Quantizing applicable components\n\nVLMs have unique considerations:\n* Vision encoders often stay in higher precision\n* Language model benefits most from quantization\n* Projector layers may be trained from scratch\n\n=== Usage ===\n\nLoad VLMs when fine-tuning for:\n* Image-text tasks (captioning, VQA)\n* Document understanding (OCR, charts)\n* Multi-image reasoning\n* Video understanding (frame-based)\n\nKey differences from text models:\n* Returns AutoProcessor (not tokenizer)\n* Handles image preprocessing\n* May have separate vision/language LoRA options\n\n== Theoretical Basis ==\n\n=== VLM Architecture ===\n\nTypical VLM structure:\n\n<math>\n\\text{Image} \\xrightarrow{\\text{Vision Encoder}} \\text{Visual Tokens} \\xrightarrow{\\text{Projector}} \\text{Language Space} \\xrightarrow{\\text{LLM}} \\text{Output}\n</math>\n\nComponents:\n* **Vision Encoder**: Extracts image features (ViT, SigLIP)\n* **Projector**: Maps visual to language space (MLP, cross-attention)\n* **LLM**: Generates text conditioned on visual + text tokens\n\n=== Quantization Strategy ===\n\nDifferent components have different quantization sensitivity:\n\n| Component | Quantization | Reason |\n|-----------|--------------|--------|\n| Vision Encoder | Keep higher precision | Visual features sensitive |\n| Projector | Trainable (not quantized) | Small, critical |\n| LLM | 4-bit NF4 | Large, benefits most |\n\n=== Memory Estimation ===\n\n<math>\n\\text{Memory} = \\text{Vision}_{fp16} + \\text{Projector}_{fp16} + \\text{LLM}_{4bit}\n</math>\n\nFor 11B VLM:\n* Vision: ~600MB (ViT-L/14)\n* Projector: ~200MB\n* LLM: ~6GB (4-bit)\n* Total: ~7GB (vs 22GB at fp16)\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_FastVisionModel_from_pretrained]]\n",
      "domains": [
        "Computer Vision",
        "NLP",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LLaVA",
          "url": "https://arxiv.org/abs/2304.08485"
        },
        {
          "type": "Paper",
          "title": "Qwen-VL",
          "url": "https://arxiv.org/abs/2308.12966"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastVisionModel_from_pretrained"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Vision_Model_Saving",
      "page_title": "Unslothai Unsloth Vision Model Saving",
      "page_type": "Principle",
      "overview": "Techniques for serializing fine-tuned Vision-Language Models with all components (vision encoder, projector, language model) and processor.",
      "content": "# Principle: Vision_Model_Saving\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|HuggingFace Saving|https://huggingface.co/docs/transformers/main_classes/model]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Model_Serialization]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTechniques for serializing fine-tuned Vision-Language Models with all components (vision encoder, projector, language model) and processor.\n\n=== Description ===\n\nVision Model Saving extends text model saving to include:\n* **Vision encoder** weights (if fine-tuned)\n* **Projector** weights\n* **Language model** weights\n* **AutoProcessor** (tokenizer + image processor)\n\nSame save methods apply (lora, merged_16bit) but GGUF export has limited VLM support.\n\n=== Usage ===\n\nSave VLMs after training. Note that GGUF export for VLMs is limited to certain architectures.\n\n== Theoretical Basis ==\n\n=== VLM Components to Save ===\n\n| Component | LoRA Save | Merged Save |\n|-----------|-----------|-------------|\n| Vision Encoder | If adapted | Full weights |\n| Projector | Full | Full |\n| Language Model | LoRA weights | Merged weights |\n| Processor | Config only | Config only |\n\n=== GGUF Limitations ===\n\nllama.cpp has limited VLM support. Currently supported:\n* LLaVA-style architectures\n* Qwen2-VL (partial)\n\nNot supported:\n* Most newer VLM architectures\n* Multi-image models\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_save_pretrained_vision]]\n",
      "domains": [
        "Computer Vision",
        "Model Serialization",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "HuggingFace Saving",
          "url": "https://huggingface.co/docs/transformers/main_classes/model"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_save_pretrained_vision"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Vision_Training",
      "page_title": "Unslothai Unsloth Vision Training",
      "page_type": "Principle",
      "overview": "Training methodology for Vision-Language Models using supervised fine-tuning on image-text pairs with specialized data collation.",
      "content": "# Principle: Vision_Training\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Paper|LLaVA|https://arxiv.org/abs/2304.08485]]\n* [[source::Doc|TRL SFTTrainer|https://huggingface.co/docs/trl/sft_trainer]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Training]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nTraining methodology for Vision-Language Models using supervised fine-tuning on image-text pairs with specialized data collation.\n\n=== Description ===\n\nVision Training adapts VLMs to specific tasks through supervised learning on multimodal data. Key differences from text-only training:\n\n1. **Data collation**: Must handle variable-size images and batch them\n2. **Processing**: AutoProcessor for both images and text\n3. **Memory**: Higher due to image embeddings\n4. **Loss masking**: Only on text responses, not visual tokens\n\n=== Usage ===\n\nApply Vision Training when:\n* Fine-tuning VLMs for specific visual tasks\n* Adapting to new image domains\n* Teaching new output formats for visual inputs\n\n== Theoretical Basis ==\n\n=== VLM Loss Function ===\n\nLoss computed on text response tokens:\n\n<math>\n\\mathcal{L} = -\\sum_{t \\in \\text{text}} \\log P(y_t | \\text{images}, \\text{prompt}, y_{<t})\n</math>\n\nVisual tokens serve as context but are not predicted.\n\n=== Data Collation ===\n\nUnslothVisionDataCollator handles:\n* Padding images to consistent size\n* Creating attention masks for variable lengths\n* Batching pixel values efficiently\n* Preserving image-text alignment\n\n=== Memory Considerations ===\n\nVLM training uses more memory due to:\n* Image tensors (pixel values)\n* Vision encoder activations\n* Cross-modal attention patterns\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_SFTTrainer_vision]]\n",
      "domains": [
        "Computer Vision",
        "Training",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Paper",
          "title": "LLaVA",
          "url": "https://arxiv.org/abs/2304.08485"
        },
        {
          "type": "Doc",
          "title": "TRL SFTTrainer",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_SFTTrainer_vision"
        }
      ]
    },
    {
      "id": "Principle/Unslothai_Unsloth_Vision_Training_Mode",
      "page_title": "Unslothai Unsloth Vision Training Mode",
      "page_type": "Principle",
      "overview": "Configuration of Vision-Language Models for gradient-enabled training mode with memory-efficient checkpointing.",
      "content": "# Principle: Vision_Training_Mode\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|PyTorch Training|https://pytorch.org/docs/stable/notes/autograd.html]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::Training]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConfiguration of Vision-Language Models for gradient-enabled training mode with memory-efficient checkpointing.\n\n=== Description ===\n\nVision Training Mode ensures the model is properly configured for fine-tuning:\n* **Gradient computation** enabled for trainable parameters\n* **Gradient checkpointing** activated for memory efficiency\n* **Training mode** set (affects dropout, batch norm)\n\nVLMs require explicit mode switching because inference and training have different requirements.\n\n=== Usage ===\n\nSet training mode before starting the training loop. Unsloth typically handles this automatically, but explicit calls may be needed for custom training loops.\n\n== Theoretical Basis ==\n\n=== Training vs Inference Mode ===\n\n| Aspect | Training Mode | Inference Mode |\n|--------|---------------|----------------|\n| Gradients | Enabled | Disabled |\n| Checkpointing | Active | Disabled |\n| Memory | Higher | Lower |\n| Speed | Slower | Faster |\n| Dropout | Active | Disabled |\n\n=== Gradient Checkpointing ===\n\nTrades compute for memory:\n\n<math>\n\\text{Memory}_{checkpoint} = O(\\sqrt{L}) \\text{ vs } O(L)\n</math>\n\nActivations are recomputed during backward pass instead of stored.\n\n== Related Pages ==\n\n=== Implemented By ===\n* [[implemented_by::Implementation:Unslothai_Unsloth_FastBaseModel_for_training]]\n",
      "domains": [
        "Deep Learning",
        "Training",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "PyTorch Training",
          "url": "https://pytorch.org/docs/stable/notes/autograd.html"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implemented_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastBaseModel_for_training"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_ALLOWED_QUANTS",
      "page_title": "Unslothai Unsloth ALLOWED QUANTS",
      "page_type": "Implementation",
      "overview": "Concrete reference for available GGUF quantization methods in Unsloth.",
      "content": "# Implementation: ALLOWED_QUANTS\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::GGUF]], [[domain::Model_Compression]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete reference for available GGUF quantization methods in Unsloth.\n\n=== Description ===\n\n`ALLOWED_QUANTS` is a dictionary mapping quantization method names to human-readable descriptions. It defines all valid quantization options for `save_pretrained_gguf` and `push_to_hub_gguf`.\n\n=== Usage ===\n\nReference this when selecting quantization_method parameter for GGUF export.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/save.py\n* '''Lines:''' L104-131\n\n=== Definition ===\n<syntaxhighlight lang=\"python\">\nALLOWED_QUANTS = {\n    \"not_quantized\": \"Recommended. Fast conversion. Slow inference, big files.\",\n    \"fast_quantized\": \"Recommended. Fast conversion. OK inference, OK file size.\",\n    \"quantized\": \"Recommended. Slow conversion. Fast inference, small files.\",\n    \"f32\": \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\n    \"bf16\": \"Bfloat16 - Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\n    \"f16\": \"Float16  - Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\n    \"q8_0\": \"Fast conversion. High resource use, but generally acceptable.\",\n    \"q4_k_m\": \"Recommended. Uses Q6_K for half of attention.wv and feed_forward.w2, else Q4_K\",\n    \"q5_k_m\": \"Recommended. Uses Q6_K for half of attention.wv and feed_forward.w2, else Q5_K\",\n    \"q2_k\": \"Uses Q4_K for attention.vw and feed_forward.w2, Q2_K for other tensors.\",\n    \"q3_k_l\": \"Uses Q5_K for attention.wv, attention.wo, feed_forward.w2, else Q3_K\",\n    \"q3_k_m\": \"Uses Q4_K for attention.wv, attention.wo, feed_forward.w2, else Q3_K\",\n    \"q3_k_s\": \"Uses Q3_K for all tensors\",\n    \"q4_0\": \"Original quant method, 4-bit.\",\n    \"q4_1\": \"Higher accuracy than q4_0 but not as high as q5_0. Quicker inference than q5.\",\n    \"q4_k_s\": \"Uses Q4_K for all tensors\",\n    \"q4_k\": \"alias for q4_k_m\",\n    \"q5_k\": \"alias for q5_k_m\",\n    \"q5_0\": \"Higher accuracy, higher resource usage and slower inference.\",\n    \"q5_1\": \"Even higher accuracy, resource usage and slower inference.\",\n    \"q5_k_s\": \"Uses Q5_K for all tensors\",\n    \"q6_k\": \"Uses Q8_K for all tensors\",\n    \"q3_k_xs\": \"3-bit extra small quantization\",\n}\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Alias Resolution ===\n{| class=\"wikitable\"\n|-\n! Alias !! Resolves To !! Description\n|-\n| \"not_quantized\" || \"f16\" or \"bf16\" || Full precision based on model dtype\n|-\n| \"fast_quantized\" || \"q8_0\" || Quick conversion, good quality\n|-\n| \"quantized\" || \"q4_k_m\" || Best balance of size/quality\n|-\n| \"q4_k\" || \"q4_k_m\" || Alias\n|-\n| \"q5_k\" || \"q5_k_m\" || Alias\n|}\n\n== Usage Examples ==\n\n=== List Available Methods ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.save import ALLOWED_QUANTS, print_quantization_methods\n\n# Print all available methods with descriptions\nprint_quantization_methods()\n\n# Access programmatically\nfor method, description in ALLOWED_QUANTS.items():\n    print(f\"{method}: {description}\")\n</syntaxhighlight>\n\n=== Select Method for Export ===\n<syntaxhighlight lang=\"python\">\n# Recommended for most use cases\nquantization_method = \"q4_k_m\"  # 4-bit with mixed precision\n\n# For higher quality\nquantization_method = \"q8_0\"  # 8-bit\n\n# Multiple formats at once\nquantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\"]\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Quantization_Selection]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n",
      "domains": [
        "Quantization",
        "GGUF",
        "Model Compression"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Quantization_Selection"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Attention_Dispatch",
      "page_title": "Unslothai Unsloth Attention Dispatch",
      "page_type": "Implementation",
      "overview": "Unified attention backend selection and execution with support for FlashAttention, xFormers, and PyTorch SDPA.",
      "content": "# Implementation: Attention_Dispatch\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Attention]], [[domain::Infrastructure]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nUnified attention backend selection and execution with support for FlashAttention, xFormers, and PyTorch SDPA.\n\n=== Description ===\nThis module provides a unified interface for running attention operations across different backends. It automatically selects the best available backend (FlashAttention > xFormers > SDPA) and handles packed sequences, grouped query attention, and sliding window attention transparently.\n\n=== Usage ===\nUsed by all model attention forward passes to dispatch to the optimal attention implementation based on available libraries and input characteristics.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/utils/attention_dispatch.py unsloth/utils/attention_dispatch.py]\n* '''Lines:''' 1-275\n\n=== Key Classes ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass AttentionConfig:\n    \"\"\"Per-layer attention metadata.\"\"\"\n    backend: str                    # Selected backend\n    n_kv_heads: int                # Number of KV heads\n    n_groups: int                  # GQA groups\n    flash_dense_kwargs: dict       # FlashAttention dense kwargs\n    flash_varlen_kwargs: dict      # FlashAttention varlen kwargs\n    sdpa_kwargs: dict              # SDPA kwargs\n    xformers_kwargs: dict          # xFormers kwargs\n\n@dataclass\nclass AttentionContext:\n    \"\"\"Per-call info required to run attention.\"\"\"\n    bsz: int                       # Batch size\n    q_len: int                     # Query sequence length\n    kv_seq_len: int               # KV sequence length\n    n_heads: int                   # Number of attention heads\n    head_dim: int                  # Head dimension\n    requires_grad: bool            # Training vs inference\n    seq_info: Optional[Tuple]      # Packed sequence info\n    attention_mask: Optional[Tensor]\n    causal_mask: Optional[Any]\n    sliding_window: Optional[int]  # Sliding window size\n</syntaxhighlight>\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef select_attention_backend(use_varlen: bool = False) -> str:\n    \"\"\"\n    Return attention backend based on availability.\n\n    Priority: flash_varlen > flash_dense > xformers > sdpa\n\n    Args:\n        use_varlen: Prefer varlen flash for packed sequences\n\n    Returns:\n        Backend name: \"flash_varlen\", \"flash_dense\", \"xformers\", or \"sdpa\"\n    \"\"\"\n\ndef run_attention(\n    *,\n    config: AttentionConfig,\n    context: AttentionContext,\n    Q: Tensor,\n    K: Tensor,\n    V: Tensor,\n) -> Tensor:\n    \"\"\"\n    Run attention using config/context info.\n\n    Handles GQA expansion, packed sequence masking, and\n    sliding window attention for each backend.\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.utils.attention_dispatch import (\n    AttentionConfig,\n    AttentionContext,\n    select_attention_backend,\n    run_attention,\n    FLASH_VARLEN,\n    FLASH_DENSE,\n    XFORMERS,\n    SDPA,\n)\n</syntaxhighlight>\n\n== Backend Priority ==\n\n{| class=\"wikitable\"\n|-\n! Priority !! Backend !! Constant !! Condition\n|-\n| 1 || FlashAttention varlen || FLASH_VARLEN || HAS_FLASH_ATTENTION and seq_info present\n|-\n| 2 || FlashAttention dense || FLASH_DENSE || HAS_FLASH_ATTENTION\n|-\n| 3 || xFormers || XFORMERS || HAS_XFORMERS\n|-\n| 4 || PyTorch SDPA || SDPA || Always available (fallback)\n|}\n\n== Usage Examples ==\n\n=== Select Backend ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.utils.attention_dispatch import select_attention_backend\n\n# For packed sequences (sample packing)\nbackend = select_attention_backend(use_varlen=True)\n# Returns \"flash_varlen\" if FlashAttention installed\n\n# For standard batches\nbackend = select_attention_backend(use_varlen=False)\n# Returns \"flash_dense\" or \"xformers\" or \"sdpa\"\n</syntaxhighlight>\n\n=== Run Attention ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.utils.attention_dispatch import (\n    AttentionConfig, AttentionContext, run_attention\n)\n\nconfig = AttentionConfig(\n    backend=\"flash_dense\",\n    n_kv_heads=8,\n    n_groups=4,  # 32 Q heads / 8 KV heads\n    flash_dense_kwargs={\"causal\": True},\n)\n\ncontext = AttentionContext(\n    bsz=2,\n    q_len=512,\n    kv_seq_len=512,\n    n_heads=32,\n    head_dim=128,\n    requires_grad=True,\n    seq_info=None,\n    attention_mask=None,\n    causal_mask=None,\n)\n\noutput = run_attention(config=config, context=context, Q=Q, K=K, V=V)\n# output shape: [bsz, q_len, n_heads, head_dim]\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Attention",
        "Infrastructure",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_CLI",
      "page_title": "Unslothai Unsloth CLI",
      "page_type": "Implementation",
      "overview": "Command-line interface for fine-tuning language models using Unsloth's optimized training pipeline.",
      "content": "# Implementation: CLI\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Docs|https://docs.unsloth.ai]]\n|-\n! Domains\n| [[domain::CLI]], [[domain::Training]], [[domain::NLP]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nCommand-line interface for fine-tuning language models using Unsloth's optimized training pipeline.\n\n=== Description ===\nThe `unsloth-cli.py` script provides a complete CLI entry point for the Unsloth fine-tuning workflow. It wraps `FastLanguageModel.from_pretrained()`, `get_peft_model()`, and `SFTTrainer` into a single command with configurable arguments for model loading, LoRA configuration, training parameters, and model saving/export.\n\n=== Usage ===\nUse this CLI when you want to fine-tune a model without writing Python code, or for scripted automation of training jobs. Supports HuggingFace datasets, local raw text files, and ModelScope datasets.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth-cli.py unsloth-cli.py]\n* '''Lines:''' 1-473\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef run(args):\n    \"\"\"\n    Main entry point for CLI fine-tuning.\n\n    Args:\n        args: Parsed argparse namespace with:\n            - model_name: HuggingFace model ID\n            - max_seq_length: Maximum sequence length\n            - dtype: Data type (auto-detected if None)\n            - load_in_4bit: Enable 4-bit quantization\n            - r: LoRA rank\n            - lora_alpha: LoRA alpha parameter\n            - lora_dropout: Dropout rate\n            - per_device_train_batch_size: Batch size\n            - learning_rate: Learning rate\n            - max_steps: Maximum training steps\n            - save_model: Whether to save model\n            - save_gguf: Whether to export as GGUF\n            - quantization: GGUF quantization method(s)\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"bash\">\n# Run directly from command line\npython unsloth-cli.py --model_name \"unsloth/llama-3-8b\" --load_in_4bit\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| --model_name || str || No (default: unsloth/llama-3-8b) || HuggingFace model ID\n|-\n| --max_seq_length || int || No (default: 2048) || Maximum sequence length\n|-\n| --load_in_4bit || flag || No || Enable 4-bit QLoRA quantization\n|-\n| --dataset || str || No (default: yahma/alpaca-cleaned) || Dataset name or path\n|-\n| --r || int || No (default: 16) || LoRA rank\n|-\n| --lora_alpha || int || No (default: 16) || LoRA alpha scaling\n|-\n| --max_steps || int || No (default: 400) || Maximum training steps\n|-\n| --learning_rate || float || No (default: 2e-4) || Learning rate\n|-\n| --save_model || flag || No || Save model after training\n|-\n| --save_gguf || flag || No || Export to GGUF format\n|-\n| --quantization || str/list || No (default: q8_0) || GGUF quantization method(s)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| Trained model || Directory || Saved to --save_path (default: model/)\n|-\n| GGUF files || Files || If --save_gguf enabled, GGUF model files\n|-\n| Logs || Directory || Training logs at --output_dir (default: outputs/)\n|}\n\n== Usage Examples ==\n\n=== Basic Fine-Tuning ===\n<syntaxhighlight lang=\"bash\">\n# Basic QLoRA fine-tuning with defaults\npython unsloth-cli.py \\\n    --model_name \"unsloth/llama-3-8b\" \\\n    --load_in_4bit \\\n    --max_steps 100 \\\n    --save_model \\\n    --save_path \"./my_model\"\n</syntaxhighlight>\n\n=== Full Configuration Example ===\n<syntaxhighlight lang=\"bash\">\n# Full configuration with GGUF export\npython unsloth-cli.py \\\n    --model_name \"unsloth/llama-3-8b\" \\\n    --max_seq_length 8192 \\\n    --load_in_4bit \\\n    --r 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.1 \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 8 \\\n    --warmup_steps 5 \\\n    --max_steps 400 \\\n    --learning_rate 2e-6 \\\n    --optim \"adamw_8bit\" \\\n    --weight_decay 0.005 \\\n    --lr_scheduler_type \"linear\" \\\n    --output_dir \"outputs\" \\\n    --report_to \"tensorboard\" \\\n    --save_model \\\n    --save_gguf \\\n    --save_path \"model\" \\\n    --quantization \"q4_k_m\" \"q8_0\"\n</syntaxhighlight>\n\n=== Raw Text Training ===\n<syntaxhighlight lang=\"bash\">\n# Train on raw text file\npython unsloth-cli.py \\\n    --model_name \"unsloth/llama-3-8b\" \\\n    --load_in_4bit \\\n    --raw_text_file \"./my_corpus.txt\" \\\n    --chunk_size 2048 \\\n    --stride 512 \\\n    --max_steps 200 \\\n    --save_model\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "CLI",
        "Training",
        "NLP"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Docs",
          "url": "https://docs.unsloth.ai"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Device_Type",
      "page_title": "Unslothai Unsloth Device Type",
      "page_type": "Implementation",
      "overview": "Device detection and capability module supporting NVIDIA CUDA, AMD HIP, and Intel XPU accelerators.",
      "content": "# Implementation: Device_Type\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::Device]], [[domain::Compatibility]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nDevice detection and capability module supporting NVIDIA CUDA, AMD HIP, and Intel XPU accelerators.\n\n=== Description ===\nThis module provides device detection utilities to determine the available GPU type (CUDA, HIP, XPU) and configure Unsloth appropriately. It handles AMD Instinct vs Radeon differences, bitsandbytes compatibility, and quantization block size requirements across different GPU architectures.\n\n=== Usage ===\nImported automatically by Unsloth to determine device capabilities. Used throughout the codebase to condition behavior on GPU type.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/device_type.py unsloth/device_type.py]\n* '''Lines:''' 1-128\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef is_hip() -> bool:\n    \"\"\"Check if running on AMD HIP (ROCm).\"\"\"\n\ndef get_device_type() -> str:\n    \"\"\"\n    Detect available GPU accelerator.\n\n    Returns:\n        str: One of \"cuda\", \"hip\", or \"xpu\"\n\n    Raises:\n        NotImplementedError: If no supported GPU found\n    \"\"\"\n\n# Module-level constants\nDEVICE_TYPE: str        # Detected device type\nDEVICE_TYPE_TORCH: str  # Device type for torch functions (hip->cuda)\nDEVICE_COUNT: int       # Number of GPUs\nALLOW_PREQUANTIZED_MODELS: bool  # Whether blocksize 64 quantization works\nALLOW_BITSANDBYTES: bool  # Whether bitsandbytes is compatible\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.device_type import (\n    is_hip,\n    get_device_type,\n    DEVICE_TYPE,\n    DEVICE_COUNT,\n    ALLOW_PREQUANTIZED_MODELS,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Outputs (Module Constants) ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| DEVICE_TYPE || str || \"cuda\", \"hip\", or \"xpu\"\n|-\n| DEVICE_TYPE_TORCH || str || Same as DEVICE_TYPE but \"hip\" -> \"cuda\" for torch compatibility\n|-\n| DEVICE_COUNT || int || Number of available GPUs\n|-\n| ALLOW_PREQUANTIZED_MODELS || bool || Whether blocksize 64 quantization is supported\n|-\n| ALLOW_BITSANDBYTES || bool || Whether bitsandbytes is compatible with current setup\n|}\n\n== GPU Compatibility Notes ==\n\n{| class=\"wikitable\"\n|-\n! Device !! Warp Size !! Block Size !! 4-bit QLoRA\n|-\n| NVIDIA CUDA || 32 || 64 || \u2705 Full support\n|-\n| AMD Radeon (Navi) || 32 || 64 || \u2705 Full support (bnb >= 0.49)\n|-\n| AMD Instinct (MI) || 64 || 128 || \u26a0\ufe0f Limited (WIP)\n|-\n| Intel XPU || - || - || \u2705 Requires torch >= 2.6\n|}\n\n== Usage Examples ==\n\n=== Check Device Type ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.device_type import DEVICE_TYPE, DEVICE_COUNT\n\nprint(f\"Running on {DEVICE_TYPE} with {DEVICE_COUNT} GPU(s)\")\n\nif DEVICE_TYPE == \"hip\":\n    print(\"AMD GPU detected - some features may be limited\")\n</syntaxhighlight>\n\n=== Conditional Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.device_type import ALLOW_PREQUANTIZED_MODELS\n\nif ALLOW_PREQUANTIZED_MODELS:\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"unsloth/llama-3-8b-bnb-4bit\",  # Pre-quantized\n        load_in_4bit=True,\n    )\nelse:\n    # Fall back to on-the-fly quantization or 16-bit\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"meta-llama/Llama-3-8B\",\n        load_in_4bit=True,  # Will quantize on load\n    )\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Infrastructure",
        "Device",
        "Compatibility"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FP8_Kernels",
      "page_title": "Unslothai Unsloth FP8 Kernels",
      "page_type": "Implementation",
      "overview": "Triton and FBGEMM-based kernels for FP8 (8-bit floating point) matrix multiplication with block-wise and row-wise quantization support.",
      "content": "# Implementation: FP8_Kernels\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|DeepSeek-V3|https://arxiv.org/abs/2412.19437]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::Quantization]], [[domain::FP8]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nTriton and FBGEMM-based kernels for FP8 (8-bit floating point) matrix multiplication with block-wise and row-wise quantization support.\n\n=== Description ===\nThis module provides optimized FP8 matmul implementations for training and inference with quantized models (e.g., DeepSeek-V3). It includes Triton kernels for block-wise FP8 quantization/dequantization, autograd-compatible linear layers, and automatic backend selection (FBGEMM > TorchAO > Triton) based on availability and GPU compatibility.\n\n=== Usage ===\nUse these functions when working with FP8-quantized models like DeepSeek-V3. The module automatically selects the fastest available backend for your GPU.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/fp8.py unsloth/kernels/fp8.py]\n* '''Lines:''' 1-615\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef weight_dequant(\n    x: torch.Tensor,      # FP8 weight tensor\n    s: torch.Tensor,      # Scale tensor\n    dtype: torch.dtype = torch.bfloat16,\n) -> torch.Tensor:\n    \"\"\"Dequantize FP8 weights to higher precision.\"\"\"\n\ndef act_quant(\n    x: torch.Tensor,\n    block_size: int = 128,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Quantize activations to FP8 with per-block scales.\"\"\"\n\ndef w8a8_block_fp8_matmul_triton(\n    A: torch.Tensor,      # FP8 activations\n    B: torch.Tensor,      # FP8 weights\n    As: torch.Tensor,     # Activation scales\n    Bs: torch.Tensor,     # Weight scales\n    block_size: List[int],  # [block_n, block_k]\n    output_dtype: torch.dtype = torch.float32,\n) -> torch.Tensor:\n    \"\"\"Block-wise FP8 matmul using Triton kernels.\"\"\"\n\ndef fp8_linear(\n    X: torch.Tensor,\n    weight: torch.Tensor,\n    weight_scale: torch.Tensor,\n    bias: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    \"\"\"Unified FP8 linear layer (auto-selects block/row mode).\"\"\"\n\nclass FP8BlockQuantLinear(torch.autograd.Function):\n    \"\"\"Autograd function for block-quantized FP8 linear with backward support.\"\"\"\n\nclass FbgemmFp8Linear_matmul(torch.autograd.Function):\n    \"\"\"Autograd function for FBGEMM row-quantized FP8 linear.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.fp8 import (\n    fp8_linear,\n    weight_dequant,\n    act_quant,\n    w8a8_block_fp8_matmul_triton,\n    fp8_block_quant_linear,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs (fp8_linear) ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| X || Tensor || Yes || Input activations [batch, seq_len, hidden_dim]\n|-\n| weight || Tensor || Yes || FP8 weight tensor [out_features, in_features]\n|-\n| weight_scale || Tensor || Yes || Scale tensor (2D for block, 1D for row quantization)\n|-\n| bias || Tensor || No || Optional bias tensor\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| output || Tensor || Linear layer output in X.dtype\n|}\n\n== Backend Selection ==\n\nThe module automatically selects the best FP8 backend:\n\n{| class=\"wikitable\"\n|-\n! Priority !! Backend !! Condition !! Notes\n|-\n| 1 || FBGEMM f8f8bf16_blockwise || fbgemm_gpu >= 1.4.0, test_has_fbgemm() passes || 15% faster than TorchAO\n|-\n| 2 || TorchAO blockwise_fp8_gemm || torchao available || 3x faster than pure Triton\n|-\n| 3 || Triton w8a8_block_fp8_matmul || Always available || Fallback implementation\n|}\n\nFBGEMM is automatically disabled on:\n- Consumer GPUs (RTX 4090/5090) - CUTLASS kernel failures\n- SM100 Blackwell GPUs - Architecture mismatch errors\n- fbgemm_gpu < 1.4.0 - Numerical precision issues\n\n== Usage Examples ==\n\n=== Basic FP8 Linear ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.fp8 import fp8_linear\nimport torch\n\n# Assume FP8 weights from a quantized model\n# weight: [out_features, in_features] in torch.float8_e4m3fn\n# weight_scale: [out_features // 128, in_features // 128] for block quant\n# or [out_features, 1] for row quant\n\nx = torch.randn(2, 512, 4096, dtype=torch.bfloat16, device=\"cuda\")\noutput = fp8_linear(x, weight, weight_scale)\n# output: [2, 512, out_features] in bfloat16\n</syntaxhighlight>\n\n=== Manual Activation Quantization ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.fp8 import act_quant, w8a8_block_fp8_matmul_triton\n\n# Quantize activations\nx_fp8, x_scale = act_quant(x, block_size=128)\n\n# Perform FP8 matmul\noutput = w8a8_block_fp8_matmul_triton(\n    x_fp8, weight, x_scale, weight_scale,\n    block_size=[128, 128],\n    output_dtype=torch.bfloat16,\n)\n</syntaxhighlight>\n\n=== Dequantize Weights ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.fp8 import weight_dequant\n\n# Dequantize for debugging or non-FP8 operations\nweight_bf16 = weight_dequant(weight_fp8, weight_scale, dtype=torch.bfloat16)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "Quantization",
        "FP8"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "DeepSeek-V3",
          "url": "https://arxiv.org/abs/2412.19437"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastBaseModel_for_inference",
      "page_title": "Unslothai Unsloth FastBaseModel for inference",
      "page_type": "Implementation",
      "overview": "Concrete tool for configuring Vision-Language Models for efficient inference mode after training.",
      "content": "# Implementation: FastBaseModel_for_inference\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Inference]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for configuring Vision-Language Models for efficient inference mode after training.\n\n=== Description ===\n\n`FastVisionModel.for_inference` prepares a VLM for generation by:\n* Disabling gradient computation\n* Disabling gradient checkpointing\n* Setting model to evaluation mode\n\nThis reduces memory usage and speeds up generation.\n\n=== Usage ===\n\nCall after training to switch to inference mode for evaluation or deployment.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/vision.py\n* '''Lines:''' L1191-1250\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@staticmethod\ndef for_inference(model: PreTrainedModel) -> None:\n    \"\"\"\n    Configure model for inference mode.\n\n    Args:\n        model: VLM with LoRA adapters\n\n    Returns:\n        None (modifies model in place)\n    \"\"\"\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Switch to Inference Mode ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\n# After training...\ntrainer.train()\n\n# Switch to inference mode\nFastVisionModel.for_inference(model)\n# Or: model.for_inference()\n\n# Now generate\nwith torch.no_grad():\n    outputs = model.generate(\n        **processor(images=image, text=prompt, return_tensors=\"pt\"),\n        max_new_tokens=256,\n    )\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Vision_Inference_Mode]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Computer Vision",
        "Inference",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Inference_Mode"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastBaseModel_for_training",
      "page_title": "Unslothai Unsloth FastBaseModel for training",
      "page_type": "Implementation",
      "overview": "Concrete tool for configuring Vision-Language Models for training mode with gradient checkpointing and proper gradient computation enabled.",
      "content": "# Implementation: FastBaseModel_for_training\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Training]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for configuring Vision-Language Models for training mode with gradient checkpointing and proper gradient computation enabled.\n\n=== Description ===\n\n`FastVisionModel.for_training` (implemented in `FastBaseModel`) prepares a VLM for fine-tuning by:\n* Enabling gradient computation\n* Activating gradient checkpointing\n* Setting model to training mode\n\nThis is necessary before passing the model to SFTTrainer.\n\n=== Usage ===\n\nCall before starting training if the model was previously in inference mode. Typically called automatically by Unsloth, but can be called explicitly.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/vision.py\n* '''Lines:''' L1190-1250\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@staticmethod\ndef for_training(\n    model: PreTrainedModel,\n    use_gradient_checkpointing: bool = True,\n) -> None:\n    \"\"\"\n    Configure model for training mode.\n\n    Args:\n        model: VLM with LoRA adapters\n        use_gradient_checkpointing: Enable gradient checkpointing\n\n    Returns:\n        None (modifies model in place)\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n# Called as: FastVisionModel.for_training(model) or model.for_training()\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Explicit Training Mode ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\nmodel, processor = FastVisionModel.from_pretrained(...)\nmodel = FastVisionModel.get_peft_model(model, ...)\n\n# Explicitly enable training mode\nFastVisionModel.for_training(model)\n\n# Or use the attached method\nmodel.for_training()\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Vision_Training_Mode]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Computer Vision",
        "Training",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Training_Mode"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastBaseModel_get_peft_model",
      "page_title": "Unslothai Unsloth FastBaseModel get peft model",
      "page_type": "Implementation",
      "overview": "Concrete tool for injecting LoRA adapters into Vision-Language Models with fine-grained control over which components (vision, language, attention, MLP) receive adapters.",
      "content": "# Implementation: FastBaseModel_get_peft_model\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::NLP]], [[domain::Parameter_Efficient_Finetuning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for injecting LoRA adapters into Vision-Language Models with fine-grained control over which components (vision, language, attention, MLP) receive adapters.\n\n=== Description ===\n\n`FastVisionModel.get_peft_model` (implemented in `FastBaseModel`) applies LoRA adapters to VLMs with additional parameters for controlling:\n* Vision encoder LoRA (`finetune_vision_layers`)\n* Language model LoRA (`finetune_language_layers`)\n* Attention module targeting (`finetune_attention_modules`)\n* MLP module targeting (`finetune_mlp_modules`)\n\nThis fine-grained control is essential because different tasks may benefit from different adapter placements.\n\n=== Usage ===\n\nCall after loading VLM with `FastVisionModel.from_pretrained`. Configure which components to adapt based on your task:\n* OCR/document: Focus on language layers\n* Image understanding: Include vision layers\n* General VQA: Both vision and language\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/vision.py\n* '''Lines:''' L921-1076\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@staticmethod\ndef get_peft_model(\n    model: PreTrainedModel,\n    r: int = 16,\n    target_modules: Optional[Union[List[str], str]] = None,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.0,\n    bias: str = \"none\",\n    finetune_vision_layers: bool = True,\n    finetune_language_layers: bool = True,\n    finetune_attention_modules: bool = True,\n    finetune_mlp_modules: bool = True,\n    layers_to_transform: Optional[List[int]] = None,\n    layers_pattern: Optional[str] = None,\n    use_gradient_checkpointing: str = \"unsloth\",\n    random_state: int = 3407,\n    use_rslora: bool = False,\n    modules_to_save: Optional[List[str]] = None,\n    **kwargs,\n) -> PeftModelForCausalLM:\n    \"\"\"\n    Apply LoRA adapters to a Vision-Language Model.\n\n    Args:\n        model: VLM from FastVisionModel.from_pretrained\n        r: LoRA rank\n        finetune_vision_layers: Apply LoRA to vision encoder\n        finetune_language_layers: Apply LoRA to language model\n        finetune_attention_modules: Target attention projections\n        finetune_mlp_modules: Target MLP projections\n        target_modules: Override auto-detection with specific modules\n        use_gradient_checkpointing: \"unsloth\" for memory efficiency\n\n    Returns:\n        PeftModelForCausalLM with LoRA adapters\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n# Called as: FastVisionModel.get_peft_model(model, ...)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || VLM from FastVisionModel.from_pretrained\n|-\n| r || int || No (default: 16) || LoRA rank\n|-\n| finetune_vision_layers || bool || No (default: True) || Apply LoRA to vision encoder\n|-\n| finetune_language_layers || bool || No (default: True) || Apply LoRA to language model\n|-\n| finetune_attention_modules || bool || No (default: True) || Target attention projections\n|-\n| finetune_mlp_modules || bool || No (default: True) || Target MLP projections\n|-\n| target_modules || List[str] or str || No || Override with specific module names\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PeftModelForCausalLM || VLM with LoRA adapters attached\n|}\n\n== Usage Examples ==\n\n=== Full VLM LoRA (Vision + Language) ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\nmodel, processor = FastVisionModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n    load_in_4bit = True,\n)\n\n# Apply LoRA to both vision and language\nmodel = FastVisionModel.get_peft_model(\n    model,\n    r = 16,\n    finetune_vision_layers = True,\n    finetune_language_layers = True,\n    finetune_attention_modules = True,\n    finetune_mlp_modules = True,\n    use_gradient_checkpointing = \"unsloth\",\n)\n\nmodel.print_trainable_parameters()\n</syntaxhighlight>\n\n=== Language-Only LoRA (OCR/Document) ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\nmodel, processor = FastVisionModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n    load_in_4bit = True,\n)\n\n# Only adapt language model for text-heavy tasks\nmodel = FastVisionModel.get_peft_model(\n    model,\n    r = 16,\n    finetune_vision_layers = False,   # Keep vision frozen\n    finetune_language_layers = True,\n    finetune_attention_modules = True,\n    finetune_mlp_modules = True,\n)\n</syntaxhighlight>\n\n=== Attention-Only LoRA ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\nmodel, processor = FastVisionModel.from_pretrained(\n    model_name = \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n    load_in_4bit = True,\n)\n\n# Only attention, not MLP (fewer parameters)\nmodel = FastVisionModel.get_peft_model(\n    model,\n    r = 32,\n    finetune_vision_layers = True,\n    finetune_language_layers = True,\n    finetune_attention_modules = True,\n    finetune_mlp_modules = False,  # Skip MLP\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Vision_LoRA_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Computer Vision",
        "NLP",
        "Parameter Efficient Finetuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_LoRA_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastCohereModel",
      "page_title": "Unslothai Unsloth FastCohereModel",
      "page_type": "Implementation",
      "overview": "Optimized patching module for Cohere Command R/R+ models with QK normalization and fast attention.",
      "content": "# Implementation: FastCohereModel\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Cohere Command R|https://huggingface.co/CohereForAI/c4ai-command-r-plus]]\n|-\n! Domains\n| [[domain::Models]], [[domain::NLP]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching module for Cohere Command R/R+ models with QK normalization and fast attention.\n\n=== Description ===\nThis module provides Unsloth-optimized implementations for Cohere models (Command R, Command R+). It includes optimized attention with QK normalization support, fast layernorm inference, and efficient memory management. Cohere models use a unique architecture with QK normalization in the attention layer.\n\n=== Usage ===\nUsed automatically when loading Cohere models through `FastLanguageModel.from_pretrained()`. The patches are applied to the HuggingFace transformers Cohere implementation.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/cohere.py unsloth/models/cohere.py]\n* '''Lines:''' 1-526\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef CohereAttention_fast_forward(\n    self,\n    hidden_states: torch.Tensor,\n    causal_mask: Optional[BlockDiagonalCausalMask] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    padding_mask: Optional[torch.LongTensor] = None,\n    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    \"\"\"\n    Fast forward pass for Cohere attention with QK normalization.\n    \"\"\"\n\ndef fast_layernorm_inference(\n    self,\n    X: torch.Tensor,\n    out_weight: torch.Tensor = None,\n) -> torch.Tensor:\n    \"\"\"\n    Fast FP32 layernorm for inference.\n    Used for QK normalization in Cohere attention.\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Usually accessed through FastLanguageModel\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"CohereForAI/c4ai-command-r-plus\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n# Cohere patches applied automatically\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs (Attention) ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| hidden_states || Tensor || Yes || [batch, seq_len, hidden_dim]\n|-\n| attention_mask || Tensor || No || Attention mask\n|-\n| position_ids || Tensor || No || Position indices\n|-\n| past_key_value || Tuple || No || KV cache for generation\n|-\n| position_embeddings || Tuple || No || (cos, sin) for RoPE\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| attn_output || Tensor || [batch, seq_len, hidden_dim]\n|-\n| attn_weights || Tensor/None || Attention weights if output_attentions\n|-\n| past_key_value || Tuple/None || Updated KV cache if use_cache\n|}\n\n== Cohere Architecture Notes ==\n\nCohere models have unique characteristics handled by this module:\n\n{| class=\"wikitable\"\n|-\n! Feature !! Description\n|-\n| QK Normalization || LayerNorm applied to Q and K before attention\n|-\n| Grouped Query Attention || Multiple query heads per KV head\n|-\n| Rotary Position Embeddings || RoPE for position encoding\n|-\n| Logit Softcapping || (Some variants) Soft cap on attention logits\n|}\n\n== Usage Examples ==\n\n=== Load and Fine-tune Cohere Model ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load with 4-bit quantization\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"CohereForAI/c4ai-command-r-v01\",\n    max_seq_length=4096,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n)\n\n# Train with SFTTrainer...\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "NLP",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Cohere Command R",
          "url": "https://huggingface.co/CohereForAI/c4ai-command-r-plus"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastFalconH1Model",
      "page_title": "Unslothai Unsloth FastFalconH1Model",
      "page_type": "Implementation",
      "overview": "Optimized patching module for Falcon H1 hybrid Mamba-Attention models.",
      "content": "# Implementation: FastFalconH1Model\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Models]], [[domain::Hybrid]], [[domain::Mamba]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching module for Falcon H1 hybrid Mamba-Attention models.\n\n=== Description ===\nThis module provides Unsloth-optimized implementations for Falcon H1 models, which use a hybrid architecture combining Mamba state-space layers with traditional attention layers. The patching includes optimized attention forward passes and support for the FalconHybridMambaAttentionDynamicCache.\n\n=== Usage ===\nUsed automatically when loading Falcon H1 models through `FastLanguageModel.from_pretrained()`. Requires transformers >= 4.53.0.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/falcon_h1.py unsloth/models/falcon_h1.py]\n* '''Lines:''' 1-764\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef FalconH1Attention_fast_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Fast forward pass for Falcon H1 attention layers.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"tiiuae/Falcon-H1-1B\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n</syntaxhighlight>\n\n== Falcon H1 Architecture Notes ==\n\n{| class=\"wikitable\"\n|-\n! Feature !! Description\n|-\n| Hybrid Architecture || Combines Mamba SSM layers with Attention layers\n|-\n| Dynamic Cache || Uses FalconHybridMambaAttentionDynamicCache\n|-\n| RoPE || Rotary Position Embeddings for attention layers\n|-\n| Minimum Transformers || Requires transformers >= 4.53.0\n|}\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "Hybrid",
        "Mamba"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastGemma2Model",
      "page_title": "Unslothai Unsloth FastGemma2Model",
      "page_type": "Implementation",
      "overview": "Optimized patching module for Google Gemma 2 models with logit softcapping and sliding window attention support.",
      "content": "# Implementation: FastGemma2Model\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|Gemma2|https://arxiv.org/abs/2408.00118]]\n|-\n! Domains\n| [[domain::Models]], [[domain::NLP]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching module for Google Gemma 2 models with logit softcapping and sliding window attention support.\n\n=== Description ===\nThis module provides Unsloth-optimized implementations for Gemma 2 models (9B, 27B). It includes support for logit softcapping in attention (a Gemma2 innovation), sliding window attention, and optimized inference with GEGLU activations.\n\n=== Usage ===\nUsed automatically when loading Gemma 2 models through `FastLanguageModel.from_pretrained()`.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/gemma2.py unsloth/models/gemma2.py]\n* '''Lines:''' 1-654\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"google/gemma-2-9b\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n</syntaxhighlight>\n\n== Gemma 2 Architecture Notes ==\n\n{| class=\"wikitable\"\n|-\n! Feature !! Description\n|-\n| Logit Softcapping || Soft cap on attention logits: t * tanh(logits / t)\n|-\n| Sliding Window || Alternating global and local attention\n|-\n| Query Pre-Attention Scaling || Uses query_pre_attn_scalar from config\n|-\n| GEGLU Activation || GELU-gated linear unit in MLP\n|}\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "NLP",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "Gemma2",
          "url": "https://arxiv.org/abs/2408.00118"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastGemmaModel",
      "page_title": "Unslothai Unsloth FastGemmaModel",
      "page_type": "Implementation",
      "overview": "Optimized patching module for Google Gemma models with fast GEGLU inference and attention.",
      "content": "# Implementation: FastGemmaModel\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Gemma|https://ai.google.dev/gemma]]\n|-\n! Domains\n| [[domain::Models]], [[domain::NLP]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching module for Google Gemma models with fast GEGLU inference and attention.\n\n=== Description ===\nThis module provides Unsloth-optimized implementations for Gemma models (1.0). It includes fast GEGLU activation inference (using tanh approximation), optimized decoder layer forward passes, and efficient attention with sample packing support.\n\n=== Usage ===\nUsed automatically when loading Gemma models through `FastLanguageModel.from_pretrained()`. Requires transformers >= 4.38.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/gemma.py unsloth/models/gemma.py]\n* '''Lines:''' 1-474\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef fast_geglu_inference(self, X: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Fast GEGLU inference for Gemma MLP.\n    Uses tanh approximation: gelu(gate) * up\n    \"\"\"\n\ndef GemmaDecoderLayer_fast_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: Optional[bool] = False,\n    use_cache: Optional[bool] = False,\n) -> Tuple[torch.Tensor, ...]:\n    \"\"\"Fast forward pass for Gemma decoder layer.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"google/gemma-7b\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n</syntaxhighlight>\n\n== Gemma Architecture Notes ==\n\n{| class=\"wikitable\"\n|-\n! Feature !! Description\n|-\n| GEGLU Activation || Uses GELU-gated linear unit in MLP\n|-\n| RoPE || Rotary Position Embeddings\n|-\n| RMSNorm || Root Mean Square normalization\n|-\n| Minimum Transformers || Requires transformers >= 4.38\n|}\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "NLP",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Gemma",
          "url": "https://ai.google.dev/gemma"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastGraniteModel",
      "page_title": "Unslothai Unsloth FastGraniteModel",
      "page_type": "Implementation",
      "overview": "Optimized patching module for IBM Granite models.",
      "content": "# Implementation: FastGraniteModel\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Models]], [[domain::NLP]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching module for IBM Granite models.\n\n=== Description ===\nThis module provides Unsloth-optimized implementations for IBM Granite models. It includes optimized attention forward passes and efficient inference mode with memory management.\n\n=== Usage ===\nUsed automatically when loading Granite models through `FastLanguageModel.from_pretrained()`.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/granite.py unsloth/models/granite.py]\n* '''Lines:''' 1-610\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"ibm-granite/granite-3b-code-instruct\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "NLP",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastLanguageModel_from_pretrained",
      "page_title": "Unslothai Unsloth FastLanguageModel from pretrained",
      "page_type": "Implementation",
      "overview": "Concrete tool for loading Large Language Models with 4-bit quantization for memory-efficient QLoRA fine-tuning, provided by the Unsloth library.",
      "content": "# Implementation: FastLanguageModel_from_pretrained\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Unsloth Docs|https://docs.unsloth.ai]]\n* [[source::Paper|QLoRA|https://arxiv.org/abs/2305.14314]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Model_Loading]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for loading Large Language Models with 4-bit quantization for memory-efficient QLoRA fine-tuning, provided by the Unsloth library.\n\n=== Description ===\n\n`FastLanguageModel.from_pretrained` is the primary entry point for loading pre-trained language models with Unsloth's optimizations. It handles automatic 4-bit NF4 quantization via bitsandbytes, device mapping, attention backend selection (Flash Attention 2, xformers, or SDPA), and model-specific patches. The function returns both the model and tokenizer in a single call, ready for LoRA adapter injection.\n\nKey capabilities for QLoRA fine-tuning:\n* **4-bit NF4 quantization** - Reduces memory footprint by ~75% while maintaining accuracy\n* **Automatic model detection** - Routes to appropriate model class (LLaMA, Mistral, Qwen, Gemma, etc.)\n* **Unsloth gradient checkpointing** - Memory-efficient alternative to standard gradient checkpointing\n* **Tokenizer patching** - Fixes common tokenizer issues (padding, special tokens)\n\n=== Usage ===\n\nUse this function when starting a QLoRA fine-tuning workflow. Import and call at the beginning of your training script to load a base model for fine-tuning. This is the first step in the standard QLoRA training pipeline before applying LoRA adapters with `get_peft_model`.\n\nNOT for:\n* Reinforcement learning workflows (use `fast_inference=True` variant instead)\n* Full fine-tuning (set `full_finetuning=True` which routes to FastModel)\n* Vision-language models (use `FastVisionModel.from_pretrained`)\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/loader.py\n* '''Lines:''' L121-700\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass FastLanguageModel(FastLlamaModel):\n    @staticmethod\n    def from_pretrained(\n        model_name: str = \"unsloth/Llama-3.2-1B-Instruct\",\n        max_seq_length: int = 2048,\n        dtype: Optional[torch.dtype] = None,\n        load_in_4bit: bool = True,\n        load_in_8bit: bool = False,\n        load_in_16bit: bool = False,\n        full_finetuning: bool = False,\n        token: Optional[str] = None,\n        device_map: str = \"sequential\",\n        rope_scaling: Optional[dict] = None,\n        fix_tokenizer: bool = True,\n        trust_remote_code: bool = False,\n        use_gradient_checkpointing: str = \"unsloth\",\n        resize_model_vocab: Optional[int] = None,\n        revision: Optional[str] = None,\n        use_exact_model_name: bool = False,\n        offload_embedding: bool = False,\n        float32_mixed_precision: Optional[bool] = None,\n        fast_inference: bool = False,\n        gpu_memory_utilization: float = 0.5,\n        float8_kv_cache: bool = False,\n        random_state: int = 3407,\n        max_lora_rank: int = 64,\n        disable_log_stats: bool = True,\n        qat_scheme: Optional[str] = None,\n        load_in_fp8: bool = False,\n        unsloth_tiled_mlp: bool = False,\n        **kwargs,\n    ) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n        \"\"\"\n        Load a pretrained language model with Unsloth optimizations.\n\n        Args:\n            model_name: HuggingFace model ID or local path\n            max_seq_length: Maximum sequence length for training\n            dtype: Compute dtype (None for auto-detection)\n            load_in_4bit: Enable 4-bit NF4 quantization for QLoRA\n            load_in_8bit: Enable 8-bit quantization\n            load_in_16bit: Load in float16 without quantization\n            full_finetuning: Enable full parameter training (no LoRA)\n            token: HuggingFace token for private models\n            device_map: Device placement strategy\n            use_gradient_checkpointing: \"unsloth\" for optimized checkpointing\n            fix_tokenizer: Apply tokenizer fixes\n            trust_remote_code: Allow custom model code\n\n        Returns:\n            Tuple of (model, tokenizer) with optimizations applied\n        \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model_name || str || Yes || HuggingFace model ID (e.g., \"unsloth/Llama-3.2-1B-Instruct\") or local path\n|-\n| max_seq_length || int || No (default: 2048) || Maximum sequence length for RoPE scaling and attention\n|-\n| load_in_4bit || bool || No (default: True) || Enable 4-bit NF4 quantization via bitsandbytes\n|-\n| dtype || torch.dtype || No (default: None) || Compute dtype; None for auto-detection based on GPU\n|-\n| token || str || No || HuggingFace Hub token for private/gated models\n|-\n| use_gradient_checkpointing || str || No (default: \"unsloth\") || Gradient checkpointing mode\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PreTrainedModel || Quantized model with Unsloth patches applied\n|-\n| tokenizer || PreTrainedTokenizer || Tokenizer with padding and special token fixes\n|}\n\n== Usage Examples ==\n\n=== Basic QLoRA Loading ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load model with 4-bit quantization for QLoRA fine-tuning\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,   # Enable QLoRA quantization\n    dtype = None,          # Auto-detect (float16 or bfloat16)\n)\n\n# Model is now ready for LoRA adapter injection\nprint(f\"Model dtype: {model.dtype}\")\nprint(f\"Tokenizer vocab size: {len(tokenizer)}\")\n</syntaxhighlight>\n\n=== Loading with Custom Sequence Length ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load for longer context fine-tuning\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.1-8B-Instruct\",\n    max_seq_length = 8192,  # Extended context\n    load_in_4bit = True,\n    dtype = None,\n    use_gradient_checkpointing = \"unsloth\",  # Memory-efficient\n)\n</syntaxhighlight>\n\n=== Loading Private/Gated Models ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load gated model with HuggingFace token\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"meta-llama/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    token = \"hf_your_token_here\",  # Required for gated models\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Model_Loading]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Gradient_Checkpointing_Tip]]\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_BFloat16_vs_Float16_Tip]]\n",
      "domains": [
        "NLP",
        "Model Loading",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Unsloth Docs",
          "url": "https://docs.unsloth.ai"
        },
        {
          "type": "Paper",
          "title": "QLoRA",
          "url": "https://arxiv.org/abs/2305.14314"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Loading"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Gradient_Checkpointing_Tip"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_BFloat16_vs_Float16_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastLanguageModel_from_pretrained_vllm",
      "page_title": "Unslothai Unsloth FastLanguageModel from pretrained vllm",
      "page_type": "Implementation",
      "overview": "Concrete tool for loading Large Language Models with vLLM fast inference backend enabled for reinforcement learning training workflows, provided by Unsloth.",
      "content": "# Implementation: FastLanguageModel_from_pretrained_vllm\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|vLLM Documentation|https://docs.vllm.ai]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Reinforcement_Learning]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for loading Large Language Models with vLLM fast inference backend enabled for reinforcement learning training workflows, provided by Unsloth.\n\n=== Description ===\n\n`FastLanguageModel.from_pretrained` with `fast_inference=True` loads a model with vLLM engine integration for high-throughput generation during RL training. vLLM enables efficient batch generation with continuous batching and PagedAttention, critical for GRPO training which requires multiple completions per prompt.\n\nKey capabilities for RL training:\n* **vLLM engine attachment** - Model gains `model.vllm_engine` for fast generation\n* **Higher LoRA rank support** - `max_lora_rank` parameter for vLLM LoRA\n* **GPU memory partitioning** - `gpu_memory_utilization` controls vLLM cache allocation\n* **Continuous batching** - Efficient generation of multiple completions per prompt\n\nThis is an angle-specific documentation of `from_pretrained` for the RL Model Loading principle.\n\n=== Usage ===\n\nUse when setting up GRPO, PPO, or other RL training workflows that require fast generation sampling. The vLLM backend enables generating 6-16 completions per prompt efficiently, which is essential for GRPO's group-relative optimization.\n\nNOT for:\n* Standard SFT training (use default `fast_inference=False`)\n* Systems without vLLM installed\n* Very limited GPU memory (vLLM requires additional VRAM for KV cache)\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/loader.py\n* '''Lines:''' L121-700 (same function, different parameters)\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@staticmethod\ndef from_pretrained(\n    model_name: str = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length: int = 2048,\n    dtype: Optional[torch.dtype] = None,\n    load_in_4bit: bool = True,\n    # RL-specific parameters\n    fast_inference: bool = True,  # Enable vLLM\n    gpu_memory_utilization: float = 0.5,  # vLLM GPU fraction\n    max_lora_rank: int = 64,  # Maximum LoRA rank for vLLM\n    float8_kv_cache: bool = False,  # FP8 KV cache for vLLM\n    disable_log_stats: bool = True,\n    # Standard parameters\n    token: Optional[str] = None,\n    use_gradient_checkpointing: str = \"unsloth\",\n    **kwargs,\n) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n    \"\"\"\n    Load model with vLLM fast inference backend for RL training.\n\n    Args:\n        model_name: HuggingFace model ID or local path\n        max_seq_length: Maximum sequence length\n        load_in_4bit: Enable 4-bit quantization\n        fast_inference: Enable vLLM backend (True for RL)\n        gpu_memory_utilization: Fraction of GPU for vLLM (0.0-1.0)\n        max_lora_rank: Maximum LoRA rank supported by vLLM\n        float8_kv_cache: Use FP8 for KV cache (memory optimization)\n\n    Returns:\n        Tuple of (model, tokenizer) with model.vllm_engine attached\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model_name || str || Yes || HuggingFace model ID\n|-\n| fast_inference || bool || Yes (True) || Must be True to enable vLLM\n|-\n| gpu_memory_utilization || float || No (default: 0.5) || Fraction of GPU for vLLM KV cache (0.0-1.0)\n|-\n| max_lora_rank || int || No (default: 64) || Maximum LoRA rank; must be >= actual LoRA rank\n|-\n| max_seq_length || int || No (default: 2048) || Maximum sequence length for generation\n|-\n| load_in_4bit || bool || No (default: True) || Enable 4-bit quantization\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PreTrainedModel || Model with `model.vllm_engine` for fast generation\n|-\n| tokenizer || PreTrainedTokenizer || Tokenizer with padding configured\n|}\n\n== Usage Examples ==\n\n=== vLLM-Enabled Loading for GRPO ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load model with vLLM for GRPO training\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    fast_inference = True,          # Enable vLLM\n    max_lora_rank = 64,             # Support up to rank 64 LoRA\n    gpu_memory_utilization = 0.6,   # 60% GPU for vLLM cache\n)\n\n# Verify vLLM engine is attached\nprint(f\"vLLM engine: {hasattr(model, 'vllm_engine')}\")\n</syntaxhighlight>\n\n=== Higher Memory Utilization for Large Models ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load larger model with more GPU allocation to vLLM\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.1-8B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    fast_inference = True,\n    max_lora_rank = 32,              # Lower rank to fit in memory\n    gpu_memory_utilization = 0.75,   # 75% GPU for vLLM\n    float8_kv_cache = True,          # FP8 cache to save memory\n)\n</syntaxhighlight>\n\n=== Complete GRPO Setup ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template\n\n# 1. Load with vLLM\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 1024,\n    load_in_4bit = True,\n    fast_inference = True,\n    max_lora_rank = 64,\n    gpu_memory_utilization = 0.5,\n)\n\n# 2. Apply LoRA (rank must be <= max_lora_rank)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,  # <= max_lora_rank from loading\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n)\n\n# 3. Configure chat template\ntokenizer = get_chat_template(tokenizer, chat_template=\"llama-3\")\n\n# Model is now ready for GRPOTrainer\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_RL_Model_Loading]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_vLLM_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_LoRA_Rank_Selection_Tip]]\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Gradient_Checkpointing_Tip]]\n",
      "domains": [
        "NLP",
        "Reinforcement Learning",
        "Inference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "vLLM Documentation",
          "url": "https://docs.vllm.ai"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_Model_Loading"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_vLLM_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_LoRA_Rank_Selection_Tip"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Gradient_Checkpointing_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastMistralModel",
      "page_title": "Unslothai Unsloth FastMistralModel",
      "page_type": "Implementation",
      "overview": "Optimized patching module for Mistral models with sliding window attention and dynamic RoPE extension.",
      "content": "# Implementation: FastMistralModel\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|Mistral|https://arxiv.org/abs/2310.06825]]\n|-\n! Domains\n| [[domain::Models]], [[domain::NLP]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching module for Mistral models with sliding window attention and dynamic RoPE extension.\n\n=== Description ===\nThis module provides Unsloth-optimized implementations for Mistral models (7B, 8x7B MoE variants). It includes optimized attention with sliding window support, dynamic RoPE embedding extension, and packed sequence boundary masking.\n\n=== Usage ===\nUsed automatically when loading Mistral models through `FastLanguageModel.from_pretrained()`.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/mistral.py unsloth/models/mistral.py]\n* '''Lines:''' 1-469\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef MistralAttention_fast_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    \"\"\"\n    Fast forward pass for Mistral attention.\n    Supports sliding window attention and dynamic RoPE extension.\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"mistralai/Mistral-7B-v0.3\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n</syntaxhighlight>\n\n== Mistral Architecture Notes ==\n\n{| class=\"wikitable\"\n|-\n! Feature !! Description\n|-\n| Sliding Window Attention || Default 4096 token window\n|-\n| Grouped Query Attention || Multiple Q heads per KV head\n|-\n| RoPE || Rotary Position Embeddings with dynamic extension\n|-\n| SwiGLU || Swish-gated linear unit in MLP\n|}\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "NLP",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "Mistral",
          "url": "https://arxiv.org/abs/2310.06825"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastQwen2Model",
      "page_title": "Unslothai Unsloth FastQwen2Model",
      "page_type": "Implementation",
      "overview": "Optimized patching class for Qwen 2 models reusing LLaMA optimizations.",
      "content": "# Implementation: FastQwen2Model\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Qwen|https://qwenlm.github.io/]]\n|-\n! Domains\n| [[domain::Models]], [[domain::NLP]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching class for Qwen 2 models reusing LLaMA optimizations.\n\n=== Description ===\n`FastQwen2Model` provides Unsloth optimizations for Qwen 2 models by inheriting from `FastLlamaModel` and applying Qwen2-specific patches. Since Qwen 2 shares architectural similarities with LLaMA, it reuses the LLaMA attention and decoder layer fast forward implementations.\n\n=== Usage ===\nUsed through `FastLanguageModel.from_pretrained()` when loading Qwen 2 models. Can also be used directly for more control.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/qwen2.py unsloth/models/qwen2.py]\n* '''Lines:''' 1-101\n\n=== Key Methods ===\n<syntaxhighlight lang=\"python\">\nclass FastQwen2Model(FastLlamaModel):\n    @staticmethod\n    def pre_patch():\n        \"\"\"\n        Apply Qwen2-specific patches.\n\n        Patches applied:\n        - Qwen2Attention.forward -> LlamaAttention_fast_forward\n        - Qwen2DecoderLayer.forward -> LlamaDecoderLayer_fast_forward\n        - Qwen2Model.forward -> LlamaModel_fast_forward\n        - Qwen2ForCausalLM.forward -> CausalLM_fast_forward\n        - Qwen2RotaryEmbedding -> LlamaRotaryEmbedding\n        \"\"\"\n\n    @staticmethod\n    def from_pretrained(\n        model_name: str = \"Qwen/Qwen2-7B\",\n        max_seq_length: int = 4096,\n        dtype = None,\n        load_in_4bit: bool = True,\n        token: str = None,\n        device_map: str = \"sequential\",\n        rope_scaling = None,  # Qwen2 does not support RoPE scaling\n        fix_tokenizer: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Load Qwen2 model with Unsloth optimizations.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.models.qwen2 import FastQwen2Model\n\n# Or through the unified API\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen2-7B\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== from_pretrained Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model_name || str || No || Model ID (default: Qwen/Qwen2-7B)\n|-\n| max_seq_length || int || No || Max sequence length (default: 4096)\n|-\n| load_in_4bit || bool || No || Enable 4-bit quantization (default: True)\n|-\n| dtype || dtype || No || Data type (auto-detected if None)\n|-\n| rope_scaling || None || No || Not supported for Qwen2\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PreTrainedModel || Qwen2 model with patches applied\n|-\n| tokenizer || PreTrainedTokenizer || Qwen2 tokenizer\n|}\n\n== Usage Examples ==\n\n=== Basic Loading ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen2-7B-Instruct\",\n    max_seq_length=4096,\n    dtype=None,\n    load_in_4bit=True,\n)\n\n# Add LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "NLP",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Qwen",
          "url": "https://qwenlm.github.io/"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastQwen3Model",
      "page_title": "Unslothai Unsloth FastQwen3Model",
      "page_type": "Implementation",
      "overview": "Optimized patching module for Qwen 3 models with RMSNorm and optimized attention.",
      "content": "# Implementation: FastQwen3Model\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Qwen|https://qwenlm.github.io/]]\n|-\n! Domains\n| [[domain::Models]], [[domain::NLP]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching module for Qwen 3 models with RMSNorm and optimized attention.\n\n=== Description ===\nThis module provides Unsloth-optimized implementations for Qwen 3 models. It includes optimized attention forward passes with grouped query attention, dynamic RoPE extension, and efficient inference mode. Requires transformers >= 4.50.3.\n\n=== Usage ===\nUsed automatically when loading Qwen 3 models through `FastLanguageModel.from_pretrained()`.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/qwen3.py unsloth/models/qwen3.py]\n* '''Lines:''' 1-457\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef Qwen3Attention_fast_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Fast forward pass for Qwen 3 attention.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen3-8B\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n</syntaxhighlight>\n\n== Qwen 3 Architecture Notes ==\n\n{| class=\"wikitable\"\n|-\n! Feature !! Description\n|-\n| QK Normalization || RMSNorm on Q and K before attention\n|-\n| Grouped Query Attention || Multiple Q heads per KV head\n|-\n| RoPE || Rotary Position Embeddings\n|-\n| SwiGLU || Swish-gated linear unit in MLP\n|-\n| Minimum Transformers || Requires transformers >= 4.50.3\n|}\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "NLP",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Qwen",
          "url": "https://qwenlm.github.io/"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastQwen3MoeModel",
      "page_title": "Unslothai Unsloth FastQwen3MoeModel",
      "page_type": "Implementation",
      "overview": "Optimized patching module for Qwen 3 Mixture-of-Experts (MoE) models.",
      "content": "# Implementation: FastQwen3MoeModel\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Models]], [[domain::MoE]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized patching module for Qwen 3 Mixture-of-Experts (MoE) models.\n\n=== Description ===\nThis module provides Unsloth-optimized implementations for Qwen 3 MoE variants. It extends the base Qwen 3 optimizations with support for the MoE architecture including optimized expert routing and grouped GEMM operations.\n\n=== Usage ===\nUsed automatically when loading Qwen 3 MoE models through `FastLanguageModel.from_pretrained()`.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/models/qwen3_moe.py unsloth/models/qwen3_moe.py]\n* '''Lines:''' 1-243\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen3-MoE-15B\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n</syntaxhighlight>\n\n== Qwen 3 MoE Architecture Notes ==\n\n{| class=\"wikitable\"\n|-\n! Feature !! Description\n|-\n| Mixture of Experts || Multiple expert FFN layers per block\n|-\n| Top-K Routing || Selects top experts per token\n|-\n| Shared Expert || Some variants include shared experts\n|-\n| QK Normalization || RMSNorm on Q and K before attention\n|}\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "MoE",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_FastVisionModel_from_pretrained",
      "page_title": "Unslothai Unsloth FastVisionModel from pretrained",
      "page_type": "Implementation",
      "overview": "Concrete tool for loading Vision-Language Models (VLMs) with 4-bit quantization for memory-efficient multimodal fine-tuning, provided by Unsloth.",
      "content": "# Implementation: FastVisionModel_from_pretrained\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|Transformers Vision Models|https://huggingface.co/docs/transformers/model_doc/llava]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::NLP]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for loading Vision-Language Models (VLMs) with 4-bit quantization for memory-efficient multimodal fine-tuning, provided by Unsloth.\n\n=== Description ===\n\n`FastVisionModel.from_pretrained` loads pre-trained VLMs (Llama-3.2-Vision, Qwen2-VL, Pixtral, etc.) with Unsloth optimizations. Unlike `FastLanguageModel`, it returns an `AutoProcessor` instead of a tokenizer, handling both text and image preprocessing.\n\nKey capabilities:\n* **4-bit quantization** - Reduces VLM memory footprint significantly\n* **AutoProcessor** - Combined tokenizer + image processor\n* **Multi-architecture support** - Llama 3.2 Vision, Qwen2-VL, Pixtral, Gemma3\n* **Gradient checkpointing** - Memory-efficient training\n\n=== Usage ===\n\nUse when fine-tuning vision-language models for tasks like:\n* Image captioning\n* Visual question answering\n* Document/OCR understanding\n* Chart/diagram comprehension\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/loader.py (FastVisionModel), unsloth/models/vision.py (FastBaseModel)\n* '''Lines:''' L702-900 (loader.py), L321-918 (vision.py)\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass FastVisionModel:\n    @staticmethod\n    def from_pretrained(\n        model_name: str = \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n        max_seq_length: int = 2048,\n        dtype: Optional[torch.dtype] = None,\n        load_in_4bit: bool = True,\n        load_in_8bit: bool = False,\n        load_in_16bit: bool = False,\n        full_finetuning: bool = False,\n        token: Optional[str] = None,\n        device_map: str = \"sequential\",\n        trust_remote_code: bool = False,\n        use_gradient_checkpointing: str = \"unsloth\",\n        fast_inference: bool = False,\n        gpu_memory_utilization: float = 0.5,\n        **kwargs,\n    ) -> Tuple[PreTrainedModel, AutoProcessor]:\n        \"\"\"\n        Load a Vision-Language Model with Unsloth optimizations.\n\n        Args:\n            model_name: HuggingFace VLM ID or local path\n            max_seq_length: Maximum sequence length\n            load_in_4bit: Enable 4-bit quantization\n            use_gradient_checkpointing: \"unsloth\" for optimized checkpointing\n\n        Returns:\n            Tuple of (model, processor) where processor is AutoProcessor\n        \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model_name || str || Yes || HuggingFace VLM ID (e.g., \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\")\n|-\n| max_seq_length || int || No (default: 2048) || Maximum sequence length for text\n|-\n| load_in_4bit || bool || No (default: True) || Enable 4-bit quantization\n|-\n| use_gradient_checkpointing || str || No (default: \"unsloth\") || Memory-efficient checkpointing\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PreTrainedModel || Vision-language model with Unsloth patches\n|-\n| processor || AutoProcessor || Combined tokenizer and image processor\n|}\n\n== Usage Examples ==\n\n=== Basic VLM Loading ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\n# Load Llama 3.2 Vision model\nmodel, processor = FastVisionModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# processor handles both text and images\nprint(f\"Processor: {type(processor)}\")\n</syntaxhighlight>\n\n=== Loading Qwen2-VL ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\n# Load Qwen2-VL for document understanding\nmodel, processor = FastVisionModel.from_pretrained(\n    model_name = \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n    max_seq_length = 4096,  # Longer for documents\n    load_in_4bit = True,\n)\n</syntaxhighlight>\n\n=== Loading with Custom Memory Settings ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\n# Load for limited GPU memory\nmodel, processor = FastVisionModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n    max_seq_length = 1024,\n    load_in_4bit = True,\n    use_gradient_checkpointing = \"unsloth\",  # Memory-efficient\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Vision_Model_Loading]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Gradient_Checkpointing_Tip]]\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_BFloat16_vs_Float16_Tip]]\n",
      "domains": [
        "Computer Vision",
        "NLP",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "Transformers Vision Models",
          "url": "https://huggingface.co/docs/transformers/model_doc/llava"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Model_Loading"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Gradient_Checkpointing_Tip"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_BFloat16_vs_Float16_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Flex_Attention",
      "page_title": "Unslothai Unsloth Flex Attention",
      "page_type": "Implementation",
      "overview": "Optimized attention kernel implementations supporting logit softcapping for Gemma2-style models using PyTorch's Flex Attention API.",
      "content": "# Implementation: Flex_Attention\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|Gemma2|https://arxiv.org/abs/2408.00118]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::Attention]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nOptimized attention kernel implementations supporting logit softcapping for Gemma2-style models using PyTorch's Flex Attention API.\n\n=== Description ===\nThis module provides attention implementations with logit softcapping, a technique used by Gemma2 models to prevent attention logits from becoming too large. It provides both a fast path using PyTorch's `flex_attention` (torch 2.5+) and a fallback `torch.compile`-optimized implementation for older PyTorch versions. The softcapping applies tanh scaling: `t * tanh(logits / t)` before softmax.\n\n=== Usage ===\nUse these functions when working with Gemma2 or similar models that require logit softcapping in attention. The module automatically selects the best implementation based on PyTorch version.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/flex_attention.py unsloth/kernels/flex_attention.py]\n* '''Lines:''' 1-187\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef slow_attention_softcapping(\n    Q: torch.Tensor,       # [batch, n_heads, seq_len, head_dim]\n    K: torch.Tensor,       # [batch, n_kv_heads, seq_len, head_dim]\n    V: torch.Tensor,       # [batch, n_kv_heads, seq_len, head_dim]\n    causal_mask: torch.Tensor,  # [seq_len, seq_len] or block_mask\n    self,                  # Model layer (for config access)\n    bsz: int,             # Batch size\n    q_len: int,           # Query sequence length\n) -> torch.Tensor:        # [batch, seq_len, hidden_dim]\n    \"\"\"\n    Compute attention with logit softcapping.\n\n    Uses config values:\n        - self.config.query_pre_attn_scalar (s): Scaling factor\n        - self.config.attn_logit_softcapping (t): Softcap threshold\n\n    Applies: t * tanh(QK^T / (sqrt(s) * t)) before softmax.\n    \"\"\"\n\ndef slow_inference_attention_softcapping(...) -> torch.Tensor:\n    \"\"\"Non-compiled version for inference (no torch.compile overhead).\"\"\"\n\ndef create_flex_attention_causal_mask(max_seq_length: int = 8192):\n    \"\"\"Create causal mask for flex_attention (torch 2.5+).\"\"\"\n\ndef create_flex_attention_sliding_window_mask(\n    max_seq_length: int = 8192,\n    sliding_window: int = 4096,\n):\n    \"\"\"Create sliding window causal mask for flex_attention.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.flex_attention import (\n    slow_attention_softcapping,\n    slow_inference_attention_softcapping,\n    create_flex_attention_causal_mask,\n    HAS_FLEX_ATTENTION,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| Q || Tensor || Yes || Query tensor [batch, n_heads, seq_len, head_dim]\n|-\n| K || Tensor || Yes || Key tensor [batch, n_kv_heads, seq_len, head_dim]\n|-\n| V || Tensor || Yes || Value tensor [batch, n_kv_heads, seq_len, head_dim]\n|-\n| causal_mask || Tensor || Yes || Causal attention mask\n|-\n| self || nn.Module || Yes || Layer module with config for softcap params\n|-\n| bsz || int || Yes || Batch size\n|-\n| q_len || int || Yes || Query sequence length\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| output || Tensor || Attention output [batch, seq_len, n_heads * head_dim]\n|}\n\n== Usage Examples ==\n\n=== Using Softcapped Attention ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.flex_attention import slow_attention_softcapping\n\n# Inside a Gemma2-style attention forward pass\n# Q, K, V shapes: [batch, n_heads, seq_len, head_dim]\noutput = slow_attention_softcapping(\n    Q=query_states,\n    K=key_states,\n    V=value_states,\n    causal_mask=attention_mask,\n    self=self,  # Has config.query_pre_attn_scalar and config.attn_logit_softcapping\n    bsz=batch_size,\n    q_len=seq_length,\n)\n# output shape: [batch, seq_len, hidden_size]\n</syntaxhighlight>\n\n=== Check Flex Attention Availability ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.flex_attention import HAS_FLEX_ATTENTION\n\nif HAS_FLEX_ATTENTION:\n    print(\"Using PyTorch 2.5+ flex_attention (faster)\")\nelse:\n    print(\"Using torch.compile fallback\")\n</syntaxhighlight>\n\n=== Create Block Masks ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.flex_attention import (\n    create_flex_attention_causal_mask,\n    create_flex_attention_sliding_window_mask,\n)\n\n# Standard causal mask\ncausal_mask = create_flex_attention_causal_mask(max_seq_length=8192)\n\n# Sliding window mask (e.g., for Mistral-style attention)\nsliding_mask = create_flex_attention_sliding_window_mask(\n    max_seq_length=8192,\n    sliding_window=4096,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "Attention",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "Gemma2",
          "url": "https://arxiv.org/abs/2408.00118"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_GEGLU_Kernels",
      "page_title": "Unslothai Unsloth GEGLU Kernels",
      "page_type": "Implementation",
      "overview": "Triton kernels for GEGLU (Gated Gaussian Error Linear Unit) activation functions with forward and backward passes.",
      "content": "# Implementation: GEGLU_Kernels\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|GLU Variants|https://arxiv.org/abs/2002.05202]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::Activation]], [[domain::Triton]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nTriton kernels for GEGLU (Gated Gaussian Error Linear Unit) activation functions with forward and backward passes.\n\n=== Description ===\nThis module provides optimized Triton implementations of GEGLU activation used in modern transformer FFN layers. It includes both exact (using erf) and approximate (using tanh) variants, matching the HuggingFace implementations. The kernels handle both forward and backward passes with fused operations for efficiency.\n\n=== Usage ===\nUse these kernels when training or running inference on models with GEGLU activation (e.g., LLaMA, Mistral, Phi). They're automatically used by Unsloth's model patches.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/geglu.py unsloth/kernels/geglu.py]\n* '''Lines:''' 1-290\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef geglu_exact_forward_kernel(\n    gate: torch.Tensor,   # [batch, seq_len, hidden_dim]\n    up: torch.Tensor,     # [batch, seq_len, hidden_dim]\n) -> torch.Tensor:        # [batch, seq_len, hidden_dim]\n    \"\"\"\n    Exact GEGLU forward using erf.\n    f = 0.5 * gate * (1 + erf(gate / sqrt(2)))\n    output = f * up\n    \"\"\"\n\ndef geglu_exact_backward_kernel(\n    DW: torch.Tensor,     # Gradient from upstream\n    e: torch.Tensor,      # Gate values (modified in-place)\n    g: torch.Tensor,      # Up values (modified in-place)\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Exact GEGLU backward pass with fused derivative computation.\"\"\"\n\ndef geglu_approx_forward_kernel(\n    gate: torch.Tensor,\n    up: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Approximate GEGLU forward using tanh (faster).\n    f = 0.5 * gate * (1 + tanh(sqrt(2/pi) * gate * (1 + 0.044715 * gate^2)))\n    output = f * up\n    \"\"\"\n\ndef geglu_approx_backward_kernel(\n    DW: torch.Tensor,\n    e: torch.Tensor,\n    g: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Approximate GEGLU backward with tanh derivative.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.geglu import (\n    geglu_exact_forward_kernel,\n    geglu_exact_backward_kernel,\n    geglu_approx_forward_kernel,\n    geglu_approx_backward_kernel,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs (Forward) ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| gate || Tensor || Yes || Gate projection [batch, seq_len, hidden_dim]\n|-\n| up || Tensor || Yes || Up projection [batch, seq_len, hidden_dim]\n|}\n\n=== Inputs (Backward) ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| DW || Tensor || Yes || Upstream gradient (modified in-place with h = f*g)\n|-\n| e || Tensor || Yes || Gate tensor (modified in-place with df = DW*f)\n|-\n| g || Tensor || Yes || Up tensor (modified in-place with de)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| (forward) || Tensor || GEGLU output [batch, seq_len, hidden_dim]\n|-\n| (backward) || Tuple || (h, df, de) stored in-place in input buffers\n|}\n\n== Mathematical Details ==\n\n=== Exact GEGLU (using erf) ===\n<syntaxhighlight lang=\"text\">\nForward:\n  f = 0.5 * x * (1 + erf(x / sqrt(2)))\n  h = f * up\n\nBackward:\n  df/dx = 0.5 * (1 + erf(x/sqrt(2))) + (1/sqrt(2*pi)) * x * exp(-0.5 * x^2)\n</syntaxhighlight>\n\n=== Approximate GEGLU (using tanh) ===\n<syntaxhighlight lang=\"text\">\nForward:\n  s = sqrt(2/pi) = 0.7978845608...\n  f = 0.5 * x * (1 + tanh(s * x * (1 + 0.044715 * x^2)))\n  h = f * up\n\nBackward:\n  Uses sech^2(x) = 1 - tanh^2(x) for efficient derivative computation\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Forward Pass ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.geglu import geglu_exact_forward_kernel\nimport torch\n\n# In an MLP forward pass\nbatch, seq_len, hidden = 2, 512, 4096\n\n# After linear projections\ngate = torch.randn(batch, seq_len, hidden, device=\"cuda\", dtype=torch.bfloat16)\nup = torch.randn(batch, seq_len, hidden, device=\"cuda\", dtype=torch.bfloat16)\n\n# GEGLU activation\noutput = geglu_exact_forward_kernel(gate, up)\n# output shape: [2, 512, 4096]\n</syntaxhighlight>\n\n=== Custom Autograd Function ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.geglu import (\n    geglu_exact_forward_kernel,\n    geglu_exact_backward_kernel,\n)\n\nclass GEGLU(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, gate, up):\n        output = geglu_exact_forward_kernel(gate, up)\n        ctx.save_for_backward(gate, up)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        gate, up = ctx.saved_tensors\n        # Clone for in-place modification\n        DW = grad_output.clone()\n        e = gate.clone()\n        g = up.clone()\n        geglu_exact_backward_kernel(DW, e, g)\n        # e now contains d_gate, g contains d_up\n        return e, g\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "Activation",
        "Triton"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "GLU Variants",
          "url": "https://arxiv.org/abs/2002.05202"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Grouped_GEMM_Autotuning",
      "page_title": "Unslothai Unsloth Grouped GEMM Autotuning",
      "page_type": "Implementation",
      "overview": "Triton autotuning configurations for grouped GEMM kernels used in MoE architectures.",
      "content": "# Implementation: Grouped_GEMM_Autotuning\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::MoE]], [[domain::Autotuning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nTriton autotuning configurations for grouped GEMM kernels used in MoE architectures.\n\n=== Description ===\nThis module defines autotuning search spaces and configurations for the grouped GEMM Triton kernels. It includes configurations for forward pass, dW (weight gradient), and dX (input gradient) kernels with various block sizes, warp counts, and pipeline stages.\n\n=== Usage ===\nUsed internally by the grouped GEMM interface when `autotune=True` is passed. Defines the search space Triton explores to find optimal kernel configurations.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/kernels/autotuning.py unsloth/kernels/moe/grouped_gemm/kernels/autotuning.py]\n* '''Lines:''' 1-396\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.kernels.autotuning import (\n    get_autotuning_configs_forward,\n    get_autotuning_configs_backward_dW,\n    get_autotuning_configs_backward_dX,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "MoE",
        "Autotuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Grouped_GEMM_Backward",
      "page_title": "Unslothai Unsloth Grouped GEMM Backward",
      "page_type": "Implementation",
      "overview": "Triton kernels for grouped GEMM backward passes (dW and dX) used in MoE training.",
      "content": "# Implementation: Grouped_GEMM_Backward\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::MoE]], [[domain::GEMM]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nTriton kernels for grouped GEMM backward passes (dW and dX) used in MoE training.\n\n=== Description ===\nThis module implements the backward pass kernels for grouped GEMM operations. It includes separate kernels for computing weight gradients (dW) and input gradients (dX), with TMA support for SM90+ GPUs.\n\n=== Usage ===\nCalled automatically during backward pass of MoE layers when using grouped GEMM autograd functions.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/kernels/backward.py unsloth/kernels/moe/grouped_gemm/kernels/backward.py]\n* '''Lines:''' 1-502\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.kernels.backward import (\n    _grouped_gemm_dW_kernel,\n    _grouped_gemm_dX_kernel,\n    _autotuned_grouped_gemm_dW_kernel,\n    _autotuned_grouped_gemm_dX_kernel,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "MoE",
        "GEMM"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Grouped_GEMM_Forward",
      "page_title": "Unslothai Unsloth Grouped GEMM Forward",
      "page_type": "Implementation",
      "overview": "Triton kernels for grouped GEMM forward pass with MoE-specific fusions.",
      "content": "# Implementation: Grouped_GEMM_Forward\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::MoE]], [[domain::GEMM]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nTriton kernels for grouped GEMM forward pass with MoE-specific fusions.\n\n=== Description ===\nThis module implements the forward pass Triton kernels for grouped GEMM operations. It supports various fusions for MoE workloads including input permutation (permute_x), output permutation (permute_y), and TMA (Tensor Memory Accelerator) operations on SM90+ GPUs.\n\n=== Usage ===\nCalled by the grouped_gemm_forward interface function for MoE layer computations.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/kernels/forward.py unsloth/kernels/moe/grouped_gemm/kernels/forward.py]\n* '''Lines:''' 1-265\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.kernels.forward import (\n    _grouped_gemm_forward_kernel,\n    _autotuned_grouped_gemm_forward_kernel,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "MoE",
        "GEMM"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Grouped_GEMM_Interface",
      "page_title": "Unslothai Unsloth Grouped GEMM Interface",
      "page_type": "Implementation",
      "overview": "High-level interface for grouped GEMM operations optimized for Mixture-of-Experts (MoE) model MLPs with TMA support.",
      "content": "# Implementation: Grouped_GEMM_Interface\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::MoE]], [[domain::GEMM]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nHigh-level interface for grouped GEMM operations optimized for Mixture-of-Experts (MoE) model MLPs with TMA support.\n\n=== Description ===\nThis module provides the main entry points for grouped GEMM operations used in MoE architectures. It supports various fusions (permute_x, permute_y, fuse_mul_post) specific to MoE workflows, autotuning, and Tensor Memory Accelerator (TMA) on SM90+ GPUs. The interface abstracts over forward, dW (weight gradient), and dX (input gradient) kernels.\n\n=== Usage ===\nUse these functions when implementing MoE layers or when you need batched matrix multiplications with variable-size groups. The grouped_gemm_forward handles the expert dispatch pattern efficiently.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/interface.py unsloth/kernels/moe/grouped_gemm/interface.py]\n* '''Lines:''' 1-968\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef grouped_gemm_forward(\n    X: torch.Tensor,              # Input activations\n    W: torch.Tensor,              # Expert weights [E, N, K]\n    topk: int,                    # Number of experts per token\n    m_sizes: torch.Tensor,        # Tokens per expert\n    gather_indices: torch.Tensor = None,  # Token-to-expert indices\n    topk_weights: torch.Tensor = None,    # Expert routing weights\n    # Fusions\n    permute_x: bool = False,      # Fuse input permutation\n    permute_y: bool = False,      # Fuse output permutation\n    fuse_mul_post: bool = False,  # Fuse routing weight multiplication\n    # Autotuning\n    autotune: bool = False,\n    # Manual kernel params\n    BLOCK_SIZE_M: int = 32,\n    BLOCK_SIZE_N: int = 32,\n    BLOCK_SIZE_K: int = 32,\n    num_warps: int = 4,\n    num_stages: int = 2,\n    # TMA options (SM90+)\n    use_tma_load_w: bool = False,\n    use_tma_load_x: bool = False,\n    use_tma_store: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Grouped GEMM forward pass for MoE MLPs.\n\n    Returns:\n        y: (total_tokens, N) output of grouped GEMM\n    \"\"\"\n\ndef supports_tma() -> bool:\n    \"\"\"Check if GPU supports Tensor Memory Accelerator (SM90+).\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.interface import (\n    grouped_gemm_forward,\n    supports_tma,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| X || Tensor || Yes || Input [num_tokens, K] or [total_tokens, K]\n|-\n| W || Tensor || Yes || Expert weights [E, N, K]\n|-\n| topk || int || Yes || Number of experts per token\n|-\n| m_sizes || Tensor || Yes || Number of tokens per expert [E]\n|-\n| gather_indices || Tensor || When permute_x/y || Token indices [total_tokens]\n|-\n| topk_weights || Tensor || When fuse_mul_post || Routing weights [total_tokens]\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| y || Tensor || Output [total_tokens, N] or [num_tokens, N] if permute_y\n|}\n\n== Fusion Options ==\n\n{| class=\"wikitable\"\n|-\n! Option !! Description !! Use Case\n|-\n| permute_x || Fuse token-to-expert permutation || First GEMM in MoE MLP\n|-\n| permute_y || Fuse expert-to-token unpermutation || Second GEMM in MoE MLP\n|-\n| fuse_mul_post || Multiply output by routing weights || Inference only (not training)\n|}\n\n== Usage Examples ==\n\n=== Basic MoE Forward ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.interface import grouped_gemm_forward\nimport torch\n\n# Example MoE configuration\nnum_experts = 8\ntopk = 2\nhidden_dim = 4096\nintermediate_dim = 14336\nnum_tokens = 512\n\n# Expert weights\nW = torch.randn(num_experts, intermediate_dim, hidden_dim,\n                device=\"cuda\", dtype=torch.bfloat16)\n\n# Input activations (already permuted to expert order)\nX = torch.randn(num_tokens * topk, hidden_dim,\n                device=\"cuda\", dtype=torch.bfloat16)\n\n# Tokens per expert\nm_sizes = torch.tensor([128, 128, 128, 128, 128, 128, 128, 128],\n                       device=\"cuda\", dtype=torch.int32)\n\n# Run grouped GEMM\noutput = grouped_gemm_forward(\n    X=X, W=W, topk=topk, m_sizes=m_sizes,\n    BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64,\n    num_warps=4, num_stages=2,\n)\n# output shape: [1024, 14336]\n</syntaxhighlight>\n\n=== With Input Permutation Fusion ===\n<syntaxhighlight lang=\"python\">\n# Input in token order (not yet sorted by expert)\nX_token_order = torch.randn(num_tokens, hidden_dim,\n                            device=\"cuda\", dtype=torch.bfloat16)\n\n# Token-to-expert assignment indices\ngather_indices = torch.randint(0, num_tokens, (num_tokens * topk,),\n                               device=\"cuda\", dtype=torch.int32)\n\noutput = grouped_gemm_forward(\n    X=X_token_order,\n    W=W,\n    topk=topk,\n    m_sizes=m_sizes,\n    gather_indices=gather_indices,\n    permute_x=True,  # Fuse permutation\n)\n</syntaxhighlight>\n\n=== With Autotuning ===\n<syntaxhighlight lang=\"python\">\noutput = grouped_gemm_forward(\n    X=X, W=W, topk=topk, m_sizes=m_sizes,\n    autotune=True,  # Let Triton find best kernel config\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "MoE",
        "GEMM"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Grouped_GEMM_Tuning",
      "page_title": "Unslothai Unsloth Grouped GEMM Tuning",
      "page_type": "Implementation",
      "overview": "Kernel configuration dataclasses for manually tuned grouped GEMM parameters.",
      "content": "# Implementation: Grouped_GEMM_Tuning\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::MoE]], [[domain::Configuration]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nKernel configuration dataclasses for manually tuned grouped GEMM parameters.\n\n=== Description ===\nThis module defines dataclasses for kernel configuration parameters used when manually tuning grouped GEMM kernels (without autotuning). It includes separate configs for forward, dW backward, and dX backward kernels with fields for block sizes, warp counts, and pipeline stages.\n\n=== Usage ===\nUsed when passing manual kernel configurations to grouped_gemm_forward instead of using autotune=True.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/kernels/tuning.py unsloth/kernels/moe/grouped_gemm/kernels/tuning.py]\n* '''Lines:''' 1-277\n\n=== Key Classes ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass KernelConfigForward:\n    BLOCK_SIZE_M: int = 32\n    BLOCK_SIZE_N: int = 32\n    BLOCK_SIZE_K: int = 32\n    num_warps: int = 4\n    num_stages: int = 2\n\n@dataclass\nclass KernelConfigBackward_dW:\n    # Similar configuration for weight gradient kernel\n    ...\n\n@dataclass\nclass KernelConfigBackward_dX:\n    # Similar configuration for input gradient kernel\n    ...\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.kernels.tuning import (\n    KernelConfigForward,\n    KernelConfigBackward_dW,\n    KernelConfigBackward_dX,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "MoE",
        "Configuration"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Import_Fixes",
      "page_title": "Unslothai Unsloth Import Fixes",
      "page_type": "Implementation",
      "overview": "Module providing runtime patches and compatibility fixes for various library versions and GPU architectures.",
      "content": "# Implementation: Import_Fixes\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Compatibility]], [[domain::Patching]], [[domain::Infrastructure]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nModule providing runtime patches and compatibility fixes for various library versions and GPU architectures.\n\n=== Description ===\n`import_fixes.py` contains a collection of patches that are applied at import time to fix compatibility issues between different versions of dependencies (transformers, vLLM, TRL, xformers, fbgemm, etc.) and to handle GPU-specific quirks (SM90/SM100 architecture differences, PDL bugs on Blackwell GPUs). It also provides logging configuration and warning suppression.\n\n=== Usage ===\nThis module is automatically imported by Unsloth's `__init__.py`. Users don't typically interact with it directly, but understanding its functions helps debug compatibility issues.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/import_fixes.py unsloth/import_fixes.py]\n* '''Lines:''' 1-696\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef Version(version: str) -> TrueVersion:\n    \"\"\"Parse version string handling dev/alpha/beta/rc suffixes.\"\"\"\n\ndef fix_message_factory_issue() -> None:\n    \"\"\"Patch protobuf MessageFactory for tensorflow compatibility.\"\"\"\n\ndef fix_xformers_performance_issue() -> None:\n    \"\"\"Patch xformers cutlass.py to fix num_splits_key performance bug.\"\"\"\n\ndef fix_vllm_aimv2_issue() -> None:\n    \"\"\"Patch vLLM ovis.py to fix aimv2 config name collision.\"\"\"\n\ndef fix_vllm_guided_decoding_params() -> None:\n    \"\"\"Alias StructuredOutputsParams back to GuidedDecodingParams.\"\"\"\n\ndef patch_enable_input_require_grads() -> None:\n    \"\"\"Patch transformers for vision model NotImplementedError.\"\"\"\n\ndef torchvision_compatibility_check() -> None:\n    \"\"\"Verify torch/torchvision version compatibility.\"\"\"\n\ndef fix_vllm_pdl_blackwell() -> None:\n    \"\"\"Disable PDL (Programmatic Dependent Launch) on SM100 Blackwell GPUs.\"\"\"\n\ndef check_fbgemm_gpu_version() -> None:\n    \"\"\"Check fbgemm_gpu version and disable if too old.\"\"\"\n\nclass HideLoggingMessage(logging.Filter):\n    \"\"\"Filter to hide specific log messages.\"\"\"\n\nclass HidePrintMessage:\n    \"\"\"Stream wrapper to filter stderr messages.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Automatically applied when importing unsloth\nimport unsloth\n\n# Or import specific fixes\nfrom unsloth.import_fixes import (\n    Version,\n    fix_vllm_pdl_blackwell,\n    torchvision_compatibility_check,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| (automatic) || - || - || Module applies patches on import\n|-\n| UNSLOTH_ENABLE_LOGGING || env var || No || Set to \"1\" to enable verbose logging\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| Patches applied || Side effects || Various library modules patched in-place\n|-\n| Warnings suppressed || Side effects || Noisy warnings filtered from stderr\n|-\n| Compatibility errors || Exception || Raised if incompatible versions detected\n|}\n\n== Patches Applied ==\n\n=== Library Version Fixes ===\n{| class=\"wikitable\"\n|-\n! Library !! Condition !! Fix\n|-\n| protobuf || MessageFactory missing GetPrototype || Add GetPrototype method\n|-\n| xformers || < 0.0.29 || Fix num_splits_key=-1 to None\n|-\n| vLLM || < 0.10.1 || Fix aimv2 config name collision\n|-\n| vLLM || All versions || Alias GuidedDecodingParams\n|-\n| TRL || OpenEnv 0.26 || Fix SamplingParams not defined\n|-\n| datasets || 4.4.0 - 4.5.0 || Raise error (recursion bug)\n|-\n| executorch || Missing torchtune || Add get_mapped_key stub\n|}\n\n=== GPU Architecture Fixes ===\n{| class=\"wikitable\"\n|-\n! GPU !! Issue !! Fix\n|-\n| SM100 (Blackwell) || PDL bug in Triton || Set TRITON_DISABLE_PDL=1, patch supports_pdl()\n|-\n| fbgemm_gpu < 1.4.0 || Numerical precision || Disable FBGEMM, use Triton kernels\n|}\n\n== Usage Examples ==\n\n=== Check Version Compatibility ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.import_fixes import Version\n\n# Compare versions safely\ntorch_ver = Version(\"2.5.0\")\nif torch_ver >= Version(\"2.4.0\"):\n    print(\"Torch 2.4+ features available\")\n</syntaxhighlight>\n\n=== Enable Verbose Logging ===\n<syntaxhighlight lang=\"bash\">\n# Set before importing unsloth\nexport UNSLOTH_ENABLE_LOGGING=1\npython your_script.py\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Compatibility",
        "Patching",
        "Infrastructure"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Kernel_Utils",
      "page_title": "Unslothai Unsloth Kernel Utils",
      "page_type": "Implementation",
      "overview": "Core utility functions for Triton kernels including block size calculation, device management, and bitsandbytes integration.",
      "content": "# Implementation: Kernel_Utils\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::Infrastructure]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nCore utility functions for Triton kernels including block size calculation, device management, and bitsandbytes integration.\n\n=== Description ===\n`utils.py` provides foundational utilities used throughout Unsloth's kernel implementations. It includes Triton configuration helpers, CUDA/XPU stream management, bitsandbytes dequantization bindings, and device-agnostic abstractions for multi-GPU and Intel XPU support.\n\n=== Usage ===\nImport utility functions when writing custom Triton kernels or when you need low-level control over quantization operations. Most users won't interact with this directly.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py unsloth/kernels/utils.py]\n* '''Lines:''' 1-1034\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef calculate_settings(n: int) -> Tuple[int, int]:\n    \"\"\"\n    Calculate optimal Triton block size and warp count for dimension n.\n\n    Args:\n        n: Feature dimension size\n\n    Returns:\n        (BLOCK_SIZE, num_warps): Optimal settings for Triton kernel launch\n\n    Raises:\n        RuntimeError: If n exceeds MAX_FUSED_SIZE (65536)\n    \"\"\"\n\ndef torch_gpu_device(device):\n    \"\"\"\n    Context manager for device-specific operations.\n    Returns nullcontext for single-GPU, torch.cuda.device for multi-GPU.\n    \"\"\"\n\n# Triton version-specific tanh implementation\ntriton_tanh = ...  # libdevice.tanh for Triton 3.0+, tl.math.tanh otherwise\n\n# AMP decorators\ntorch_amp_custom_fwd = ...  # torch.amp.custom_fwd for torch 2.4+\ntorch_amp_custom_bwd = ...  # torch.amp.custom_bwd for torch 2.4+\n\n# Bitsandbytes operations\ncdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\ncdequantize_blockwise_fp16_nf4 = bnb.functional.lib.cdequantize_blockwise_fp16_nf4\ncdequantize_blockwise_bf16_nf4 = bnb.functional.lib.cdequantize_blockwise_bf16_nf4\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.utils import (\n    calculate_settings,\n    torch_gpu_device,\n    triton_tanh,\n    torch_amp_custom_fwd,\n    torch_amp_custom_bwd,\n    MAX_FUSED_SIZE,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== calculate_settings ===\n{| class=\"wikitable\"\n|-\n! Input !! Type !! Description\n|-\n| n || int || Feature dimension size\n|}\n\n{| class=\"wikitable\"\n|-\n! Output !! Type !! Description\n|-\n| BLOCK_SIZE || int || Power of 2, capped at 65536\n|-\n| num_warps || int || 4, 8, 16, or 32 based on BLOCK_SIZE\n|}\n\n== Key Constants ==\n\n{| class=\"wikitable\"\n|-\n! Constant !! Value !! Description\n|-\n| MAX_FUSED_SIZE || 65536 || Maximum CUDA block size\n|-\n| CUDA_STREAMS || tuple || Pre-allocated CUDA stream pointers per device\n|-\n| WEIGHT_BUFFERS || list || Per-device weight buffer storage\n|-\n| ABSMAX_BUFFERS || list || Per-device absmax buffer storage\n|}\n\n== Usage Examples ==\n\n=== Calculate Kernel Settings ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.utils import calculate_settings\n\n# For a hidden dimension of 4096\nBLOCK_SIZE, num_warps = calculate_settings(4096)\nprint(f\"BLOCK_SIZE={BLOCK_SIZE}, num_warps={num_warps}\")\n# Output: BLOCK_SIZE=4096, num_warps=8\n</syntaxhighlight>\n\n=== Multi-GPU Context Manager ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.utils import torch_gpu_device\n\ndevice = torch.device(\"cuda:1\")\nwith torch_gpu_device(device):\n    # Operations execute on cuda:1\n    my_triton_kernel[grid](...)\n</syntaxhighlight>\n\n=== Custom Triton Kernel with Utils ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.utils import (\n    calculate_settings,\n    torch_gpu_device,\n    triton_tanh,\n)\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef my_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Use triton_tanh for version compatibility\n    out = triton_tanh(x)\n    tl.store(out_ptr + offsets, out, mask=mask)\n\ndef launch_kernel(x):\n    n = x.numel()\n    BLOCK_SIZE, num_warps = calculate_settings(min(n, 65536))\n    out = torch.empty_like(x)\n    grid = lambda meta: (triton.cdiv(n, meta[\"BLOCK_SIZE\"]),)\n    with torch_gpu_device(x.device):\n        my_kernel[grid](x, out, n, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n    return out\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "Infrastructure",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_LayerNorm_Kernel",
      "page_title": "Unslothai Unsloth LayerNorm Kernel",
      "page_type": "Implementation",
      "overview": "Triton-based Layer Normalization kernel with optimized forward and backward passes for FP32 precision.",
      "content": "# Implementation: LayerNorm_Kernel\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Blog|llm.c LayerNorm|https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::Normalization]], [[domain::Triton]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nTriton-based Layer Normalization kernel with optimized forward and backward passes for FP32 precision.\n\n=== Description ===\nThis module provides an optimized Triton implementation of Layer Normalization that maintains FP32 precision for numerical stability (matching torchtune's Fp32LayerNorm). It includes both forward and backward kernels with efficient memory access patterns and supports the full autograd interface.\n\n=== Usage ===\nUse this kernel as a drop-in replacement for `torch.nn.LayerNorm` when you need faster layer normalization with guaranteed FP32 internal computation.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/layernorm.py unsloth/kernels/layernorm.py]\n* '''Lines:''' 1-225\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\nclass Fast_Layernorm(torch.autograd.Function):\n    \"\"\"Autograd function for fast LayerNorm with Triton kernels.\"\"\"\n\n    @staticmethod\n    def forward(\n        ctx,\n        X: torch.Tensor,      # Input tensor [*, hidden_dim]\n        W: torch.Tensor,      # Weight (gamma) [hidden_dim]\n        b: torch.Tensor,      # Bias (beta) [hidden_dim]\n        eps: float,           # Epsilon for numerical stability\n    ) -> torch.Tensor:\n        \"\"\"Forward pass computing LayerNorm with FP32 precision.\"\"\"\n\n    @staticmethod\n    def backward(ctx, dY: torch.Tensor) -> Tuple[torch.Tensor, None, None, None]:\n        \"\"\"Backward pass following Karpathy's llm.c derivation.\"\"\"\n\n\ndef fast_layernorm(\n    layernorm: nn.LayerNorm,  # PyTorch LayerNorm module\n    X: torch.Tensor,           # Input tensor\n) -> torch.Tensor:\n    \"\"\"\n    Apply fast LayerNorm using Triton kernels.\n\n    Args:\n        layernorm: nn.LayerNorm instance (must have elementwise_affine=True)\n        X: Input tensor of any shape with last dim matching layernorm\n\n    Returns:\n        Normalized tensor with same shape as X\n    \"\"\"\n\n\ndef test_layernorm(\n    dim: int = 1024,\n    eps: float = 1e-5,\n    dtype: torch.dtype = torch.float16,\n    bsz: int = 21,\n    random_state: int = 3407,\n    seqlen: int = 3341,\n) -> None:\n    \"\"\"Test function verifying gradient correctness.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.layernorm import fast_layernorm, Fast_Layernorm\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| X || Tensor || Yes || Input tensor [..., hidden_dim]\n|-\n| W || Tensor || Yes || Weight/gamma parameter [hidden_dim]\n|-\n| b || Tensor || Yes || Bias/beta parameter [hidden_dim]\n|-\n| eps || float || Yes || Epsilon (typically 1e-5 or 1e-6)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| output || Tensor || Normalized tensor with same shape as X\n|}\n\n== Mathematical Details ==\n\n<syntaxhighlight lang=\"text\">\nForward:\n  mean = sum(X) / n_cols\n  var = sum((X - mean)^2) / n_cols\n  inv_var = rsqrt(var + eps)\n  Y = (X - mean) * inv_var * W + b\n\nBackward (following llm.c):\n  normed = (X - mean) * inv_var\n  dY_W = dY * W\n  dX = inv_var * (dY_W - mean(dY_W) - normed * mean(dY_W * normed))\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Basic Usage ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.layernorm import fast_layernorm\nimport torch.nn as nn\n\n# Create standard LayerNorm\nlayernorm = nn.LayerNorm(4096, eps=1e-5, device=\"cuda\", dtype=torch.float16)\n\n# Input tensor\nx = torch.randn(2, 512, 4096, device=\"cuda\", dtype=torch.float16)\n\n# Apply fast LayerNorm\noutput = fast_layernorm(layernorm, x)\n# Equivalent to layernorm(x) but faster with FP32 internals\n</syntaxhighlight>\n\n=== Direct Autograd Usage ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.layernorm import Fast_Layernorm\n\n# Get layernorm parameters\nweight = layernorm.weight\nbias = layernorm.bias\neps = layernorm.eps\n\n# Direct call\noutput = Fast_Layernorm.apply(x, weight, bias, eps)\n</syntaxhighlight>\n\n=== Verify Correctness ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.layernorm import test_layernorm, testing_suite_layernorm\n\n# Single test\ntest_layernorm(dim=1024, dtype=torch.bfloat16)\n\n# Full test suite\ntesting_suite_layernorm()  # Tests multiple dims, dtypes, seqlens\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "Normalization",
        "Triton"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Blog",
          "title": "llm.c LayerNorm",
          "url": "https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Llama4_MoE_Layer",
      "page_title": "Unslothai Unsloth Llama4 MoE Layer",
      "page_type": "Implementation",
      "overview": "Reference implementation for LLaMA 4 style Mixture-of-Experts layer using grouped GEMM.",
      "content": "# Implementation: Llama4_MoE_Layer\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Models]], [[domain::MoE]], [[domain::Reference]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nReference implementation for LLaMA 4 style Mixture-of-Experts layer using grouped GEMM.\n\n=== Description ===\nThis module provides a reference implementation of the LLaMA 4 MoE layer architecture. It includes the expert routing logic, grouped GEMM integration, and weight multiplication patterns specific to the LLaMA 4 MoE design.\n\n=== Usage ===\nUsed as a reference/baseline for testing grouped GEMM kernels against expected LLaMA 4 MoE behavior.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/reference/layers/llama4_moe.py unsloth/kernels/moe/grouped_gemm/reference/layers/llama4_moe.py]\n* '''Lines:''' 1-437\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.reference.layers.llama4_moe import (\n    Llama4MoELayer,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "MoE",
        "Reference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_MoE_Block",
      "page_title": "Unslothai Unsloth MoE Block",
      "page_type": "Implementation",
      "overview": "Generic MoE block implementation serving as a base for model-specific MoE layers.",
      "content": "# Implementation: MoE_Block\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Models]], [[domain::MoE]], [[domain::Reference]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nGeneric MoE block implementation serving as a base for model-specific MoE layers.\n\n=== Description ===\nThis module provides a generic MoE block implementation that can be subclassed for specific model architectures. It handles common MoE patterns like expert selection, routing, and output aggregation.\n\n=== Usage ===\nUsed as a base class for model-specific MoE implementations like Llama4MoELayer and Qwen3MoELayer.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/reference/moe_block.py unsloth/kernels/moe/grouped_gemm/reference/moe_block.py]\n* '''Lines:''' 1-161\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.reference.moe_block import MoEBlock\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "MoE",
        "Reference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_MoE_Ops",
      "page_title": "Unslothai Unsloth MoE Ops",
      "page_type": "Implementation",
      "overview": "Low-level MoE routing operations including top-k selection and token-expert assignment.",
      "content": "# Implementation: MoE_Ops\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Models]], [[domain::MoE]], [[domain::Operations]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nLow-level MoE routing operations including top-k selection and token-expert assignment.\n\n=== Description ===\nThis module provides primitive operations used in MoE routing, including top-k expert selection, computing token-to-expert assignments, and aggregating expert outputs with routing weights.\n\n=== Usage ===\nCalled by MoE layers for computing expert routing and aggregating results.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/reference/moe_ops.py unsloth/kernels/moe/grouped_gemm/reference/moe_ops.py]\n* '''Lines:''' 1-151\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.reference.moe_ops import (\n    compute_expert_assignment,\n    aggregate_expert_outputs,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "MoE",
        "Operations"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Model_Registry",
      "page_title": "Unslothai Unsloth Model Registry",
      "page_type": "Implementation",
      "overview": "Model registration system for tracking supported models with their quantization variants.",
      "content": "# Implementation: Model_Registry\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Infrastructure]], [[domain::Models]], [[domain::Registry]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nModel registration system for tracking supported models with their quantization variants.\n\n=== Description ===\nThis module provides a registry system for Unsloth-supported models. It defines dataclasses for model metadata (`ModelInfo`, `ModelMeta`), quantization types (`QuantType`), and functions to register and look up models. The registry maps model paths to their metadata including organization, version, size, and quantization type.\n\n=== Usage ===\nUsed internally to manage the catalog of supported models and their pre-quantized variants on the Unsloth HuggingFace organization.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/registry/registry.py unsloth/registry/registry.py]\n* '''Lines:''' 1-191\n\n=== Key Classes ===\n<syntaxhighlight lang=\"python\">\nclass QuantType(Enum):\n    \"\"\"Quantization type enum.\"\"\"\n    BNB = \"bnb\"           # bitsandbytes 4-bit\n    UNSLOTH = \"unsloth\"   # Dynamic 4-bit\n    GGUF = \"GGUF\"         # GGUF format\n    NONE = \"none\"         # No quantization\n    BF16 = \"bf16\"         # BF16 (DeepSeek V3)\n\n@dataclass\nclass ModelInfo:\n    \"\"\"Information about a specific model variant.\"\"\"\n    org: str              # Organization (e.g., \"unsloth\", \"meta-llama\")\n    base_name: str        # Base model name\n    version: str          # Version string\n    size: int             # Model size (e.g., 7, 8, 70)\n    name: str = None      # Full model name\n    is_multimodal: bool = False\n    instruct_tag: str = None\n    quant_type: QuantType = None\n\n@dataclass\nclass ModelMeta:\n    \"\"\"Metadata for registering model families.\"\"\"\n    org: str\n    base_name: str\n    model_version: str\n    model_info_cls: type[ModelInfo]\n    model_sizes: list[str]\n    instruct_tags: list[str]\n    quant_types: list[QuantType]\n    is_multimodal: bool = False\n</syntaxhighlight>\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef register_model(\n    model_info_cls: ModelInfo,\n    org: str,\n    base_name: str,\n    version: str,\n    size: int,\n    instruct_tag: str = None,\n    quant_type: QuantType = None,\n    is_multimodal: bool = False,\n    name: str = None,\n) -> None:\n    \"\"\"Register a model in MODEL_REGISTRY.\"\"\"\n\nMODEL_REGISTRY: dict[str, ModelInfo]\n    \"\"\"Global registry mapping model paths to ModelInfo.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.registry.registry import (\n    MODEL_REGISTRY,\n    ModelInfo,\n    ModelMeta,\n    QuantType,\n    register_model,\n)\n</syntaxhighlight>\n\n== Quantization Tags ==\n\n{| class=\"wikitable\"\n|-\n! QuantType !! HF Tag !! Example Path\n|-\n| BNB || bnb-4bit || unsloth/llama-3-8b-bnb-4bit\n|-\n| UNSLOTH || unsloth-bnb-4bit || unsloth/llama-3-8b-unsloth-bnb-4bit\n|-\n| GGUF || GGUF || unsloth/llama-3-8b-GGUF\n|-\n| BF16 || bf16 || unsloth/DeepSeek-V3-bf16\n|-\n| NONE || (none) || meta-llama/Llama-3-8B\n|}\n\n== Usage Examples ==\n\n=== Check if Model is Registered ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.registry.registry import MODEL_REGISTRY\n\nmodel_path = \"unsloth/llama-3-8b-bnb-4bit\"\nif model_path in MODEL_REGISTRY:\n    info = MODEL_REGISTRY[model_path]\n    print(f\"Size: {info.size}B, Quant: {info.quant_type}\")\n</syntaxhighlight>\n\n=== Register Custom Model Family ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.registry.registry import (\n    ModelMeta, ModelInfo, QuantType, _register_models\n)\n\ncustom_meta = ModelMeta(\n    org=\"my-org\",\n    base_name=\"my-model\",\n    model_version=\"v1\",\n    model_info_cls=ModelInfo,\n    model_sizes=[\"7B\", \"13B\"],\n    instruct_tags=[\"Instruct\"],\n    quant_types=[QuantType.BNB, QuantType.NONE],\n)\n_register_models(custom_meta)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Infrastructure",
        "Models",
        "Registry"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_OLLAMA_TEMPLATES",
      "page_title": "Unslothai Unsloth OLLAMA TEMPLATES",
      "page_type": "Implementation",
      "overview": "Concrete collection of Ollama Modelfile templates for different chat formats.",
      "content": "# Implementation: OLLAMA_TEMPLATES\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Deployment]], [[domain::Ollama]], [[domain::Chat_Templates]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete collection of Ollama Modelfile templates for different chat formats.\n\n=== Description ===\n\n`OLLAMA_TEMPLATES` is a dictionary mapping chat template names to Ollama Modelfile strings. Each template includes:\n* FROM directive placeholder\n* TEMPLATE with Go template syntax\n* PARAMETER stop tokens\n* Optional SYSTEM message\n\n=== Usage ===\n\nReferenced automatically during GGUF export to generate appropriate Modelfile.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/ollama_template_mappers.py\n* '''Lines:''' L1-500\n\n=== Exports ===\n<syntaxhighlight lang=\"python\">\n__all__ = [\n    \"OLLAMA_TEMPLATES\",\n    \"OLLAMA_TEMPLATE_TO_MODEL_MAPPER\",\n    \"MODEL_TO_OLLAMA_TEMPLATE_MAPPER\",\n]\n\nOLLAMA_TEMPLATES = {}  # Template name -> Modelfile string\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Template Structure ===\n<syntaxhighlight lang=\"text\">\nFROM {__FILE_LOCATION__}\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n{{ .Response }}<|im_end|>\n\"\"\"\nPARAMETER stop \"<|im_start|>\"\nPARAMETER stop \"<|im_end|>\"\nPARAMETER temperature 1.5\nPARAMETER min_p 0.1\n</syntaxhighlight>\n\n=== Placeholders ===\n{| class=\"wikitable\"\n|-\n! Placeholder !! Description\n|-\n| `{__FILE_LOCATION__}` || Replaced with GGUF file path\n|-\n| `{__EOS_TOKEN__}` || Replaced with model's EOS token\n|}\n\n== Usage Examples ==\n\n=== ChatML Template ===\n<syntaxhighlight lang=\"python\">\nchatml_ollama = '''\nFROM {__FILE_LOCATION__}\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n{{ .Response }}<|im_end|>\n\"\"\"\nPARAMETER stop \"<|im_start|>\"\nPARAMETER stop \"<|im_end|>\"\nPARAMETER temperature 1.5\nPARAMETER min_p 0.1\n'''\nOLLAMA_TEMPLATES[\"chatml\"] = chatml_ollama\n</syntaxhighlight>\n\n=== Mistral Template ===\n<syntaxhighlight lang=\"python\">\nmistral_ollama = '''\nFROM {__FILE_LOCATION__}\nTEMPLATE \"\"\"[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]\"\"\"\nPARAMETER stop \"[INST]\"\nPARAMETER stop \"[/INST]\"\n'''\nOLLAMA_TEMPLATES[\"mistral\"] = mistral_ollama\n</syntaxhighlight>\n\n=== Using Generated Modelfile ===\n<syntaxhighlight lang=\"bash\">\n# After GGUF export, Modelfile is created in output directory\n# Register with Ollama:\nollama create my-model -f ./Modelfile\n\n# Run the model:\nollama run my-model\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Ollama_Modelfile]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n",
      "domains": [
        "Deployment",
        "Ollama",
        "Chat Templates"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Ollama_Modelfile"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_Qwen3_MoE_Layer",
      "page_title": "Unslothai Unsloth Qwen3 MoE Layer",
      "page_type": "Implementation",
      "overview": "Reference implementation for Qwen 3 style Mixture-of-Experts layer using grouped GEMM.",
      "content": "# Implementation: Qwen3_MoE_Layer\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Models]], [[domain::MoE]], [[domain::Reference]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nReference implementation for Qwen 3 style Mixture-of-Experts layer using grouped GEMM.\n\n=== Description ===\nThis module provides a reference implementation of the Qwen 3 MoE layer architecture. It includes the expert routing logic and grouped GEMM integration specific to Qwen 3 MoE design patterns.\n\n=== Usage ===\nUsed as a reference/baseline for testing grouped GEMM kernels against expected Qwen 3 MoE behavior.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/moe/grouped_gemm/reference/layers/qwen3_moe.py unsloth/kernels/moe/grouped_gemm/reference/layers/qwen3_moe.py]\n* '''Lines:''' 1-348\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.moe.grouped_gemm.reference.layers.qwen3_moe import (\n    Qwen3MoELayer,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Models",
        "MoE",
        "Reference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_RawTextDataLoader",
      "page_title": "Unslothai Unsloth RawTextDataLoader",
      "page_type": "Implementation",
      "overview": "Utility class for loading raw text files and converting them into chunked datasets suitable for causal language model training.",
      "content": "# Implementation: RawTextDataLoader\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Data_Preparation]], [[domain::NLP]], [[domain::Preprocessing]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nUtility class for loading raw text files and converting them into chunked datasets suitable for causal language model training.\n\n=== Description ===\n`RawTextDataLoader` handles loading raw text from various file formats (.txt, .md, .json, .jsonl, .csv) and splitting it into overlapping chunks with proper tokenization. It supports both tokenized output (for direct training) and text output (for further processing). The chunking algorithm respects token boundaries and adds EOS tokens appropriately.\n\n=== Usage ===\nUse this class when you want to train a language model on raw text data (e.g., books, articles, code) rather than structured instruction-response pairs. It handles the chunking and tokenization automatically.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/dataprep/raw_text.py unsloth/dataprep/raw_text.py]\n* '''Lines:''' 37-242\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass RawTextDataLoader:\n    def __init__(\n        self,\n        tokenizer,\n        chunk_size: int = 2048,\n        stride: int = 512,\n        return_tokenized: bool = True\n    ):\n        \"\"\"\n        Initialize the raw text data loader.\n\n        Args:\n            tokenizer: HuggingFace tokenizer for the target model\n            chunk_size: Maximum tokens per chunk (default: 2048)\n            stride: Overlap between consecutive chunks (default: 512)\n            return_tokenized: Return tokenized dicts vs text strings (default: True)\n        \"\"\"\n\n    def load_from_file(self, file_path: str, return_tokenized: bool = None) -> Dataset:\n        \"\"\"Load raw text from a single file and return a HuggingFace Dataset.\"\"\"\n\n    def load_from_files(self, file_paths: List[str], return_tokenized: bool = None) -> Dataset:\n        \"\"\"Load raw text from multiple files.\"\"\"\n\n    def smart_chunk_text(\n        self,\n        text: str,\n        chunk_size: int,\n        stride: int,\n        return_tokenized: bool = True\n    ) -> List[Union[str, Dict]]:\n        \"\"\"Split text into overlapping chunks with proper token boundaries.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import RawTextDataLoader\n# or\nfrom unsloth.dataprep.raw_text import RawTextDataLoader\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| tokenizer || PreTrainedTokenizer || Yes || Tokenizer for chunking and tokenization\n|-\n| chunk_size || int || No (default: 2048) || Maximum tokens per chunk\n|-\n| stride || int || No (default: 512) || Overlap between chunks (must be < chunk_size)\n|-\n| return_tokenized || bool || No (default: True) || Return tokenized dicts or text strings\n|-\n| file_path || str || Yes (for load_from_file) || Path to text file\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| Dataset || datasets.Dataset || HuggingFace Dataset with chunks\n|-\n| (tokenized mode) || Dict || {\"input_ids\": List[int], \"attention_mask\": List[int], \"labels\": List[int]}\n|-\n| (text mode) || Dict || {\"text\": str}\n|}\n\n== Usage Examples ==\n\n=== Basic Usage ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel, RawTextDataLoader\n\n# Load model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n)\n\n# Create data loader\nloader = RawTextDataLoader(\n    tokenizer,\n    chunk_size=2048,\n    stride=512,\n)\n\n# Load dataset from raw text file\ndataset = loader.load_from_file(\"./my_corpus.txt\")\nprint(f\"Created {len(dataset)} training chunks\")\n</syntaxhighlight>\n\n=== Multiple Files ===\n<syntaxhighlight lang=\"python\">\n# Load from multiple files\nfiles = [\"book1.txt\", \"book2.txt\", \"article.md\"]\ndataset = loader.load_from_files(files)\n</syntaxhighlight>\n\n=== JSON Lines Format ===\n<syntaxhighlight lang=\"python\">\n# Automatically handles JSON lines format\n# Extracts text from common fields: \"text\", \"content\", \"message\", \"body\"\ndataset = loader.load_from_file(\"./data.jsonl\")\n</syntaxhighlight>\n\n=== Text Mode (Non-Tokenized) ===\n<syntaxhighlight lang=\"python\">\n# Get text chunks instead of tokenized output\nloader = RawTextDataLoader(\n    tokenizer,\n    chunk_size=2048,\n    stride=512,\n    return_tokenized=False,  # Return text strings\n)\ndataset = loader.load_from_file(\"./corpus.txt\")\n# dataset[0] = {\"text\": \"chunk content here...\"}\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Data Preparation",
        "NLP",
        "Preprocessing"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_SFTTrainer_train",
      "page_title": "Unslothai Unsloth SFTTrainer train",
      "page_type": "Implementation",
      "overview": "Wrapper usage pattern for TRL's SFTTrainer within Unsloth's optimized training pipeline, automatically enhanced with fused kernels and efficient gradient computation.",
      "content": "# Implementation: SFTTrainer_train\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|TRL SFTTrainer|https://huggingface.co/docs/trl/sft_trainer]]\n* [[source::Repo|TRL|https://github.com/huggingface/trl]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Training]], [[domain::Supervised_Learning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nWrapper usage pattern for TRL's SFTTrainer within Unsloth's optimized training pipeline, automatically enhanced with fused kernels and efficient gradient computation.\n\n=== Description ===\n\n`SFTTrainer` is TRL's supervised fine-tuning trainer. When Unsloth is imported, it patches SFTTrainer at import time to use optimized kernels for:\n* Fused cross-entropy loss (chunked for large vocabularies)\n* Optimized RMS normalization\n* Efficient gradient checkpointing\n\nThis is a Wrapper Doc - it documents how Unsloth uses and extends TRL's SFTTrainer.\n\nKey Unsloth enhancements:\n* **Automatic patching** - Import Unsloth first to apply optimizations\n* **Sample packing** - Pack multiple sequences into single training examples\n* **Padding-free training** - Avoid computation on padding tokens\n* **UnslothTrainer** - Custom trainer subclass with embedding LR support\n\n=== Usage ===\n\nImport Unsloth before TRL to apply patches. Create SFTTrainer with model, tokenizer, dataset, and training arguments. Call `trainer.train()` to execute the training loop. Unsloth optimizations are applied automatically.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/trainer.py\n* '''Lines:''' L182-408 (UnslothTrainer class)\n\n=== External Reference ===\n* '''Library:''' [https://github.com/huggingface/trl TRL]\n* '''Documentation:''' [https://huggingface.co/docs/trl/sft_trainer SFTTrainer Docs]\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n# SFTTrainer is from TRL, patched by Unsloth\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model: PreTrainedModel,\n    tokenizer: PreTrainedTokenizer,  # or processing_class in newer TRL\n    train_dataset: Dataset,\n    args: TrainingArguments,\n    packing: bool = False,\n    dataset_text_field: str = \"text\",\n    max_seq_length: int = 2048,\n    dataset_num_proc: int = 2,\n    data_collator: Optional[DataCollator] = None,\n    **kwargs,\n)\n\n# Execute training\ntrain_output = trainer.train()\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# IMPORTANT: Import Unsloth FIRST to apply patches\nfrom unsloth import FastLanguageModel\n\n# Then import SFTTrainer (now patched)\nfrom trl import SFTTrainer\n\n# Or use UnslothTrainer for embedding_learning_rate support\nfrom unsloth import UnslothTrainer\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || Model with LoRA adapters from get_peft_model\n|-\n| tokenizer || PreTrainedTokenizer || Yes || Tokenizer with chat template configured\n|-\n| train_dataset || Dataset || Yes || Dataset with \"text\" column (formatted conversations)\n|-\n| args || TrainingArguments || Yes || Training configuration\n|-\n| packing || bool || No (default: False) || Enable sample packing for efficiency\n|-\n| max_seq_length || int || No (default: 2048) || Maximum sequence length\n|-\n| dataset_text_field || str || No (default: \"text\") || Column containing formatted text\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| trainer || SFTTrainer || Configured trainer instance\n|-\n| train_output || TrainOutput || Training metrics (loss, steps, runtime)\n|}\n\n== Usage Examples ==\n\n=== Basic SFT Training ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel, UnslothTrainingArguments\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\n\n# 1. Load model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# 2. Apply LoRA\nmodel = FastLanguageModel.get_peft_model(model, r=16)\n\n# 3. Load dataset (pre-formatted with chat template)\ndataset = load_dataset(\"json\", data_files=\"train.jsonl\")[\"train\"]\n\n# 4. Configure training\ntraining_args = UnslothTrainingArguments(\n    output_dir = \"./outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    max_steps = 60,\n    learning_rate = 2e-4,\n    optim = \"adamw_8bit\",\n    bf16 = True,\n)\n\n# 5. Create trainer and train\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    args = training_args,\n    dataset_text_field = \"text\",\n    max_seq_length = 2048,\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== With Sample Packing ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, SFTConfig\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\nmodel = FastLanguageModel.get_peft_model(model, r=16)\n\n# Enable sample packing for better GPU utilization\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    args = SFTConfig(\n        output_dir = \"./outputs\",\n        packing = True,  # Pack multiple sequences\n        max_seq_length = 2048,\n        per_device_train_batch_size = 2,\n        bf16 = True,\n    ),\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== With UnslothTrainer for Embedding LR ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel, UnslothTrainer, UnslothTrainingArguments\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Train embeddings too\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    modules_to_save = [\"embed_tokens\", \"lm_head\"],\n)\n\ntraining_args = UnslothTrainingArguments(\n    output_dir = \"./outputs\",\n    learning_rate = 2e-4,\n    embedding_learning_rate = 5e-5,  # Lower LR for embeddings\n    max_steps = 100,\n    bf16 = True,\n)\n\n# Use UnslothTrainer for embedding_learning_rate support\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    args = training_args,\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Supervised_Finetuning]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Sample_Packing_Tip]]\n",
      "domains": [
        "NLP",
        "Training",
        "Supervised Learning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "TRL SFTTrainer",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        },
        {
          "type": "Repo",
          "title": "TRL",
          "url": "https://github.com/huggingface/trl"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Supervised_Finetuning"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Sample_Packing_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_SFTTrainer_vision",
      "page_title": "Unslothai Unsloth SFTTrainer vision",
      "page_type": "Implementation",
      "overview": "Wrapper usage pattern for SFTTrainer with Vision-Language Models, using UnslothVisionDataCollator for proper image handling.",
      "content": "# Implementation: SFTTrainer_vision\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|TRL SFTTrainer|https://huggingface.co/docs/trl/sft_trainer]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Training]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nWrapper usage pattern for SFTTrainer with Vision-Language Models, using UnslothVisionDataCollator for proper image handling.\n\n=== Description ===\n\nSFTTrainer for VLM training requires:\n* **processing_class** instead of tokenizer (AutoProcessor)\n* **UnslothVisionDataCollator** for batching images and text\n* **remove_unused_columns=False** to preserve image data\n\nThis is a **Wrapper Doc** documenting the VLM-specific usage of SFTTrainer.\n\n=== Usage ===\n\nCreate SFTTrainer with VLM model, AutoProcessor, multimodal dataset, and the vision data collator.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/trainer.py\n* '''External:''' unsloth_zoo/vision_utils.py (UnslothVisionDataCollator)\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom trl import SFTTrainer, SFTConfig\nfrom unsloth import UnslothVisionDataCollator\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Complete Vision Training ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel, UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\n\n# 1. Load VLM\nmodel, processor = FastVisionModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n    load_in_4bit = True,\n)\n\n# 2. Apply LoRA\nmodel = FastVisionModel.get_peft_model(\n    model,\n    r = 16,\n    finetune_vision_layers = True,\n    finetune_language_layers = True,\n)\n\n# 3. Set training mode\nFastVisionModel.for_training(model)\n\n# 4. Configure training\ntraining_args = SFTConfig(\n    output_dir = \"./vision_outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    max_steps = 100,\n    learning_rate = 2e-4,\n    bf16 = True,\n    remove_unused_columns = False,  # CRITICAL for VLM\n    dataset_kwargs = {\"skip_prepare_dataset\": True},\n)\n\n# 5. Create trainer with vision collator\ntrainer = SFTTrainer(\n    model = model,\n    processing_class = processor,  # NOT tokenizer\n    train_dataset = vision_dataset,\n    data_collator = UnslothVisionDataCollator(processor),\n    args = training_args,\n)\n\n# 6. Train\ntrainer.train()\n</syntaxhighlight>\n\n=== With Sample Outputs ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel, UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\n\nmodel, processor = FastVisionModel.from_pretrained(...)\nmodel = FastVisionModel.get_peft_model(model, ...)\n\ntraining_args = SFTConfig(\n    output_dir = \"./vision_outputs\",\n    per_device_train_batch_size = 1,\n    max_steps = 50,\n    bf16 = True,\n    remove_unused_columns = False,\n    logging_steps = 10,\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    processing_class = processor,\n    train_dataset = vision_dataset,\n    data_collator = UnslothVisionDataCollator(processor),\n    args = training_args,\n)\n\ntrainer.train()\n\n# Test generation after training\nFastVisionModel.for_inference(model)\n# ... generate with model\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Vision_Training]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Computer Vision",
        "Training",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "TRL SFTTrainer",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Training"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_SwiGLU_Kernels",
      "page_title": "Unslothai Unsloth SwiGLU Kernels",
      "page_type": "Implementation",
      "overview": "Triton kernels for SwiGLU (Swish-Gated Linear Unit) activation functions used in LLaMA-style transformer FFN layers.",
      "content": "# Implementation: SwiGLU_Kernels\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|GLU Variants|https://arxiv.org/abs/2002.05202]]\n|-\n! Domains\n| [[domain::Kernels]], [[domain::Activation]], [[domain::Triton]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nTriton kernels for SwiGLU (Swish-Gated Linear Unit) activation functions used in LLaMA-style transformer FFN layers.\n\n=== Description ===\nThis module provides optimized Triton implementations of SwiGLU activation, which combines the Swish activation (x * sigmoid(x)) with gating. It includes both forward and backward passes with fused operations. SwiGLU is used in LLaMA, Mistral, and other modern LLMs.\n\n=== Usage ===\nUse these kernels when training or running inference on models with SwiGLU activation in their MLP layers. They're automatically used by Unsloth's model patches.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/swiglu.py unsloth/kernels/swiglu.py]\n* '''Lines:''' 1-143\n\n=== Key Functions ===\n<syntaxhighlight lang=\"python\">\ndef swiglu_fg_kernel(\n    e: torch.Tensor,   # Gate projection [batch, seq_len, hidden_dim]\n    g: torch.Tensor,   # Up projection [batch, seq_len, hidden_dim]\n) -> torch.Tensor:     # Output [batch, seq_len, hidden_dim]\n    \"\"\"\n    SwiGLU forward pass.\n    f = e * sigmoid(e)  # Swish activation\n    h = f * g           # Gating\n    \"\"\"\n\ndef swiglu_DWf_DW_dfg_kernel(\n    DW: torch.Tensor,  # Upstream gradient (modified in-place with h)\n    e: torch.Tensor,   # Gate tensor (modified in-place with df)\n    g: torch.Tensor,   # Up tensor (modified in-place with de)\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    SwiGLU backward pass with fused derivative computation.\n    Stores results in-place in input buffers.\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.swiglu import (\n    swiglu_fg_kernel,\n    swiglu_DWf_DW_dfg_kernel,\n)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs (Forward) ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| e || Tensor || Yes || Gate projection [batch, seq_len, hidden_dim]\n|-\n| g || Tensor || Yes || Up projection [batch, seq_len, hidden_dim]\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| h || Tensor || SwiGLU output [batch, seq_len, hidden_dim]\n|}\n\n== Mathematical Details ==\n\n<syntaxhighlight lang=\"text\">\nForward:\n  se = sigmoid(e) = 1 / (1 + exp(-e))\n  f = e * se       # Swish activation\n  h = f * g        # Gating\n\nBackward:\n  df/de = se * (1 + e * (1 - se))\n  d_gate = dh * g * df/de\n  d_up = dh * f\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Forward Pass ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.kernels.swiglu import swiglu_fg_kernel\nimport torch\n\n# In an MLP forward pass\nbatch, seq_len, hidden = 2, 512, 4096\n\n# After linear projections\ngate = torch.randn(batch, seq_len, hidden, device=\"cuda\", dtype=torch.bfloat16)\nup = torch.randn(batch, seq_len, hidden, device=\"cuda\", dtype=torch.bfloat16)\n\n# SwiGLU activation\noutput = swiglu_fg_kernel(gate, up)\n# output shape: [2, 512, 4096]\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Kernels",
        "Activation",
        "Triton"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "GLU Variants",
          "url": "https://arxiv.org/abs/2002.05202"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_SyntheticDataKit",
      "page_title": "Unslothai Unsloth SyntheticDataKit",
      "page_type": "Implementation",
      "overview": "Tool for generating synthetic Q&A training data using a vLLM-powered language model server.",
      "content": "# Implementation: SyntheticDataKit\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Data_Preparation]], [[domain::Synthetic_Data]], [[domain::NLP]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 15:46 GMT]]\n|}\n\n== Overview ==\nTool for generating synthetic Q&A training data using a vLLM-powered language model server.\n\n=== Description ===\n`SyntheticDataKit` manages a vLLM server subprocess to generate synthetic question-answer pairs from raw text documents. It handles server lifecycle (startup, health checks, shutdown), text chunking based on model context limits, and configuration for Q&A generation parameters like temperature and top_p.\n\n=== Usage ===\nUse this class when you need to generate synthetic training data from raw documents (PDFs, HTML, text files) for fine-tuning. It's particularly useful for creating domain-specific Q&A datasets.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth Unslothai_Unsloth]\n* '''File:''' [https://github.com/unslothai/unsloth/blob/main/unsloth/dataprep/synthetic.py unsloth/dataprep/synthetic.py]\n* '''Lines:''' 153-465\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass SyntheticDataKit:\n    def __init__(\n        self,\n        model_name: str = \"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n        max_seq_length: int = 2048,\n        gpu_memory_utilization: float = 0.98,\n        float8_kv_cache: bool = False,\n        conservativeness: float = 1.0,\n        token: Optional[str] = None,\n        timeout: int = 1200,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize synthetic data generation kit with vLLM server.\n\n        Args:\n            model_name: HuggingFace model ID for generation\n            max_seq_length: Maximum sequence length\n            gpu_memory_utilization: GPU memory fraction for vLLM\n            float8_kv_cache: Use FP8 KV cache for efficiency\n            conservativeness: Memory allocation conservativeness\n            token: HuggingFace token for gated models\n            timeout: Server startup timeout in seconds\n        \"\"\"\n\n    @staticmethod\n    def from_pretrained(...) -> \"SyntheticDataKit\":\n        \"\"\"Alternative constructor matching HF API style.\"\"\"\n\n    def chunk_data(self, filename: str) -> List[str]:\n        \"\"\"Split a document into chunks respecting model context limits.\"\"\"\n\n    def prepare_qa_generation(\n        self,\n        output_folder: str = \"data\",\n        max_generation_tokens: int = 512,\n        temperature: float = 0.7,\n        top_p: float = 0.95,\n        overlap: int = 64,\n        default_num_pairs: int = 25,\n        cleanup_threshold: float = 1.0,\n        cleanup_batch_size: int = 4,\n        cleanup_temperature: float = 0.3,\n    ) -> None:\n        \"\"\"Configure and prepare directories for Q&A generation.\"\"\"\n\n    def cleanup(self) -> None:\n        \"\"\"Terminate vLLM server and free GPU memory.\"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import SyntheticDataKit\n# or\nfrom unsloth.dataprep.synthetic import SyntheticDataKit\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model_name || str || No || Model for generation (default: Llama-3.1-8B-Instruct)\n|-\n| max_seq_length || int || No || Maximum context length (default: 2048)\n|-\n| gpu_memory_utilization || float || No || GPU memory fraction (default: 0.98)\n|-\n| filename || str || Yes (for chunk_data) || Document path to process\n|-\n| output_folder || str || No || Output directory (default: \"data\")\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| chunk_data() || List[str] || List of chunk filenames created\n|-\n| prepare_qa_generation() || None || Creates config file and output directories\n|-\n| Output directories || Directories || pdf/, html/, txt/, output/, generated/, cleaned/, final/\n|}\n\n== Usage Examples ==\n\n=== Basic Usage with Context Manager ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import SyntheticDataKit\n\n# Use context manager for automatic cleanup\nwith SyntheticDataKit(\n    model_name=\"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n    max_seq_length=4096,\n    gpu_memory_utilization=0.9,\n) as kit:\n    # Chunk a document\n    chunks = kit.chunk_data(\"./my_document.txt\")\n    print(f\"Created {len(chunks)} chunks\")\n\n    # Prepare for Q&A generation\n    kit.prepare_qa_generation(\n        output_folder=\"./synthetic_data\",\n        max_generation_tokens=512,\n        temperature=0.7,\n        default_num_pairs=25,\n    )\n# Server automatically cleaned up\n</syntaxhighlight>\n\n=== Manual Lifecycle Management ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import SyntheticDataKit\n\n# Alternative constructor\nkit = SyntheticDataKit.from_pretrained(\n    model_name=\"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n    max_seq_length=4096,\n)\n\ntry:\n    # Check server status\n    if kit.check_vllm_status():\n        print(\"vLLM server is ready\")\n\n    # Process documents\n    chunks = kit.chunk_data(\"./corpus.txt\")\n    kit.prepare_qa_generation(output_folder=\"./data\")\nfinally:\n    # Manual cleanup\n    kit.cleanup()\n</syntaxhighlight>\n\n== Related Pages ==\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_vLLM_Environment]]\n",
      "domains": [
        "Data Preparation",
        "Synthetic Data",
        "NLP"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 15:46 GMT",
      "outgoing_links": [
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_vLLM_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_UnslothGRPOConfig",
      "page_title": "Unslothai Unsloth UnslothGRPOConfig",
      "page_type": "Implementation",
      "overview": "Wrapper for TRL's GRPOConfig that adds Unsloth-specific parameters for optimized GRPO reinforcement learning training.",
      "content": "# Implementation: UnslothGRPOConfig\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|TRL GRPOConfig|https://huggingface.co/docs/trl/grpo_trainer]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::Reinforcement_Learning]], [[domain::Training]], [[domain::Hyperparameters]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nWrapper for TRL's GRPOConfig that adds Unsloth-specific parameters for optimized GRPO reinforcement learning training.\n\n=== Description ===\n\n`UnslothGRPOConfig` extends TRL's `GRPOConfig` with Unsloth-specific parameters for vLLM integration and memory optimization. It's dynamically generated by Unsloth to match the installed TRL version.\n\nThis is a **Wrapper Doc** - it documents how Unsloth extends TRL's configuration class.\n\nKey Unsloth additions:\n* `vllm_sampling_params` - Direct vLLM SamplingParams configuration\n* `unsloth_num_chunks` - Memory chunking for gradient computation\n\n=== Usage ===\n\nCreate `UnslothGRPOConfig` with GRPO hyperparameters and pass to `UnslothGRPOTrainer`. Key parameters to configure:\n* `num_generations` - Completions per prompt (typically 6-16)\n* `max_completion_length` - Maximum tokens to generate\n* `beta` - KL divergence penalty coefficient\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/rl.py\n* '''Lines:''' L309-334 (dynamically generated template)\n\n=== External Reference ===\n* '''Library:''' [https://github.com/huggingface/trl TRL]\n* '''Base Class:''' `trl.GRPOConfig`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@dataclass\nclass UnslothGRPOConfig(GRPOConfig):\n    \"\"\"\n    GRPO configuration with Unsloth extensions.\n    \"\"\"\n    # Unsloth-specific\n    vllm_sampling_params: Optional[Any] = field(\n        default=None,\n        metadata={'help': 'vLLM SamplingParams object'}\n    )\n    unsloth_num_chunks: Optional[int] = field(\n        default=-1,\n        metadata={'help': 'Chunk size for memory efficiency. -1 is most efficient.'}\n    )\n\n    # Standard GRPOConfig parameters\n    output_dir: str = None\n    per_device_train_batch_size: int = 1\n    gradient_accumulation_steps: int = 1\n    num_generations: int = 6  # Completions per prompt\n    max_prompt_length: int = 256\n    max_completion_length: int = 200\n    beta: float = 0.1  # KL penalty coefficient\n    learning_rate: float = 5e-6\n    num_train_epochs: float = 1.0\n    max_steps: int = -1\n    logging_steps: int = 1\n    save_strategy: str = \"no\"\n    bf16: bool = False\n    fp16: bool = False\n    **kwargs\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothGRPOConfig\n# Or access after patching:\nfrom trl import GRPOConfig  # Returns UnslothGRPOConfig after import unsloth\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| output_dir || str || Yes || Directory for checkpoints\n|-\n| num_generations || int || No (default: 6) || Completions generated per prompt\n|-\n| max_completion_length || int || No (default: 200) || Maximum tokens to generate\n|-\n| max_prompt_length || int || No (default: 256) || Maximum prompt token length\n|-\n| beta || float || No (default: 0.1) || KL divergence penalty coefficient\n|-\n| learning_rate || float || No (default: 5e-6) || Learning rate for policy\n|-\n| per_device_train_batch_size || int || No (default: 1) || Prompts per batch per GPU\n|-\n| gradient_accumulation_steps || int || No (default: 1) || Steps before gradient update\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| config || UnslothGRPOConfig || Configuration object for UnslothGRPOTrainer\n|}\n\n== Usage Examples ==\n\n=== Basic GRPO Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothGRPOConfig\n\ngrpo_config = UnslothGRPOConfig(\n    output_dir = \"./grpo_outputs\",\n    num_generations = 6,           # 6 completions per prompt\n    max_completion_length = 200,   # Max tokens per completion\n    max_prompt_length = 256,       # Max prompt tokens\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 1,\n    beta = 0.1,                    # KL penalty\n    learning_rate = 5e-6,\n    max_steps = 100,\n    logging_steps = 1,\n    bf16 = True,\n)\n</syntaxhighlight>\n\n=== High-Generation Configuration (Math Reasoning) ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothGRPOConfig\n\n# More generations for better reward estimation\ngrpo_config = UnslothGRPOConfig(\n    output_dir = \"./math_grpo\",\n    num_generations = 16,           # More samples for math tasks\n    max_completion_length = 500,    # Longer for reasoning\n    max_prompt_length = 200,\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 4,  # Effective batch = 4\n    beta = 0.05,                    # Lower KL for more exploration\n    learning_rate = 1e-5,\n    max_steps = 500,\n    bf16 = True,\n)\n</syntaxhighlight>\n\n=== With Custom vLLM Sampling ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothGRPOConfig, vLLMSamplingParams\n\n# Custom vLLM sampling parameters\nsampling_params = vLLMSamplingParams(\n    temperature = 0.8,\n    top_p = 0.95,\n    max_tokens = 300,\n)\n\ngrpo_config = UnslothGRPOConfig(\n    output_dir = \"./grpo_outputs\",\n    num_generations = 8,\n    max_completion_length = 300,\n    vllm_sampling_params = sampling_params,  # Unsloth extension\n    learning_rate = 5e-6,\n    max_steps = 200,\n    bf16 = True,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_GRPO_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_vLLM_Environment]]\n",
      "domains": [
        "Reinforcement Learning",
        "Training",
        "Hyperparameters"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "TRL GRPOConfig",
          "url": "https://huggingface.co/docs/trl/grpo_trainer"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GRPO_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_vLLM_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_UnslothGRPOTrainer",
      "page_title": "Unslothai Unsloth UnslothGRPOTrainer",
      "page_type": "Implementation",
      "overview": "Wrapper for TRL's GRPOTrainer that integrates Unsloth's vLLM optimization and efficient gradient computation for GRPO reinforcement learning.",
      "content": "# Implementation: UnslothGRPOTrainer\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|TRL GRPOTrainer|https://huggingface.co/docs/trl/grpo_trainer]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::Reinforcement_Learning]], [[domain::Training]], [[domain::NLP]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nWrapper for TRL's GRPOTrainer that integrates Unsloth's vLLM optimization and efficient gradient computation for GRPO reinforcement learning.\n\n=== Description ===\n\n`UnslothGRPOTrainer` extends TRL's `GRPOTrainer` with:\n* **vLLM integration** - Uses model's attached vLLM engine for fast generation\n* **Inference mode switching** - Automatic for_inference/for_training mode management\n* **Memory optimization** - Chunked gradient computation via `unsloth_num_chunks`\n\nThis is a **Wrapper Doc** - it documents how Unsloth extends TRL's trainer.\n\n=== Usage ===\n\nCreate `UnslothGRPOTrainer` with the vLLM-enabled model, tokenizer, reward functions, and config. Call `trainer.train()` to execute GRPO training.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/rl.py\n* '''Lines:''' L240-700 (dynamically generated)\n\n=== External Reference ===\n* '''Library:''' [https://github.com/huggingface/trl TRL]\n* '''Base Class:''' `trl.GRPOTrainer`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass UnslothGRPOTrainer(GRPOTrainer):\n    \"\"\"\n    GRPO trainer with Unsloth optimizations.\n    \"\"\"\n    def __init__(\n        self,\n        model: PreTrainedModel,  # Model with vLLM engine\n        processing_class: PreTrainedTokenizer,  # Tokenizer\n        reward_funcs: List[Callable],  # List of reward functions\n        args: UnslothGRPOConfig = None,\n        train_dataset: Dataset = None,\n        eval_dataset: Optional[Dataset] = None,\n        data_collator: Optional[DataCollator] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize GRPO trainer.\n\n        Args:\n            model: Model with vLLM from from_pretrained(fast_inference=True)\n            processing_class: Tokenizer (named processing_class in TRL >= 0.13)\n            reward_funcs: List of reward functions [f(completions, prompts) -> scores]\n            args: UnslothGRPOConfig with training hyperparameters\n            train_dataset: Dataset with \"prompt\" column\n\n        Note: TRL >= 0.13 renamed tokenizer to processing_class\n        \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothGRPOTrainer, UnslothGRPOConfig\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || Model with vLLM engine and LoRA adapters\n|-\n| processing_class || PreTrainedTokenizer || Yes || Tokenizer with chat template\n|-\n| reward_funcs || List[Callable] || Yes || Reward functions (scores summed)\n|-\n| args || UnslothGRPOConfig || No || Training configuration\n|-\n| train_dataset || Dataset || Yes || Dataset with \"prompt\" column\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| trainer || UnslothGRPOTrainer || Configured trainer\n|-\n| train() returns || TrainOutput || Training metrics (rewards, loss, etc.)\n|}\n\n== Usage Examples ==\n\n=== Complete GRPO Training ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import (\n    FastLanguageModel,\n    UnslothGRPOTrainer,\n    UnslothGRPOConfig,\n)\nfrom unsloth.chat_templates import get_chat_template\nimport re\n\n# 1. Load model with vLLM\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 1024,\n    load_in_4bit = True,\n    fast_inference = True,\n    max_lora_rank = 64,\n    gpu_memory_utilization = 0.5,\n)\n\n# 2. Apply LoRA\nmodel = FastLanguageModel.get_peft_model(model, r=64)\n\n# 3. Configure chat template\ntokenizer = get_chat_template(tokenizer, chat_template=\"llama-3\")\n\n# 4. Define reward functions\ndef format_reward(completions, prompts, **kwargs):\n    rewards = []\n    for c in completions:\n        score = 0.5 if \"<think>\" in c else 0.0\n        score += 0.5 if \"\\\\boxed{\" in c else 0.0\n        rewards.append(score)\n    return rewards\n\ndef correctness_reward(completions, prompts, **kwargs):\n    # Placeholder - implement actual verification\n    return [0.0] * len(completions)\n\n# 5. Configure GRPO\ngrpo_config = UnslothGRPOConfig(\n    output_dir = \"./grpo_outputs\",\n    num_generations = 6,\n    max_completion_length = 200,\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 1,\n    beta = 0.1,\n    learning_rate = 5e-6,\n    max_steps = 100,\n    logging_steps = 1,\n    bf16 = True,\n)\n\n# 6. Create trainer\ntrainer = UnslothGRPOTrainer(\n    model = model,\n    processing_class = tokenizer,  # Use processing_class for TRL >= 0.13\n    reward_funcs = [format_reward, correctness_reward],\n    args = grpo_config,\n    train_dataset = dataset,  # Must have \"prompt\" column\n)\n\n# 7. Train\ntrainer.train()\n\n# 8. Save model\nmodel.save_pretrained_merged(\"./grpo_model\", tokenizer, save_method=\"merged_16bit\")\n</syntaxhighlight>\n\n=== Math Reasoning Training ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothGRPOTrainer, UnslothGRPOConfig\n\n# Assume model and tokenizer already loaded with vLLM\n\n# Math-specific reward functions\ndef format_reward(completions, prompts, **kwargs):\n    \"\"\"Check for proper reasoning format.\"\"\"\n    rewards = []\n    for c in completions:\n        score = 0.0\n        if \"<think>\" in c and \"</think>\" in c:\n            score += 0.3\n        if \"\\\\boxed{\" in c:\n            score += 0.2\n        rewards.append(score)\n    return rewards\n\ndef correctness_reward(completions, prompts, answers, **kwargs):\n    \"\"\"Check if answer is correct.\"\"\"\n    rewards = []\n    for completion, answer in zip(completions, answers):\n        match = re.search(r\"\\\\boxed\\{(.+?)\\}\", completion)\n        if match and match.group(1).strip() == str(answer).strip():\n            rewards.append(1.0)\n        else:\n            rewards.append(0.0)\n    return rewards\n\ngrpo_config = UnslothGRPOConfig(\n    output_dir = \"./math_grpo\",\n    num_generations = 16,  # More generations for math\n    max_completion_length = 500,  # Longer for reasoning\n    beta = 0.05,\n    learning_rate = 1e-5,\n    max_steps = 500,\n    bf16 = True,\n)\n\ntrainer = UnslothGRPOTrainer(\n    model = model,\n    processing_class = tokenizer,\n    reward_funcs = [format_reward, correctness_reward],\n    args = grpo_config,\n    train_dataset = math_dataset,\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_GRPO_Execution]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_vLLM_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_LoRA_Rank_Selection_Tip]]\n",
      "domains": [
        "Reinforcement Learning",
        "Training",
        "NLP"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "TRL GRPOTrainer",
          "url": "https://huggingface.co/docs/trl/grpo_trainer"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GRPO_Execution"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_vLLM_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_LoRA_Rank_Selection_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_UnslothTrainingArguments",
      "page_title": "Unslothai Unsloth UnslothTrainingArguments",
      "page_type": "Implementation",
      "overview": "Wrapper for TRL's SFTConfig that adds Unsloth-specific training arguments for optimized fine-tuning, particularly embedding layer learning rate control.",
      "content": "# Implementation: UnslothTrainingArguments\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|TRL SFTConfig|https://huggingface.co/docs/trl/sft_trainer]]\n* [[source::Doc|Transformers TrainingArguments|https://huggingface.co/docs/transformers/main_classes/trainer]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Training]], [[domain::Hyperparameters]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nWrapper for TRL's SFTConfig that adds Unsloth-specific training arguments for optimized fine-tuning, particularly embedding layer learning rate control.\n\n=== Description ===\n\n`UnslothTrainingArguments` extends TRL's `SFTConfig` (or `TrainingArguments` for older versions) with Unsloth-specific parameters. The primary addition is `embedding_learning_rate`, which enables separate learning rates for embedding layers (`embed_tokens`, `lm_head`) when training them via `modules_to_save`.\n\nThis is a Wrapper Doc - it documents how Unsloth uses and extends an external library's configuration class.\n\n=== Usage ===\n\nUse `UnslothTrainingArguments` when configuring training with `SFTTrainer`. It accepts all standard `SFTConfig` parameters plus Unsloth extensions. Pass the resulting config object to `SFTTrainer` via the `args` parameter.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/trainer.py\n* '''Lines:''' L133-137\n\n=== External Reference ===\n* '''Library:''' [https://github.com/huggingface/trl TRL]\n* '''Base Class:''' `trl.SFTConfig` or `transformers.TrainingArguments`\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\nclass UnslothTrainingArguments(TrainingArguments):\n    def __init__(\n        self,\n        # Unsloth-specific\n        embedding_learning_rate: float = None,\n        # Standard TrainingArguments/SFTConfig parameters\n        output_dir: str = None,\n        per_device_train_batch_size: int = 8,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 5e-5,\n        num_train_epochs: float = 3.0,\n        max_steps: int = -1,\n        warmup_steps: int = 0,\n        warmup_ratio: float = 0.0,\n        lr_scheduler_type: str = \"linear\",\n        weight_decay: float = 0.0,\n        optim: str = \"adamw_torch\",\n        logging_steps: int = 500,\n        save_steps: int = 500,\n        save_total_limit: int = None,\n        fp16: bool = False,\n        bf16: bool = False,\n        seed: int = 42,\n        **kwargs,\n    ):\n        \"\"\"\n        Training arguments with Unsloth extensions.\n\n        Args:\n            embedding_learning_rate: Separate LR for embed_tokens/lm_head (if in modules_to_save)\n            output_dir: Directory for checkpoints and outputs\n            per_device_train_batch_size: Batch size per GPU\n            gradient_accumulation_steps: Steps before gradient update\n            learning_rate: Peak learning rate for LoRA parameters\n            num_train_epochs: Total training epochs\n            max_steps: Override epochs with fixed step count\n            warmup_steps: Linear warmup steps\n            lr_scheduler_type: LR schedule (\"linear\", \"cosine\", \"constant\")\n            optim: Optimizer (\"adamw_8bit\" recommended for QLoRA)\n            fp16/bf16: Mixed precision training\n\n        Note: All trl.SFTConfig parameters are supported via **kwargs\n        \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothTrainingArguments\n# Or use SFTConfig directly with embedding_learning_rate if needed\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| output_dir || str || Yes || Directory for saving checkpoints\n|-\n| per_device_train_batch_size || int || No (default: 8) || Batch size per GPU\n|-\n| gradient_accumulation_steps || int || No (default: 1) || Steps before weight update\n|-\n| learning_rate || float || No (default: 5e-5) || Peak learning rate\n|-\n| warmup_steps || int || No (default: 0) || Linear warmup steps\n|-\n| max_steps || int || No (default: -1) || Fixed step count (-1 for epoch-based)\n|-\n| optim || str || No || Optimizer name (\"adamw_8bit\" recommended)\n|-\n| embedding_learning_rate || float || No || Separate LR for embedding layers\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| config || UnslothTrainingArguments || Configuration object for SFTTrainer\n|}\n\n== Usage Examples ==\n\n=== Standard QLoRA Training Config ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothTrainingArguments\n\ntraining_args = UnslothTrainingArguments(\n    output_dir = \"./outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,  # Effective batch = 8\n    warmup_steps = 5,\n    max_steps = 60,\n    learning_rate = 2e-4,\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 1,\n    optim = \"adamw_8bit\",  # Memory-efficient optimizer\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n)\n</syntaxhighlight>\n\n=== With Embedding Layer Training ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothTrainingArguments\n\n# When using modules_to_save = [\"embed_tokens\", \"lm_head\"]\ntraining_args = UnslothTrainingArguments(\n    output_dir = \"./outputs\",\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    learning_rate = 2e-4,\n    embedding_learning_rate = 5e-5,  # Lower LR for embeddings\n    max_steps = 100,\n    optim = \"adamw_8bit\",\n    bf16 = True,\n)\n</syntaxhighlight>\n\n=== Long Training Run ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothTrainingArguments\n\ntraining_args = UnslothTrainingArguments(\n    output_dir = \"./outputs\",\n    per_device_train_batch_size = 4,\n    gradient_accumulation_steps = 2,\n    num_train_epochs = 3,\n    learning_rate = 1e-4,\n    warmup_ratio = 0.03,  # 3% warmup\n    lr_scheduler_type = \"cosine\",\n    save_strategy = \"epoch\",\n    logging_steps = 10,\n    optim = \"adamw_8bit\",\n    bf16 = True,\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Training_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Sample_Packing_Tip]]\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_Embedding_Learning_Rate_Tip]]\n",
      "domains": [
        "NLP",
        "Training",
        "Hyperparameters"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "TRL SFTConfig",
          "url": "https://huggingface.co/docs/trl/sft_trainer"
        },
        {
          "type": "Doc",
          "title": "Transformers TrainingArguments",
          "url": "https://huggingface.co/docs/transformers/main_classes/trainer"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Training_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Sample_Packing_Tip"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_Embedding_Learning_Rate_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_dataset_mapping_pattern",
      "page_title": "Unslothai Unsloth dataset mapping pattern",
      "page_type": "Implementation",
      "overview": "Pattern specification for preparing datasets for GRPO reinforcement learning, transforming raw data into the prompt format required by GRPOTrainer.",
      "content": "# Implementation: dataset_mapping_pattern\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|HuggingFace Datasets|https://huggingface.co/docs/datasets]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Reinforcement_Learning]], [[domain::Data_Engineering]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nPattern specification for preparing datasets for GRPO reinforcement learning, transforming raw data into the prompt format required by GRPOTrainer.\n\n=== Description ===\n\nThis is a **Pattern Doc** - it documents a user-defined interface that must be implemented for GRPO training. The pattern specifies how to transform raw datasets into the format GRPOTrainer expects: a dataset with a \"prompt\" column containing formatted prompts ready for generation.\n\nKey requirements:\n* Output must have \"prompt\" column\n* Prompts should use the configured chat template\n* Should not include assistant responses (model generates these)\n\n=== Usage ===\n\nImplement this pattern as a mapping function applied to your raw dataset. The function should format each example as a prompt using `tokenizer.apply_chat_template()`.\n\n== Interface Specification ==\n\n=== Required Output Format ===\n<syntaxhighlight lang=\"python\">\n# Dataset must have \"prompt\" column\n{\n    \"prompt\": str  # Formatted prompt for generation\n}\n</syntaxhighlight>\n\n=== Mapping Function Signature ===\n<syntaxhighlight lang=\"python\">\ndef format_for_grpo(example: Dict[str, Any]) -> Dict[str, str]:\n    \"\"\"\n    Transform a dataset example into GRPO format.\n\n    Args:\n        example: Raw dataset row with problem/question fields\n\n    Returns:\n        Dict with \"prompt\" key containing formatted string\n    \"\"\"\n    # Format as chat messages\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": example[\"question\"]}\n    ]\n\n    # Apply template\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True  # Add assistant turn start\n    )\n\n    return {\"prompt\": prompt}\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Math Reasoning Dataset ===\n<syntaxhighlight lang=\"python\">\nfrom datasets import load_dataset\n\n# Define system prompt for reasoning\nSYSTEM_PROMPT = \"\"\"You are a helpful assistant. For math problems:\n1. Think step-by-step inside <think></think> tags\n2. Provide final answer in \\\\boxed{} format\"\"\"\n\n# Load raw dataset\ndataset = load_dataset(\"openai/gsm8k\", \"main\")[\"train\"]\n\n# Define mapping function\ndef format_prompt(example):\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": example[\"question\"]}\n    ]\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    return {\"prompt\": prompt}\n\n# Apply mapping\ndataset = dataset.map(format_prompt)\n\n# Now dataset has \"prompt\" column ready for GRPOTrainer\nprint(dataset[0][\"prompt\"])\n</syntaxhighlight>\n\n=== Code Generation Dataset ===\n<syntaxhighlight lang=\"python\">\nfrom datasets import load_dataset\n\nSYSTEM_PROMPT = \"You are a coding assistant. Write clean, correct code.\"\n\ndataset = load_dataset(\"my_code_problems\")[\"train\"]\n\ndef format_code_prompt(example):\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": f\"Problem: {example['problem']}\\n\\nWrite a Python solution:\"}\n    ]\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    return {\"prompt\": prompt}\n\ndataset = dataset.map(format_code_prompt)\n</syntaxhighlight>\n\n=== Instruction Following Dataset ===\n<syntaxhighlight lang=\"python\">\nfrom datasets import load_dataset\n\n# No system prompt, simple instruction format\ndataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")[\"train\"]\n\ndef format_instruction(example):\n    # Take first user message as prompt\n    user_message = example[\"messages\"][0][\"content\"]\n    messages = [{\"role\": \"user\", \"content\": user_message}]\n\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    return {\"prompt\": prompt}\n\ndataset = dataset.map(format_instruction)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_RL_Dataset_Preparation]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_vLLM_Environment]]\n",
      "domains": [
        "NLP",
        "Reinforcement Learning",
        "Data Engineering"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Datasets",
          "url": "https://huggingface.co/docs/datasets"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_Dataset_Preparation"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_vLLM_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_get_chat_template",
      "page_title": "Unslothai Unsloth get chat template",
      "page_type": "Implementation",
      "overview": "Concrete tool for applying chat templates to tokenizers, enabling consistent conversation formatting for instruction-tuned model fine-tuning, provided by Unsloth.",
      "content": "# Implementation: get_chat_template\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|HuggingFace Chat Templates|https://huggingface.co/docs/transformers/chat_templating]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Data_Preprocessing]], [[domain::Tokenization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for applying chat templates to tokenizers, enabling consistent conversation formatting for instruction-tuned model fine-tuning, provided by Unsloth.\n\n=== Description ===\n\n`get_chat_template` configures a tokenizer with a specific chat template format (ChatML, Llama-3, Alpaca, etc.). It handles special token mapping, EOS token configuration, and dataset field remapping for consistent training data formatting.\n\nKey capabilities:\n* **20+ built-in templates** - ChatML, Llama-3, Alpaca, Gemma, Phi, Mistral, etc.\n* **EOS token mapping** - Maps template stop tokens to existing tokenizer vocabulary\n* **Dataset field mapping** - Remaps ShareGPT format fields (role\u2192from, content\u2192value)\n* **Custom template support** - Pass (template_string, stop_token) tuple\n\n=== Usage ===\n\nCall after loading model and tokenizer, before dataset preparation. This ensures all training data uses consistent formatting that matches the model's instruction-tuning format.\n\nTemplate selection guidelines:\n* Use model's native format when available (e.g., \"llama-3\" for Llama-3 models)\n* Use \"chatml\" as a universal fallback for most models\n* Match template to your inference deployment (Ollama uses chatml by default)\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/chat_templates.py\n* '''Lines:''' L2123-2400\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef get_chat_template(\n    tokenizer: PreTrainedTokenizer,\n    chat_template: str = \"chatml\",\n    mapping: dict = {\n        \"role\": \"role\",\n        \"content\": \"content\",\n        \"user\": \"user\",\n        \"assistant\": \"assistant\"\n    },\n    map_eos_token: bool = True,\n    system_message: Optional[str] = None,\n) -> PreTrainedTokenizer:\n    \"\"\"\n    Apply a chat template to a tokenizer.\n\n    Args:\n        tokenizer: Tokenizer from FastLanguageModel.from_pretrained\n        chat_template: Template name or (template_string, stop_token) tuple\n        mapping: Field name remapping for dataset columns\n        map_eos_token: Map template stop token to tokenizer's EOS\n        system_message: Optional system message to prepend\n\n    Returns:\n        Tokenizer with chat_template attribute configured\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.chat_templates import get_chat_template\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| tokenizer || PreTrainedTokenizer || Yes || Tokenizer from model loading step\n|-\n| chat_template || str or tuple || No (default: \"chatml\") || Template name or (template, stop_token) tuple\n|-\n| mapping || dict || No || Field remapping: {\"role\": \"from\", \"content\": \"value\"} for ShareGPT\n|-\n| map_eos_token || bool || No (default: True) || Map template's stop token to tokenizer EOS\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| tokenizer || PreTrainedTokenizer || Tokenizer with chat_template configured for apply_chat_template()\n|}\n\n== Usage Examples ==\n\n=== Standard ChatML Template ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Apply ChatML template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"chatml\",  # <|im_start|>role\\ncontent<|im_end|>\n)\n\n# Test the template\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n    {\"role\": \"assistant\", \"content\": \"4\"},\n]\nformatted = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(formatted)\n# <|im_start|>user\n# What is 2+2?<|im_end|>\n# <|im_start|>assistant\n# 4<|im_end|>\n</syntaxhighlight>\n\n=== Llama-3 Native Template ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Use Llama-3's native template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3\",\n)\n\n# Produces: <|begin_of_text|><|start_header_id|>user<|end_header_id|>...\n</syntaxhighlight>\n\n=== ShareGPT Dataset Mapping ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Map ShareGPT format: {\"from\": \"human\", \"value\": \"...\"}\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"chatml\",\n    mapping = {\n        \"role\": \"from\",\n        \"content\": \"value\",\n        \"user\": \"human\",\n        \"assistant\": \"gpt\",\n    },\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Data_Formatting]]\n* [[implements::Principle:Unslothai_Unsloth_Chat_Template_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "NLP",
        "Data Preprocessing",
        "Tokenization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Chat Templates",
          "url": "https://huggingface.co/docs/transformers/chat_templating"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Data_Formatting"
        },
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Chat_Template_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_get_peft_model",
      "page_title": "Unslothai Unsloth get peft model",
      "page_type": "Implementation",
      "overview": "Concrete tool for injecting LoRA (Low-Rank Adaptation) adapters into a quantized language model for parameter-efficient fine-tuning, provided by Unsloth's optimized PEFT wrapper.",
      "content": "# Implementation: get_peft_model\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|PEFT Documentation|https://huggingface.co/docs/peft]]\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Parameter_Efficient_Finetuning]], [[domain::LoRA]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for injecting LoRA (Low-Rank Adaptation) adapters into a quantized language model for parameter-efficient fine-tuning, provided by Unsloth's optimized PEFT wrapper.\n\n=== Description ===\n\n`FastLanguageModel.get_peft_model` wraps PEFT's LoRA implementation with Unsloth-specific optimizations. It attaches trainable low-rank matrices to specified attention and MLP modules while keeping base weights frozen. The function applies fused LoRA kernels for QKV projections and handles gradient checkpointing setup.\n\nKey features for QLoRA training:\n* **Fused LoRA operations** - Single kernel for QKV attention projections\n* **Optimized gradient checkpointing** - Unsloth's memory-efficient implementation\n* **Automatic target module detection** - Defaults to all attention + MLP projections\n* **Support for embed_tokens/lm_head training** - Via `modules_to_save` parameter\n\n=== Usage ===\n\nCall this function immediately after loading a model with `FastLanguageModel.from_pretrained`. Pass the model and configure LoRA hyperparameters (rank, alpha, target modules). The returned model has LoRA adapters attached and is ready for training.\n\nTypical LoRA rank values:\n* r=8-16: Standard fine-tuning tasks\n* r=32-64: Complex tasks requiring more capacity\n* r=128+: Full fine-tuning approximation (rare)\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/llama.py\n* '''Lines:''' L2577-3100\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@staticmethod\ndef get_peft_model(\n    model: PreTrainedModel,\n    r: int = 16,\n    target_modules: List[str] = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.0,\n    bias: str = \"none\",\n    layers_to_transform: Optional[List[int]] = None,\n    layers_pattern: Optional[str] = None,\n    use_gradient_checkpointing: str = \"unsloth\",\n    random_state: int = 3407,\n    max_seq_length: int = 2048,\n    use_rslora: bool = False,\n    modules_to_save: Optional[List[str]] = None,\n    init_lora_weights: bool = True,\n    loftq_config: dict = {},\n    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\n    qat_scheme: Optional[str] = None,\n    ensure_weight_tying: bool = False,\n    **kwargs,\n) -> PeftModelForCausalLM:\n    \"\"\"\n    Apply LoRA adapters to a pretrained model.\n\n    Args:\n        model: Base model from FastLanguageModel.from_pretrained\n        r: LoRA rank (dimensionality of low-rank matrices)\n        target_modules: List of module names to apply LoRA\n        lora_alpha: LoRA scaling factor (effective scale = alpha/r)\n        lora_dropout: Dropout probability (0.0 recommended for Unsloth)\n        bias: Bias training mode (\"none\", \"all\", \"lora_only\")\n        use_gradient_checkpointing: \"unsloth\" for optimized checkpointing\n        use_rslora: Enable rank-stabilized LoRA scaling\n        modules_to_save: Modules to train fully (e.g., embed_tokens, lm_head)\n\n    Returns:\n        PeftModelForCausalLM with LoRA adapters attached\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n# Called as: FastLanguageModel.get_peft_model(model, ...)\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || Base model from FastLanguageModel.from_pretrained\n|-\n| r || int || No (default: 16) || LoRA rank - dimensionality of adaptation matrices\n|-\n| target_modules || List[str] || No || Modules to apply LoRA; defaults to all attention + MLP\n|-\n| lora_alpha || int || No (default: 16) || LoRA scaling factor; effective scale = alpha/r\n|-\n| lora_dropout || float || No (default: 0.0) || Dropout rate; 0.0 recommended for fast patching\n|-\n| use_rslora || bool || No (default: False) || Use rank-stabilized LoRA (scale = alpha/sqrt(r))\n|-\n| modules_to_save || List[str] || No || Full-precision trainable modules (embed_tokens, lm_head)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PeftModelForCausalLM || Model with LoRA adapters; only adapter weights are trainable\n|}\n\n== Usage Examples ==\n\n=== Standard QLoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load base model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Apply LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,                # LoRA rank\n    lora_alpha = 16,       # Scaling factor (effective scale = 1.0)\n    lora_dropout = 0,      # No dropout for fast patching\n    target_modules = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()\n# Output: trainable params: 13,631,488 || all params: 1,249,902,592 || trainable%: 1.0906%\n</syntaxhighlight>\n\n=== Training Embeddings (Extended Vocabulary) ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Train LoRA + embedding layers for new tokens\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    lora_alpha = 16,\n    target_modules = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    modules_to_save = [\"embed_tokens\", \"lm_head\"],  # Train embeddings\n    use_gradient_checkpointing = \"unsloth\",\n)\n</syntaxhighlight>\n\n=== High-Rank Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Higher rank for complex tasks\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,                # Higher rank for more capacity\n    lora_alpha = 64,       # Keep alpha = r for scale = 1.0\n    use_rslora = True,     # Rank-stabilized scaling\n    target_modules = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_LoRA_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_LoRA_Rank_Selection_Tip]]\n",
      "domains": [
        "NLP",
        "Parameter Efficient Finetuning",
        "LoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "PEFT Documentation",
          "url": "https://huggingface.co/docs/peft"
        },
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_LoRA_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_LoRA_Rank_Selection_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_get_peft_model_rl",
      "page_title": "Unslothai Unsloth get peft model rl",
      "page_type": "Implementation",
      "overview": "Concrete tool for injecting LoRA adapters into a vLLM-enabled model for reinforcement learning training, with rank constraints matching vLLM's pre-allocated limits.",
      "content": "# Implementation: get_peft_model_rl\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|LoRA|https://arxiv.org/abs/2106.09685]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Reinforcement_Learning]], [[domain::Parameter_Efficient_Finetuning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for injecting LoRA adapters into a vLLM-enabled model for reinforcement learning training, with rank constraints matching vLLM's pre-allocated limits.\n\n=== Description ===\n\n`FastLanguageModel.get_peft_model` for RL workflows applies LoRA adapters with specific constraints for vLLM compatibility. The LoRA rank must not exceed `max_lora_rank` specified during model loading, as vLLM pre-allocates memory for adapter weights.\n\nThis is an angle-specific documentation of `get_peft_model` for the RL LoRA Configuration context.\n\nKey RL-specific considerations:\n* **Rank constraint** - `r <= max_lora_rank` from model loading\n* **Higher ranks typical** - RL often uses r=32-64 for more capacity\n* **vLLM memory** - LoRA weights affect vLLM's available GPU memory\n\n=== Usage ===\n\nCall after loading model with `fast_inference=True`. The LoRA rank must be <= `max_lora_rank` from loading. Higher ranks are common for RL as the model needs more adaptability for policy optimization.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/models/llama.py\n* '''Lines:''' L2577-3100 (same function, RL context)\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\n@staticmethod\ndef get_peft_model(\n    model: PreTrainedModel,  # Model with vLLM engine\n    r: int = 16,  # Must be <= max_lora_rank from loading\n    target_modules: List[str] = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.0,\n    bias: str = \"none\",\n    use_gradient_checkpointing: str = \"unsloth\",\n    use_rslora: bool = False,\n    **kwargs,\n) -> PeftModelForCausalLM:\n    \"\"\"\n    Apply LoRA adapters for RL training.\n\n    Args:\n        model: Model with vLLM engine from from_pretrained(fast_inference=True)\n        r: LoRA rank (MUST be <= max_lora_rank from loading)\n        target_modules: Modules to apply LoRA\n        lora_alpha: LoRA scaling factor\n        use_rslora: Use rank-stabilized scaling\n\n    Returns:\n        PeftModelForCausalLM ready for GRPOTrainer\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| model || PreTrainedModel || Yes || Model with vLLM engine from fast_inference=True loading\n|-\n| r || int || No (default: 16) || LoRA rank; MUST be <= max_lora_rank from loading\n|-\n| target_modules || List[str] || No || Modules for LoRA; defaults to attention + MLP\n|-\n| lora_alpha || int || No (default: 16) || Scaling factor; effective scale = alpha/r\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| model || PeftModelForCausalLM || vLLM-enabled model with LoRA adapters\n|}\n\n== Usage Examples ==\n\n=== Standard RL LoRA Configuration ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load with vLLM and max_lora_rank=64\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 1024,\n    load_in_4bit = True,\n    fast_inference = True,\n    max_lora_rank = 64,\n    gpu_memory_utilization = 0.5,\n)\n\n# Apply LoRA with rank <= 64\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,          # Max rank, equal to max_lora_rank\n    lora_alpha = 64,\n    target_modules = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    use_gradient_checkpointing = \"unsloth\",\n)\n</syntaxhighlight>\n\n=== Lower Rank for Memory Efficiency ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# Load with max_lora_rank=64\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.1-8B-Instruct\",\n    max_seq_length = 1024,\n    load_in_4bit = True,\n    fast_inference = True,\n    max_lora_rank = 64,\n    gpu_memory_utilization = 0.6,\n)\n\n# Use lower rank to save memory\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,          # Lower than max for memory\n    lora_alpha = 32,\n    target_modules = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_RL_LoRA_Configuration]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_vLLM_Environment]]\n",
      "domains": [
        "NLP",
        "Reinforcement Learning",
        "Parameter Efficient Finetuning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "LoRA",
          "url": "https://arxiv.org/abs/2106.09685"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_RL_LoRA_Configuration"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_vLLM_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_llama_cli_validation",
      "page_title": "Unslothai Unsloth llama cli validation",
      "page_type": "Implementation",
      "overview": "External tool documentation for validating GGUF exports using llama.cpp CLI.",
      "content": "# Implementation: llama_cli_validation\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Doc|llama.cpp|https://github.com/ggerganov/llama.cpp]]\n|-\n! Domains\n| [[domain::Testing]], [[domain::GGUF]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nExternal tool documentation for validating GGUF exports using llama.cpp CLI.\n\n=== Description ===\n\n`llama-cli` (formerly `main`) is the llama.cpp command-line inference tool. Use it to verify GGUF exports before deployment.\n\n=== Usage ===\n\nRun after GGUF export to validate model functionality.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/ggerganov/llama.cpp llama.cpp]\n* '''File:''' examples/main/main.cpp\n* '''Binary:''' llama-cli (built from source)\n\n=== Installation ===\n<syntaxhighlight lang=\"bash\">\n# Clone and build llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\nmake -j\n\n# Or with CUDA support\nmake -j LLAMA_CUDA=1\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== CLI Arguments ===\n{| class=\"wikitable\"\n|-\n! Argument !! Description\n|-\n| -m, --model || Path to GGUF model file\n|-\n| -p, --prompt || Input prompt text\n|-\n| -n, --n-predict || Number of tokens to generate\n|-\n| --chat || Enable chat mode with template\n|-\n| -c, --ctx-size || Context size (default: 512)\n|-\n| -t, --threads || Number of CPU threads\n|-\n| --gpu-layers || Layers to offload to GPU\n|}\n\n== Usage Examples ==\n\n=== Basic Generation Test ===\n<syntaxhighlight lang=\"bash\">\n# Test text generation\nllama-cli \\\n  --model ./model-Q4_K_M.gguf \\\n  --prompt \"The capital of France is\" \\\n  --n-predict 50\n\n# Expected: Coherent continuation about Paris\n</syntaxhighlight>\n\n=== Chat Mode Test ===\n<syntaxhighlight lang=\"bash\">\n# Test chat template\nllama-cli \\\n  --model ./model-Q4_K_M.gguf \\\n  --chat \\\n  --prompt \"Hello, how are you?\"\n\n# Should use model's chat template\n</syntaxhighlight>\n\n=== VLM Test ===\n<syntaxhighlight lang=\"bash\">\n# Test vision-language model\nllama-mtmd-cli \\\n  --model ./model-Q4_K_M.gguf \\\n  --mmproj ./mmproj-Q4_K_M.gguf\n\n# Interactive mode:\n# /image path/to/image.jpg\n# What do you see in this image?\n</syntaxhighlight>\n\n=== Ollama Test ===\n<syntaxhighlight lang=\"bash\">\n# Register with Ollama\nollama create my-model -f ./Modelfile\n\n# Test with Ollama\nollama run my-model \"What is 2+2?\"\n</syntaxhighlight>\n\n=== Quick Validation Script ===\n<syntaxhighlight lang=\"python\">\nimport subprocess\nimport sys\n\ndef validate_gguf(model_path: str) -> bool:\n    \"\"\"Quick validation of GGUF export.\"\"\"\n    try:\n        result = subprocess.run(\n            [\n                \"llama-cli\",\n                \"-m\", model_path,\n                \"-p\", \"Hello\",\n                \"-n\", \"10\",\n            ],\n            capture_output=True,\n            text=True,\n            timeout=60,\n        )\n        if result.returncode == 0:\n            print(f\"\u2713 Model loaded successfully\")\n            print(f\"Output: {result.stdout[:200]}...\")\n            return True\n        else:\n            print(f\"\u2717 Error: {result.stderr}\")\n            return False\n    except FileNotFoundError:\n        print(\"\u2717 llama-cli not found. Install llama.cpp first.\")\n        return False\n    except subprocess.TimeoutExpired:\n        print(\"\u2717 Timeout during inference\")\n        return False\n\nif __name__ == \"__main__\":\n    validate_gguf(sys.argv[1])\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_GGUF_Verification]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_llama_cpp_Environment]]\n\n",
      "domains": [
        "Testing",
        "GGUF",
        "Inference"
      ],
      "sources": [
        {
          "type": "Doc",
          "title": "llama.cpp",
          "url": "https://github.com/ggerganov/llama.cpp"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Verification"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_llama_cpp_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_multimodal_dataset_pattern",
      "page_title": "Unslothai Unsloth multimodal dataset pattern",
      "page_type": "Implementation",
      "overview": "Pattern specification for preparing datasets for Vision-Language Model fine-tuning with interleaved image and text content.",
      "content": "# Implementation: multimodal_dataset_pattern\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|HuggingFace Datasets|https://huggingface.co/docs/datasets]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Data_Engineering]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nPattern specification for preparing datasets for Vision-Language Model fine-tuning with interleaved image and text content.\n\n=== Description ===\n\nThis is a **Pattern Doc** - it documents the dataset format expected by VLM training. Multimodal datasets must provide:\n* Messages with `{\"type\": \"image\"}` and `{\"type\": \"text\"}` content\n* Images as PIL.Image objects or file paths\n* Proper message structure for chat templates\n\n=== Usage ===\n\nPrepare your dataset to match this format before passing to SFTTrainer with a VLM.\n\n== Interface Specification ==\n\n=== Required Format ===\n<syntaxhighlight lang=\"python\">\n# Each dataset row should have:\n{\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},  # Placeholder for image\n                {\"type\": \"text\", \"text\": \"What is in this image?\"}\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"This image shows...\"}\n            ]\n        }\n    ],\n    \"images\": [PIL.Image or path]  # Actual image data\n}\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Image Captioning Dataset ===\n<syntaxhighlight lang=\"python\">\nfrom datasets import Dataset\nfrom PIL import Image\n\ndef create_vlm_dataset(image_paths, captions):\n    \"\"\"Create dataset for VLM fine-tuning.\"\"\"\n    data = []\n    for img_path, caption in zip(image_paths, captions):\n        data.append({\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"image\"},\n                        {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n                    ]\n                },\n                {\n                    \"role\": \"assistant\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": caption}\n                    ]\n                }\n            ],\n            \"images\": [Image.open(img_path)]\n        })\n    return Dataset.from_list(data)\n</syntaxhighlight>\n\n=== VQA Dataset ===\n<syntaxhighlight lang=\"python\">\nfrom datasets import Dataset\nfrom PIL import Image\n\ndef create_vqa_dataset(examples):\n    \"\"\"Create VQA dataset with questions and answers.\"\"\"\n    data = []\n    for ex in examples:\n        data.append({\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"image\"},\n                        {\"type\": \"text\", \"text\": ex[\"question\"]}\n                    ]\n                },\n                {\n                    \"role\": \"assistant\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": ex[\"answer\"]}\n                    ]\n                }\n            ],\n            \"images\": [Image.open(ex[\"image_path\"])]\n        })\n    return Dataset.from_list(data)\n</syntaxhighlight>\n\n=== Multi-Turn Conversation ===\n<syntaxhighlight lang=\"python\">\nfrom datasets import Dataset\nfrom PIL import Image\n\ndef create_conversation_dataset(conversations):\n    \"\"\"Create multi-turn conversation dataset.\"\"\"\n    data = []\n    for conv in conversations:\n        messages = []\n        for turn in conv[\"turns\"]:\n            content = []\n            if turn.get(\"has_image\"):\n                content.append({\"type\": \"image\"})\n            content.append({\"type\": \"text\", \"text\": turn[\"text\"]})\n\n            messages.append({\n                \"role\": turn[\"role\"],\n                \"content\": content\n            })\n\n        data.append({\n            \"messages\": messages,\n            \"images\": [Image.open(p) for p in conv.get(\"image_paths\", [])]\n        })\n    return Dataset.from_list(data)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Multimodal_Data_Preparation]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Computer Vision",
        "Data Engineering",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Datasets",
          "url": "https://huggingface.co/docs/datasets"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Multimodal_Data_Preparation"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_push_to_hub_gguf",
      "page_title": "Unslothai Unsloth push to hub gguf",
      "page_type": "Implementation",
      "overview": "Concrete tool for uploading GGUF models to HuggingFace Hub.",
      "content": "# Implementation: push_to_hub_gguf\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Model_Sharing]], [[domain::HuggingFace_Hub]], [[domain::GGUF]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for uploading GGUF models to HuggingFace Hub.\n\n=== Description ===\n\n`push_to_hub_gguf` converts models to GGUF format and uploads directly to HuggingFace Hub. It combines:\n* GGUF conversion (via save_pretrained_gguf)\n* Repository creation\n* File upload with progress tracking\n* README generation\n\n=== Usage ===\n\nCall after training to export and share GGUF models in one step.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/save.py\n* '''Lines:''' L2060-2300\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef push_to_hub_gguf(\n    self,\n    repo_id: str,\n    tokenizer = None,\n    quantization_method: Union[str, List[str]] = \"fast_quantized\",\n    first_conversion: Optional[str] = None,\n    use_temp_dir: Optional[bool] = None,\n    commit_message: Optional[str] = \"Trained with Unsloth\",\n    private: Optional[bool] = None,\n    token: Union[bool, str, None] = None,\n    max_shard_size: Union[int, str] = \"5GB\",\n    create_pr: bool = False,\n    safe_serialization: bool = True,\n    revision: str = None,\n    commit_description: str = \"Upload model trained with Unsloth 2x faster\",\n    tags: Optional[List[str]] = None,\n    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\n    maximum_memory_usage: float = 0.85,\n) -> str:\n    \"\"\"\n    Convert to GGUF and upload to HuggingFace Hub.\n\n    Returns:\n        URL of the uploaded model repository\n    \"\"\"\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Parameter !! Type !! Description\n|-\n| repo_id || str || HuggingFace repo ID (user/model-name)\n|-\n| tokenizer || PreTrainedTokenizer || Model tokenizer\n|-\n| quantization_method || Union[str, List[str]] || Quant method(s) to upload\n|-\n| private || Optional[bool] || Create private repository\n|-\n| token || Union[bool, str, None] || HuggingFace API token\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Output !! Type !! Description\n|-\n| repo_url || str || URL of uploaded repository\n|}\n\n== Usage Examples ==\n\n=== Basic Hub Upload ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# After training...\n# Upload to HuggingFace Hub with q4_k_m\nmodel.push_to_hub_gguf(\n    \"username/my-model-GGUF\",\n    tokenizer,\n    quantization_method = \"q4_k_m\",\n    token = \"hf_...\",  # Your HF token\n)\n</syntaxhighlight>\n\n=== Multiple Quantizations ===\n<syntaxhighlight lang=\"python\">\n# Upload multiple quantization levels\nmodel.push_to_hub_gguf(\n    \"username/my-model-GGUF\",\n    tokenizer,\n    quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\"],\n    private = False,\n    commit_message = \"Add GGUF quantizations\",\n)\n</syntaxhighlight>\n\n=== Private Repository ===\n<syntaxhighlight lang=\"python\">\n# Create private GGUF repository\nmodel.push_to_hub_gguf(\n    \"username/my-private-model-GGUF\",\n    tokenizer,\n    quantization_method = \"q4_k_m\",\n    private = True,\n    token = \"hf_...\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_GGUF_Hub_Upload]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_GGUF_Quantization_Selection_Tip]]\n\n",
      "domains": [
        "Model Sharing",
        "HuggingFace Hub",
        "GGUF"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Hub_Upload"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_GGUF_Quantization_Selection_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_reward_function_pattern",
      "page_title": "Unslothai Unsloth reward function pattern",
      "page_type": "Implementation",
      "overview": "Pattern specification for user-defined reward functions that score model completions during GRPO reinforcement learning training.",
      "content": "# Implementation: reward_function_pattern\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Paper|GRPO|https://arxiv.org/abs/2402.03300]]\n* [[source::Doc|TRL GRPOTrainer|https://huggingface.co/docs/trl/grpo_trainer]]\n|-\n! Domains\n| [[domain::Reinforcement_Learning]], [[domain::Reward_Modeling]], [[domain::NLP]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nPattern specification for user-defined reward functions that score model completions during GRPO reinforcement learning training.\n\n=== Description ===\n\nThis is a **Pattern Doc** - it documents a user-defined interface that must be implemented for GRPO training. Reward functions are the core mechanism for providing learning signal in reinforcement learning. They take model completions and return numerical scores indicating quality.\n\nKey requirements:\n* Input: Lists of completions and prompts\n* Output: List of float scores (one per completion)\n* Should be deterministic for training stability\n* Can use heuristics, external models, or verification\n\n=== Usage ===\n\nDefine one or more reward functions and pass them to GRPOTrainer via the `reward_funcs` parameter. Multiple reward functions can be combined (their scores are summed or weighted).\n\n== Interface Specification ==\n\n=== Required Signature ===\n<syntaxhighlight lang=\"python\">\ndef reward_function(\n    completions: List[str],  # Model-generated completions\n    prompts: List[str],      # Original prompts (for context)\n    **kwargs                 # Optional: metadata, model outputs\n) -> List[float]:            # Reward scores, one per completion\n    \"\"\"\n    Score model completions for GRPO training.\n\n    Args:\n        completions: List of generated text strings\n        prompts: List of corresponding input prompts\n        **kwargs: Optional additional context\n\n    Returns:\n        List of float rewards (same length as completions)\n        Higher scores = better completions\n    \"\"\"\n    ...\n</syntaxhighlight>\n\n=== Score Guidelines ===\n{| class=\"wikitable\"\n|-\n! Property !! Guideline\n|-\n| Range || Typically -1 to 1, or 0 to 1\n|-\n| Scale || Consistent across examples\n|-\n| Meaning || Higher = better completion\n|-\n| Determinism || Prefer deterministic for stability\n|}\n\n== Usage Examples ==\n\n=== Format Checking Reward (Rule-Based) ===\n<syntaxhighlight lang=\"python\">\nimport re\n\ndef format_reward(completions, prompts, **kwargs):\n    \"\"\"\n    Reward for correct output format (reasoning + boxed answer).\n    \"\"\"\n    rewards = []\n    for completion in completions:\n        score = 0.0\n\n        # Check for reasoning tags\n        if \"<think>\" in completion and \"</think>\" in completion:\n            score += 0.5\n\n        # Check for boxed answer\n        if re.search(r\"\\\\boxed\\{.+\\}\", completion):\n            score += 0.5\n\n        rewards.append(score)\n\n    return rewards\n</syntaxhighlight>\n\n=== Answer Correctness Reward ===\n<syntaxhighlight lang=\"python\">\nimport re\n\ndef correctness_reward(completions, prompts, **kwargs):\n    \"\"\"\n    Reward for correct math answers.\n    Requires ground truth answers in kwargs or dataset.\n    \"\"\"\n    rewards = []\n    answers = kwargs.get(\"answers\", [None] * len(completions))\n\n    for completion, answer in zip(completions, answers):\n        if answer is None:\n            rewards.append(0.0)\n            continue\n\n        # Extract model's answer from \\boxed{}\n        match = re.search(r\"\\\\boxed\\{(.+?)\\}\", completion)\n        if match:\n            model_answer = match.group(1).strip()\n            # Compare (simple string match)\n            score = 1.0 if model_answer == str(answer).strip() else 0.0\n        else:\n            score = 0.0\n\n        rewards.append(score)\n\n    return rewards\n</syntaxhighlight>\n\n=== Length Penalty Reward ===\n<syntaxhighlight lang=\"python\">\ndef length_reward(completions, prompts, max_length=500, **kwargs):\n    \"\"\"\n    Penalize overly long or short completions.\n    \"\"\"\n    rewards = []\n    for completion in completions:\n        length = len(completion)\n\n        if length < 50:  # Too short\n            score = -0.5\n        elif length > max_length:  # Too long\n            score = -0.3\n        else:\n            score = 0.0  # Good length range\n\n        rewards.append(score)\n\n    return rewards\n</syntaxhighlight>\n\n=== Model-Based Reward ===\n<syntaxhighlight lang=\"python\">\ndef model_reward(completions, prompts, reward_model, tokenizer, **kwargs):\n    \"\"\"\n    Use a trained reward model to score completions.\n    \"\"\"\n    rewards = []\n    for completion, prompt in zip(completions, prompts):\n        # Prepare input for reward model\n        full_text = prompt + completion\n        inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True)\n\n        # Get reward score\n        with torch.no_grad():\n            score = reward_model(**inputs).logits.item()\n\n        rewards.append(score)\n\n    return rewards\n</syntaxhighlight>\n\n=== Combining Multiple Rewards ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import UnslothGRPOTrainer, UnslothGRPOConfig\n\n# Define multiple reward functions\ndef format_reward(completions, prompts, **kwargs):\n    # ... format checking\n    return format_scores\n\ndef correctness_reward(completions, prompts, **kwargs):\n    # ... answer verification\n    return correctness_scores\n\n# Pass as list to trainer\ntrainer = UnslothGRPOTrainer(\n    model = model,\n    processing_class = tokenizer,\n    train_dataset = dataset,\n    reward_funcs = [format_reward, correctness_reward],  # Combined\n    args = UnslothGRPOConfig(...),\n)\n\n# Rewards are summed: total = format + correctness\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Reward_Functions]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_vLLM_Environment]]\n",
      "domains": [
        "Reinforcement Learning",
        "Reward Modeling",
        "NLP"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Paper",
          "title": "GRPO",
          "url": "https://arxiv.org/abs/2402.03300"
        },
        {
          "type": "Doc",
          "title": "TRL GRPOTrainer",
          "url": "https://huggingface.co/docs/trl/grpo_trainer"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Reward_Functions"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_vLLM_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_save_pretrained",
      "page_title": "Unslothai Unsloth save pretrained",
      "page_type": "Implementation",
      "overview": "Concrete tool for saving fine-tuned models in various formats (LoRA adapters, merged 16-bit, merged 4-bit) for deployment or further training, provided by Unsloth.",
      "content": "# Implementation: save_pretrained\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|HuggingFace Model Saving|https://huggingface.co/docs/transformers/main_classes/model]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Model_Serialization]], [[domain::Deployment]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for saving fine-tuned models in various formats (LoRA adapters, merged 16-bit, merged 4-bit) for deployment or further training, provided by Unsloth.\n\n=== Description ===\n\n`model.save_pretrained` and `model.save_pretrained_merged` provide flexible model saving options after QLoRA fine-tuning:\n\n* **LoRA only** (`save_method=\"lora\"`): Saves only the trained adapter weights (~50-100MB). Fastest, smallest, requires base model at inference.\n* **Merged 16-bit** (`save_method=\"merged_16bit\"`): Merges LoRA into base and saves as float16. Required for GGUF export.\n* **Merged 4-bit** (`save_method=\"merged_4bit\"`): Merges LoRA into quantized weights. For direct inference without base model.\n\n=== Usage ===\n\nCall after training completes. Choose save method based on deployment needs:\n* For continued training or HuggingFace deployment: `lora`\n* For llama.cpp/GGUF export: `merged_16bit`\n* For direct 4-bit inference: `merged_4bit`\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/save.py\n* '''Lines:''' L235-860\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef unsloth_save_model(\n    model,\n    tokenizer,\n    save_directory: Union[str, os.PathLike],\n    save_method: str = \"lora\",  # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\n    push_to_hub: bool = False,\n    token: Optional[Union[str, bool]] = None,\n    is_main_process: bool = True,\n    state_dict: Optional[dict] = None,\n    save_function: Callable = torch.save,\n    max_shard_size: Union[int, str] = \"5GB\",\n    safe_serialization: bool = True,\n    variant: Optional[str] = None,\n    save_peft_format: bool = True,\n    # Push to hub\n    use_temp_dir: Optional[bool] = None,\n    commit_message: Optional[str] = \"Trained with Unsloth\",\n    private: Optional[bool] = None,\n    create_pr: bool = False,\n    revision: str = None,\n    commit_description: str = \"Upload model trained with Unsloth 2x faster\",\n    tags: List[str] = None,\n    # Unsloth-specific\n    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\n    maximum_memory_usage: float = 0.9,\n) -> None:\n    \"\"\"\n    Save model with optional LoRA merging.\n\n    Args:\n        model: Trained model with LoRA adapters\n        tokenizer: Tokenizer to save alongside model\n        save_directory: Output path or HuggingFace repo ID\n        save_method: \"lora\", \"merged_16bit\", or \"merged_4bit\"\n        push_to_hub: Upload to HuggingFace Hub\n        token: HuggingFace API token\n        max_shard_size: Maximum file size per shard\n        safe_serialization: Use safetensors format\n        maximum_memory_usage: RAM limit for merging (0.0-0.95)\n\n    Returns:\n        None (files saved to disk or uploaded to Hub)\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# save_pretrained is a method on the model\nmodel.save_pretrained(\"./output\")\nmodel.save_pretrained_merged(\"./output\", tokenizer, save_method=\"merged_16bit\")\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| save_directory || str || Yes || Local path or HuggingFace repo ID\n|-\n| tokenizer || PreTrainedTokenizer || Yes (for merged) || Tokenizer to save with model\n|-\n| save_method || str || No (default: \"lora\") || \"lora\", \"merged_16bit\", or \"merged_4bit\"\n|-\n| push_to_hub || bool || No (default: False) || Upload to HuggingFace Hub\n|-\n| token || str || Conditional || Required if push_to_hub=True\n|-\n| max_shard_size || str || No (default: \"5GB\") || Maximum size per weight shard\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| Files || Directory || Model weights, config, tokenizer files saved to disk\n|-\n| Hub URL || str || Repository URL if push_to_hub=True\n|}\n\n== Usage Examples ==\n\n=== Save LoRA Adapters Only ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# After training...\n# Save just the LoRA adapters (smallest, fastest)\nmodel.save_pretrained(\"./lora_model\")\ntokenizer.save_pretrained(\"./lora_model\")\n\n# Load later with:\n# model, tokenizer = FastLanguageModel.from_pretrained(\"./lora_model\")\n</syntaxhighlight>\n\n=== Save Merged 16-bit for GGUF Export ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# After training...\n# Merge LoRA and save as float16 (required for GGUF)\nmodel.save_pretrained_merged(\n    \"./merged_model\",\n    tokenizer,\n    save_method = \"merged_16bit\",\n    max_shard_size = \"5GB\",\n)\n\n# Now ready for: model.save_pretrained_gguf(...)\n</syntaxhighlight>\n\n=== Push to HuggingFace Hub ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# After training...\n# Upload LoRA adapters to Hub\nmodel.save_pretrained_merged(\n    \"username/my-fine-tuned-model\",  # Repo ID\n    tokenizer,\n    save_method = \"lora\",\n    push_to_hub = True,\n    token = \"hf_your_token\",\n    private = False,\n)\n</syntaxhighlight>\n\n=== Save Merged for Direct Inference ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# After training...\n# Merge LoRA into 4-bit weights (smallest merged)\nmodel.save_pretrained_merged(\n    \"./merged_4bit\",\n    tokenizer,\n    save_method = \"merged_4bit_forced\",  # Acknowledge accuracy warning\n    max_shard_size = \"2GB\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Model_Saving]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "NLP",
        "Model Serialization",
        "Deployment"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "HuggingFace Model Saving",
          "url": "https://huggingface.co/docs/transformers/main_classes/model"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Saving"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_save_pretrained_vision",
      "page_title": "Unslothai Unsloth save pretrained vision",
      "page_type": "Implementation",
      "overview": "Concrete tool for saving fine-tuned Vision-Language Models with processor (tokenizer + image processor) included.",
      "content": "# Implementation: save_pretrained_vision\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Computer_Vision]], [[domain::Model_Serialization]], [[domain::Multimodal]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for saving fine-tuned Vision-Language Models with processor (tokenizer + image processor) included.\n\n=== Description ===\n\nVLM saving uses the same `save_pretrained` / `save_pretrained_merged` APIs as text models, but saves the AutoProcessor (which includes both tokenizer and image processor) alongside the model weights.\n\n=== Usage ===\n\nCall after training, passing the processor instead of tokenizer.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/save.py\n* '''Lines:''' L235-860\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\n# Same API as text models\nmodel.save_pretrained_merged(\"./output\", processor, save_method=\"merged_16bit\")\n</syntaxhighlight>\n\n== Usage Examples ==\n\n=== Save VLM with LoRA ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\n# After training...\n# Save LoRA adapters\nmodel.save_pretrained(\"./vlm_lora\")\nprocessor.save_pretrained(\"./vlm_lora\")\n</syntaxhighlight>\n\n=== Save Merged VLM ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\n# After training...\n# Merge and save\nmodel.save_pretrained_merged(\n    \"./vlm_merged\",\n    processor,  # Pass processor, not tokenizer\n    save_method = \"merged_16bit\",\n)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Vision_Model_Saving]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "Computer Vision",
        "Model Serialization",
        "Multimodal"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Vision_Model_Saving"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_save_to_gguf",
      "page_title": "Unslothai Unsloth save to gguf",
      "page_type": "Implementation",
      "overview": "Concrete tool for converting fine-tuned models to GGUF format using llama.cpp.",
      "content": "# Implementation: save_to_gguf\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Model_Export]], [[domain::GGUF]], [[domain::Quantization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for converting fine-tuned models to GGUF format using llama.cpp.\n\n=== Description ===\n\n`save_pretrained_gguf` orchestrates the full GGUF conversion pipeline:\n1. Merges LoRA adapters to float16\n2. Installs llama.cpp if needed\n3. Converts HF format to GGUF\n4. Applies quantization\n5. Creates Ollama Modelfile\n\n=== Usage ===\n\nCall after training to export model for llama.cpp/Ollama deployment.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/save.py\n* '''Lines:''' L1070-1300 (save_to_gguf), L1760-2058 (save_pretrained_gguf)\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef save_pretrained_gguf(\n    self,\n    save_directory: str,\n    tokenizer = None,\n    quantization_method: Union[str, List[str]] = \"fast_quantized\",\n    first_conversion: Optional[str] = None,\n    push_to_hub: bool = False,\n    token: Optional[str] = None,\n    is_main_process: bool = True,\n    state_dict: Optional[dict] = None,\n    save_function = safetensors.torch.save_file,\n    max_shard_size: Union[int, str] = \"5GB\",\n    safe_serialization: bool = True,\n    variant: Optional[str] = None,\n    save_peft_format: bool = True,\n    tags: List[str] = None,\n    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\n    maximum_memory_usage: float = 0.85,\n) -> dict:\n    \"\"\"\n    Convert model to GGUF format.\n\n    Returns dict with:\n    - save_directory: Output path\n    - gguf_files: List of GGUF file paths\n    - modelfile_location: Ollama Modelfile path\n    - is_vlm: Whether model is vision-language\n    \"\"\"\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Parameter !! Type !! Description\n|-\n| save_directory || str || Output directory for GGUF files\n|-\n| tokenizer || PreTrainedTokenizer || Model tokenizer\n|-\n| quantization_method || Union[str, List[str]] || Quant method(s) from ALLOWED_QUANTS\n|-\n| first_conversion || Optional[str] || Initial precision (auto-detected)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Output !! Type !! Description\n|-\n| gguf_files || List[str] || Paths to generated GGUF files\n|-\n| modelfile_location || str || Path to Ollama Modelfile\n|-\n| is_vlm || bool || Whether model is VLM\n|}\n\n== Usage Examples ==\n\n=== Basic GGUF Export ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# After training...\n# Export to GGUF with q4_k_m quantization\nmodel.save_pretrained_gguf(\n    \"./model_gguf\",\n    tokenizer,\n    quantization_method = \"q4_k_m\",\n)\n# Creates: ./model_gguf/model-Q4_K_M.gguf\n</syntaxhighlight>\n\n=== Multiple Quantizations ===\n<syntaxhighlight lang=\"python\">\n# Export multiple quantization levels at once\nmodel.save_pretrained_gguf(\n    \"./model_gguf\",\n    tokenizer,\n    quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\"],\n)\n# Creates: model-Q4_K_M.gguf, model-Q8_0.gguf, model-Q5_K_M.gguf\n</syntaxhighlight>\n\n=== VLM Export (Partial Support) ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastVisionModel\n\n# After training VLM...\n# Note: GGUF VLM support is limited to LLaVA-style architectures\nmodel.save_pretrained_gguf(\n    \"./vlm_gguf\",\n    processor,\n    quantization_method = \"q4_k_m\",\n)\n# Creates: model-Q4_K_M.gguf + mmproj file for vision encoder\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_GGUF_Export]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n=== Uses Heuristics ===\n* [[uses_heuristic::Heuristic:Unslothai_Unsloth_GGUF_Quantization_Selection_Tip]]\n\n",
      "domains": [
        "Model Export",
        "GGUF",
        "Quantization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_GGUF_Export"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        },
        {
          "edge_type": "uses_heuristic",
          "target_type": "Heuristic",
          "target_id": "Unslothai_Unsloth_GGUF_Quantization_Selection_Tip"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_train_on_responses_only",
      "page_title": "Unslothai Unsloth train on responses only",
      "page_type": "Implementation",
      "overview": "Concrete tool for masking loss computation to only assistant responses, enabling effective SFT warm-up before GRPO reinforcement learning training.",
      "content": "# Implementation: train_on_responses_only\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|TRL Documentation|https://huggingface.co/docs/trl]]\n|-\n! Domains\n| [[domain::NLP]], [[domain::Training]], [[domain::Supervised_Learning]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for masking loss computation to only assistant responses, enabling effective SFT warm-up before GRPO reinforcement learning training.\n\n=== Description ===\n\n`train_on_responses_only` modifies an SFTTrainer to compute loss only on assistant response tokens, masking user and system messages. This is critical for:\n\n1. **Pre-RL SFT warm-up**: Initialize policy with good generation patterns before RL\n2. **Efficient learning**: Don't waste capacity learning to predict user messages\n3. **Chat format compliance**: Learn response structure without instruction repetition\n\n=== Usage ===\n\nCall after creating SFTTrainer but before training. Pass the delimiters that mark user vs assistant turns in your chat template.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/chat_templates.py (imported from unsloth_zoo.dataset_utils)\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef train_on_responses_only(\n    trainer: SFTTrainer,\n    instruction_part: str,\n    response_part: str,\n) -> SFTTrainer:\n    \"\"\"\n    Modify trainer to compute loss only on response tokens.\n\n    Args:\n        trainer: SFTTrainer instance to modify\n        instruction_part: String marking user/instruction turn start\n        response_part: String marking assistant/response turn start\n\n    Returns:\n        Modified trainer (also modifies in place)\n    \"\"\"\n</syntaxhighlight>\n\n=== Import ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth.chat_templates import train_on_responses_only\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Required !! Description\n|-\n| trainer || SFTTrainer || Yes || Configured SFTTrainer instance\n|-\n| instruction_part || str || Yes || Delimiter for instruction/user turn\n|-\n| response_part || str || Yes || Delimiter for response/assistant turn\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Name !! Type !! Description\n|-\n| trainer || SFTTrainer || Modified trainer with response-only loss masking\n|}\n\n== Usage Examples ==\n\n=== ChatML Template ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template, train_on_responses_only\nfrom trl import SFTTrainer, SFTConfig\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Apply ChatML template\ntokenizer = get_chat_template(tokenizer, chat_template=\"chatml\")\n\n# Create trainer\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    args = SFTConfig(...),\n)\n\n# Mask loss to responses only (ChatML delimiters)\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|im_start|>user\\n\",\n    response_part = \"<|im_start|>assistant\\n\",\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== Llama-3 Template ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template, train_on_responses_only\nfrom trl import SFTTrainer, SFTConfig\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\n\n# Apply Llama-3 template\ntokenizer = get_chat_template(tokenizer, chat_template=\"llama-3\")\n\ntrainer = SFTTrainer(...)\n\n# Llama-3 specific delimiters\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)\n\ntrainer.train()\n</syntaxhighlight>\n\n=== Pre-GRPO Warm-up ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template, train_on_responses_only\nfrom trl import SFTTrainer, SFTConfig\n\n# 1. Load with vLLM for later GRPO\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n    max_seq_length = 1024,\n    load_in_4bit = True,\n    fast_inference = True,\n    max_lora_rank = 64,\n)\n\n# 2. Apply LoRA\nmodel = FastLanguageModel.get_peft_model(model, r=64)\n\n# 3. SFT warm-up with response-only loss\ntokenizer = get_chat_template(tokenizer, chat_template=\"chatml\")\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = sft_dataset,\n    args = SFTConfig(max_steps=100, per_device_train_batch_size=2),\n)\n\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|im_start|>user\\n\",\n    response_part = \"<|im_start|>assistant\\n\",\n)\n\ntrainer.train()\n\n# 4. Now ready for GRPO training\n# GRPOTrainer(model=model, ...)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_SFT_Pretraining]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n",
      "domains": [
        "NLP",
        "Training",
        "Supervised Learning"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "TRL Documentation",
          "url": "https://huggingface.co/docs/trl"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_SFT_Pretraining"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Implementation/Unslothai_Unsloth_unsloth_save_model_merged",
      "page_title": "Unslothai Unsloth unsloth save model merged",
      "page_type": "Implementation",
      "overview": "Concrete tool for merging LoRA adapters into base model weights in preparation for GGUF export.",
      "content": "# Implementation: unsloth_save_model_merged\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n|-\n! Domains\n| [[domain::Model_Serialization]], [[domain::GGUF]], [[domain::LoRA]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 16:00 GMT]]\n|}\n\n== Overview ==\n\nConcrete tool for merging LoRA adapters into base model weights in preparation for GGUF export.\n\n=== Description ===\n\n`save_pretrained_merged` merges LoRA adapters into the base model and saves to float16/bfloat16 format. For GGUF export, this creates the intermediate representation that llama.cpp converts to GGUF.\n\n=== Usage ===\n\nCall before GGUF export to create merged 16-bit weights from a 4-bit LoRA model.\n\n== Code Reference ==\n\n=== Source Location ===\n* '''Repository:''' [https://github.com/unslothai/unsloth unsloth]\n* '''File:''' unsloth/save.py\n* '''Lines:''' L235-860\n\n=== Signature ===\n<syntaxhighlight lang=\"python\">\ndef save_pretrained_merged(\n    self,\n    save_directory: str,\n    tokenizer = None,\n    save_method: str = \"merged_16bit\",  # For GGUF prep\n    push_to_hub: bool = False,\n    token: Optional[str] = None,\n    is_main_process: bool = True,\n    state_dict: Optional[dict] = None,\n    save_function = safetensors.torch.save_file,\n    max_shard_size: Union[int, str] = \"5GB\",\n    safe_serialization: bool = True,\n    variant: Optional[str] = None,\n    save_peft_format: bool = True,\n    tags: Optional[List[str]] = None,\n    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\n    maximum_memory_usage: float = 0.85,\n) -> Tuple[str, Optional[str]]:\n    \"\"\"\n    Merge LoRA adapters and save model.\n\n    For GGUF export, use save_method=\"merged_16bit\" to create\n    float16 weights that llama.cpp can convert.\n    \"\"\"\n</syntaxhighlight>\n\n== I/O Contract ==\n\n=== Inputs ===\n{| class=\"wikitable\"\n|-\n! Parameter !! Type !! Description\n|-\n| save_directory || str || Output directory for merged model\n|-\n| tokenizer || PreTrainedTokenizer || Tokenizer to save alongside model\n|-\n| save_method || str || \"merged_16bit\" for GGUF prep\n|-\n| maximum_memory_usage || float || RAM usage limit (0.85 = 85%)\n|}\n\n=== Outputs ===\n{| class=\"wikitable\"\n|-\n! Output !! Type !! Description\n|-\n| save_directory || str || Path where model was saved\n|-\n| commit_url || Optional[str] || Hub URL if push_to_hub=True\n|}\n\n== Usage Examples ==\n\n=== Merge for GGUF Export ===\n<syntaxhighlight lang=\"python\">\nfrom unsloth import FastLanguageModel\n\n# After training...\n# Merge LoRA and save as 16-bit (required for GGUF)\nmodel.save_pretrained_merged(\n    \"./merged_model\",\n    tokenizer,\n    save_method = \"merged_16bit\",\n    maximum_memory_usage = 0.9,  # Use up to 90% RAM\n)\n# Now ready for: model.save_pretrained_gguf(...)\n</syntaxhighlight>\n\n== Related Pages ==\n\n=== Implements Principle ===\n* [[implements::Principle:Unslothai_Unsloth_Model_Preparation]]\n\n=== Requires Environment ===\n* [[requires_env::Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n",
      "domains": [
        "Model Serialization",
        "GGUF",
        "LoRA"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        }
      ],
      "last_updated": "2026-01-09 16:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "implements",
          "target_type": "Principle",
          "target_id": "Unslothai_Unsloth_Model_Preparation"
        },
        {
          "edge_type": "requires_env",
          "target_type": "Environment",
          "target_id": "Unslothai_Unsloth_CUDA_GPU_Environment"
        }
      ]
    },
    {
      "id": "Environment/Unslothai_Unsloth_CUDA_GPU_Environment",
      "page_title": "Unslothai Unsloth CUDA GPU Environment",
      "page_type": "Environment",
      "overview": "GPU-accelerated environment supporting NVIDIA CUDA, AMD ROCm (HIP), and Intel XPU for training and fine-tuning LLMs with Unsloth.",
      "content": "# Environment: CUDA_GPU_Environment\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|device_type.py|https://github.com/unslothai/unsloth/blob/main/unsloth/device_type.py]]\n* [[source::Doc|loader.py|https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::LLMs]], [[domain::Infrastructure]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nGPU-accelerated environment supporting NVIDIA CUDA, AMD ROCm (HIP), and Intel XPU for training and fine-tuning LLMs with Unsloth.\n\n=== Description ===\nThis environment provides the core GPU acceleration context for all Unsloth training workflows. It supports three GPU backends:\n\n1. **NVIDIA CUDA** - Primary support with full feature set\n2. **AMD ROCm (HIP)** - Support via bitsandbytes >= 0.48.3, with some limitations on pre-quantized models\n3. **Intel XPU** - Requires PyTorch >= 2.6.0\n\nThe environment auto-detects available GPU accelerators and configures the appropriate compute backend. BFloat16 support is automatically detected based on GPU compute capability.\n\n=== Usage ===\nUse this environment for **QLoRA fine-tuning**, **Vision fine-tuning**, and **GGUF export** workflows. This is the standard prerequisite for running `FastLanguageModel.from_pretrained()`, `FastVisionModel.from_pretrained()`, and all model saving operations.\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n| OS || Linux (Ubuntu 20.04+) || Windows via WSL2 only\n|-\n| Hardware || NVIDIA/AMD/Intel GPU || Minimum 16GB VRAM recommended for 7B models\n|-\n| CUDA || >= 11.8 || For NVIDIA GPUs\n|-\n| ROCm || >= 6.0 || For AMD GPUs\n|-\n| Disk || 50GB SSD || For model caching and checkpoints\n|}\n\n== Dependencies ==\n=== System Packages ===\n* `cuda-toolkit` >= 11.8 (NVIDIA)\n* `rocm` >= 6.0 (AMD)\n* `intel-extension-for-pytorch` (Intel XPU)\n\n=== Python Packages ===\n* `torch` >= 2.4.0\n* `transformers` >= 4.45.0 (4.37+ for 4-bit support, 4.43.2+ for Llama 3.1)\n* `bitsandbytes` >= 0.43.3 (0.48.3+ for AMD ROCm)\n* `peft` >= 0.10.0\n* `trl` >= 0.11.0\n* `triton` >= 3.0.0\n* `accelerate`\n* `datasets`\n* `huggingface_hub`\n\n== Credentials ==\nThe following environment variables may be required:\n* `HF_TOKEN`: HuggingFace API token for private model access and Hub uploads\n* `WANDB_API_KEY`: Weights & Biases API key for training logging (optional)\n\n== Quick Install ==\n<syntaxhighlight lang=\"bash\">\n# Install all required packages\npip install torch>=2.4.0 transformers>=4.45.0 bitsandbytes>=0.43.3 peft>=0.10.0 trl>=0.11.0 accelerate triton>=3.0.0 datasets\n\n# For Unsloth\npip install unsloth\n</syntaxhighlight>\n\n== Code Evidence ==\n\nDevice detection from `device_type.py:37-59`:\n<syntaxhighlight lang=\"python\">\n@functools.cache\ndef get_device_type():\n    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n        if is_hip():\n            return \"hip\"\n        return \"cuda\"\n    elif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n        return \"xpu\"\n    # Check torch.accelerator\n    if hasattr(torch, \"accelerator\"):\n        if not torch.accelerator.is_available():\n            raise NotImplementedError(\n                \"Unsloth cannot find any torch accelerator? You need a GPU.\"\n            )\n    raise NotImplementedError(\n        \"Unsloth currently only works on NVIDIA, AMD and Intel GPUs.\"\n    )\n</syntaxhighlight>\n\nIntel XPU version check from `kernels/utils.py:41-44`:\n<syntaxhighlight lang=\"python\">\nif DEVICE_TYPE == \"xpu\" and Version(torch.__version__) < Version(\"2.6.0\"):\n    raise RuntimeError(\n        \"Intel xpu currently supports unsloth with torch.version >= 2.6.0\"\n    )\n</syntaxhighlight>\n\nTransformers version checks from `loader.py:68-79`:\n<syntaxhighlight lang=\"python\">\nSUPPORTS_FOURBIT = transformers_version >= Version(\"4.37\")\nSUPPORTS_GEMMA = transformers_version >= Version(\"4.38\")\nSUPPORTS_GEMMA2 = transformers_version >= Version(\"4.42\")\nSUPPORTS_LLAMA31 = transformers_version >= Version(\"4.43.2\")\nSUPPORTS_LLAMA32 = transformers_version > Version(\"4.45.0\")\nSUPPORTS_GRANITE = transformers_version >= Version(\"4.46.0\")\nSUPPORTS_QWEN3 = transformers_version >= Version(\"4.50.3\")\n</syntaxhighlight>\n\n== Common Errors ==\n\n{| class=\"wikitable\"\n|-\n! Error Message !! Cause !! Solution\n|-\n|| `Unsloth cannot find any torch accelerator? You need a GPU.` || No GPU detected || Ensure CUDA/ROCm drivers are installed and GPU is available\n|-\n|| `Intel xpu currently supports unsloth with torch.version >= 2.6.0` || Old PyTorch for Intel || Upgrade: `pip install torch>=2.6.0`\n|-\n|| `transformers version does not support Llama 3.1` || transformers < 4.43.2 || Upgrade: `pip install transformers>=4.43.2`\n|-\n|| `AMD currently is not stable with 4bit bitsandbytes` || BnB < 0.48.3 on AMD || Upgrade: `pip install bitsandbytes>=0.48.3`\n|-\n|| `Device does not support bfloat16` || Older GPU (pre-Ampere) || Unsloth auto-switches to float16\n|}\n\n== Compatibility Notes ==\n\n* **AMD GPUs (ROCm):** Requires bitsandbytes >= 0.48.3. Pre-quantized models with blocksize 64 may not work on Instinct GPUs (MI series) which use blocksize 128. Radeon (Navi) GPUs are supported.\n* **Intel XPU:** Requires PyTorch >= 2.6.0. Uses `torch.xpu` device type.\n* **Windows:** Not officially supported; use WSL2.\n* **BFloat16:** Auto-detected based on GPU capability. Falls back to float16 on older GPUs.\n* **Multi-GPU:** Supported via device_map=\"sequential\". DEVICE_COUNT is auto-detected.\n\n== Related Pages ==\n* [[required_by::Implementation:Unslothai_Unsloth_FastLanguageModel_from_pretrained]]\n* [[required_by::Implementation:Unslothai_Unsloth_get_peft_model]]\n* [[required_by::Implementation:Unslothai_Unsloth_get_chat_template]]\n* [[required_by::Implementation:Unslothai_Unsloth_UnslothTrainingArguments]]\n* [[required_by::Implementation:Unslothai_Unsloth_SFTTrainer_train]]\n* [[required_by::Implementation:Unslothai_Unsloth_save_pretrained]]\n* [[required_by::Implementation:Unslothai_Unsloth_FastVisionModel_from_pretrained]]\n* [[required_by::Implementation:Unslothai_Unsloth_FastBaseModel_get_peft_model]]\n* [[required_by::Implementation:Unslothai_Unsloth_multimodal_dataset_pattern]]\n* [[required_by::Implementation:Unslothai_Unsloth_FastBaseModel_for_training]]\n* [[required_by::Implementation:Unslothai_Unsloth_SFTTrainer_vision]]\n* [[required_by::Implementation:Unslothai_Unsloth_FastBaseModel_for_inference]]\n* [[required_by::Implementation:Unslothai_Unsloth_save_pretrained_vision]]\n* [[required_by::Implementation:Unslothai_Unsloth_unsloth_save_model_merged]]\n* [[required_by::Implementation:Unslothai_Unsloth_ALLOWED_QUANTS]]\n* [[required_by::Implementation:Unslothai_Unsloth_save_to_gguf]]\n* [[required_by::Implementation:Unslothai_Unsloth_OLLAMA_TEMPLATES]]\n* [[required_by::Implementation:Unslothai_Unsloth_push_to_hub_gguf]]\n",
      "domains": [
        "Deep Learning",
        "LLMs",
        "Infrastructure"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "device_type.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/device_type.py"
        },
        {
          "type": "Doc",
          "title": "loader.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastLanguageModel_from_pretrained"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_peft_model"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_chat_template"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_UnslothTrainingArguments"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_SFTTrainer_train"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_save_pretrained"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastVisionModel_from_pretrained"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastBaseModel_get_peft_model"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_multimodal_dataset_pattern"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastBaseModel_for_training"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_SFTTrainer_vision"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastBaseModel_for_inference"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_save_pretrained_vision"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_unsloth_save_model_merged"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_ALLOWED_QUANTS"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_save_to_gguf"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_OLLAMA_TEMPLATES"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_push_to_hub_gguf"
        }
      ]
    },
    {
      "id": "Environment/Unslothai_Unsloth_CUDA_GPU_vLLM_Environment",
      "page_title": "Unslothai Unsloth CUDA GPU vLLM Environment",
      "page_type": "Environment",
      "overview": "Extended GPU environment with vLLM integration for fast batch inference, required for GRPO training and RL workflows.",
      "content": "# Environment: CUDA_GPU_vLLM_Environment\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|loader.py|https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py]]\n* [[source::Doc|rl.py|https://github.com/unslothai/unsloth/blob/main/unsloth/models/rl.py]]\n|-\n! Domains\n| [[domain::Deep_Learning]], [[domain::LLMs]], [[domain::Reinforcement_Learning]], [[domain::Infrastructure]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nExtended GPU environment with vLLM integration for fast batch inference, required for GRPO training and RL workflows.\n\n=== Description ===\nThis environment extends the base CUDA_GPU_Environment with vLLM support for fast batch inference. vLLM enables efficient parallel generation which is critical for RL workflows like GRPO where multiple completions must be generated per prompt.\n\nThe environment requires additional GPU memory for vLLM's KV cache alongside model training. The `gpu_memory_utilization` parameter controls how much GPU memory vLLM reserves.\n\n=== Usage ===\nUse this environment for **GRPO Training** and any workflow requiring `fast_inference=True`. This is mandatory for `UnslothGRPOTrainer` and reinforcement learning fine-tuning.\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n| OS || Linux (Ubuntu 20.04+) || Windows not supported for vLLM\n|-\n| Hardware || NVIDIA GPU || AMD ROCm has limited vLLM support\n|-\n| CUDA || >= 11.8 || Required for vLLM\n|-\n| VRAM || >= 24GB || Recommended for 7B models with vLLM overhead\n|-\n| Disk || 100GB SSD || For model caching and vLLM artifacts\n|}\n\n== Dependencies ==\n=== System Packages ===\n* `cuda-toolkit` >= 11.8\n* All packages from [[Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n\n=== Python Packages ===\n* All packages from [[Environment:Unslothai_Unsloth_CUDA_GPU_Environment]]\n* `vllm` >= 0.6.0\n* `ray` (installed with vLLM)\n\n== Credentials ==\nThe following environment variables may be required:\n* `HF_TOKEN`: HuggingFace API token for private model access\n* `WANDB_API_KEY`: Weights & Biases API key for training logging (optional)\n\n== Quick Install ==\n<syntaxhighlight lang=\"bash\">\n# Install base dependencies\npip install torch>=2.4.0 transformers>=4.45.0 bitsandbytes>=0.43.3 peft>=0.10.0 trl>=0.11.0 accelerate triton>=3.0.0\n\n# Install vLLM for fast inference\npip install vllm>=0.6.0\n\n# For Unsloth\npip install unsloth\n</syntaxhighlight>\n\n== Code Evidence ==\n\nvLLM availability check from `loader.py:234-239`:\n<syntaxhighlight lang=\"python\">\nif fast_inference:\n    if importlib.util.find_spec(\"vllm\") is None:\n        raise ImportError(\n            \"Unsloth: Please install vLLM before enabling `fast_inference`!\\n\"\n            \"You can do this in a terminal via `pip install vllm`\"\n        )\n</syntaxhighlight>\n\nDGX Spark compatibility check from `loader.py:240-249`:\n<syntaxhighlight lang=\"python\">\nif DEVICE_TYPE_TORCH == \"cuda\":\n    for i in range(DEVICE_COUNT):\n        # [TODO] DGX Spark vLLM breaks\n        if \"NVIDIA GB10\" in str(torch.cuda.get_device_name(i)).upper():\n            print(\n                \"Unsloth: DGX Spark detected - `fast_inference=True` is currently broken as of January 2026.\\n\"\n                \"Defaulting to native Unsloth inference.\"\n            )\n            fast_inference = False\n            break\n</syntaxhighlight>\n\nFP8 requires vLLM from `loader.py:252-256`:\n<syntaxhighlight lang=\"python\">\nif load_in_fp8 != False:\n    if not fast_inference:\n        raise NotImplementedError(\n            \"Unsloth: set `fast_inference = True` when doing `load_in_fp8`.\"\n        )\n</syntaxhighlight>\n\n== Common Errors ==\n\n{| class=\"wikitable\"\n|-\n! Error Message !! Cause !! Solution\n|-\n|| `Please install vLLM before enabling fast_inference` || vLLM not installed || `pip install vllm`\n|-\n|| `set fast_inference = True when doing load_in_fp8` || FP8 requires vLLM || Enable `fast_inference=True` when using `load_in_fp8`\n|-\n|| `DGX Spark detected - fast_inference is currently broken` || DGX Spark GPU limitation || Use native Unsloth inference (automatic fallback)\n|-\n|| `CUDA out of memory` || Insufficient VRAM for model + vLLM || Reduce `gpu_memory_utilization` to 0.3-0.5\n|-\n|| `vLLM initialization failed` || vLLM version incompatibility || Upgrade: `pip install --upgrade vllm`\n|}\n\n== Compatibility Notes ==\n\n* **DGX Spark (GB10):** vLLM is currently broken on DGX Spark as of January 2026. Unsloth auto-falls back to native inference.\n* **AMD ROCm:** vLLM has limited support on AMD GPUs. Check vLLM documentation for compatibility.\n* **Memory Usage:** vLLM reserves GPU memory for KV cache. Use `gpu_memory_utilization` (default 0.5) to control allocation.\n* **LoRA Rank:** When using vLLM with LoRA, ensure `r` <= `max_lora_rank` (default 64).\n* **Blackwell GPUs (SM100):** PDL (Programmatic Dependent Launch) fix applied automatically via `TRITON_DISABLE_PDL=1`.\n\n== Related Pages ==\n* [[required_by::Implementation:Unslothai_Unsloth_FastLanguageModel_from_pretrained_vllm]]\n* [[required_by::Implementation:Unslothai_Unsloth_get_peft_model_rl]]\n* [[required_by::Implementation:Unslothai_Unsloth_dataset_mapping_pattern]]\n* [[required_by::Implementation:Unslothai_Unsloth_reward_function_pattern]]\n* [[required_by::Implementation:Unslothai_Unsloth_train_on_responses_only]]\n* [[required_by::Implementation:Unslothai_Unsloth_UnslothGRPOConfig]]\n* [[required_by::Implementation:Unslothai_Unsloth_UnslothGRPOTrainer]]\n",
      "domains": [
        "Deep Learning",
        "LLMs",
        "Reinforcement Learning",
        "Infrastructure"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "loader.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py"
        },
        {
          "type": "Doc",
          "title": "rl.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/models/rl.py"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastLanguageModel_from_pretrained_vllm"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_peft_model_rl"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_dataset_mapping_pattern"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_reward_function_pattern"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_train_on_responses_only"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_UnslothGRPOConfig"
        },
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_UnslothGRPOTrainer"
        }
      ]
    },
    {
      "id": "Environment/Unslothai_Unsloth_llama_cpp_Environment",
      "page_title": "Unslothai Unsloth llama cpp Environment",
      "page_type": "Environment",
      "overview": "Environment for GGUF export and validation using llama.cpp tools for local LLM deployment.",
      "content": "# Environment: llama_cpp_Environment\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|save.py|https://github.com/unslothai/unsloth/blob/main/unsloth/save.py]]\n* [[source::Repo|llama.cpp|https://github.com/ggerganov/llama.cpp]]\n|-\n! Domains\n| [[domain::Deployment]], [[domain::Quantization]], [[domain::Inference]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nEnvironment for GGUF export and validation using llama.cpp tools for local LLM deployment.\n\n=== Description ===\nThis environment provides the tools needed to convert trained models to GGUF format and validate them using llama.cpp. GGUF is the standard format for running LLMs locally with tools like Ollama, LM Studio, and llama.cpp.\n\nThe environment includes:\n- `llama.cpp` conversion scripts for GGUF export\n- Quantization tools for various precision levels (q4_k_m, q5_k_m, q8_0, etc.)\n- `llama-cli` for validation and inference testing\n\n=== Usage ===\nUse this environment for the **GGUF_Export** workflow, specifically for the verification step using `llama-cli`. This environment can run on CPU-only systems for inference testing, though GPU acceleration is recommended for faster quantization.\n\n== System Requirements ==\n{| class=\"wikitable\"\n! Category !! Requirement !! Notes\n|-\n| OS || Linux, macOS, Windows || Cross-platform support\n|-\n| Hardware || CPU (GPU optional) || GPU accelerates quantization\n|-\n| RAM || >= 16GB || For loading models during conversion\n|-\n| Disk || 50GB SSD || For model files and GGUF outputs\n|}\n\n== Dependencies ==\n=== System Packages ===\n* `cmake` >= 3.14\n* `gcc` / `clang` (C++ compiler)\n* `make`\n* CUDA toolkit (optional, for GPU-accelerated quantization)\n\n=== Python Packages ===\n* `sentencepiece` (for tokenizer conversion)\n* `psutil` (for memory monitoring)\n* `numpy`\n\n=== External Tools ===\n* `llama.cpp` (build from source or use pre-built binaries)\n* `llama-cli` (included with llama.cpp)\n\n== Credentials ==\n* `HF_TOKEN`: HuggingFace API token for `push_to_hub_gguf` operations\n\n== Quick Install ==\n<syntaxhighlight lang=\"bash\">\n# Clone and build llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\nmake -j\n\n# Or with CUDA support\nmake -j LLAMA_CUDA=1\n\n# Install Python dependencies for Unsloth GGUF export\npip install sentencepiece psutil numpy\n</syntaxhighlight>\n\n== Code Evidence ==\n\nGGUF quantization methods from `save.py:104-131`:\n<syntaxhighlight lang=\"python\">\nALLOWED_QUANTS = {\n    \"not_quantized\": \"No quantization (float16)\",\n    \"fast_quantized\": \"Q8_0 quantization (8-bit)\",\n    \"quantized\": \"Q4_K_M quantization (4-bit)\",\n    \"q4_k_m\": \"Q4_K_M quantization\",\n    \"q5_k_m\": \"Q5_K_M quantization\",\n    \"q8_0\": \"Q8_0 quantization\",\n    \"q2_k\": \"Q2_K quantization (2-bit)\",\n    \"q3_k_m\": \"Q3_K_M quantization\",\n    \"q6_k\": \"Q6_K quantization\",\n    \"f16\": \"Float16 (no quantization)\",\n    \"bf16\": \"BFloat16 (no quantization)\",\n}\n</syntaxhighlight>\n\n== Common Errors ==\n\n{| class=\"wikitable\"\n|-\n! Error Message !! Cause !! Solution\n|-\n|| `llama.cpp not found` || llama.cpp not in PATH || Build llama.cpp and add to PATH\n|-\n|| `Unsupported quantization method` || Invalid quant string || Use one of: q4_k_m, q5_k_m, q8_0, q2_k, q3_k_m, q6_k, f16, bf16\n|-\n|| `sentencepiece not installed` || Missing Python package || `pip install sentencepiece`\n|-\n|| `Memory error during quantization` || Insufficient RAM || Close other applications or use machine with more RAM\n|-\n|| `GGUF validation failed` || Corrupted conversion || Re-run conversion with different quantization\n|}\n\n== Compatibility Notes ==\n\n* **Cross-Platform:** llama.cpp works on Linux, macOS, and Windows.\n* **CPU Inference:** GGUF files can run on CPU-only systems, making them ideal for deployment on edge devices.\n* **GPU Acceleration:** For faster quantization, build llama.cpp with CUDA/Metal/OpenCL support.\n* **Ollama Integration:** Unsloth automatically generates Ollama Modelfiles for easy deployment.\n* **Model Size:** Quantization significantly reduces model size:\n  - q4_k_m: ~4 bits per weight (~25% of original size)\n  - q8_0: ~8 bits per weight (~50% of original size)\n  - f16: ~16 bits per weight (full precision)\n\n== Related Pages ==\n* [[required_by::Implementation:Unslothai_Unsloth_llama_cli_validation]]\n",
      "domains": [
        "Deployment",
        "Quantization",
        "Inference"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "save.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/save.py"
        },
        {
          "type": "Repo",
          "title": "llama.cpp",
          "url": "https://github.com/ggerganov/llama.cpp"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "required_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_llama_cli_validation"
        }
      ]
    },
    {
      "id": "Heuristic/Unslothai_Unsloth_BFloat16_vs_Float16_Tip",
      "page_title": "Unslothai Unsloth BFloat16 vs Float16 Tip",
      "page_type": "Heuristic",
      "overview": "Use BFloat16 on Ampere+ GPUs (3000 series+) for more stable training; Float16 on older GPUs.",
      "content": "# Heuristic: BFloat16_vs_Float16_Tip\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|loader.py|https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py]]\n|-\n! Domains\n| [[domain::Training]], [[domain::Hardware]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nUse BFloat16 on Ampere+ GPUs (3000 series+) for more stable training; Float16 on older GPUs.\n\n=== Description ===\nBFloat16 (Brain Floating Point 16) and Float16 (FP16) are both 16-bit floating point formats, but with different precision characteristics:\n\n- **BFloat16:** Same exponent range as FP32 (8 bits), less mantissa precision (7 bits)\n- **Float16:** More mantissa precision (10 bits), smaller exponent range (5 bits)\n\nBFloat16's larger exponent range prevents overflow/underflow issues common in FP16 training, making it more stable for deep learning.\n\n=== Usage ===\nUse this heuristic when:\n- **Choosing dtype:** Setting `dtype` parameter in model loading\n- **Debugging NaN loss:** FP16 can overflow; try BFloat16\n- **New GPU purchase:** Consider Ampere+ for native BFloat16 support\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `dtype` parameter in `FastLanguageModel.from_pretrained()` (or let Unsloth auto-detect)\n* **Value:**\n  - **Ampere+ GPUs (RTX 3000/4000/5000, A100, H100):** Use `dtype=torch.bfloat16` or `dtype=None` (auto-detect)\n  - **Pre-Ampere GPUs (RTX 2000, V100, T4):** Use `dtype=torch.float16`\n* **Trade-off:** BFloat16 more stable but slightly less precise; Float16 more precise but overflow risk\n* **Auto-detection:** Unsloth automatically selects based on GPU capability\n\n== Reasoning ==\nDeep learning training involves very large and very small numbers:\n\n1. **Gradient accumulation:** Gradients can become very large\n2. **Normalization layers:** Values can span many orders of magnitude\n3. **Loss scaling:** FP16 requires loss scaling to prevent underflow\n\nBFloat16 solves these issues by maintaining FP32's dynamic range while using only 16 bits. This eliminates:\n- Need for loss scaling\n- Overflow during gradient accumulation\n- Underflow in small gradient updates\n\n**GPU Support:**\n- **NVIDIA:** Ampere (SM80+), i.e., RTX 3000 series, A100, H100\n- **AMD:** MI100+ with ROCm 5.0+\n- **Intel:** XPU with PyTorch 2.6+\n\n== Code Evidence ==\n\nBFloat16 support detection from `_utils.py:154-165`:\n<syntaxhighlight lang=\"python\">\ndef is_bfloat16_supported():\n    if DEVICE_TYPE == \"cuda\":\n        major, minor = torch.cuda.get_device_capability()\n        return major >= 8  # Ampere and later\n    elif DEVICE_TYPE == \"xpu\":\n        return True  # Intel XPU supports bfloat16\n    elif DEVICE_TYPE == \"hip\":\n        return True  # AMD MI series supports bfloat16\n    return False\n</syntaxhighlight>\n\nAuto dtype selection in `loader.py`:\n<syntaxhighlight lang=\"python\">\nif dtype is None:\n    dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n</syntaxhighlight>\n\n== Related Pages ==\n* [[used_by::Implementation:Unslothai_Unsloth_FastLanguageModel_from_pretrained]]\n* [[used_by::Implementation:Unslothai_Unsloth_FastVisionModel_from_pretrained]]\n",
      "domains": [
        "Training",
        "Hardware",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "loader.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastLanguageModel_from_pretrained"
        },
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastVisionModel_from_pretrained"
        }
      ]
    },
    {
      "id": "Heuristic/Unslothai_Unsloth_Embedding_Learning_Rate_Tip",
      "page_title": "Unslothai Unsloth Embedding Learning Rate Tip",
      "page_type": "Heuristic",
      "overview": "Use a lower learning rate for embedding layers (5e-5) compared to LoRA parameters (2e-4) for stable training.",
      "content": "# Heuristic: Embedding_Learning_Rate_Tip\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|trainer.py|https://github.com/unslothai/unsloth/blob/main/unsloth/trainer.py]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Training]], [[domain::Embeddings]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nUse a lower learning rate for embedding layers (5e-5) compared to LoRA parameters (2e-4) for stable training.\n\n=== Description ===\nWhen training with embeddings unfrozen (via `use_exact_model_tokens=True` in `get_peft_model`), embedding layers require a lower learning rate than LoRA adapter parameters. This is because embeddings are pre-trained on large corpora and drastic updates can destabilize the model.\n\nUnsloth provides `embedding_learning_rate` in `UnslothTrainingArguments` to set a separate learning rate for embedding layers.\n\n=== Usage ===\nUse this heuristic when:\n- **Training with unfrozen embeddings:** Using `use_exact_model_tokens=True`\n- **Adding new tokens:** Extending vocabulary for domain-specific terms\n- **Observing training instability:** Loss spikes or NaN values\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `embedding_learning_rate` in `UnslothTrainingArguments`\n* **Value:**\n  - Embedding LR: `5e-5` (default recommendation)\n  - Main LR: `2e-4` (standard for LoRA)\n  - Ratio: embedding_lr \u2248 0.25 \u00d7 main_lr\n* **Trade-off:** Lower embedding LR = more stable but slower embedding adaptation\n\n== Reasoning ==\nEmbedding layers have fundamentally different properties than LoRA adapters:\n\n1. **Pre-trained knowledge:** Embeddings encode word/token semantics learned from massive corpora\n2. **High impact:** Small embedding changes affect every layer of the model\n3. **Different gradients:** Embedding gradients tend to be larger in magnitude\n\nUsing the same learning rate for embeddings and LoRA often causes:\n- Loss instability\n- Catastrophic forgetting of language understanding\n- Embedding drift that degrades generation quality\n\nThe 5e-5 default is empirically validated to balance adaptation speed with stability.\n\n== Code Evidence ==\n\nEmbedding optimizer from `trainer.py:139-179`:\n<syntaxhighlight lang=\"python\">\ndef _create_unsloth_optimizer(\n    model,\n    optimizer_cls,\n    optimizer_kwargs,\n    embedding_lr = 5e-5,\n):\n    lr = optimizer_kwargs[\"lr\"]\n    weight_decay = optimizer_kwargs.get(\"weight_decay\", 0.0)\n\n    param_groups = {\n        \"non_embeddings\": {},\n        \"embeddings\": {},\n    }\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if name.endswith(\"modules_to_save.default.weight\"):\n            partial_name = name[: -len(\".modules_to_save.default.weight\")]\n            partial_name = partial_name[partial_name.rfind(\".\") + 1 :]\n            print(\n                f\"Unsloth: Setting lr = {embedding_lr:.2e} instead of {lr:.2e} for {partial_name}.\"\n            )\n            param_groups[\"embeddings\"][name] = param\n        else:\n            param_groups[\"non_embeddings\"][name] = param\n\n    optimizer_grouped_parameters = [\n        {\n            \"params\": list(param_groups[\"non_embeddings\"].values()),\n            \"weight_decay\": weight_decay,\n            \"lr\": lr,\n        },\n        {\n            \"params\": list(param_groups[\"embeddings\"].values()),\n            \"weight_decay\": weight_decay,\n            \"lr\": embedding_lr,\n        },\n    ]\n    optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n    return optimizer\n</syntaxhighlight>\n\n== Related Pages ==\n* [[used_by::Implementation:Unslothai_Unsloth_UnslothTrainingArguments]]\n",
      "domains": [
        "Optimization",
        "Training",
        "Embeddings"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "trainer.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/trainer.py"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_UnslothTrainingArguments"
        }
      ]
    },
    {
      "id": "Heuristic/Unslothai_Unsloth_GGUF_Quantization_Selection_Tip",
      "page_title": "Unslothai Unsloth GGUF Quantization Selection Tip",
      "page_type": "Heuristic",
      "overview": "Use q4_k_m for balanced quality/size, q8_0 for maximum quality, or q2_k for maximum compression.",
      "content": "# Heuristic: GGUF_Quantization_Selection_Tip\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|save.py|https://github.com/unslothai/unsloth/blob/main/unsloth/save.py]]\n|-\n! Domains\n| [[domain::Quantization]], [[domain::Deployment]], [[domain::Optimization]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nUse q4_k_m for balanced quality/size, q8_0 for maximum quality, or q2_k for maximum compression.\n\n=== Description ===\nGGUF quantization reduces model file size for local deployment. The quantization method determines the trade-off between model quality (perplexity) and file size/inference speed.\n\nUnsloth supports all standard llama.cpp quantization methods through the `quantization_method` parameter in `save_pretrained_gguf()`.\n\n=== Usage ===\nUse this heuristic when:\n- **Deploying to consumer hardware:** Need to fit model in limited VRAM/RAM\n- **Optimizing inference speed:** Lower precision = faster inference on CPU\n- **Balancing quality vs size:** Choosing between perplexity loss and model size\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `quantization_method` in `save_pretrained_gguf()` or `push_to_hub_gguf()`\n* **Values:**\n\n{| class=\"wikitable\"\n|-\n! Method !! Bits/Weight !! Use Case !! Quality Loss\n|-\n| `f16` || 16 || Maximum quality, baseline || None\n|-\n| `bf16` || 16 || Maximum quality (if supported) || None\n|-\n| `q8_0` || 8 || High quality, moderate size || Minimal\n|-\n| `q6_k` || 6.5 || Good quality, smaller size || Very low\n|-\n| `q5_k_m` || 5.5 || Balanced quality/size || Low\n|-\n| `q4_k_m` || 4.5 || **Recommended default** || Acceptable\n|-\n| `q3_k_m` || 3.4 || Small models, some quality loss || Moderate\n|-\n| `q2_k` || 2.6 || Maximum compression || Noticeable\n|}\n\n* **Trade-off:** Lower bits = smaller file = faster inference = lower quality\n* **Default:** `q4_k_m` (\"quantized\" preset)\n\n== Reasoning ==\nQuantization works by representing weights with fewer bits:\n\n1. **k-quant methods** (q4_k_m, q5_k_m, etc.) use mixed precision where important weights get more bits\n2. **m suffix** indicates medium quality variant (vs s=small, l=large)\n3. **8-bit (q8_0)** provides near-lossless quality at 50% size reduction\n4. **4-bit (q4_k_m)** provides 75% size reduction with acceptable quality loss\n\n**Recommendation Matrix:**\n\n| Scenario | Method |\n|----------|--------|\n| Quality-critical applications | q8_0 or f16 |\n| General purpose chat | q4_k_m |\n| Resource-constrained deployment | q3_k_m or q2_k |\n| Ollama/LM Studio default | q4_k_m |\n\n== Code Evidence ==\n\nQuantization options from `save.py:104-131`:\n<syntaxhighlight lang=\"python\">\nALLOWED_QUANTS = {\n    \"not_quantized\": \"No quantization (float16)\",\n    \"fast_quantized\": \"Q8_0 quantization (8-bit)\",\n    \"quantized\": \"Q4_K_M quantization (4-bit)\",\n    \"q4_k_m\": \"Q4_K_M quantization\",\n    \"q5_k_m\": \"Q5_K_M quantization\",\n    \"q8_0\": \"Q8_0 quantization\",\n    \"q2_k\": \"Q2_K quantization (2-bit)\",\n    \"q3_k_m\": \"Q3_K_M quantization\",\n    \"q6_k\": \"Q6_K quantization\",\n    \"f16\": \"Float16 (no quantization)\",\n    \"bf16\": \"BFloat16 (no quantization)\",\n}\n</syntaxhighlight>\n\n== Related Pages ==\n* [[used_by::Implementation:Unslothai_Unsloth_save_to_gguf]]\n* [[used_by::Implementation:Unslothai_Unsloth_push_to_hub_gguf]]\n* [[used_by::Implementation:Unslothai_Unsloth_ALLOWED_QUANTS]]\n",
      "domains": [
        "Quantization",
        "Deployment",
        "Optimization"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "save.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/save.py"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_save_to_gguf"
        },
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_push_to_hub_gguf"
        },
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_ALLOWED_QUANTS"
        }
      ]
    },
    {
      "id": "Heuristic/Unslothai_Unsloth_Gradient_Checkpointing_Tip",
      "page_title": "Unslothai Unsloth Gradient Checkpointing Tip",
      "page_type": "Heuristic",
      "overview": "Use `use_gradient_checkpointing=\"unsloth\"` for optimized memory-compute trade-off during training.",
      "content": "# Heuristic: Gradient_Checkpointing_Tip\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|loader.py|https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Memory_Management]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nUse `use_gradient_checkpointing=\"unsloth\"` for optimized memory-compute trade-off during training.\n\n=== Description ===\nUnsloth provides a custom gradient checkpointing implementation that is more efficient than the standard HuggingFace/PyTorch implementation. The \"unsloth\" mode intelligently checkpoints activations to reduce VRAM usage by ~50-60% while minimizing the compute overhead (typically only ~20% slower training).\n\nStandard gradient checkpointing stores fewer intermediate activations during the forward pass and recomputes them during the backward pass. Unsloth's implementation optimizes which layers are checkpointed based on the model architecture.\n\n=== Usage ===\nUse this heuristic when:\n- **VRAM constrained:** Getting CUDA OOM errors during training\n- **Training large models:** Fine-tuning 7B+ parameter models on consumer GPUs (RTX 3090/4090)\n- **Maximizing batch size:** Need to fit larger batches in memory\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `use_gradient_checkpointing=\"unsloth\"` in `FastLanguageModel.from_pretrained()` or `FastVisionModel.from_pretrained()`\n* **Value:** String `\"unsloth\"` (not boolean `True`)\n* **Trade-off:** Reduces VRAM usage by ~50-60% at the cost of ~20% slower training speed\n* **Compatibility:** Requires `use_cache=False` during training (automatically set)\n* **Default:** Enabled by default in Unsloth\n\n== Reasoning ==\nDeep Transformers have massive activation maps (Batch \u00d7 SeqLen \u00d7 Hidden). Storing all activations for backpropagation is the primary VRAM bottleneck during training. By selectively recomputing activations during the backward pass:\n\n1. Peak VRAM is significantly reduced\n2. Larger batch sizes become possible\n3. Training throughput may actually improve due to better memory efficiency\n\nUnsloth's implementation is optimized specifically for the LLM architectures it supports, resulting in better trade-offs than generic implementations.\n\n**Benchmarks:** On Llama-2-7B, VRAM typically drops from ~22GB to ~11GB with this flag enabled.\n\n== Code Evidence ==\n\nFrom `loader.py:562-563`:\n<syntaxhighlight lang=\"python\">\nif use_gradient_checkpointing == \"unsloth\":\n    patch_unsloth_smart_gradient_checkpointing(dtype = dtype)\n</syntaxhighlight>\n\nFrom `FastLanguageModel.from_pretrained` signature in `loader.py:136`:\n<syntaxhighlight lang=\"python\">\nuse_gradient_checkpointing = \"unsloth\",  # Default value\n</syntaxhighlight>\n\n== Related Pages ==\n* [[used_by::Implementation:Unslothai_Unsloth_FastLanguageModel_from_pretrained]]\n* [[used_by::Implementation:Unslothai_Unsloth_FastVisionModel_from_pretrained]]\n",
      "domains": [
        "Optimization",
        "Memory Management",
        "Training"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "loader.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastLanguageModel_from_pretrained"
        },
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_FastVisionModel_from_pretrained"
        }
      ]
    },
    {
      "id": "Heuristic/Unslothai_Unsloth_LoRA_Rank_Selection_Tip",
      "page_title": "Unslothai Unsloth LoRA Rank Selection Tip",
      "page_type": "Heuristic",
      "overview": "Use LoRA rank between 8-128, with r=16 as a solid default for most fine-tuning tasks.",
      "content": "# Heuristic: LoRA_Rank_Selection_Tip\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|loader.py|https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::LoRA]], [[domain::Training]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nUse LoRA rank between 8-128, with r=16 as a solid default for most fine-tuning tasks.\n\n=== Description ===\nThe LoRA (Low-Rank Adaptation) rank parameter `r` controls the dimensionality of the low-rank matrices used for adaptation. Higher ranks increase model capacity but also increase memory usage and may lead to overfitting. Lower ranks are more parameter-efficient but may not capture complex adaptations.\n\nThe `lora_alpha` parameter should typically be set equal to `r` or `2*r` for stable training.\n\n=== Usage ===\nUse this heuristic when:\n- **Choosing initial LoRA configuration:** Starting a new fine-tuning project\n- **Debugging underfitting:** Model not learning task well \u2192 increase rank\n- **Reducing memory usage:** Need to fit training in less VRAM \u2192 decrease rank\n- **vLLM inference:** Must use rank \u2264 64 for vLLM LoRA support\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `r` parameter in `get_peft_model()`\n* **Value:**\n  - **r=8**: Minimal, good for simple tasks or extremely limited VRAM\n  - **r=16**: Solid default for most fine-tuning tasks\n  - **r=32**: Better for complex tasks or chat models\n  - **r=64**: Maximum for vLLM compatibility, good for complex reasoning tasks\n  - **r=128+**: Only for non-vLLM training, may overfit\n* **Trade-off:** Higher rank = more parameters = more VRAM = risk of overfitting\n* **Alpha:** Set `lora_alpha = r` or `lora_alpha = 2*r`\n\n== Reasoning ==\nLoRA's effectiveness comes from the observation that the weight updates during fine-tuning have low intrinsic rank. The rank `r` determines how many dimensions of this low-rank subspace are captured:\n\n1. **Too low (r < 8):** May not capture enough task-specific information\n2. **Sweet spot (r = 16-64):** Balances capacity vs efficiency for most tasks\n3. **Too high (r > 128):** Diminishing returns, increased overfitting risk\n\nResearch shows that r=16 captures most of the adaptation capacity needed for typical instruction-tuning tasks.\n\n**vLLM Constraint:** When using `fast_inference=True` for GRPO or other RL workflows, vLLM's LoRA implementation has a maximum rank of 64. Exceeding this will cause runtime errors.\n\n== Code Evidence ==\n\nDefault LoRA targets from `loader.py:795-803`:\n<syntaxhighlight lang=\"python\">\ntarget_modules = [\n    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n    \"gate_proj\", \"up_proj\", \"down_proj\",\n]\n# ...\nr = 16,\nlora_alpha = 16,\n</syntaxhighlight>\n\nvLLM rank constraint from `rl.py:145-155`:\n<syntaxhighlight lang=\"python\">\nmax_lora_rank = lora_request.max_lora_rank if lora_request is not None else 64\n# LoRA rank must not exceed vLLM's max_lora_rank\nif r > max_lora_rank:\n    raise ValueError(\n        f\"LoRA rank {r} exceeds vLLM max_lora_rank {max_lora_rank}\"\n    )\n</syntaxhighlight>\n\n== Related Pages ==\n* [[used_by::Implementation:Unslothai_Unsloth_get_peft_model]]\n* [[used_by::Implementation:Unslothai_Unsloth_get_peft_model_rl]]\n",
      "domains": [
        "Optimization",
        "LoRA",
        "Training"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "loader.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_peft_model"
        },
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_get_peft_model_rl"
        }
      ]
    },
    {
      "id": "Heuristic/Unslothai_Unsloth_Sample_Packing_Tip",
      "page_title": "Unslothai Unsloth Sample Packing Tip",
      "page_type": "Heuristic",
      "overview": "Enable sample packing with `packing=True` for >2x faster training and reduced VRAM usage.",
      "content": "# Heuristic: Sample_Packing_Tip\n\n{| class=\"wikitable\" style=\"float:right; margin-left:1em; width:300px;\"\n|-\n! Knowledge Sources\n|\n* [[source::Repo|Unsloth|https://github.com/unslothai/unsloth]]\n* [[source::Doc|trainer.py|https://github.com/unslothai/unsloth/blob/main/unsloth/trainer.py]]\n|-\n! Domains\n| [[domain::Optimization]], [[domain::Training]], [[domain::Efficiency]]\n|-\n! Last Updated\n| [[last_updated::2026-01-09 12:00 GMT]]\n|}\n\n== Overview ==\nEnable sample packing with `packing=True` for >2x faster training and reduced VRAM usage.\n\n=== Description ===\nSample packing concatenates multiple shorter training samples into a single sequence up to `max_seq_length`. Instead of padding each sample individually (wasting compute on pad tokens), packing fills sequences efficiently by combining multiple samples.\n\nUnsloth auto-enables padding-free batching when packing is not explicitly set, providing similar efficiency gains with automatic setup.\n\n=== Usage ===\nUse this heuristic when:\n- **Training on short samples:** Average sample length << `max_seq_length`\n- **Optimizing training speed:** Want >2x faster training throughput\n- **Reducing VRAM usage:** Packing reduces wasted compute on padding\n\n**Do NOT use when:**\n- Training vision-language models (VLMs)\n- Using custom data collators\n- Model type is in blocklist (gemma2, gpt_oss)\n- Need to return logits (`UNSLOTH_RETURN_LOGITS=1`)\n\n== The Insight (Rule of Thumb) ==\n* **Action:** Set `packing=True` in `SFTConfig` or `SFTTrainer`\n* **Value:** Boolean `True`\n* **Trade-off:**\n  - Faster training (2x or more)\n  - May slightly change loss computation due to cross-contamination between packed samples\n  - Requires `dataset_text_field` or formatted dataset\n* **Alternative:** If not setting `packing`, Unsloth auto-enables `padding_free` mode\n\n== Reasoning ==\nTraining on short sequences with individual padding wastes significant compute:\n- A batch of 8 samples with lengths [128, 256, 192, 64, 320, 96, 160, 224] padded to 512 wastes ~68% of compute\n- Packing combines these into fewer, fuller sequences\n\nUnsloth's implementation:\n1. Auto-detects when packing is beneficial\n2. Falls back gracefully if packing fails\n3. Auto-enables padding-free mode as alternative\n\n**Model Blocklist:** Some models don't work correctly with packing:\n- `gemma2`: Uses slow_attention_softcapping with torch.compile issues\n- `gpt_oss`: Uses Flex Attention which doesn't handle padding_free correctly\n\n== Code Evidence ==\n\nPacking blocklist from `trainer.py:57-60`:\n<syntaxhighlight lang=\"python\">\nPADDING_FREE_BLOCKLIST = {\n    \"gemma2\",  # - gemma2:  Uses slow_attention_softcapping which has torch.compile issues\n    \"gpt_oss\",  # - gpt_oss: Uses Flex Attention which doesn't handle padding_free correctly\n}\n</syntaxhighlight>\n\nAuto packing detection from `trainer.py:346-349`:\n<syntaxhighlight lang=\"python\">\nif _should_pack(config_arg) and not blocked:\n    configure_sample_packing(config_arg)\n    packing_active = True\n    logger.info(\"Unsloth: Sample packing enabled for SFTTrainer instance.\")\n</syntaxhighlight>\n\nUser-facing message from `trainer.py:393-396`:\n<syntaxhighlight lang=\"python\">\nif trainer_packing and (packing_active or _should_pack(trainer_args)):\n    enable_sample_packing(self.model, self)\n    print(\n        \"\ud83e\udda5 Unsloth: Packing enabled - training is >2x faster and uses less VRAM!\"\n    )\n</syntaxhighlight>\n\n== Related Pages ==\n* [[used_by::Implementation:Unslothai_Unsloth_SFTTrainer_train]]\n* [[used_by::Implementation:Unslothai_Unsloth_UnslothTrainingArguments]]\n",
      "domains": [
        "Optimization",
        "Training",
        "Efficiency"
      ],
      "sources": [
        {
          "type": "Repo",
          "title": "Unsloth",
          "url": "https://github.com/unslothai/unsloth"
        },
        {
          "type": "Doc",
          "title": "trainer.py",
          "url": "https://github.com/unslothai/unsloth/blob/main/unsloth/trainer.py"
        }
      ],
      "last_updated": "2026-01-09 12:00 GMT",
      "outgoing_links": [
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_SFTTrainer_train"
        },
        {
          "edge_type": "used_by",
          "target_type": "Implementation",
          "target_id": "Unslothai_Unsloth_UnslothTrainingArguments"
        }
      ]
    }
  ],
  "metadata": {
    "collection": "KGWikiPages",
    "embedding_model": "text-embedding-3-large"
  }
}