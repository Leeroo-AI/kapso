# Workflow: DeepSpeed

{| class="wikitable" style="float:right; margin-left:1em; width:300px;"
|-
! Knowledge Sources
|
* [[source::Repo|DeepSpeed|https://github.com/deepspeedai/DeepSpeed]]
|-
! Domains
| [[domain::deep_learning]], [[domain::distributed_training]], [[domain::LLM_inference]], [[domain::PyTorch]], [[domain::GPU_acceleration]]
|-
! Stars
| 41,200
|-
! Last Updated
| [[last_updated::2026-01-12]]
|}

== Overview ==
DeepSpeed is a deep learning optimization library focused on making large-scale distributed training and inference easier, faster, and more memory-efficient. It provides system and algorithmic optimiz...

=== Description ===
DeepSpeed is a deep learning optimization library focused on making large-scale distributed training and inference easier, faster, and more memory-efficient. It provides system and algorithmic optimizations (e.g., ZeRO-family techniques, parallelism/offload capabilities, and performance-focused CUDA/C++ extensions) primarily for PyTorch-based workflows.

=== Usage ===
It is directly applicable to ML engineering workflows because practitioners can integrate it into PyTorch training loops to train larger models, reduce memory usage, and improve multi-GPU/multi-node efficiency.

== Github URL ==
https://github.com/deepspeedai/DeepSpeed

== Related Pages ==
* Repository serves as a starter template for ML workflows
* Can be cloned and adapted for specific use cases
