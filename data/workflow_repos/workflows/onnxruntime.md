# Workflow: onnxruntime

{| class="wikitable" style="float:right; margin-left:1em; width:300px;"
|-
! Knowledge Sources
|
* [[source::Repo|onnxruntime|https://github.com/microsoft/onnxruntime]]
|-
! Domains
| [[domain::machine_learning]], [[domain::model_inference]], [[domain::onnx]], [[domain::runtime]], [[domain::acceleration]]
|-
! Stars
| 18,900
|-
! Last Updated
| [[last_updated::2026-01-12]]
|}

== Overview ==
ONNX Runtime is a cross-platform, high-performance machine learning inference and training engine for running models in the ONNX format. It provides graph optimizations and supports multiple hardware ...

=== Description ===
ONNX Runtime is a cross-platform, high-performance machine learning inference and training engine for running models in the ONNX format. It provides graph optimizations and supports multiple hardware execution providers (e.g., CPU/GPU/accelerators) to speed up deployment and, in some cases, training workflows.

=== Usage ===
It is directly applicable to ML engineering and data science workflows for deploying and optimizing trained models from common frameworks (e.

== Github URL ==
https://github.com/microsoft/onnxruntime

== Related Pages ==
* Repository serves as a starter template for ML workflows
* Can be cloned and adapted for specific use cases
