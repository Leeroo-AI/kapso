# Environment Index: huggingface_transformers

> Index of Environment pages for the huggingface_transformers wiki.

## Pages

| Page | File | Connections | Notes |
|------|------|-------------|-------|
| huggingface_transformers_PyTorch | [→](./environments/huggingface_transformers_PyTorch.md) | ✅Impl:huggingface_transformers_AutoConfig_from_pretrained, ✅Impl:huggingface_transformers_AutoTokenizer_from_pretrained, ✅Impl:huggingface_transformers_Trainer_init, ✅Impl:huggingface_transformers_pipeline_factory | Base Python/PyTorch environment |
| huggingface_transformers_CUDA | [→](./environments/huggingface_transformers_CUDA.md) | ✅Impl:huggingface_transformers_Trainer_train, ✅Impl:huggingface_transformers_BitsAndBytesConfig, ✅Impl:huggingface_transformers_get_hf_quantizer, ✅Impl:huggingface_transformers_quantizer_preprocess_model | NVIDIA GPU with CUDA |
| huggingface_transformers_BitsAndBytes | [→](./environments/huggingface_transformers_BitsAndBytes.md) | ✅Impl:huggingface_transformers_BitsAndBytesConfig, ✅Impl:huggingface_transformers_get_hf_quantizer, ✅Impl:huggingface_transformers_quantizer_preprocess_model, ✅Impl:huggingface_transformers_quantizer_postprocess_model | INT8/INT4 quantization |
| huggingface_transformers_FlashAttention | [→](./environments/huggingface_transformers_FlashAttention.md) | ✅Impl:huggingface_transformers_PreTrainedModel_from_config, ✅Impl:huggingface_transformers_Pipeline_forward | Flash Attention for efficient attention |

---

**Legend:** `✅Type:Name` = page exists | `⬜Type:Name` = page needs creation
