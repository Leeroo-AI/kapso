# src/transformers/initialization.py

## Understanding

**Status:** âœ… Explored

**Purpose:** Implements a guarded initialization system that prevents re-initialization of already-loaded model parameters, ensuring weights from pretrained checkpoints are preserved during model construction.

**Mechanism:** The file provides wrapped versions of all PyTorch initialization functions:
- **TORCH_INIT_FUNCTIONS**: Dictionary storing original PyTorch initialization functions (`uniform_`, `normal_`, `kaiming_uniform_`, etc.)
- **Guarded wrappers**: Each initialization function checks for the `_is_hf_initialized` flag on tensors before applying initialization
- **Context manager `guard_torch_init_functions()`**: Temporarily patches PyTorch's init functions across multiple modules (torch.nn.init, torch.nn.modules.*) to use the guarded versions
- **Module patching**: TORCH_MODULES_TO_PATCH lists all torch modules that need patching to ensure comprehensive protection
- **Tensor attribute**: Uses `_is_hf_initialized` flag to mark tensors that should not be re-initialized

**Significance:** This module solves a critical problem in model loading: when calling model constructors during checkpoint loading, PyTorch's default initialization would overwrite the loaded weights. The guarded initialization system ensures that pretrained weights remain intact while still allowing proper initialization of new parameters (e.g., when adding new layers). This is essential for transfer learning, fine-tuning, and loading models with custom architectures.
