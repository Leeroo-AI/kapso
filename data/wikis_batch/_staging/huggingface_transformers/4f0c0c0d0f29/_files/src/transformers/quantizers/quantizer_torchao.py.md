**Status:** âœ… Explored

**Purpose:** Implements PyTorch's torchao quantization library integration supporting multiple quantization schemes (int4/int8 weight-only, dynamic activation-int8, autoquant). Handles both prequantized model loading and on-the-fly quantization with serialization support.

**Mechanism:** The TorchAoHfQuantizer class (requires torchao>=0.15.0 for serialization) validates torch>=2.5.0 for weights_only loading and handles AffineQuantizedTensor/LinearActivationQuantizedTensor formats. It determines quantization bit-width through fuzzy_match_size() regex matching config names, maps to INT4/int8 custom dtypes, and adjusts max_memory to 90% for quantization parameters. For autoquant mode, it compiles the model with max-autotune and applies autoquant() post-loading. The quantizer handles FqnToConfig for fine-grained per-module quantization (>=0.15.0), flattens state_dict through flatten_tensor_state_dict() for safetensors compatibility, and implements TorchAoDeserialize for weight conversion with patterns matching _weight_qdata, _weight_scale, etc.

**Significance:** TorchAo provides PyTorch's official quantization framework with extensive flexibility, supporting multiple quantization strategies and automatic optimization through autoquant. The serialization support enables model sharing via safetensors, while the compile integration and support for both static and dynamic quantization make it suitable for production deployment. The trainability for int8 variants enables continued fine-tuning, and the FqnToConfig support allows surgical quantization control, making it highly versatile for both research and production use cases.
