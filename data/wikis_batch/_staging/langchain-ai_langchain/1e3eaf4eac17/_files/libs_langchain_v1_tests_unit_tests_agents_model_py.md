# File: `libs/langchain_v1/tests/unit_tests/agents/model.py`

**Category:** other

| Property | Value |
|----------|-------|
| Lines | 109 |
| Classes | `FakeToolCallingModel` |
| Imports | collections, dataclasses, json, langchain_core, pydantic, typing |

## Understanding

**Status:** âœ… Explored

**Purpose:** Provides a mock chat model implementation for testing agent behaviors without requiring actual LLM API calls. The `FakeToolCallingModel` simulates tool calling capabilities in unit tests.

**Mechanism:** Inherits from `BaseChatModel` and `Generic[StructuredResponseT]` to provide a configurable fake model that:
- Returns predefined tool calls from a list (cycling through them with an index)
- Supports both OpenAI and Anthropic tool calling styles
- Can return structured responses (JSON) when `response_format` is specified
- Implements `bind_tools()` to mimic real model tool binding behavior
- Generates deterministic AIMessage responses based on input messages

**Significance:** Critical testing infrastructure that enables fast, deterministic unit tests for agent functionality. By providing predictable tool call sequences, tests can verify agent behavior, tool execution flow, error handling, and message passing without the cost, latency, or non-determinism of real LLM calls.
