# File: `examples/offline_inference/qwen_1m.py`

**Category:** example

| Property | Value |
|----------|-------|
| Lines | 70 |
| Functions | `load_prompt`, `process_requests`, `initialize_engine`, `main` |
| Imports | os, urllib, vllm |
| Executable | Yes (`__main__`) |

## Understanding

**Status:** âœ… Explored

**Purpose:** Demonstrates Qwen model's 1 million token context window capability using extremely long prompts.

**Mechanism:** Downloads a ~400K token prompt file and processes it with Qwen model configured for max_model_len=1000000. Uses LLMEngine to handle the ultra-long context, demonstrating vLLM's ability to process prompts far exceeding typical context limits.

**Significance:** Showcases vLLM's support for million-token context windows, important for long document analysis, large codebase understanding, and other extreme context applications. Tests memory management and attention mechanisms at scale.
