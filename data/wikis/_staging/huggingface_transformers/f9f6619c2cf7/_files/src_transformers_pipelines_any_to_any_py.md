# File: `src/transformers/pipelines/any_to_any.py`

**Category:** other

| Property | Value |
|----------|-------|
| Lines | 505 |
| Classes | `ReturnType`, `Chat`, `AnyToAnyPipeline` |
| Imports | audio_utils, base, enum, generation, image_utils, numpy, processing_utils, typing, utils, video_utils |

## Understanding

**Status:** âœ… Explored

**Purpose:** Implements a multimodal pipeline that accepts any combination of text, images, video, and audio inputs to generate text, audio, or image outputs using multimodal language models.

**Mechanism:** The AnyToAnyPipeline extends the base Pipeline class to handle mixed modality inputs. It supports both single-turn prompts and multi-turn chat conversations (with the Chat wrapper class). During preprocessing, it uses processors to encode multimodal inputs into model-ready tensors, supports chat template application with optional assistant prefill, and handles various input formats. The pipeline leverages the model's generate() method and supports different generation modes (text/audio/image output). Return types can be controlled via ReturnType enum for tensors, new text only, or full text including prompts.

**Significance:** This is the most flexible pipeline in transformers, enabling the new generation of multimodal models (like Gemma 3n, LLaVA) that can process and generate across multiple modalities. It represents the future direction of unified multimodal AI where arbitrary combinations of inputs and outputs are supported through a single interface.
