# File: `conftest.py`

**Category:** other

| Property | Value |
|----------|-------|
| Lines | 152 |
| Classes | `CustomOutputChecker` |
| Functions | `pytest_configure`, `pytest_collection_modifyitems`, `pytest_addoption`, `pytest_terminal_summary`, `pytest_sessionfinish` |
| Imports | _pytest, doctest, os, pytest, sys, transformers, warnings |

## Understanding

**Status:** âœ… Explored

**Purpose:** Configures pytest behavior and test environment for the entire transformers test suite. This pytest configuration file defines custom markers, test collection rules, output handling, and environment setup that applies globally to all tests.

**Mechanism:** The file implements pytest hooks: `pytest_configure` registers custom markers (pipeline tests, staging tests, accelerate tests, device tests, torch compile/export tests, flash attention tests, training CI), `pytest_collection_modifyitems` automatically marks certain tests as "not_device_test" based on test name patterns (tokenization, processing, configuration tests that don't need GPU/device), `pytest_addoption` adds command-line options, `pytest_terminal_summary` generates test reports, and `pytest_sessionfinish` handles exit status (converting pytest's "no tests collected" exit code 5 to 0 to prevent CI failures). It also customizes doctest behavior by creating a `CustomOutputChecker` that supports an `IGNORE_RESULT` flag, patches doctest modules with HuggingFace-specific implementations, and sets up PyTorch environment (disables TF32, patches torch.compile for fullgraph mode). The script adds the repo's src directory to sys.path for development workflow.

**Significance:** This is the central test configuration that ensures consistent test behavior across local development and CI environments. The automatic marking of device-agnostic tests prevents unnecessary GPU resource usage, the custom doctest handling allows flexible documentation testing, and the pytest hooks enable sophisticated test reporting and collection. This configuration is crucial for managing the complexity of testing a large ML library with diverse hardware requirements and test categories.
