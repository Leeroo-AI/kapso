{| class="wikitable" style="float:right; margin-left:1em; width:300px;"
|-
! Knowledge Sources
|
* [[source::Repo|vllm-project_vllm|https://github.com/vllm-project/vllm]]
|-
! Domains
| [[domain::Examples]], [[domain::Online Serving]], [[domain::UI]], [[domain::Gradio]], [[domain::Legacy]]
|-
! Last Updated
| [[last_updated::2025-12-18 12:00 GMT]]
|}

== Overview ==
Demonstrates a simple Gradio-based web interface for interacting with vLLM's legacy API server, providing a user-friendly text completion UI.

=== Description ===
This example creates a basic web interface using Gradio that connects to vLLM's demonstration API server (<code>vllm.entrypoints.api_server</code>). Users can enter prompts in a text box and see streaming completions generated by the model in real-time.

The interface is intentionally minimal, showcasing the core pattern for building custom UIs on top of vLLM. It demonstrates HTTP request handling, streaming response processing, and Gradio's reactive interface patterns.

'''Note:''' This example uses the legacy API server, which is deprecated. For production applications, consider using the OpenAI-compatible API with modern UI frameworks or tools like Chainlit, Streamlit, or custom React applications.

=== Usage ===
Use this example when:
* Quickly prototyping a web interface for model testing
* Learning how to integrate vLLM with Gradio
* Building internal tools or demos for non-technical users
* Creating simple interactive interfaces for model evaluation
* Experimenting with streaming text generation in a web UI

'''Not recommended for production''' - use modern frameworks with the OpenAI API instead.

== Code Reference ==

=== Source Location ===
* '''Repository:''' [https://github.com/vllm-project/vllm vllm-project_vllm]
* '''File:''' [https://github.com/vllm-project/vllm/blob/main/examples/online_serving/gradio_webserver.py examples/online_serving/gradio_webserver.py]

=== CLI Usage ===
<syntaxhighlight lang="bash">
# First, start the vLLM API server
python -m vllm.entrypoints.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000

# In another terminal, start the Gradio interface
python examples/online_serving/gradio_webserver.py

# With custom configuration
python examples/online_serving/gradio_webserver.py \
    --host 0.0.0.0 \
    --port 8001 \
    --model-url http://localhost:8000/generate

# Access the interface at http://localhost:8001
</syntaxhighlight>

== Key Concepts ==

=== Gradio Integration ===
Gradio provides:
* Automatic UI generation from Python functions
* Built-in support for streaming outputs
* Share functionality for creating public demos
* Minimal code required for interactive interfaces

Key components used:
* <code>gr.Blocks</code>: Container for custom layouts
* <code>gr.Textbox</code>: Input and output text fields
* <code>submit</code>: Event handler for form submission

=== Streaming Pattern ===
The example uses a generator function for streaming:
<syntaxhighlight lang="python">
def http_bot(prompt):
    # Make streaming HTTP request
    response = requests.post(model_url, json=pload, stream=True)

    # Yield incremental updates
    for chunk in response.iter_lines(...):
        if chunk:
            data = json.loads(chunk.decode("utf-8"))
            output = data["text"][0]
            yield output  # Updates UI automatically
</syntaxhighlight>

Gradio automatically updates the output textbox with each yielded value.

=== Share Functionality ===
The <code>share=True</code> parameter:
* Creates a temporary public URL via Gradio's tunneling service
* Useful for demos and sharing with remote collaborators
* Requires downloading the frpc binary (may be blocked by antivirus)
* Automatically expires after 72 hours

== Usage Examples ==

=== Basic Gradio Interface ===
<syntaxhighlight lang="python">
import argparse
import json
import gradio as gr
import requests

def http_bot(prompt):
    headers = {"User-Agent": "vLLM Client"}
    pload = {
        "prompt": prompt,
        "stream": True,
        "max_tokens": 128,
    }
    model_url = "http://localhost:8000/generate"
    response = requests.post(model_url, headers=headers, json=pload, stream=True)

    for chunk in response.iter_lines(
        chunk_size=8192, decode_unicode=False, delimiter=b"\n"
    ):
        if chunk:
            data = json.loads(chunk.decode("utf-8"))
            output = data["text"][0]
            yield output

def build_demo():
    with gr.Blocks() as demo:
        gr.Markdown("# vLLM text completion demo\n")
        inputbox = gr.Textbox(label="Input", placeholder="Enter text and press ENTER")
        outputbox = gr.Textbox(
            label="Output", placeholder="Generated result from the model"
        )
        inputbox.submit(http_bot, [inputbox], [outputbox])
    return demo

if __name__ == "__main__":
    demo = build_demo()
    demo.queue().launch(server_name="localhost", server_port=8001, share=True)
</syntaxhighlight>

=== Enhanced Interface with Controls ===
<syntaxhighlight lang="python">
import gradio as gr
import requests
import json

def http_bot(prompt, max_tokens, temperature):
    headers = {"User-Agent": "vLLM Client"}
    pload = {
        "prompt": prompt,
        "stream": True,
        "max_tokens": max_tokens,
        "temperature": temperature,
    }
    response = requests.post(
        "http://localhost:8000/generate",
        headers=headers,
        json=pload,
        stream=True
    )

    for chunk in response.iter_lines(
        chunk_size=8192, decode_unicode=False, delimiter=b"\n"
    ):
        if chunk:
            data = json.loads(chunk.decode("utf-8"))
            output = data["text"][0]
            yield output

def build_demo():
    with gr.Blocks() as demo:
        gr.Markdown("# vLLM Text Completion")

        with gr.Row():
            with gr.Column():
                inputbox = gr.Textbox(
                    label="Prompt",
                    placeholder="Enter your prompt...",
                    lines=5
                )
                max_tokens = gr.Slider(
                    minimum=1,
                    maximum=512,
                    value=128,
                    step=1,
                    label="Max Tokens"
                )
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=2.0,
                    value=0.7,
                    step=0.1,
                    label="Temperature"
                )
                submit_btn = gr.Button("Generate")

            with gr.Column():
                outputbox = gr.Textbox(
                    label="Generated Text",
                    lines=10,
                    interactive=False
                )

        submit_btn.click(
            http_bot,
            [inputbox, max_tokens, temperature],
            [outputbox]
        )

    return demo

demo = build_demo()
demo.queue().launch()
</syntaxhighlight>

=== Chat-Style Interface ===
<syntaxhighlight lang="python">
import gradio as gr

def chat_completion(message, history):
    # Convert chat history to prompt
    prompt = "\n".join([f"User: {h[0]}\nAssistant: {h[1]}" for h in history])
    prompt += f"\nUser: {message}\nAssistant:"

    # Generate response
    headers = {"User-Agent": "vLLM Client"}
    pload = {
        "prompt": prompt,
        "stream": True,
        "max_tokens": 256,
    }
    response = requests.post(
        "http://localhost:8000/generate",
        headers=headers,
        json=pload,
        stream=True
    )

    # Stream response
    full_response = ""
    for chunk in response.iter_lines(
        chunk_size=8192, decode_unicode=False, delimiter=b"\n"
    ):
        if chunk:
            data = json.loads(chunk.decode("utf-8"))
            full_response = data["text"][0].strip()
            yield full_response

# Create chat interface
demo = gr.ChatInterface(
    chat_completion,
    title="vLLM Chat Demo",
    description="Chat with a language model powered by vLLM",
)

demo.launch()
</syntaxhighlight>

=== Multi-Model Interface ===
<syntaxhighlight lang="python">
def build_multi_model_demo():
    models = {
        "Llama-2-7B": "http://localhost:8000/generate",
        "Mistral-7B": "http://localhost:8001/generate",
    }

    def generate_with_model(prompt, model_name):
        url = models[model_name]
        response = requests.post(
            url,
            json={"prompt": prompt, "stream": True, "max_tokens": 128},
            stream=True
        )
        for chunk in response.iter_lines(decode_unicode=False, delimiter=b"\n"):
            if chunk:
                data = json.loads(chunk.decode("utf-8"))
                yield data["text"][0]

    with gr.Blocks() as demo:
        gr.Markdown("# Multi-Model Comparison")

        model_selector = gr.Dropdown(
            choices=list(models.keys()),
            value=list(models.keys())[0],
            label="Select Model"
        )
        inputbox = gr.Textbox(label="Prompt", lines=3)
        outputbox = gr.Textbox(label="Output", lines=5)

        inputbox.submit(generate_with_model, [inputbox, model_selector], [outputbox])

    return demo
</syntaxhighlight>

== Gradio Features ==

=== Queue System ===
The <code>.queue()</code> method:
* Enables request queuing for concurrent users
* Required for streaming outputs
* Provides better UX under load
* Automatically manages websocket connections

=== Share and Deployment ===
<syntaxhighlight lang="python">
# Local development
demo.launch(server_name="localhost", server_port=8001)

# Accessible from other machines
demo.launch(server_name="0.0.0.0", server_port=8001)

# Create temporary public link
demo.launch(share=True)

# Custom authentication
demo.launch(auth=("username", "password"))

# Embedded iframe support
demo.launch(inline=True, inbrowser=True)
</syntaxhighlight>

=== Error Handling ===
<syntaxhighlight lang="python">
def http_bot_with_errors(prompt):
    try:
        response = requests.post(model_url, json=pload, stream=True, timeout=60)
        response.raise_for_status()

        for chunk in response.iter_lines(...):
            if chunk:
                data = json.loads(chunk.decode("utf-8"))
                yield data["text"][0]

    except requests.exceptions.ConnectionError:
        yield "Error: Could not connect to vLLM server. Is it running?"
    except requests.exceptions.Timeout:
        yield "Error: Request timed out"
    except Exception as e:
        yield f"Error: {str(e)}"
</syntaxhighlight>

== Troubleshooting ==

=== Gradio Installation ===
<syntaxhighlight lang="bash">
# Install Gradio
pip install --upgrade gradio

# If import fails, try:
pip uninstall gradio
pip install gradio==4.0.0  # Use specific version
</syntaxhighlight>

=== frpc Download Issues ===
If antivirus blocks frpc download for share functionality:

<syntaxhighlight lang="bash">
# Manual download
wget https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64
mv frpc_linux_amd64 frpc_linux_amd64_v0.3

# Move to Gradio cache
mkdir -p ~/.cache/huggingface/gradio/frpc
mv frpc_linux_amd64_v0.3 ~/.cache/huggingface/gradio/frpc/
chmod +x ~/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3
</syntaxhighlight>

=== Connection Issues ===
<syntaxhighlight lang="bash">
# Verify vLLM server is running
curl http://localhost:8000/generate \
    -H "Content-Type: application/json" \
    -d '{"prompt": "test", "max_tokens": 5}'

# Check Gradio can reach server
python -c "import requests; print(requests.get('http://localhost:8000').status_code)"
</syntaxhighlight>

== Modern Alternatives ==

=== OpenAI API + Gradio ===
Instead of the legacy API, use OpenAI-compatible endpoints:

<syntaxhighlight lang="python">
from openai import OpenAI
import gradio as gr

client = OpenAI(api_key="EMPTY", base_url="http://localhost:8000/v1")

def generate(prompt):
    stream = client.completions.create(
        model="meta-llama/Llama-2-7b-chat-hf",
        prompt=prompt,
        max_tokens=128,
        stream=True,
    )

    full_text = ""
    for chunk in stream:
        if chunk.choices[0].text:
            full_text += chunk.choices[0].text
            yield full_text

demo = gr.Interface(fn=generate, inputs="text", outputs="text")
demo.launch()
</syntaxhighlight>

=== Other UI Frameworks ===
* '''Streamlit''': More features, better for data apps
* '''Chainlit''': Purpose-built for LLM chat interfaces
* '''Mesop''': Google's UI framework with streaming support
* '''Custom React/Vue''': Full control, professional UIs

== Related Pages ==
* [[requires_env::Environment:vllm-project_vllm_GPU_Environment]]
* [[related::Implementation:vllm-project_vllm_LegacyAPIClient]]
