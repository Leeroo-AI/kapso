{| class="wikitable" style="float:right; margin-left:1em; width:300px;"
|-
! Knowledge Sources
|
* [[source::Repo|langchain-ai_langchain|https://github.com/langchain-ai/langchain]]
|-
! Domains
| [[domain::Prompts]], [[domain::Few_Shot_Learning]], [[domain::Data_Generation]]
|-
! Last Updated
| [[last_updated::2025-12-18 12:00 GMT]]
|}

== Overview ==

Utility function `generate_example` that uses an LLM to generate new examples from existing few-shot examples.

=== Description ===

The `generate_example` function creates new example data by providing existing examples to an LLM in a few-shot format and asking it to generate another similar example. This is useful for data augmentation, creating test cases, or expanding few-shot prompt examples programmatically.

=== Usage ===

Use this function when you have a set of example inputs for a task and want to automatically generate more examples with the same format and style. The function uses LCEL composition internally.

== Code Reference ==

=== Source Location ===
* '''Repository:''' [https://github.com/langchain-ai/langchain langchain-ai_langchain]
* '''File:''' [https://github.com/langchain-ai/langchain/blob/main/libs/langchain/langchain_classic/chains/example_generator.py libs/langchain/langchain_classic/chains/example_generator.py]
* '''Lines:''' 1-22

=== Signature ===
<syntaxhighlight lang="python">
def generate_example(
    examples: list[dict],
    llm: BaseLanguageModel,
    prompt_template: PromptTemplate,
) -> str:
    """Return another example given a list of examples for a prompt.

    Args:
        examples: List of example dictionaries to use as few-shot context.
        llm: Language model to use for generation.
        prompt_template: Template for formatting each example.

    Returns:
        A new example string generated by the LLM.
    """
</syntaxhighlight>

=== Import ===
<syntaxhighlight lang="python">
from langchain_classic.chains.example_generator import generate_example
</syntaxhighlight>

== I/O Contract ==

=== Inputs ===
{| class="wikitable"
|-
! Name !! Type !! Required !! Description
|-
| examples || list[dict] || Yes || List of example dictionaries
|-
| llm || BaseLanguageModel || Yes || Language model for generation
|-
| prompt_template || PromptTemplate || Yes || Template to format each example
|}

=== Outputs ===
{| class="wikitable"
|-
! Name !! Type !! Description
|-
| return || str || New generated example as a string
|}

== Usage Examples ==

=== Generating More Examples ===
<syntaxhighlight lang="python">
from langchain_classic.chains.example_generator import generate_example
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI

# Define how each example should be formatted
example_template = PromptTemplate(
    input_variables=["question", "answer"],
    template="Q: {question}\nA: {answer}"
)

# Existing examples
examples = [
    {"question": "What is 2+2?", "answer": "4"},
    {"question": "What is the capital of France?", "answer": "Paris"},
    {"question": "Who wrote Romeo and Juliet?", "answer": "William Shakespeare"},
]

# Generate a new example
llm = OpenAI()
new_example = generate_example(examples, llm, example_template)
print(new_example)
# Output might be: "Q: What is the largest planet?\nA: Jupiter"
</syntaxhighlight>

=== Building a Dataset ===
<syntaxhighlight lang="python">
# Generate multiple new examples
new_examples = []
for _ in range(5):
    new_example = generate_example(examples, llm, example_template)
    new_examples.append(new_example)
    # Optionally parse and add to examples for diversity
</syntaxhighlight>

== Related Pages ==
* [[requires_env::Environment:langchain-ai_langchain_LangChain_Runtime_Environment]]

